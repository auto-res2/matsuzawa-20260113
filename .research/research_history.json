{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data-efficient image classification",
    "regularization image classification few-shot",
    "data augmentation regularization",
    "mixup cutmix regularization",
    "self-supervised data-efficient classification",
    "semi-supervised image classification regularization",
    "few-shot image augmentation",
    "consistency regularization image classification",
    "AutoAugment image classification",
    "RandAugment data-efficient"
  ],
  "research_study_list": [
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification",
      "full_text": "Contextual Squeeze-and-Excitation for Efﬁcient Few-Shot Image Classiﬁcation Massimiliano Patacchiola University of Cambridge mp2008@cam.ac.uk John Bronskill University of Cambridge jfb54@cam.ac.uk Aliaksandra Shysheya University of Cambridge as2975@cam.ac.uk Katja Hofmann Microsoft Research kahofman@microsoft.com Sebastian Nowozin∗ nowozin@gmail.com Richard E. Turner University of Cambridge ret26@cam.ac.uk Abstract Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personaliza- tion, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a speciﬁc user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classiﬁcation setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to signiﬁcantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a ﬁne-tuning routine to adapt a linear head, deﬁning a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading ﬁne-tuning methods with the beneﬁt of orders of magnitude lower adaptation cost. 1 Introduction In recent years, the growth of industrial applications based on recommendation systems (Bennett et al., 2007), speech recognition (Xiong et al., 2018), and personalization (Massiceti et al., 2021) has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a speciﬁc user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classiﬁcation task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user’s device (context). In these applications the goal is twofold: maximize the classiﬁcation accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources. Methods developed to face these challenges in the few-shot classiﬁcation setting can be grouped in two categories: meta-learning and ﬁne-tuning. Meta-learning is based on the idea of learning-how-to- learn by improving the algorithm itself (Schmidhuber, 1987; Hospedales et al., 2020). Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates (Chen et al., 2019; Triantaﬁllou et al., 2019; Tian et al., 2020; Kolesnikov et al., 2020; Dumoulin et al., 2021). ∗Work done while the author was at Microsoft Research – Cambridge (UK) 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.09843v3  [cs.CV]  11 Jan 2023We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. Figure 1 illustrates this comparison by showing on the vertical axis the average classiﬁcation accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, Dumoulin et al. 2021), and on the horizontal axis the adaptation cost measured as the number of multiply–accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, ﬁne-tuners achieve a higher classiﬁcation accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, Kolesnikov et al. 2020) and LITE (Bronskill et al., 2021), shows a substantial performance gap of 14% in favor of the ﬁne-tuner but at a much higher adaptation cost, with BiT requiring 526 ×1012 MACs and LITE only 0.2 ×1012 MACs. Figure 1: Accuracy and adaptation cost on VTAB for meta-learners (blue), ﬁne- tuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE nar- rows the gap with the leading ﬁne-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost. It is crucial to ﬁnd solutions that retain the best of both worlds: the accuracy of ﬁne-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of ﬁne-tuners is the need for multiple gradi- ent adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) signiﬁcantly speeds up ﬁne-tuning, but it harms perfor- mance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is there- fore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a ﬁne-tuning routine for optimizing the head. At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by Hu et al. (2018) to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive ﬁne-tuning routines for the linear head, similarly to methods like MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). Figure 1 shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, ﬁne-tuners such as MD-Transfer (Triantaﬁllou et al., 2019), and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classiﬁcation performance. Our contributions can be summarized as follows: 1. We introduce a new adapter calledContextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by Hu et al. (2018), that outperforms other adaptation mechanisms (e.g. the FiLM generators used in Bronskill et al. 2021) in terms of parameter efﬁciency (a 75% reduction in the number of adaptation parameters) and classiﬁcation accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1. 2. We use CaSE adaptive blocks in conjuction with a ﬁne-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classiﬁcation accuracy compared to the SOTA meta-learner (Bronskill et al., 2021) on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT (Kolesnikov et al., 2020) with the beneﬁt of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset (Massiceti et al., 2021), where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT). 1https://github.com/mpatacchiola/contextual-squeeze-and-excitation 22 Contextual Squeeze-and-Excitation (CaSE) Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us deﬁne a collection of meta-training tasks as D= {τ1,...,τ D}where τi = (Ci,Ti) represents a generic task composed of a context set Ci = {(x,y)1,..., (x,y)M}and a target set Ti = {(x,y)1,..., (x,y)D}of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task τ∗ = {C∗,x∗}the goal is to predict the true label y∗ of the unlabeled target point x∗ conditioned on the context set C∗. In ﬁne-tuning methods, we are given a neural network fθ(·), with parameters θ estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task τ∗ adaptation consists of minimizing the lossL(·) via gradient updates to ﬁnd the task-speciﬁc parameters θτ∗ ←G(ϵ,L,τ∗,fθ), where ϵis a learning rate, and G(·) is a functional representing an iterative routine that returns the adapted parameters θτ∗ (used for prediction). This procedure is particularly effective because it can exploit efﬁcient mini-batching, parallelization, and large pretrained models. In meta-learning methods training and evaluation are performed episodically (Vinyals et al., 2016), with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to deﬁne a hierarchy. The parameters are divided in two groups: φtask-common parameters shared across all tasks (top of the hierarchy), and ψτ task-speciﬁc parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way φand ψτ come into play is method dependent; they can be estimated via gradient updates (e.g. MAML, Finn et al. 2017), learned metrics (e.g. ProtoNets, Snell et al. 2017), or Bayesian methods (Gordon et al., 2018; Patacchiola et al., 2020; Sendera et al., 2021). Standard Squeeze-Excite (SE) We brieﬂy introduce standard SE (Hu et al., 2018), as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of Llayers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function gφ(·). The number of hidden units in the MLP is deﬁned by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B×C×H×W where Cis the number of channels, Hthe height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H1,..., HB}with Hi ∈RC×H×W. To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B×C×1 ×1; this can be interpreted as a set of vectors {h1,..., hB}with hi ∈RC. For each layer l, the set is passed to the associated MLP that will generate an individual scale vector γi ∈RC, where γ(l) 1 = g(l) φ ( h(l) 1 ) ··· γ(l) B = g(l) φ ( h(l) B ) . (1) An elementwise product is then performed between the scale vector and the original tensor ˆH(l) 1 = H(l) 1 ∗γ(l) 1 ··· ˆH(l) B = H(l) B ∗γ(l) B , (2) with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in Figure 2 (left). Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to deﬁne a new version of SE for task-based channel-wise feature adaptation. For a task τ = (C,T), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H1,..., HN}with Hi ∈RC×H×W. As in standard SE, we ﬁrst apply a spatial pooling to each tensor Hi which produces N vectors {h1,..., hN}of shape hi ∈RC. Then a context pooling is performed; this corresponds to an empirical mean over {h1,..., hN}(see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer γ(l) = g(l) φ ( ¯h(l) ) with ¯h(l) = 1 N ( h(l) 1 + ··· + h(l) N ) , (3) 3Figure 2: Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, ∗elementwise multiplication. which is then multiplied elementwise by the original tensors ˆH(l) 1 = H(l) 1 ∗γ(l) ··· ˆH(l) N = H(l) N ∗γ(l). (4) The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of Figure 2. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in Figure 2 (right), additional technical details are provided in Appendix A. Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, Perez et al. 2018). Those mechanisms perform adaptation using a separate convolutional set-encoder to produce an embedding of the context set. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM (Oreshkin et al., 2018), CNAPs (Requeima et al., 2019), SimpleCNAPs (Bateni et al., 2020), CA VIA (Zintgraf et al., 2019), and LITE (Bronskill et al., 2021). We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efﬁcient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method (LITE, Bronskill et al. 2021), showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters. 3 UpperCaSE: system description and optimization protocol We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate- Descent (CD). We call this protocolhybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a ﬁne-tuning routine to estimate the task-speciﬁc parameters (head). Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), deﬁned as bθ(·) where θare the pretrained parameters. CaSE blocks, 4parameterized by φ, are added to the model at speciﬁc locations to give bθ,φ(·) (see Appendix A for details about this step). We are interested in learning the CaSE parameters φkeeping constant the pretrained parameters θ(omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks τ = {C,T}∼D , where Dis the training set. The number of classes (way) is calculated from the context set and used to deﬁne a linear classiﬁcation head hψτ (·) parameterized by ψτ. The complete model is obtained by nesting the two functions as hψτ (bφ(·)). We indicate a forward pass through the body over the context inputs with the shorthand bφ(Cx) →{z1,..., zN}, where zn is the context embedding for the input xn. All the context embeddings and the associated labels are stored in M= {(zn,yn)}N n=1. Optimization challenges We have two sets of learnable parameters,φthe CaSE parameters, and ψτ the parameters of the linear head for the taskτ. While φis shared across all tasks (task-common), ψτ must be inferred on the task at hand (task-speciﬁc). In both cases, the objective is the minimization of a classiﬁcation loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient dL dφ = ∑ τ (∂Lτ ∂ψτ dψτ dφ + ∂Lτ ∂φ ) . (5) The ﬁrst term ∂Lτ/∂ψτ (sensitivity of the loss w.r.t. the head) and the direct gradient ∂L/∂φ (sensitivity of the loss w.r.t. the adaptation parameters with a ﬁxed head) can be obtained with auto-differentiation as usual. The second term dψτ/dφ(sensitivity of the head w.r.t. the adaptation parameters) is problematic because ψτ is obtained iteratively after a sequence of gradient updates. Backpropagating the gradients to φincludes a backpropagation through all the gradient steps per- formed to obtain the task-speciﬁc ψτ. Previous work has showed that this produces instability, vanishing gradients, and high memory consumption (Antoniou et al., 2018; Rajeswaran et al., 2019). Meta-training via Coordinate-Descent A potential solution to these issues is the use of implicit gradients (Chen et al., 2020; Rajeswaran et al., 2019; Chen et al., 2022). The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy’s implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). These methods share the idea of inner-loop-head/outer- loop-body meta-training, and they ﬁnd the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) (Wright, 2015). The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. φand ψτ as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ψτ ﬁrst, we reach a local minimum where ∂Lτ/∂ψτ ≈0. Therefore CD induces a direct optimization objective w.r.t.φ, with Equation (5) reducing to ∂Lτ/∂φ(no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details. In practice, at each training iteration we sample a task τ = (C,T) ∼D, perform a forward pass on the body (with CaSE in adaptive mode) to get bφ(Cx) →{z1,..., zN}. (6) The context embeddings are temporarily stored in a buffer with their associated labels M = {(zn,yn)}N n=1 to avoid expensive calls to bφ(·). We then set the head parameters to zero, and solve the ﬁrst minimization problem (inner-loop), obtaining the task-speciﬁc parameters ψτ via ψτ ←G ( α,M,L,hψτ ) (7) where αis a learning rate, and G(·) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation(7) only relies on the headhψτ (·) and not on the bodybφ(·), which is the primary source of memory savings and the crucial difference with common ﬁne-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efﬁcient mini-batching. 5We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t.φ(outer-loop) given support/target points and the task-speciﬁc parameters ψτ identiﬁed previously. The ﬁnal form of the equation depends on the optimizer, for a generic SGD the update is given by φ←φ−β∇φL ( Cy ∪Qy,hψτ ,bφ ) , (8) where βis a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B. Inference on unseen tasks After the training phase, we are given an unseen task τ∗ = (C∗,x∗) where x∗ is a single target input and y∗ the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation (6) and embeddings/labels stored in M, (ii) estimation of the task-speciﬁc parameters ψ∗ via iterative updates as in Equation (7), and (iii) inference of the target-point membership via a forward pass over body and head ˆy∗ = hψ∗ (bφ(x∗)) with CaSE in inference mode. 4 Related work Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details (Hospedales et al., 2020). LITE (Bronskill et al., 2021) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs (Bateni et al., 2020) that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classiﬁcation accuracy and parameter efﬁciency. Fine-tuning Chen et al. (2019) were the ﬁrst to expose the potential of simple ﬁne-tuning baselines for transfer learning. MD-Transfer has been proposed in Triantaﬁllou et al. (2019) as an effective ﬁne-tuning baseline for the MetaDataset benchmark. More recently Kolesnikov et al. (2020) have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classiﬁcation head, learning schedule, normalization layers, and batching. Fine-tuning only the last linear layer can be effective (Bauer et al., 2017; Tian et al., 2020). We compare against this baseline in Section 5.1, showing that adapting the body via CaSE signiﬁcantly boosts the performance. Overall, ﬁne-tuners have consistently outperformed meta-learners in terms of classiﬁcation accuracy, only under particular conditions (e.g. strong class-imbalance) the trend is reversed (Ochal et al., 2021a,b). Hybrids Hybrid methods are trained episodically like meta-learners but rely on ﬁne-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, Finn et al. 2017) ﬁnds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ (Antoniou et al., 2018), ProtoMAML (Triantaﬁllou et al., 2019), and Reptile (Nichol et al., 2018). Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs (Han et al., 2021). Adaptive components have been used in a variety of applications, such as neural compression (Veit and Belongie, 2018; Wu et al., 2018), generation of artistic styles (Dumoulin et al., 2016; Huang and Belongie, 2017), or routing (Guo et al., 2019). Residual adapters (Rebufﬁ et al., 2017, 2018) have been used in transfer learning (non few-shot) but they rely on ﬁne-tuning routines which are signiﬁcantly slow during adaptation. More recently, Li et al. (2022) have used serial and residual adapters in the few-shot setting, with the task-speciﬁc weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-speciﬁc weights in the body of the network which is costly. In Sun et al. (2019) the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via ﬁne-tuning. This method suffers of similar limitations, as the ﬁne-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. 6Figure 3: Left: CaSE vs Squeeze-and-Excitation (SE) (both methods use EfﬁcientNetB0, 84 × 84 inputs, Mahalanobis-distance head). CaSE outperforms SE in all conditions. Center: CaSE vs. FiLM generators (Bronskill et al., 2021) and a baseline with no body adaptation (all methods use EfﬁcientNetB0, 84 ×84 inputs, Mahalanobis-distance head). CaSE outperforms FiLM generators in all conditions. Right: boxplot of CaSE activations at different depth of an EfﬁcientNetB0 for 800 tasks sampled from the MDv2 test set (224 ×224 inputs, UpperCaSE). The modulation of CaSE is minimal at early stages for general-purpose ﬁlters and increases at deeper stages. 5 Experiments In this section we report on experiments on VTAB+MD (Dumoulin et al., 2021) and ORBIT (Massiceti et al., 2021). VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfﬁcientNetB0 from the ofﬁcial Torchvision repository; ResNet50x1-S released with BiT (Kolesnikov et al., 2020). We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of Bronskill et al. (2021) ( 10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10−3,10−5] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (conﬁdence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . 5.1 Analysis of CaSE blocks In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE (Hu et al., 2018) and CaSE on MDv2 and VTAB, conﬁrming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator (Bronskill et al., 2021), showing that CaSE is signiﬁcantly more efﬁcient using 75% fewer parameters while boosting the classiﬁcation accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis. Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD- v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in Bronskill et al. (2021), reporting results with a linear head in the appendix. We summarize the results in Figure 3 (left) and add a tabular breakdown in Appendix C.2. CaSE outperforms SE in all conditions, conﬁrming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks. Comparing adaptation mechanisms We perform a comparison on VTAB+MD of CaSE against FiLM generators (Bronskill et al., 2021), and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfﬁcientNetB0 backbone, and same training schedule. We show a summary of the results in 7Figure 3 (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneﬁcial. This is likely due to the strong domain shift introduced by some of the specialized datasets. Role of CaSE blocks To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfﬁcientNetB0 trained with UpperCaSE on224×224 images. In Figure 3 (right) we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose ﬁlters that are useful across all tasks. In deeper layers the variance increases, showing a task-speciﬁc modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4. 5.2 Performance evaluation of UpperCaSE In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA ﬁne-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB;2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset (Massiceti et al., 2021) for the cross-domain condition MDv2→ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars. Table 1: UpperCaSE outperforms ﬁne-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold. Cost↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. MACs all all natur. spec. struc. MD-Transfer ﬁne-tuning RN18 126 11.2 118.6 63.4 55.6 52.4 72.9 49.3 SUR ﬁne-tuning RN50 224 164.6 28.8 71.3 43.7 50.9 66.2 27.2 Big Transfer ﬁne-tuning RN50 224 23.5 526.3 73.3 65.4 69.4 81.0 54.5 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.2 76.1 58.4 69.1 80.3 39.4 Table 2: UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest num- ber of parameters per adaptive blocks . Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold. Adapt.↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. count all all natur. spec. struc. ProtoMAML hybrid RN18 126 11.2 n/a 64.2 45.0 45.7 70.7 31.5 CTX meta-learning RN34 224 21.3 n/a 71.6 50.5 61.1 67.3 34.0 ProtoNet meta-learning ENB0 224 4.0 n/a 72.7 46.1 60.9 64.2 25.9 LITE meta-learning ENB0 224 4.0 1.7 73.8 51.4 65.2 71.9 30.8 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.4 76.1 58.4 69.1 80.3 39.4 Comparison on VTAB+MDWe compare UpperCaSE against ﬁne-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results 8Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% conﬁdence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al. (2021): meta-train on MetaDataset and test on ORBIT, image-size 84 ×84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold. Cost Clean Video Evaluation (CLE-VE) Clutter Video Evaluation (CLU-VE) Method MACs ↓ frame acc.↑ FTR↓ video acc.↑ frame acc.↑ FTR↓ video acc.↑ ProtoNet 3.2 59.0 ±2.2 11.5 ±1.8 69.2 ±3.0 47.0 ±1.8 20.4 ±1.7 52.8 ±2.5 CNAPs 3.5 51.9 ±2.5 20.8 ±2.3 60.8 ±3.2 41.6 ±1.9 30.7 ±2.1 43.0 ±2.5 MAML 95.3 42.5 ±2.7 37.3 ±3.0 47.0 ±3.2 24.3 ±1.8 62.3 ±2.3 25.7 ±2.2 FineTuner 317.7 61.0±2.2 11.5 ±1.8 72.6 ±2.9 48.4 ±1.9 19.1 ±1.7 54.1 ±2.5 UpperCaSE 3.5 63.0±2.2 8.8 ±1.6 74.4 ±2.8 48.1 ±1.8 18.2 ±1.7 54.5 ±2.5 in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfﬁcientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see Table 1) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%). The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE (Bronskill et al., 2021) which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difﬁcult to compensate without ﬁne-tuning the body. It is not clear whether transfer learning is beneﬁcial on these datasets in the ﬁrst place. The results in terms of adaptation cost (see Table 1) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efﬁcient (0.2 ×1012 MACs) than all ﬁne-tuners, with BiT being the most expensive method overall (526.3 × 1012 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see Table 2) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfﬁcientNetB0) compared to LITE (Bronskill et al., 2021) which is based on FiLM generators. Comparison on ORBIT We compare UpperCaSE to other methods on ORBIT (Massiceti et al., 2021), a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classiﬁers are meta-trained on MetaDataset and tested on ORBIT. Results are reported in Table 3. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR −2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efﬁcient than FineTuner and close to the leading method (ProtoNet). 6 Conclusions We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and- Excitation (SE) block proposed by Hu et al. (2018). CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of ﬁne-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark. 9Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that ﬁne-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large. Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classiﬁ- cation settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efﬁciency of our method can reduce energy consumption and beneﬁt the environment. Certain applications require careful consideration to avoid biases that can harm speciﬁc groups of people (e.g. surveillance, legal decision-making). Acknowledgments and Disclosure of Funding Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos, Daniela Massiceti, and Shoaib Ahmed Siddiqui for providing suggestions and feedback on the preliminary version of the manuscript. References Antoniou, A., Edwards, H., and Storkey, A. (2018). How to train your maml. arXiv preprint arXiv:1810.09502. Bateni, P., Goyal, R., Masrani, V ., Wood, F., and Sigal, L. (2020). Improved few-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Bauer, M., Rojas-Carulla, M., ´Swi ˛ atkowski, J. B., Schölkopf, B., and Turner, R. E. (2017). Discrimi- native k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326. Bennett, J., Lanning, S., et al. (2007). The netﬂix prize. In Proceedings of KDD cup and workshop. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136. Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efﬁcient meta-learning with large images. Advances in Neural Information Processing Systems. Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. (2019). A closer look at few-shot classiﬁcation. arXiv preprint arXiv:1904.04232. Chen, Y ., Friesen, A. L., Behbahani, F., Doucet, A., Budden, D., Hoffman, M., and de Freitas, N. (2020). Modular meta-learning with shrinkage. Advances in Neural Information Processing Systems. Dumoulin, V ., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and Larochelle, H. (2021). Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark. arXiv preprint arXiv:2104.02638. Dumoulin, V ., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style.arXiv preprint arXiv:1610.07629. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W. (2018). Neural processes. arXiv preprint arXiv:1807.01622. 10Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. (2018). Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921. Guo, Y ., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. (2019). Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. (2020). Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439. Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Huang, X. and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In European conference on computer vision. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021a). Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021b). How sensitive are meta-learners to dataset imbalance? arXiv preprint arXiv:2104.05344. Oreshkin, B., Rodríguez López, P., and Lacoste, A. (2018). Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31. Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and Storkey, A. J. (2020). Bayesian meta- learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. (2019). Meta-learning with implicit gradients. Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2018). Efﬁcient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. 11Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. (2019). Fast and ﬂexible multi- task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems. Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München. Sendera, M., Tabor, J., Nowak, A., Bedychaj, A., Patacchiola, M., Trzcinski, T., Spurek, P., and Zieba, M. (2021). Non-gaussian gaussian processes for few-shot regression. Advances in Neural Information Processing Systems. Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems. Sun, Q., Liu, Y ., Chua, T.-S., and Schiele, B. (2019). Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. (2020). Rethinking few-shot image classiﬁcation: a good embedding is all you need? In European Conference on Computer Vision. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. Veit, A. and Belongie, S. (2018). Convolutional networks with adaptive inference graphs. In Proceedings of the European Conference on Computer Vision (ECCV). Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learning. Advances in Neural Information Processing Systems. Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Programming, 151. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L. S., Grauman, K., and Feris, R. (2018). Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. In IEEE international conference on acoustics, speech and signal processing (ICASSP). Zintgraf, L., Shiarli, K., Kurin, V ., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In International Conference on Machine Learning. 12A CaSE: additional details A.1 CaSE implementation Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer las showed in Equation (3), that is ¯h(l) ∈RC, subtracting the mean and dividing by the standard deviation. Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c= 2which extends the range to [0,2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 (Table 6). CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in Bronskill et al. (2021) for FiLM generators. In EfﬁcientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the ﬁrst layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfﬁcientNet): Conv2d→BatchNorm2d→SiLU→CaSE→SqueezeExcitation→Conv2d→BatchNorm2d This results in a total of 18 CaSE blocks for EfﬁcientNetB0. Increasing the number of blocks did not provide a signiﬁcant beneﬁt. In ResNet18 we place two CaSE blocks per each basic block as: Conv2d→BatchNorm2d→ReLU→CaSE→Conv2d→BatchNorm2d→ReLU→CaSE Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details. Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor. CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input sizeC(corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor rmin that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels. A.2 Context pooling In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2). Similarities with other methods Context pooling is a way to summarize a task with a permutation- invariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets (Snell et al., 2017) a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class. The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs (Bateni et al., 2020) a prototype is estimated as in ProtoNets but it is used to deﬁne a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family (Garnelo et al., 2018) also rely on similar permutation-invariant aggregations to deﬁne distributions over functions. 13Global vs. local context-pooling Comparing CaSE with the FiLM generators of Bronskill et al. (2021) it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of Bronskill et al. (2021) rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More speciﬁcally, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding. CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4). 14A.3 Pytorch code for CaSE Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . import torch from torch import nn class CaSE (nn. Module ): def __init__ (self , cin , reduction =32 , min_units =32 , standardize =True , out_mul =2.0, device =None , dtype = None ): \"\"\" Initialize a CaSE adaptive block . Parameters : cin ( int ): number of input channels . reduction ( int ): divider for computing number of hidden units . min_units ( int ): clip hidden units to this value (if lower ). standardize ( bool ): standardize the input for the MLP . out_mul ( float ): multiply the MLP output by this value . \"\"\" factory_kwargs = {’ device ’: device , ’dtype ’: dtype } super (CaSE , self ). __init__ () self . cin = cin self . standardize = standardize self . out_mul = out_mul hidden = max ( min_units , cin // reduction ) self . gamma_generator = nn. Sequential ( nn. Linear (cin , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , cin , bias =True , ** factory_kwargs ), nn. Sigmoid () ) self . reset_parameters () def reset_parameters ( self ): nn. init . zeros_ ( self . gamma_generator [4]. weight ) nn. init . zeros_ ( self . gamma_generator [4]. bias ) self . gamma = torch . tensor ([1.0]) def forward (self , x): if( self . training ): # adaptive mode self . gamma = torch . mean (x, dim =[2,3]) # spatial pooling self . gamma = torch . mean ( self . gamma , dim =[0])# context pooling if( self . standardize ): self . gamma = ( self . gamma - torch . mean ( self . gamma )) / \\ torch . sqrt ( torch . var ( self . gamma , unbiased = False ) + 1e-5) self . gamma = self . gamma . unsqueeze (0) self . gamma = self . gamma_generator ( self . gamma ) * self . out_mul self . gamma = self . gamma . reshape ([1,-1,1,1]) return self . gamma * x else : # inference mode self . gamma = self . gamma .to(x. device ) return self . gamma * x def extra_repr ( self ): return ’cin ={}’. format ( self . cin ) 15B UppereCaSE: additional details B.1 Algorithm of UpperCaSE Algorithm 1 UpperCaSE: training function for the few-shot classiﬁcation setting. Require: D= {τ1,...,τ D}training dataset Require: bφ() pretrained feature extractor (body) with CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α, β: step-size hyperparameters for the optimizer. 1: Set φto random values ⊿optional: set φto enforce identity in CaSE output 2: while not done do 3: Sample task τ = (C,T) ∼D 4: Forward pass over context set bφ(Cx) → z1,..., zN ⊿CaSE in adaptive mode 5: Store context embeddings and associated labels M= {(zn,yn)}N n=1 ⊿temporary memory buffer 6: Deﬁne a linear model for the head hψτ () and set ψτ to zero 7: for total inner-steps do ⊿loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B∼M 9: Update the head parameters ψτ ←step(α,L,B,hψτ ) 10: end for 11: Update the CaSE parameters φ←step(β,L,C,T,bφ,hψτ ) ⊿CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classiﬁcation setting. Require: τ∗ = (C∗,x∗) unseen test task with target input x∗ an context C∗. Require: bφ() pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α: step-size hyperparameter for the optimizer. 1: Forward pass over context set bφ(Cx ∗ ) → z1,..., zN ⊿CaSE in adaptive mode 2: Store context embeddings and associated labels M∗ = {(zn,yn)}N n=1 ⊿temporary memory buffer 3: Deﬁne a linear model for the head hψτ∗ () and set ψτ∗ to zero 4: for total inner-steps do ⊿loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B∗ ∼M∗ 6: Update the head parameters ψτ∗ ←step(α,L,B∗,hψτ∗ ) 7: end for 8: Return Prediction ˆy∗ = hψτ∗ (bφ(x∗)) ⊿CaSE in inference mode C Additional experimental details and results C.1 Additional details MACs counting MACs are proportional to the size of the task, size of the images, and number of classes. We can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ×224 ×3 generated via Gaussian noise (µ= 0,σ = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfﬁcientNetB0 backbone for the ﬁrst and a ResNet50-S for the second. For MD-Transfer we used the same parameters reported in Dumoulin et al. (2021) with images of size 126 ×126 ×3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone. VTAB+MD trainingWe follow the protocol reported in the original papers (Triantaﬁllou et al., 2019; Dumoulin et al., 2021) training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and conﬁdence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfﬁcientNetB0 from the ofﬁcial Torchvision repository 3, and a pretrained ResNet50-S 2https://github.com/microsoft/ORBIT-Dataset 3https://pytorch.org/vision 16from the BiT repository 4. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use the BiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the ofﬁcial Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). For the evaluation phase we followed the instructions reported in Massiceti et al. (2021). C.2 CaSE vs SE Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al., 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold. Model SE CaSE SE CaSE Contextual pooling No Yes No Yes Adaptation head MD MD Linear Linear Image size 84 84 224 224 MetaDataset (all) 67.8 69.6 74.6 76.2 VTAB (all) 43.6 45.3 56.6 58.2 VTAB (natural) 47.5 50.2 65.3 68.1 VTAB (specialized) 63.6 64.9 79.8 79.6 VTAB (structured) 30.6 31.8 38.6 40.1 C.3 CaSE vs other adapters Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al. (2021), and a baseline with no body adaptation. CaSE blocks are more efﬁcient in terms of adaptive and amortization parameters while providing higher classiﬁcation accuracy. All models have been trained and tested on 84 ×84 images, using a Mahalanobis distance head. Best results in bold. Adaptation type None FiLM CaSE64 CaSE32 CaSE16 Adaptive Params (M) n/a 0.02 0.01 0.01 0.01 Amortiz. Params (M) n/a 1.7 0.4 0.8 1.6 MetaDataset (all) 53.4 68.4 69.8 69.6 70.4 VTAB (all) 43.5 44.7 46.2 45.3 46.4 VTAB (natural) 45.4 49.5 52.1 50.2 52.6 VTAB (specialized) 69.4 63.8 66.3 64.9 65.5 VTAB (structured) 29.1 31.7 31.8 31.8 32.1 C.4 Ablation studies In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows: • Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in Table 6 show that the sigmoid with multiplier and the linear layer provide the best results. 4https://github.com/google-research/big_transfer 17• Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in Table 8 show that blocks with more units provide marginal gains or no gains at all. This is probably due to overﬁtting issues affecting the models with more units. • Ablation of the number of hidden layers of CaSE. The results reported in Table 7 show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overﬁtting issues affecting the models with more parameters. • Ablation of the activation function for the hidden layers. Results reported in Table 9 show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfﬁcientNet backbones). Table 6: Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfﬁcientNetB0 backbone with a linear head on images of size 224 ×224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the ﬁrst iterations). Best results in bold. Adaptive block SE SE CaSE CaSE CaSE Activation (output) Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2 MetaDataset (all) 74.2 74.6 75.8 74.9 76.2 VTAB (all) 56.8 56.6 58.4 56.8 58.2 VTAB (natural) 67.0 65.3 68.3 67.1 68.1 VTAB (specialized) 81.1 79.8 79.5 80.8 79.6 VTAB (structured) 36.9 38.6 40.3 37.1 40.1 Table 7: Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ×224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. # Hidden layers 1 2 3 4 Amortiz. Params (M) 0.420 0.426 0.432 0.438 MetaDataset (all) 76.0 76.1 75.5 75.2 VTAB (all) 58.2 58.4 58.2 58.0 VTAB (natural) 68.3 69.1 68.0 67.4 VTAB (specialized) 79.7 80.3 80.5 80.3 VTAB (structured) 40.0 39.4 39.7 39.7 18Table 8: Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is deﬁned by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Reduction factor 64 32 16 8 Clip factor 16 32 48 64 Amortiz. Params (M) 0.4 0.8 1.6 3.0 MetaDataset (all) 76.1 76.2 75.8 76.2 VTAB (all) 58.4 58.2 57.9 58.5 VTAB (natural) 69.1 68.1 67.9 68.3 VTAB (specialized) 80.3 79.6 79.4 79.0 VTAB (structured) 39.4 40.1 39.7 40.9 Table 9: Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortiza- tion parameters) and they have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Activation (hidden) SiLU ReLU Tanh MetaDataset (all) 76.1 75.8 74.8 VTAB (all) 58.4 57.8 48.2 VTAB (natural) 69.1 69.8 67.0 VTAB (specialized) 80.3 79.7 80.8 VTAB (structured) 39.4 39.4 36.4 19C.5 Role of CaSE blocks Figure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfﬁcientNetB0 (horizontal axis, with early stages on the left). The box encloses ﬁrst to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles. 20Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfﬁcientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the ﬁrst stages but diverge in the latest. The magnitude tends to increase with depth. 21C.6 UpperCaSE: results on VTAB+MD In this section we provide a full breakdown of the results for UpperCaSE vs. other methods on the VTAB+MD benchmark. Results for other methods are taken from Bronskill et al. (2021) and Dumoulin et al. (2021). UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfﬁcientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks. In Table 10 we report the results for UpperCaSE against ﬁne-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE). Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneﬁcial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and ﬁne-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set. In this case, ﬁne-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against ﬁne-tuning methods. Best result in bold. Model BiT MD-Transfer SUR UpperCaSE UpperCaSE Image Size 224 126 224 224 224 Network RN50-S RN18 RN50 ×7 ENB0 RN50-S Params (M) 23.5 11.2 164.5 4.0 23.5 Omniglot 68.0 ±4.5 82.0 ±1.3 92.8±0.5 90.7±0.4 89.1 ±0.5 Aircraft 77.4 ±3.5 76.8 ±1.2 84.4 ±0.6 89.4±0.4 87.5±0.4 Birds 90.8±1.5 61.2±1.3 75.8 ±1.0 90.4±0.4 89.6 ±0.4 DTD 85.0±2.5 66.0±1.1 74.3 ±0.7 83.4±0.4 84.8 ±0.5 QuickDraw 66.6 ±3.7 61.3 ±1.1 70.3 ±0.7 76.8±0.5 73.7±0.6 Fungi 59.4 ±4.2 35.5 ±1.1 81.7±0.6 59.3±0.8 56.8 ±0.8 Trafﬁc Sign 73.5 ±4.7 84.7±0.9 50.0±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 65.7±2.7 39.6±1.0 49.4 ±1.1 50.8 ±0.7 46.7 ±0.8 Caltech101 87.2 70.6 82.3 88.3 86.2 CIFAR100 54.4 31.3 33.7 52.7 47.0 Flowers102 83.3 66.1 55.7 85.3 83.0 Pets 87.9 49.1 76.3 89.9 89.3 Sun397 33.3 13.9 27.5 35.8 32.5 SVHN 70.4 83.2 18.7 62.7 59.8 EuroSAT 94.4 88.7 78.9 92.2 91.6 Resics45 76.1 63.7 62.4 75.5 74.4 Patch Camelyon 83.1 81.5 75.6 79.3 80.9 Retinopathy 70.2 57.6 27.9 74.3 73.7 CLEVR-count 74.0 40.3 30.0 40.3 42.0 CLEVR-dist 51.5 52.9 37.1 38.9 37.3 dSprites-loc 82.7 85.9 30.0 45.3 38.1 dSprites-ori 55.1 46.4 19.8 42.5 41.4 SmallNORB-azi 17.8 36.5 12.9 15.7 15.1 SmallNORB-elev 32.1 31.2 18.1 22.7 21.0 DMLab 43.2 37.9 33.3 38.7 36.1 KITTI-dist 79.9 58.7 52.3 71.0 69.6 MetaDataset (all) 73.3 63.4 71.0 76.1 74.9 VTAB (all) 65.4 55.6 42.9 58.4 56.6 VTAB (natural) 69.4 52.4 49.0 69.1 66.3 VTAB (specialized) 81.0 72.9 61.2 80.3 80.1 VTAB (structured) 54.5 49.4 29.2 39.4 37.6 22Table 11: Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold. Model ProtoNet ProtoMAML CTX LITE UpperCaSE UpperCaSE Image Size 224 126 224 224 224 224 Network ENB0 RN18 RN34 ENB0 ENB0 RN50-S Params (M) 4.0 11.2 21.3 4.0 4.0 23.5 Omniglot 88.3 ±0.8 90.2±0.7 84.6±0.9 86.5 ±0.8 90.7±0.4 89.1±0.5 Aircraft 85.0 ±0.7 82.1 ±0.6 85.3 ±0.8 83.6 ±0.7 89.4±0.4 87.5±0.4 Birds 90.2±0.5 73.4±0.9 72.9 ±1.1 88.6 ±0.7 90.4±0.4 89.6±0.4 DTD 81.4 ±0.6 66.3 ±0.8 77.3 ±0.7 84.1±0.7 83.4±0.4 84.8±0.5 QuickDraw 76.0±0.7 66.4±1.0 73.3 ±0.8 75.7±0.8 59.3±0.8 56.8 ±0.8 Fungi 57.4 ±1.1 46.3 ±1.1 48.0 ±1.2 56.9 ±1.2 59.3±0.8 56.8±0.8 Trafﬁc Sign 53.5 ±1.1 50.3 ±1.1 80.1±1.0 65.8±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 49.8±1.1 39.0±1.0 51.4±1.1 50.0 ±1.0 50.8 ±0.7 46.7±0.8 Caltech101 87.4 73.1 84.2 87.7 88.3 86.2 CIFAR100 43.1 29.7 37.5 48.8 52.7 47.0 Flowers102 78.2 60.2 81.8 83.5 85.3 83.0 Pets 88.6 56.6 70.9 89.3 89.9 89.3 Sun397 32.9 8.1 24.8 30.9 35.8 32.5 SVHN 35.2 46.8 67.2 51.0 62.7 59.8 EuroSAT 83.3 80.1 86.4 89.3 92.2 91.6 Resics45 68.8 53.5 67.7 76.4 75.5 74.4 Patch Camelyon 73.3 75.9 79.8 81.4 79.3 80.9 Retinopathy 31.3 73.2 35.5 40.3 74.3 73.7 CLEVR-count 27.2 32.7 27.9 31.4 40.3 42.0 CLEVR-dist 28.5 35.4 29.6 32.8 38.9 37.3 dSprites-loc 13.4 42.0 23.2 12.3 45.3 38.1 dSprites-ori 19.6 23.0 46.9 31.1 42.5 41.4 SmallNORB-azi 9.4 13.4 37.0 14.5 15.7 15.1 SmallNORB-elev 17.0 18.8 21.6 21.0 22.7 21.0 DMLab 35.8 32.5 31.9 39.4 38.7 36.1 KITTI-dist 56.5 54.4 54.3 63.9 71.0 69.6 MetaDataset (all) 72.7 64.2 71.6 73.9 76.1 74.9 VTAB (all) 46.1 45.0 50.5 51.4 58.4 56.6 VTAB (natural) 60.9 45.7 61.1 65.2 69.1 66.3 VTAB (specialized) 64.2 70.7 67.3 71.9 80.3 80.1 VTAB (structured) 25.9 31.5 34.1 30.8 39.4 37.6 23",
      "references": [],
      "meta_data": {
        "arxiv_id": "2206.09843v3",
        "authors": [
          "Massimiliano Patacchiola",
          "John Bronskill",
          "Aliaksandra Shysheya",
          "Katja Hofmann",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2022-06-20T15:25:08Z",
        "github_url": "https://github.com/mpatacchiola/contextual-squeeze-and-excitation"
      },
      "llm_extracted_info": {
        "main_contributions": "We introduce Contextual Squeeze-and-Excitation (CaSE), an adaptive block that enables rapid per-task adaptation of a pretrained network in few-shot image classification by conditioning on a small context set. We also propose UpperCaSE, a hybrid optimization protocol that uses CaSE to adapt the body (shared across tasks) via meta-training and a separate fast fine-tuning stage for the linear head. UpperCaSE achieves state-of-the-art accuracy among meta-learners on 26 datasets in VTAB+MD and strong performance on ORBIT, while drastically reducing adaptation cost compared to full fine-tuning methods.",
        "methodology": "CaSE extends the standard Squeeze-and-Excitation (SE) block by creating a per-task context vector through context pooling across the context images. This vector modulates channel-wise activations via a learned MLP, producing a scale γ that is transferred from the context to the target as an adaptive block. Unlike FiLM-based methods, CaSE uses only scale modulation (fewer parameters) and employs context pooling (per-task, not per-instance). UpperCaSE uses Coordinate Descent-style alternating optimization: inner-loop optimization updates the task-specific head ψτ while CaSE blocks are in adaptive mode; outer-loop updates update the shared CaSE parameters φ in the body. Inference on new tasks repeats the adaptive CaSE pass on the context, optimizes ψ∗ with a few gradient steps, then predicts with hψ∗(bφ(x∗)). CaSE is placed at multiple layers in backbones (e.g., EfficientNetB0, ResNet50-S) with a reduction-based bottleneck to limit parameters. The approach blends meta-training (fast body adaptation) with conventional fine-tuning (head) to achieve a favorable accuracy-cost Pareto frontier.",
        "experimental_setup": "Datasets and benchmarks: evaluation on VTAB+MD (MDv2 and VTAB datasets; 26 datasets total) and a real-world personalization benchmark ORBIT. Backbones: EfficientNetB0 and ResNet50-S pretrained on ImageNet. Training protocol: meta-training with 10K tasks (updates every 16 tasks); optimizer: Adam with linearly decaying learning rate from 1e-3 to 1e-5; CaSE parameters trained with 500 inner-loop steps for the head on mini-batches of 128; head parameters reset to zero before inner loop to avoid backprop through the entire head; evaluation uses 1200 tasks per dataset MD test; inputs sized 84x84 for ORBIT and 224x224 for VTAB; comparison against meta-learners, FiLM-based adapters (LITE), and fine-tuners (BiT, MD-Transfer, SUR). Evaluation metrics include average accuracy across datasets and adaptation cost in MACs; additional ablations and per-dataset analyses provided in appendices.",
        "limitations": "Two main limitations: (i) UpperCaSE relies on iterative gradient updates for the task head, which can be slow and hardware-dependent on devices with limited compute; (ii) VTAB per-dataset breakdown shows poorer performance on structured datasets, indicating that large domain shifts may still benefit from full-body fine-tuning. The method assumes a pretrained backbone and task-specific context availability; some gains are dataset- and backbone-dependent.",
        "future_research_directions": "Explore more granular layer-wise adapters or dynamic gating to further reduce cost; study the balance between local vs global context pooling; extend CaSE to other architectures (transformers, vision-language models) and modalities (video, audio); investigate improvements in context selection, robustness to imbalanced data, and domain shift; optimize the optimization routine (e.g., approximate/implicit gradients, faster inner loops) for mobile devices; analyze per-layer contributions to guide adapter placement and pruning; combine CaSE with other meta-learning strategies for broader applicability.",
        "experimental_code": "import torch\nfrom torch import nn\nfrom collections import OrderedDict\n\nclass CaSE(nn.Module):\n  def __init__(self, cin, reduction=64, min_units=16, standardize=True, out_mul=2.0, device=None, dtype=None):\n      \"\"\"\n      Initialize a CaSE adaptive block.\n  \n      Parameters:\n      cin (int): number of input channels.\n      reduction (int): divider for computing number of hidden units.\n      min_units (int): clip hidden units to this value (if lower).\n      standardize (bool): standardize the input for the MLP.\n      out_mul (float): multiply the MLP output by this value.\n      \"\"\"\n      factory_kwargs = {'device': device, 'dtype': dtype}\n      super(CaSE, self).__init__()\n      self.cin = cin\n      self.standardize = standardize\n      self.out_mul = out_mul\n\n      # Gamma-generator\n      hidden_features = max(min_units, cin // reduction)\n      self.gamma_generator = nn.Sequential(OrderedDict([\n          ('gamma_lin1', nn.Linear(cin, hidden_features, bias=True, **factory_kwargs)),\n          ('gamma_silu1', nn.SiLU()),\n          ('gamma_lin2', nn.Linear(hidden_features, hidden_features, bias=True, **factory_kwargs)),\n          ('gamma_silu2', nn.SiLU()),\n          ('gamma_lin3', nn.Linear(hidden_features, cin, bias=True, **factory_kwargs)),\n          ('gamma_sigmoid', nn.Sigmoid()),\n        ]))\n\n      self.gamma = torch.tensor([1.0]) # Set to one for the moment\n      self.reset_parameters()\n\n  def reset_parameters(self):      \n      torch.nn.init.zeros_(self.gamma_generator.gamma_lin3.weight)\n      torch.nn.init.zeros_(self.gamma_generator.gamma_lin3.bias)\n\n  def forward(self, x):\n      # Adaptive mode\n      if(self.training):\n          self.gamma = torch.mean(x, dim=[0,2,3]) # spatial + context pooling\n          if(self.standardize):\n                  self.gamma = (self.gamma - torch.mean(self.gamma)) / torch.sqrt(torch.var(self.gamma, unbiased=False)+1e-5)\n          self.gamma = self.gamma.unsqueeze(0) #-> [1,channels]\n          self.gamma = self.gamma_generator(self.gamma) * self.out_mul\n          self.gamma = self.gamma.reshape([1,-1,1,1])\n          return self.gamma * x # Apply gamma to the input and return\n      # Inference Mode\n      else:\n          self.gamma = self.gamma.to(x.device)\n          return self.gamma * x # Use previous gamma\n\n  def extra_repr(self) -> str:\n        return 'cin={}'.format(self.cin)\n\n\ndef _dummy():\n    pass\n",
        "experimental_info": "Experimental_info typically contains the description of the method’s experimental setup. The CaSE module and UpperCaSE are implemented in the repository. The following sections summarize the key experimental settings captured in the code and the accompanying paper description:\n\nCaSE method (implementation excerpt):\n- Gamma-generation network (per-task context conditioning): a small MLP that outputs a per-channel scale gamma with a final Sigmoid activation and an output multiplier. The hidden features count is hidden_features = max(min_units, cin // reduction).\n- Input standardization option controlled by standardize flag.\n- Context pooling: during training (adaptive mode), CaSE pools over spatial and contextual dimensions to compute a per-channel gamma, which is then transformed by gamma_generator and scaled by out_mul, producing a gamma of shape [1, C, 1, 1] that modulates the input feature map: gamma * x.\n- Inference mode uses a fixed gamma (the last computed/learned gamma).\n- Parameters: gamma_generator consists of three linear layers interleaved with SiLU activations and a final Sigmoid to produce gamma in [0,1].\n- Architecture decisions: uses a bottleneck-like gamma network with a reduction factor, and clamps hidden units with min_units.\n\nUpperCaSE method (implementation excerpt):\n- UpperCaSE implements a coordinate-descent-like meta-learning loop combining CaSE blocks with a fast linear head.\n- It collects parameters of adaptive CaSE blocks in adaptive_params_list and instantiates an Adam optimizer over those parameters, enabling fast per-task adaptation of the body.\n- Inner-loop head adaptation: for each task, a head h is initialized to zeros (weights and bias) and updated with Adam at a fixed learning rate while the CaSE blocks remain in adaptive mode. The inner loop runs tot_iterations steps (e.g., 500 in the provided example).\n- Outer-loop updates: after accumulating gradients from inner-loop, the outer optimizer (over CaSE adapter parameters) performs updates every 16 tasks (control via task_idx%16 == 0) using a linearly decaying learning rate schedule across total tasks.\n- Inference: to get predictions for a new task, CaSE blocks are activated (adaptive mode) to produce context embeddings, the head is trained on the context, and then predictions for the target set are produced by passing through the updated backbone with the learned head.\n\nExperimental_settings (as reflected in code and typical usage):\n- Datasets: The method is evaluated on meta-datasets and VTAB-like benchmarks (e.g., MD/VTAB combinations) and real-world personalization tasks such as ORBIT in literature; the repository includes a pipeline for meta-training with CaSE-based adaptation and a separate head fine-tuning phase.\n- Backbones: EfficientNetB0 and ResNet variants, with pretrained ImageNet weights; CaSE blocks are added as adaptive layers to various backbones.\n- Meta-training protocol (as described in Method and mirrored by code): 10k tasks per meta-training run, Adam optimizer with linearly decaying learning rate from 1e-3 to 1e-5; CaSE parameters trained with inner-loop of 500 steps on mini-batches of size 128; head parameters reset to zero before inner-loop to avoid backprop through entire head; 1200 tasks per dataset MD test; inputs are 224x224 for VTAB-like tasks, 84x84 for some benchmarks.\n- Inference/test protocol: per-task adaptation by running an adaptive CaSE pass on the context set, then optimizing the head for a few gradient steps, and finally predicting on the target set with the updated head.\n\nCode-examples shown here reflect the repository contents:\n- adapters/case.py contains the CaSE class with the forward method using per-task gamma (context-pooled) to modulate the input feature maps.\n- models/uppercase.py contains the UpperCaSE class implementing the two-stage optimization (inner-loop head update with CaSE in adaptive mode, outer-loop CaSE parameter updates) and the predict/learn methods used during evaluation and meta-training."
      }
    },
    {
      "title": "A Baseline for Few-Shot Image Classification",
      "full_text": "Published as a conference paper at ICLR 2020 A BASELINE FOR FEW-SHOT IMAGE CLASSIFICATION Guneet S. Dhillon1, Pratik Chaudhari2∗, Avinash Ravichandran1, Stefano Soatto1,3 1Amazon Web Services, 2University of Pennsylvania, 3University of California, Los Angeles {guneetsd, ravinash, soattos}@amazon.com, pratikac@seas.upenn.edu ABSTRACT Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When ﬁne-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered- ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the ﬁrst few-shot learning results on the ImageNet-21k dataset. We ﬁnd that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantiﬁes the “hardness” of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way. 1 I NTRODUCTION Prototypical Networks [2017] MAML [2017] LEO [2018] MetaOpt SVM [2019] Transductive Fine-Tuning 0 20 40 60 80 1001-shot, 5-way accuracy on Mini-Imagenet (%) Figure 1: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shot algorithms on the Mini-ImageNet (Vinyals et al., 2016) dataset for the 1-shot 5-way protocol. The boxes show the ± 25% quantiles of the accuracy while the notches indicate the median and its 95% conﬁdence interval. Whiskers denote the 1.5 × interquartile range which captures 99.3% of the probability mass for a normal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shot accuracies is large too. This suggests that progress may be illusory, especially considering that none outperform the simple transductive ﬁne-tuning baseline discussed in this paper (rightmost). As image classiﬁcation systems begin to tackle more and more classes, the cost of annotating a massive number of images and the difﬁculty of procuring images of rare categories increases. This has fueled interest in few-shot learning, where only few labeled samples per class are available for training. Fig. 1 displays a snapshot of the state-of-the-art. We estimated this plot by using published ∗Work done while at Amazon Web Services 1 arXiv:1909.02729v5  [cs.LG]  21 Oct 2020Published as a conference paper at ICLR 2020 numbers for the estimate of the mean accuracy, the 95% conﬁdence interval of this estimate and the number of few-shot episodes. For MAML (Finn et al., 2017) and MetaOpt SVM (Lee et al., 2019), we use the number of episodes in the author’s Github implementation. The ﬁeld appears to be progressing steadily albeit slowly based on Fig. 1. However, the variance of the estimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero (e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance of the accuracies is extremely large in Fig. 1. This suggests that progress in the past few years may be less signiﬁcant than it seems if one only looks at the mean accuracies. To compound the problem, many algorithms report results using different models for different number of ways (classes) and shots (number of labeled samples per class), with aggressive hyper-parameter optimization.1 Our goal is to develop a simple baseline for few-shot learning, one that does not require specialized training depending on the number of ways or shots, nor hyper-parameter tuning for different protocols. The simplest baseline we can think of is to pre-train a model on the meta-training dataset using the standard cross-entropy loss, and then ﬁne-tune on the few-shot dataset. Although this approach is basic and has been considered before (Vinyals et al., 2016; Chen et al., 2018), it has gone unnoticed that it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing ﬁne-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard benchmarks and few-shot protocols (cf. Table 1). Our contribution is to develop a transductive ﬁne-tuning baseline for few-shot learning, our approach works even for a single labeled example and a single test datum per class. Our baseline outperforms the state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al., 2018), all with the same hyper-parameters. Current approaches to few-shot learning are hard to scale to large datasets. We report the ﬁrst few-shot learning results on the ImageNet-21k dataset (Deng et al., 2009) which contains 14.2 million images across 21,814 classes. The rare classes in ImageNet-21k form a natural benchmark for few-shot learning. The empirical performance of this baseline, should not be understood as us suggesting that this is the right way of performing few-shot learning. We believe that sophisticated meta-training, understanding taxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective few-shot learning. The performance of the simple baseline however indicates that we need to interpret existing results2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate that, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically report performance for different few-shot protocols. 2 P ROBLEM DEFINITION AND RELATED WORK We ﬁrst introduce some notation and formalize the few-shot image classiﬁcation problem. Let (x,y) denote an image and its ground-truth label respectively. The training and test datasets are Ds = {(xi,yi)}Ns i=1 and Dq = {(xi,yi)}Nq i=1 respectively, where yi ∈Ct for some set of classes Ct. In the few-shot learning literature, training and test datasets are referred to as support and query datasets respectively, and are collectively called a few-shot episode. The number of ways, or classes, is |Ct|. The set {xi |yi = k, (xi,yi) ∈Ds}is the support of class kand its cardinality is ssupport shots (this is non-zero and is generally shortened to shots). The number sis small in the few-shot setting. The set {xi |yi = k,(xi,yi) ∈Dq}is the query of class kand its cardinality is qquery shots. The goal is to learn a function F to exploit the training set Ds to predict the label of a test datum x, 1For instance, Rusu et al. (2018) tune for different few-shot protocols, with parameters changing by up to six orders of magnitude; Oreshkin et al. (2018) use a different query shot for different few-shot protocols. 2For instance, Vinyals et al. (2016); Ravi & Larochelle (2016) use different versions of Mini-ImageNet; Oreshkin et al. (2018) report results for meta-training on the training set while Qiao et al. (2018) use both the training and validation sets; Chen et al. (2018) use full-sized images from the parent ImageNet-1k dataset (Deng et al., 2009); Snell et al. (2017); Finn et al. (2017); Oreshkin et al. (2018); Rusu et al. (2018) use different model architectures of varying sizes, which makes it difﬁcult to disentangle the effect of their algorithmic contributions. 2Published as a conference paper at ICLR 2020 where (x,y) ∈Dq, by ˆy= F(x; Ds). (1) Typical approaches for supervised learning replace Ds above with a statistic, θ∗= θ∗(Ds) that is, ideally, sufﬁcient to classify Ds, as measured by, say, the cross-entropy loss θ∗(Ds) = arg min θ 1 Ns ∑ (x,y)∈Ds −log pθ(y|x), (2) where pθ(·|x) is the probability distribution on Ct as predicted by the model in response to input x. When presented with a test datum, the classiﬁcation rule is typically chosen to be of the form Fθ∗ (x; Ds) ≜ arg max k pθ∗ (k|x), (3) where Ds is represented by θ∗. This form of the classiﬁer entails a loss of generality unless θ∗is a sufﬁcient statistic, pθ∗ (y|x) = p(y|x), which is of course never the case, especially given few labeled data in Ds. However, it conveniently separates training and inference phases, never having to revisit the training set. This might be desirable in ordinary image classiﬁcation, but not in few-shot learning. We therefore adopt the more general form of F in (1). If we call the test datum x= xNs+1 , then we can obtain the general form of the classiﬁer by ˆy= F(x; Ds) = arg min yNs+1 min θ 1 Ns + 1 Ns+1∑ i=1 −log pθ(yi|xi). (4) In addition to the training set, one typically also has a meta-training set, Dm = {(xi,yi)}Nm i=1, where yi ∈ Cm, with set of classes Cm disjoint from Ct. The goal of meta-training is to use Dm to infer the parameters of the few-shot learning model: ˆθ(Dm; (Ds,Dq)) = arg minθ 1 Nm ∑ (x,y)∈Dm ℓ(y,Fθ(x; (Ds,Dq))),where meta-training loss ℓdepends on the method. 2.1 R ELATED WORK Learning to learn: The meta-training loss is designed to make few-shot training efﬁcient (Utgoff, 1986; Schmidhuber, 1987; Baxter, 1995; Thrun, 1998). This approach partitions the problem into a base-level that performs standard supervised learning and a meta-level that accrues information from the base-level. Two main approaches have emerged to do so. Gradient-based approaches: These approaches treat the updates of the base-level as a learnable mapping (Bengio et al., 1992). This mapping can be learnt using temporal models (Hochreiter et al., 2001; Ravi & Larochelle, 2016), or one can back-propagate the gradients across the base-level updates (Maclaurin et al., 2015; Finn et al., 2017). It is challenging to perform this dual or bi-level optimization, respectively. These approaches have not been shown to be competitive on large datasets. Recent approaches learn the base-level in closed-form using SVMs (Bertinetto et al., 2018; Lee et al., 2019) which restricts the capacity of the base-level although it alleviates the optimization problem. Metric-based approaches: A majority of the state-of-the-art algorithms are metric-based approaches. These approaches learn an embedding that can be used to compare (Bromley et al., 1994; Chopra et al., 2005) or cluster (Vinyals et al., 2016; Snell et al., 2017) query samples. Recent approaches build upon this idea with increasing levels of sophistication in learning the embedding (Vinyals et al., 2016; Gidaris & Komodakis, 2018; Oreshkin et al., 2018), creating exemplars from the support set and picking a metric for the embedding (Gidaris & Komodakis, 2018; Allen et al., 2018; Ravichandran et al., 2019). There are numerous hyper-parameters involved in implementing these approaches which makes it hard to evaluate them systematically (Chen et al., 2018). Transductive learning: This approach is more efﬁcient at using few labeled data than supervised learning (Joachims, 1999; Zhou et al., 2004; Vapnik, 2013). The idea is to use information from the test datum xto restrict the hypothesis space while searching for the classiﬁer F(x,Ds) at test time. Our approach is closest to this line of work. We train a model on the meta-training set Dm and 3Published as a conference paper at ICLR 2020 initialize a classiﬁer using the support set Ds. The parameters are then ﬁne-tuned to adapt to the new test datum x. There are recent papers in few-shot learning such as Nichol et al. (2018); Liu et al. (2018a) that are motivated from transductive learning and exploit the unlabeled query samples. The former updates batch-normalization parameters using query samples while the latter uses label propagation to estimate labels of all query samples at once. Semi-supervised learning: We penalize the Shannon Entropy of the predictions on the query samples at test time. This is a simple technique in the semi-supervised learning literature, closest to Grandvalet & Bengio (2005). Modern augmentation techniques such as Miyato et al. (2015); Sajjadi et al. (2016); Dai et al. (2017) or graph-based approaches (Kipf & Welling, 2016) can also be used with our approach; we used the entropic penalty for the sake of simplicity. Semi-supervised few-shot learning is typically formulated as having access to extra unlabeled data during meta-training or few-shot training (Garcia & Bruna, 2017; Ren et al., 2018). This is different from our approach which uses the unlabeled query samples for transductive learning. Initialization for ﬁne-tuning: We use recent ideas from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Chen et al., 2018; Gidaris & Komodakis, 2018) to initialize the meta-trained model for ﬁne-tuning. These works connect the softmax cross-entropy loss with cosine distance and are discussed further in Section 3.1. 3 A PPROACH The simplest form of meta-training is pre-training with the cross-entropy loss, which yields ˆθ= arg min θ 1 Nm ∑ (x,y)∈Dm −log pθ(y|x) + R(θ), (5) where the second term denotes a regularizer, say weight decay R(θ) = ∥θ∥2/2. The model predicts logits zk(x; θ) for k ∈Cm and the distribution pθ(·|x) is computed from these logits using the softmax operator. This loss is typically minimized by stochastic gradient descent-based algorithms. If few-shot training is performed according to the general form in (4), then the optimization is identical to that above and amounts to ﬁne-tuning the pre-trained model. However, the model needs to be modiﬁed to account for the new classes. Careful initialization can make this process efﬁcient. 3.1 S UPPORT -BASED INITIALIZATION Given the pre-trained model (called the “backbone”), pθ (dropping the hat from ˆθ), we append a new fully-connected “classiﬁer” layer that takes the logits of the backbone as input and predicts the labels in Ct. For a support sample (x,y), denote the logits of the backbone by z(x; θ) ∈R|Cm|; the weights and biases of the classiﬁer by w∈R|Ct|×|Cm|and b∈R|Ct|respectively; and the kth row of wand bby wk and bk respectively. The ReLU non-linearity is denoted by (·)+. If the classiﬁer’s logits are z′ = wz(x; θ)+ + b, the ﬁrst term in the cross-entropy loss: −log pΘ(y|x) = −wyz(x; θ)+ −by + log∑ kewkz(x;θ)++bk would be the cosine distance between wy and z(x; θ)+ if both were normalized to unit ℓ2 norm and bias by = 0. This suggests wy = z(x; θ)+ ∥z(x; θ)+∥ and by = 0 (6) as a candidate for initializing the classiﬁer, along with normalizing z(x; θ)+ to unit ℓ2 norm. It is easy to see that this maximizes the cosine similarity between features z(x; θ)+ and weights wy. For multiple support samples per class, we take the Euclidean average of features z(x; θ)+ for each class in Ct, before ℓ2 normalization in (6). The logits of the classiﬁer are thus given by R|Ct|∋z(x; Θ) = w z(x; θ)+ ∥z(x; θ)+∥+ b, (7) 4Published as a conference paper at ICLR 2020 where Θ = {θ,w,b }, the combined parameters of the backbone and the classiﬁer. Note that we have added a ReLU non-linearity between the backbone and the classiﬁer, before the ℓ2 normalization. All the parameters Θ are trainable in the ﬁne-tuning phase. Remark 1 (Relation to weight imprinting). The support-based initialization is motivated from previous papers (Hu et al., 2015; Movshovitz-Attias et al., 2017; Chen et al., 2018; Gidaris & Komodakis, 2018). In particular, Qi et al. (2018) use a similar technique, with minor differences, to expand the size of the ﬁnal fully-connected layer (classiﬁer) for low-shot continual learning. The authors call their technique “weight imprinting” because wk can be thought of as a template for class k. In our case, we are only interested in performing well on the few-shot classes. Remark 2 (Using logits of the backbone instead of features as input to the classiﬁer). A natural way to adapt the backbone to predict new classes is to re-initialize its ﬁnal fully-connected layer (classiﬁer). We instead append a new classiﬁer after the logits of the backbone. This is motivated from Frosst et al. (2019) who show that for a trained backbone, outputs of all layers are entangled, without class-speciﬁc clusters; but the logits are peaked on the correct class, and are therefore well-clustered. The logits are thus better inputs to the classiﬁer as compared to the features. We explore this choice via an experiment in Appendix C.6. 3.2 T RANSDUCTIVE FINE -TUNING In (4), we assumed that there is a single query sample. However, we can also process multiple query samples together, and perform the minimization over all unknown query labels. We introduce a regularizer, similar to Grandvalet & Bengio (2005), as we seek outputs with a peaked posterior, or low Shannon Entropy H. So the transductive ﬁne-tuning phase solves for Θ∗= arg min Θ 1 Ns ∑ (x,y)∈Ds −log pΘ (y|x) + 1 Nq ∑ (x,y)∈Dq H(pΘ(·|x)). (8) Note that the data ﬁtting term uses the labeled support samples whereas the regularizer uses the unlabeled query samples. The two terms can be highly imbalanced (due to the varying range of values for the two quantities, or due to the variance in their estimates which depend onNs and Nq). To allow ﬁner control on this imbalance, one can use a coefﬁcient for the entropic term and/or a temperature in the softmax distribution of the query samples. Tuning these hyper-parameters per dataset and few-shot protocol leads to uniform improvements in the results in Section 4 by 1-2%. However, we wish to keep in line with our goal of developing a simple baseline and refrain from optimizing these hyper-parameters, and set them equal to 1 for all experiments on benchmark datasets. 4 E XPERIMENTAL RESULTS We show results of transductive ﬁne-tuning on benchmark datasets in few-shot learning, namely Mini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al., 2018), in Section 4.1. We also show large-scale experiments on the ImageNet-21k dataset (Deng et al., 2009) in Section 4.2. Along with the analysis in Section 4.3, these help us design a metric that measures the hardness of an episode in Section 4.4. We sketch key points of the experimental setup here; see Appendix A for details. Pre-training: We use the WRN-28-10 (Zagoruyko & Komodakis, 2016) model as the backbone. We pre-train using standard data augmentation, cross-entropy loss with label smoothing (Szegedy et al., 2016) of ϵ=0.1, mixup regularization (Zhang et al., 2017) of α=0.25, SGD with batch-size of 256, Nesterov’s momentum of 0.9, weight-decay of10−4 and no dropout. We use batch-normalization (Ioffe & Szegedy, 2015) but exclude its parameters from weight decay (Jia et al., 2018). We use cyclic learning rates (Smith, 2017) and half-precision distributed training on 8 GPUs (Howard et al., 2018) to reduce training time. Each dataset has a training, validation and test set consisting of disjoint sets of classes. Some algorithms use only the training set as the meta-training set (Snell et al., 2017; Oreshkin et al., 2018), while others use both training and validation sets (Rusu et al., 2018). For completeness we report 5Published as a conference paper at ICLR 2020 results using both methodologies; the former is denoted as (train) while the latter is denoted as (train + val). All experiments in Sections 4.3 and 4.4 use the (train + val) setting. Fine-tuning: We perform ﬁne-tuning on one GPU in full-precision for 25 epochs and a ﬁxed learning rate of 5 ×10−5 with Adam (Kingma & Ba, 2014) without any regularization. We make two weight updates in each epoch: one for the cross-entropy term using support samples and one for the Shannon Entropy term using query samples (cf. (8)). Hyper-parameters: We used images from ImageNet-1k belonging to the training classes of Mini- ImageNet as the validation set for pre-training the backbone for Mini-ImageNet. We used the validation set of Mini-ImageNet to choose hyper-parameters for ﬁne-tuning. All hyper-parameters are kept constant for experiments on benchmark datasets. Evaluation: Few-shot episodes contain classes sampled uniformly from classes in the test sets of the respective datasets; support and query samples are further sampled uniformly for each class; the query shot is ﬁxed to 15 for all experiments unless noted otherwise. All networks are evaluated over 1,000 few-shot episodes unless noted otherwise. To enable easy comparison with existing literature, we report an estimate of the mean accuracy and the 95% conﬁdence interval of this estimate. However, we encourage reporting the standard deviation in light of Section 1 and Fig. 1. 4.1 R ESULTS ON BENCHMARK DATASETS Table 1: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. Best results in each column are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider ResNet-12 which we denote as ResNet-12 ∗. Mini-ImageNet Tiered-ImageNet CIFAR-FS FC-100Algorithm Architecture 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%)Matching networks (Vinyals et al., 2016) conv(64)×4 46.6 60LSTM meta-learner (Ravi & Larochelle,2016) conv(64)×443.44±0.77 60.60±0.71Prototypical Networks (Snell et al., 2017) conv(64)×449.42±0.78 68.20±0.66MAML (Finn et al., 2017) conv(32)×448.70±1.84 63.11±0.92R2D2 (Bertinetto et al., 2018) conv(96k)×451.8±0.2 68.4±0.2 65.4 ±0.2 79.4±0.2TADAM (Oreshkin et al., 2018) ResNet-12 58.5±0.3 76.7±0.3 40.1 ±0.4 56.1±0.4Transductive Propagation (Liu et al.,2018b) conv(64)×455.51±0.86 69.86±0.65 59.91±0.94 73.30±0.75Transductive Propagation (Liu et al.,2018b) ResNet-12 59.46 75.64MetaOpt SVM (Lee et al., 2019) ResNet-12∗62.64±0.6178.63±0.4665.99±0.72 81.56±0.53 72.0±0.7 84.2±0.5 41.1±0.6 55.5±0.6Support-based initialization (train) WRN-28-10 56.17±0.64 73.31±0.53 67.45±0.70†82.88±0.53†70.26±0.70 83.82±0.49†36.82±0.51 49.72±0.55Fine-tuning (train) WRN-28-10 57.73±0.6278.17±0.4966.58±0.7085.55±0.4868.72±0.6786.11±0.4738.25±0.5257.19±0.57Transductive ﬁne-tuning (train) WRN-28-1065.73±0.68 78.40±0.52 73.34±0.71 85.50±0.50 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55Activation to Parameter (Qiao et al., 2018)(train + val)WRN-28-10 59.60±0.41 73.74±0.19LEO (Rusu et al., 2018) (train + val) WRN-28-10 61.76±0.08 77.59±0.12 66.33±0.05 81.44±0.09MetaOpt SVM (Lee et al., 2019) (train +val) ResNet-12∗64.09±0.6280.00±0.4565.81±0.74 81.75±0.53 72.8±0.7 85.0±0.5 47.2±0.6 62.5±0.6Support-based initialization (train + val) WRN-28-10 58.47±0.66 75.56±0.52 67.34±0.69†83.32±0.51†72.14±0.69†85.21±0.49†45.08±0.61 60.05±0.60Fine-tuning (train + val) WRN-28-10 59.62±0.6679.93±0.4766.23±0.6886.08±0.4770.07±0.6787.26±0.4543.80±0.58 64.40±0.58Transductive ﬁne-tuning (train + val) WRN-28-1068.11±0.69 80.36±0.50 72.87±0.71 86.15±0.50 78.36±0.70 87.54±0.49 50.44±0.68 65.74±0.60 Table 1 shows the results of transductive ﬁne-tuning on benchmark datasets for standard few-shot protocols. We see that this simple baseline is uniformly better than state-of-the-art algorithms. We include results for support-based initialization, which does no ﬁne-tuning; and for ﬁne-tuning, which involves optimizing only the cross-entropy term in (8) using the labeled support samples. The support-based initialization is sometimes better than or comparable to state-of-the-art algorithms (marked †). The few-shot literature has gravitated towards larger backbones (Rusu et al., 2018). Our results indicate that for large backbones even standard cross-entropy pre-training and support-based initialization work well, similar to observation made by Chen et al. (2018). 6Published as a conference paper at ICLR 2020 For the 1-shot 5-way setting, ﬁne-tuning using only the labeled support examples leads to minor improvement over the initialization, and sometimes marginal degradation. However,for the 5-shot 5-way setting non-transductive ﬁne-tuning is better than the state-of-the-art. In both (train) and (train + val) settings, transductive ﬁne-tuning leads to 2-7% improvement for 1-shot 5-way setting over the state-of-the-art for all datasets. It results in an increase of 1.5-4% for the 5-shot 5-way setting except for the Mini-ImageNet dataset, where the performance is matched. This suggests that the use of the unlabeled query samples is vital for the few-shot setting. For the Mini-ImageNet, CIFAR-FS and FC-100 datasets, using additional data from the valida- tion set to pre-train the backbone results in 2-8% improvements; the improvement is smaller for Tiered-ImageNet. This suggests that having more pre-training classes leads to improved few-shot performance as a consequence of a better embedding. See Appendix C.5 for more experiments. 4.2 L ARGE -SCALE FEW -SHOT LEARNING The ImageNet-21k dataset (Deng et al., 2009) with 14.2M images across 21,814 classes is an ideal large-scale few-shot learning benchmark due to the high class imbalance. The simplicity of our approach allows us to present the ﬁrst few-shot learning results on this large dataset. We use the 7,491 classes having more than 1,000 images each as the meta-training set and the next 13,007 classes with at least 10 images each for constructing few-shot episodes. See Appendix B for details. Table 2: Accuracy (%) on the few-shot data of ImageNet-21k. The conﬁdence intervals are large because we compute statistics only over 80 few-shot episodes so as to test for large number of ways. Way Algorithm Model Shot 5 10 20 40 80 160 Support-based initialization WRN-28-10 1 87.20±1.72 78.71±1.63 69.48±1.30 60.55±1.03 49.15±0.68 40.57±0.42 Transductive ﬁne-tuning WRN-28-10 1 89.00±1.86 79.88±1.70 69.66±1.30 60.72±1.04 48.88±0.66 40.46±0.44 Support-based initialization WRN-28-10 5 95.73±0.84 91.00±1.09 84.77±1.04 78.10±0.79 70.09±0.71 61.93±0.45 Transductive ﬁne-tuning WRN-28-10 5 95.20±0.94 90.61±1.03 84.21±1.09 77.13±0.82 68.94±0.75 60.11±0.48 Table 2 shows the mean accuracy of transductive ﬁne-tuning evaluated over 80 few-shot episodes on ImageNet-21k. The accuracy is extremely high as compared to corresponding results in Table 1 even for large way. E.g., the 1-shot 5-way accuracy on Tiered-ImageNet is 72.87 ±0.71% while it is 89 ± 1.86% here. This corroborates the results in Section 4.1 and indicates that pre-training with a large number of classes may be an effective strategy to build large-scale few-shot learning systems. The improvements of transductive ﬁne-tuning are minor for ImageNet-21k because the support-based initialization accuracies are extremely high. We noticed a slight degradation of accuracies due to transductive ﬁne-tuning at high ways because the entropic term in (8) is much larger than the the cross-entropy loss. The experiments for ImageNet-21k therefore scale down the entropic term by log |Ct|and forego the ReLU in (6) and (7). This reduces the difference in accuracies at high ways. 4.3 A NALYSIS This section presents a comprehensive analysis of transductive ﬁne-tuning on the Mini-ImageNet, Tiered-ImageNet and ImageNet-21k datasets. Robustness of transductive ﬁne-tuning to query shot: Fig. 2a shows the effect of changing the query shot on the mean accuracy. For the 1-shot 5-way setting, the entropic penalty in (8) helps as the query shot increases. This effect is minor in the 5-shot 5-way setting as more labeled data is available. Query shot of 1 achieves a relatively high mean accuracy because transductive ﬁne-tuning can adapt to those few queries. One query shot is enough to beneﬁt from transductive ﬁne-tuning : for Mini-ImageNet, the 1-shot 5-way accuracy with query shot of 1 is 66.94 ±1.55% which is better than non-transductive ﬁne-tuning (59.62 ±0.66% in Table 1) and higher than other approaches. Performance for different way and support shot: A few-shot system should be able to robustly handle different few-shot scenarios. Figs. 2b and 2c, show the performance of transductive ﬁne-tuning 7Published as a conference paper at ICLR 2020 1 5 10 15 20 Query shot 65 75 85 95Mean accuracy (%) 1 shot 5-way Mini-Imagenet 5 shot         \"  1 shot 5-way Tiered-Imagenet 5 shot         \"  (a) 101 102 Way 0 20 40 60 80 100Mean accuracy (%)  1 shot   Tiered-ImageNet  5 shot       \"  10 shot       \"   1 shot   ImageNet-21k  5 shot       \"  10 shot       \" (b) 100 101 Support Shot 20 40 60 80 100Mean accuracy (%)  5 way Tiered-Imagenet 20 way       \" 80 way       \" 160 way       \" (c) Figure 2: Mean accuracy of transductive ﬁne-tuning for different query shot, way and support shot. Fig. 2a shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for Tiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive ﬁne-tuning can specialize to those queries. Fig. 2b shows that the mean accuracy degrades logarithmically with way for ﬁxed support shot and query shot (15). Fig. 2c suggests that the mean accuracy improves logarithmically with the support shot for ﬁxed way and query shot (15). These trends suggest thumb rules for building few-shot systems. with changing way and support shot. The mean accuracy changes logarithmically with the way and support shot which provides thumb rules for building few-shot systems. Different backbone architectures: We include experiments using conv(64)×4 (Vinyals et al., 2016) and ResNet-12 (He et al., 2016a; Oreshkin et al., 2018) in Table 3, in order to facilitate comparisons for different backbone architectures. The results for transductive ﬁne-tuning are comparable or better than state-of-the-art for a given backbone architecture, except for those in Liu et al. (2018b) who use a more sophisticated transductive algorithm using graph propagation, with conv (64)×4. In line with our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results show that transductive ﬁne-tuning is a sound baseline for a variety of backbone architectures. Computational complexity: There is no free lunch and our advocated baseline has its limitations. It performs gradient updates during the ﬁne-tuning phase which makes it slow at inference time. Speciﬁcally, transductive ﬁne-tuning is about 300 ×slower (20.8 vs. 0.07 seconds) for a 1-shot 5-way episode with 15 query shot as compared to Snell et al. (2017) with the same backbone architecture (prototypical networks (Snell et al., 2017) do not update model parameters at inference time). The latency factor reduces with higher support shot. Interestingly, for a single query shot, the former takes 4 seconds vs. 0.07 seconds. This is a more reasonable factor of 50 ×, especially considering that the mean accuracy of the former is 66.2% compared to about 58% of the latter in our implementation. Experiments in Appendix C.3 suggest that using a smaller backbone architecture partially compensates for the latency with some degradation of accuracy. A number of approaches such as Ravi & Larochelle (2016); Finn et al. (2017); Rusu et al. (2018); Lee et al. (2019) also perform additional processing at inference time and are expected to be slow, along with other transductive approaches (Nichol et al., 2018; Liu et al., 2018b). Additionally, support-based initialization has the same inference time as Snell et al. (2017). 4.4 A PROPOSAL FOR REPORTING FEW -SHOT CLASSIFICATION PERFORMANCE As discussed in Section 1, we need better metrics to report the performance of few-shot algorithms. There are two main issues: (i) standard deviation of the few-shot accuracy across different sampled episodes for a given algorithm, dataset and few-shot protocol is very high (cf. Fig. 1), and (ii) different models and hyper-parameters for different few-shot protocols makes evaluating algorithmic contributions difﬁcult (cf. Table 1). This section takes a step towards resolving these issues. Hardness of an episode: Classiﬁcation performance on a few-shot episode is determined by the relative location of the features corresponding to labeled and unlabeled samples. If the unlabeled 8Published as a conference paper at ICLR 2020 features are close to the labeled features from the same class, the classiﬁer can distinguish between the classes easily to obtain a high accuracy. Otherwise, the accuracy would be low. The following deﬁnition characterizes this intuition. For training (support) set Ds and test (query) set Dq, we will deﬁne the hardness Ωϕ as the average log-odds of a test datum being classiﬁed incorrectly. More precisely, Ωϕ(Dq; Ds) = 1 Nq ∑ (x,y)∈Dq log 1 −p(y|x) p(y|x) , (9) where p(·|x) is a softmax distribution with logits zy = wϕ(x). wis the weight matrix constructed using (6) and Ds; and ϕis the ℓ2 normalized logits computed using a rich-enough feature generator, say a deep network trained for standard image classiﬁcation. This is a clustering loss where the labeled support samples form class-speciﬁc cluster centers. The cluster afﬁnities are calculated using cosine-similarities, followed by the softmax operator to get the probability distribution p(·|x). Note that Ωϕ does not depend on the few-shot learner and gives a measure of how difﬁcult the classiﬁcation problem is for any few-shot episode, using a generic feature extractor. 1 2 3 4 5 Hardness 20 40 60 80 100Accuracy (%) CIFAR-FS FC-100 Tiered-Imagenet Mini-Imagenet Imagenet-21k Figure 3: Comparing the accuracy of transductive ﬁne-tuning (solid lines) vs. support-based initialization (dotted lines) for different datasets, ways (5, 10, 20, 40, 80 and 160) and support shots (1 and 5).Abscissae are computed using (9) and a Resnet-152 (He et al., 2016b) network trained for standard image classiﬁcation on the ImageNet-1k dataset. Each marker indicates the accuracy of transductive ﬁne-tuning on a few-shot episode; markers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes different ways; ways increase from left to right (5, 10, 20, 40, 80 and 160). Size of the markers denotes different support shot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot 10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combining the episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing two algorithms for different few-shot scenarios. The areas in the ﬁrst quadrant under the ﬁtted regression lines are 295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet) and 475 vs. 484 (ImageNet-21k) for transductive ﬁne-tuning and support-based initialization. 9Published as a conference paper at ICLR 2020 Fig. 3 demonstrates how to use the hardness metric. Few-shot accuracy degrades linearly with hardness. Performance for all hardness can thus be estimated by testing for two different ways. We advocate selecting hyper-parameters using the area under the ﬁtted curve as a metric instead of tuning them speciﬁcally for each few-shot protocol. The advantage of such a test methodology is that it predicts the performance of the model across multiple few-shot protocols systematically. Different algorithms can be compared directly , e.g., transductive ﬁne-tuning (solid lines) and support-based initialization (dotted lines). For instance, the former leads to large improvements on easy episodes, the performance is similar for hard episodes, especially for Tiered-ImageNet and ImageNet-21k. The high standard deviation of accuracy of few-shot learning algorithms in Fig. 1 can be seen as the spread of the cluster corresponding to each few-shot protocol, e.g., the ellipse in Fig. 3 denotes the 5-shot 10-way protocol for ImageNet-21k. It is the nature of few-shot learning that episodes have varying hardness even if the way and shot are ﬁxed. However, episodes within the ellipse lie on a different line (with a large negative slope) which indicates that given a few-shot protocol, hardness is a good indicator of accuracy. Fig. 3 also shows that due to fewer test classes, CIFAR-FS, FC-100 and Mini-ImageNet have less diversity in the hardness of episodes while Tiered-ImageNet and ImageNet-21k allow sampling of both very hard and very easy diverse episodes. For a given few-shot protocol, the hardness of episodes in the former three is almost the same as that of the latter two datasets. This indicates that CIFAR-FS, FC-100 and Mini-ImageNet may be good benchmarks for applications with few classes. The hardness metric in (9) naturally builds upon existing ideas in deep metric learning (Qi et al., 2018). We propose it as a means to evaluate few-shot learning algorithms uniformly across different few-shot protocols for different datasets; ascertaining its efﬁcacy and comparisons to other metrics will be part of future work. 5 D ISCUSSION Our aim is to provide grounding to the practice of few-shot learning. The current literature is in the spirit of increasingly sophisticated approaches for modest improvements in mean accuracy using an inadequate evaluation methodology. This is why we set out to establish a baseline, namely transductive ﬁne-tuning, and a systematic evaluation methodology, namely the hardness metric. We would like to emphasize that our advocated baseline, namely transductive ﬁne-tuning, is not novel and yet performs better than existing algorithms on all standard benchmarks. This is indeed surprising and indicates that we need to take a step back and re-evaluate the status quo in few-shot learning. We hope to use the results in this paper as guidelines for the development of new algorithms. REFERENCES Kelsey R Allen, Hanul Shin, Evan Shelhamer, and Josh B Tenenbaum. Variadic learning by bayesian nonpara- metric deep embedding. 2018. Jonathan Baxter. Learning internal representations. Flinders University of S. Aust., 1995. Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas, 1992. Luca Bertinetto, Jo ˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. arXiv:1805.08136, 2018. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁcation using a” siamese” time delay neural network. In Advances in neural information processing systems, pp. 737–744, 1994. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. 2018. Sumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric discriminatively, with application to face veriﬁcation. In CVPR (1), pp. 539–546, 2005. 10Published as a conference paper at ICLR 2020 Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised learning that requires a bad gan. In Advances in neural information processing systems, pp. 6510–6520, 2017. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126– 1135. JMLR. org, 2017. Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft nearest neighbor loss. arXiv:1902.01889, 2019. Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv:1711.04043, 2017. Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural information processing systems, pp. 529–536, 2005. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv:1603.05027, 2016b. Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001. Jeremy Howard et al. fastai. https://github.com/fastai/fastai, 2018. Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 325–333, 2015. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015. Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al. Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes. arXiv:1807.11205, 2018. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Icml, volume 99, pp. 200–209, 1999. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv:1609.02907, 2016. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. arXiv:1904.03758, 2019. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learning. 2018a. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot learning. arXiv:1805.10002, 2018b. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983, 2016. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579–2605, 2008. Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pp. 2113–2122, 2015. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv:1710.03740, 2017. 11Published as a conference paper at ICLR 2020 Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with virtual adversarial training. arXiv:1507.00677, 2015. Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In Proceedings of the IEEE International Conference on Computer Vision, pp. 360–368, 2017. Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv:1803.02999, 2018. Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp. 719–729, 2018. Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830, 2018. Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7229–7238, 2018. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016. Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class models and shot-free meta training, 2019. Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv:1803.00676, 2018. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv:1807.05960, 2018. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163–1171, 2016. Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987. Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 464–472. IEEE, 2017. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998. Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019. Paul E Utgoff. Shift of bias for inductive concept learning.Machine learning: An artiﬁcial intelligence approach, 2:107–148, 1986. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Junyuan Xie, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. arXiv:1812.01187, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv:1605.07146, 2016. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv:1710.09412, 2017. Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf. Learning with local and global consistency. In Advances in neural information processing systems, pp. 321–328, 2004. 12Published as a conference paper at ICLR 2020 A S ETUP A.1 D ATASETS We use the following datasets for our benchmarking experiments. • The Mini-ImageNet dataset (Vinyals et al., 2016) which is a subset of ImageNet-1k (Deng et al., 2009) and consists of 84 ×84 sized images with 600 images per class. There are 64 training, 16 validation and 20 test classes. There are multiple versions of this dataset in the literature; we obtained the dataset from the authors of Gidaris & Komodakis (2018)3. • The Tiered-ImageNet dataset (Ren et al., 2018) is a larger subset of ImageNet-1k with 608 classes split as 351 training, 97 validation and 160 testing classes, each with about 1300 images of size 84 ×84. This dataset ensures that training, validation and test classes do not have a semantic overlap and is a potentially harder few-shot learning dataset. • We also consider two smaller CIFAR-100 (Krizhevsky & Hinton, 2009) derivatives, both with 32 ×32 sized images and 600 images per class. The ﬁrst is the CIFAR-FS dataset (Bertinetto et al., 2018) which splits classes randomly into 64 training, 16 validation and 20 test. The second is the FC-100 dataset (Oreshkin et al., 2018) which splits CIFAR-100 into 60 training, 20 validation and 20 test classes with minimal semantic overlap. Each dataset has a training, validation and test set. The set of classes for each of these sets are disjoint from each other. For meta-training, we ran two sets of experiments: the ﬁrst, where we only use the training set as the meta-training dataset, denoted by (train); the second, where we use both the training and validation sets as the meta-training dataset, denoted by (train + val). We use the test set to construct few-shot episodes. A.2 P RE-TRAINING We use a wide residual network (Zagoruyko & Komodakis, 2016; Qiao et al., 2018; Rusu et al., 2018) with a widening factor of 10 and a depth of 28 which we denote as WRN-28-10. The smaller networks: conv (64)×4 (Vinyals et al., 2016; Snell et al., 2017), ResNet-12 (He et al., 2016a; Oreshkin et al., 2018; Lee et al., 2019) and WRN-16-4 (Zagoruyko & Komodakis, 2016), are used for analysis in Appendix C. All networks are trained using SGD with a batch-size of 256, Nesterov’s momentum set to 0.9, no dropout, weight decay of 10−4. We use batch-normalization (Ioffe & Szegedy, 2015). We use two-cycles of learning rate annealing (Smith, 2017), these are 40 and 80 epochs each for all datasets except ImageNet-21k, which uses cycles of 8 and 16 epochs each. The learning rate is set to 10−i at the beginning of the ith cycle and decreased to 10−6 by the end of that cycle with a cosine schedule (Loshchilov & Hutter, 2016). We use data parallelism across 8 Nvidia V100 GPUs and half-precision training using techniques from Micikevicius et al. (2017); Howard et al. (2018). We use the following regularization techniques that have been discovered in the non-few-shot, standard image classiﬁcation literature (Xie et al., 2018) for pre-training the backbone. • Mixup (Zhang et al., 2017): This augments data by a linear interpolation between input images and their one-hot labels. If (x1,y1),(x2,y2) ∈D are two samples, mixup creates a new sample (˜x,˜y) where ˜x= λx1 + (1−λ)x2 and its label ˜y= λey1 + (1−λ)ey2 ; here ek is the one-hot vector with a non-zero kth entry and λ∈[0,1] is sampled from Beta(α,α) for a hyper-parameter α. • Label smoothing (Szegedy et al., 2016): When using a softmax operator, the logits can increase or decrease in an unbounded manner causing numerical instabilities while training. Label smoothing sets pθ(k|x) = 1 −ϵ if k = y and ϵ/(K −1) otherwise, for a small constant ϵ> 0 and number of classes K. The ratio between the largest and smallest output neuron is thus ﬁxed which helps large-scale training. • We exclude the batch-normalization parameters from weight-decay (Jia et al., 2018). 3https://github.com/gidariss/FewShotWithoutForgetting 13Published as a conference paper at ICLR 2020 We set ϵ=0.1 for label smoothing cross-entroy loss and α=0.25 for mixup regularization for all our experiments. A.3 F INE -TUNING HYPER -PARAMETERS We used 1-shot 5-way episodes on the validation set of Mini-ImageNet to manually tune hyper- parameters. Fine-tuning is done for 25 epochs with a ﬁxed learning rate of 5 ×10−5 with Adam (Kingma & Ba, 2014). Adam is used here as it is more robust to large changes in the magnitude of the loss and gradients which occurs if the number of classes in the few-shot episode (ways) is large. We do not use any regularization (weight-decay, mixup, dropout, or label smoothing) in the ﬁne-tuning phase. These hyper-parameters are kept constant on all benchmark datasets, namely Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100. All ﬁne-tuning and evaluation is performed on a single GPU in full-precision. We update the parameters sequentially by computing the gradient of the two terms in (8) independently. This updates both the weights of the model and the batch-normalization parameters. A.4 D ATA AUGMENTATION Input images are normalized using the mean and standard-deviation computed on ImageNet-1k. Our Data augmentation consists of left-right ﬂips with probability of 0.5, padding the image with 4px and adding brightness and contrast changes of ±40%. The augmentation is kept the same for both pre-training and ﬁne-tuning. We explored augmentation using afﬁne transforms of the images but found that adding this has minor effect with no particular trend on the numerical results. A.5 E VALUATION PROCEDURE The few-shot episode contains classes that are uniformly sampled from the test classes of correspond- ing datasets. Support and query samples are further uniformly sampled for each class. The query shot is ﬁxed to 15 for all experiments unless noted otherwise. We evaluate all networks over 1,000 episodes unless noted otherwise. For ease of comparison, we report the mean accuracy and the 95% conﬁdence interval of the estimate of the mean accuracy. B S ETUP FOR IMAGE NET-21 K The ImageNet-21k dataset (Deng et al., 2009) has 14.2M images across 21,814 classes. The blue region in Fig. 4 denotes our meta-training set with 7,491 classes, each with more than 1,000 images. The green region shows 13,007 classes with at least 10 images each, the set used to construct few-shot episodes. We do not use the red region consisting of 1,343 classes with less than 10 images each. We train the same backbone (WRN-28-10) with the same procedure as that in Appendix A on 84 × 84 resized images, albeit for only 24 epochs. Since we use the same hyper-parameters as the other benchmark datasets, we did not create validation sets for pre-training or the ﬁne-tuning phases. The few-shot episodes are constructed in the same way as Appendix A. We evaluate using fewer few-shot episodes (80) on this dataset because we would like to demonstrate the performance across a large number of different ways. C A DDITIONAL ANALYSIS This section contains additional experiments and analysis, complementing Section 4.3. All ex- periments use the (train + val) setting, pre-training on both the training and validation sets of the corresponding datasets, unless noted otherwise. 14Published as a conference paper at ICLR 2020 0 5000 10000 15000 20000 classes 100 101 102 103 images per class Figure 4: ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images while the rarest class has a single image. Figure 5: t-SNE (Maaten & Hinton, 2008) embedding of the logits for 1-shot 5-way few-shot episode of Mini-ImageNet. Colors denote the ground-truth labels; crosses denote the support samples; circles denote the query samples; translucent markers and opaque markers denote the embeddings before and after transductive ﬁne-tuning respectively. Even though query samples are far away from their respective supports in the beginning, they move towards the supports by the end of transductive ﬁne-tuning. Logits of support samples are relatively unchanged which suggests that the support-based initialization is effective. C.1 T RANSDUCTIVE FINE -TUNING CHANGES THE EMBEDDING DRAMATICALLY Fig. 5 demonstrates this effect. The logits for query samples are far from those of their respective support samples and metric-based loss functions, e.g., those for prototypical networks (Snell et al., 2017) would have a high loss on this episode; indeed the accuracy after the support-based initialization is 64%. Logits for the query samples change dramatically during transductive ﬁne-tuning and majority of the query samples cluster around their respective supports. The post transductive ﬁne-tuning accuracy of this episode is 73.3%. This suggests that modifying the embedding using the query samples is crucial to obtaining good performance on new classes. This example also demonstrates that the support-based initialization is efﬁcient, logits of the support samples are relatively unchanged during the transductive ﬁne-tuning phase. 15Published as a conference paper at ICLR 2020 C.2 L ARGE VS . SMALL BACKBONES The expressive power of the backbone plays an important role in the efﬁcacy of ﬁne-tuning. We observed that a WRN-16-4 architecture (2.7M parameters) performs worse than WRN-28-10 (36M parameters). The former obtains 63.28 ±0.68% and 77.39 ±0.5% accuracy on Mini-ImageNet and 69.04 ±0.69% and 83.55 ±0.51% accuracy on Tiered-ImageNet on 1-shot 5-way and 5-shot 5-way protocols respectively. While these numbers are comparable to those of state-of-the-art algorithms, they are lower than their counterparts for WRN-28-10 in Table 1. This suggests that a larger network is effective in learning richer features from the meta-training classes, and ﬁne-tuning is effective in taking advantage of this to further improve performance on samples belonging to few-shot classes. C.3 L ATENCY WITH A SMALLER BACKBONES The WRN-16-4 architecture (2.7M parameters) is much smaller than WRN-28-10 (36M parameters) and transductive ﬁne-tuning on the former is much faster. As compared to our implementation of Snell et al. (2017) with the same backbone, WRN-16-4 is 20-70×slower (0.87 vs. 0.04 seconds for a query shot of 1, and 2.85 vs. 0.04 seconds for a query shot of 15) for the 1-shot 5-way scenario. Compare this to the computational complexity experiment in Section 4.3. As discussed in Appendix C.2, the accuracy of WRN-16-4 is 63.28 ±0.68% and 77.39 ±0.5% for 1-shot 5-way and 5-shot 5-way on Mini-ImageNet respectively. As compared to this, our implementation of (Snell et al., 2017) using a WRN-16-4 backbone obtains 57.29 ±0.40% and 75.34 ±0.32% accuracies for the same settings respectively; the former number in particular is signiﬁcantly worse than its transductive ﬁne-tuning counterpart. C.4 C OMPARISONS AGAINST BACKBONES IN THE CURRENT LITERATURE We include experiments using conv (64)×4 and ResNet-12 in Table 3, in addition to WRN-28-10 in Section 4, in order to facilitate comparisons of the proposed baseline for different backbone architectures. Our results are comparable or better than existing results for a given backbone architecture, except for those in Liu et al. (2018b) who use a graph-based transduction algorithm, for conv (64)×4 on Mini-ImageNet. In line with our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results suggest that transductive ﬁne-tuning is a sound baseline for a variety of backbone architectures. C.5 U SING MORE META -TRAINING CLASSES In Section 4.1 we observed that having more pre-training classes improves few-shot performance. But since we append a classiﬁer on top of a pre-trained backbone and use the logits of the backbone as inputs to the classiﬁer, a backbone pre-trained on more classes would also have more parameters as compared to one pre-trained on fewer classes. However, this difference is not large: WRN-28-10 for Mini-ImageNet has 0.03% more parameters for (train + val) as compared to (train). However, in order to facilitate a fair comparison, we ran an experiment where we use the features of the backbone, instead of the logits, as inputs to the classiﬁer. By doing so, the number of parameters in the pre-trained backbone that are used for few-shot classiﬁcation remain the same for both the (train) and (train + val) settings. For Mini-ImageNet, (train + val) obtains 64.20 ±0.65% and 81.26 ±0.45%, and (train) obtains 62.55 ±0.65% and 78.89 ±0.46%, for 1-shot 5-way and 5-shot 5-way respectively. These results corroborate the original statement that more pre-training classes improves few-shot performance. C.6 U SING FEATURES OF THE BACKBONE AS INPUT TO THE CLASSIFIER Instead of re-initializing the ﬁnal fully-connected layer of the backbone to classify new classes, we simply append the classiﬁer on top of it. We implemented the former, more common, approach and found that it achieves an accuracy of 64.20 ±0.65% and 81.26 ±0.45% for 1-shot 5-way and 5-shot 5-way respectively on Mini-ImageNet, while the accuracy on Tiered-ImageNet is 67.14 ± 16Published as a conference paper at ICLR 2020 Table 3: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. The rows are grouped by the backbone architectures. Best results in each column and for a given backbone architecture are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider ResNet-12 which we denote as ResNet-12 ∗. Mini-ImageNet Tiered-ImageNet CIFAR-FS FC-100Algorithm Architecture 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%)MAML (Finn et al., 2017) conv(32)×448.70±1.84 63.11±0.92Matching networks (Vinyals et al., 2016) conv(64)×4 46.6 60LSTM meta-learner (Ravi & Larochelle,2016) conv(64)×443.44±0.77 60.60±0.71Prototypical Networks (Snell et al., 2017) conv(64)×449.42±0.78 68.20±0.66Transductive Propagation (Liu et al.,2018b) conv(64)×455.51±0.86 69.86±0.65 59.91±0.9473.30±0.75Support-based initialization (train) conv(64)×450.69±0.63 66.07±0.53 58.42±0.6973.98±0.58†61.77±0.73 76.40±0.54 36.07±0.5448.72±0.57Fine-tuning (train) conv(64)×449.43±0.62 66.42±0.53 57.45±0.6873.96±0.5659.74±0.72 76.37±0.53 35.46±0.53 49.43±0.57Transductive ﬁne-tuning (train) conv(64)×450.46±0.62 66.68±0.52 58.05±0.6874.24±0.56 61.73±0.72 76.92±0.52 36.62±0.55 50.24±0.58R2D2 (Bertinetto et al., 2018) conv(96k)×451.8±0.2 68.4±0.2 65.4 ±0.2 79.4±0.2TADAM (Oreshkin et al., 2018) ResNet-12 58.5±0.376.7±0.3 40.1±0.456.1±0.4Transductive Propagation (Liu et al.,2018b) ResNet-12 59.46 75.64Support-based initialization (train) ResNet-12 54.21±0.64 70.58±0.54 66.39±0.73 81.93±0.54 65.69±0.72 79.95±0.51 35.51±0.53 48.26±0.54Fine-tuning (train) ResNet-12 56.67±0.62 74.80±0.51 64.45±0.7083.59±0.5164.66±0.7382.13±0.5037.52±0.53 55.39±0.57Transductive ﬁne-tuning (train) ResNet-1262.35±0.6674.53±0.5468.41±0.73 83.41±0.52 70.76±0.7481.56±0.5341.89±0.5954.96±0.55MetaOpt SVM (Lee et al., 2019) ResNet-12∗62.64±0.61 78.63±0.46 65.99±0.72 81.56±0.53 72.0±0.7 84.2±0.5 41.1±0.6 55.5±0.6Support-based initialization (train) WRN-28-10 56.17±0.64 73.31±0.53 67.45±0.70 82.88±0.53 70.26±0.70 83.82±0.49 36.82±0.51 49.72±0.55Fine-tuning (train) WRN-28-10 57.73±0.6278.17±0.4966.58±0.7085.55±0.4868.72±0.6786.11±0.4738.25±0.5257.19±0.57Transductive ﬁne-tuning (train) WRN-28-1065.73±0.68 78.40±0.52 73.34±0.71 85.50±0.50 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55 Support-based initialization (train + val) conv(64)×452.77±0.64 68.29±0.54 59.08±0.70 74.62±0.57 64.01±0.71 78.46±0.53 40.25±0.5654.53±0.57Fine-tuning (train + val) conv(64)×451.40±0.6168.58±0.5258.04±0.6874.48±0.5662.12±0.7177.98±0.5239.09±0.55 54.83±0.55Transductive ﬁne-tuning (train + val) conv(64)×452.30±0.61 68.78±0.53 58.81±0.69 74.71±0.56 63.89±0.71 78.48±0.52 40.33±0.56 55.60±0.56Support-based initialization (train + val) ResNet-12 56.79±0.65 72.94±0.55 67.60±0.71 83.09±0.53 69.39±0.71 83.27±0.50 43.11±0.58 58.16±0.57Fine-tuning (train + val) ResNet-12 58.64±0.6476.83±0.5065.55±0.7084.51±0.5068.11±0.7085.19±0.4842.84±0.5763.10±0.57Transductive ﬁne-tuning (train + val) ResNet-1264.50±0.68 76.92±0.55 69.48±0.73 84.37±0.51 74.35±0.7184.57±0.5348.29±0.63 63.38±0.58MetaOpt SVM (Lee et al., 2019) (train +val) ResNet-12∗64.09±0.62 80.00±0.45 65.81±0.74 81.75±0.53 72.8±0.7 85.0±0.5 47.2±0.6 62.5±0.6 Activation to Parameter (Qiao et al., 2018)(train + val)WRN-28-10 59.60±0.41 73.74±0.19LEO (Rusu et al., 2018) (train + val) WRN-28-10 61.76±0.08 77.59±0.12 66.33±0.05 81.44±0.09Support-based initialization (train + val) WRN-28-10 58.47±0.66 75.56±0.52 67.34±0.69†83.32±0.51†72.14±0.69 85.21±0.49 45.08±0.61 60.05±0.60Fine-tuning (train + val) WRN-28-10 59.62±0.6679.93±0.4766.23±0.6886.08±0.4770.07±0.6787.26±0.4543.80±0.58 64.40±0.58Transductive ﬁne-tuning (train + val) WRN-28-1068.11±0.69 80.36±0.50 72.87±0.71 86.15±0.50 78.36±0.70 87.54±0.49 50.44±0.68 65.74±0.60 0.74% and 86.67 ±0.46% for 1-shot 5-way and 5-shot 5-way respectively. These numbers are signiﬁcantly lower for the 1-shot 5-way protocol on both datasets compared to their counterparts in Table 1. However, the 5-shot 5-way accuracy is marginally higher in this experiment than that in Table 1. As noted in Remark 2, logits of the backbone are well-clustered and that is why they work better for few-shot scenarios. 17Published as a conference paper at ICLR 2020 C.7 F REEZING THE BACKBONE RESTRICTS PERFORMANCE The previous observation suggests that the network changes a lot in the ﬁne-tuning phase. Freezing the backbone severely restricts the changes in the network to only changes to the classiﬁer. As a consequence, the accuracy of freezing the backbone is 58.38 ±0.66 % and 75.46 ±0.52% on Mini-ImageNet and 67.06 ±0.69% and 83.20 ±0.51% on Tiered-ImageNet for 1-shot 5-way and 5-shot 5-way respectively. While the 1-shot 5-way accuracies are much lower than their counterparts in Table 1, the gap in the 5-shot 5-way scenario is smaller. C.8 U SING MIXUP DURING PRE -TRAINING Mixup improves the few-shot accuracy by about 1%; the accuracy for WRN-28-10 trained without mixup is 67.06 ±0.71% and 79.29 ±0.51% on Mini-ImageNet for 1-shot 5-way and 5-shot 5-way respectively. C.9 M ORE FEW -SHOT EPISODES Fig. 1 suggests that the standard deviation of the accuracies achieved by few-shot algorithms is high. Considering this randomness, evaluations were done over 10,000 few-shot episodes as well. The accuracies on Mini-ImageNet are 67.77 ±0.21 % and 80.24 ±0.16 % and on Tiered-ImageNet are 72.36 ±0.23 % and 85.70 ±0.16 % for 1-shot 5-way and 5-shot 5-way respectively. The numbers are consistent with the ones for 1,000 few-shot episodes in Table 1, though the conﬁdence intervals decreased as the number of episodes sampled increased. C.10 E VALUATION ON META-DATASET Table 4: Few-shot accuracies on Meta-Dataset. Best results in each row are shown in bold. 600 few-shot episodes were used to compare to the results reported in Triantaﬁllou et al. (2019). Results where the support- based initialization is better than or comparable to existing algorithms are denoted by †. Best in Triantaﬁllou et al. (2019) Support-based initialization Fine-tuning Transductive ﬁne-tuningILSVRC 50.50 ±1.08 60.05±1.03† 60.42±1.03 60.53±1.03Omniglot 63.37 ±1.33 76.32±0.96† 80.73±0.9582.07±0.93Aircraft 68.69 ±1.26 61.99±0.87 70.08±0.9772.40±0.97Birds 68.79 ±1.01 80.16±0.84† 81.46±0.84 82.05±0.85Textures 69.05 ±0.90 74.31±0.67† 79.92±0.81 80.47±0.79Quick Draw 51.52±1.00 58.75±0.92† 56.10±1.04 57.36±1.04Fungi 39.96 ±1.14 48.29±1.10† 48.43±1.03 47.72±1.02VGG Flowers 87.15±0.69 91.23±0.57† 91.78±0.55 92.01±0.55Trafﬁc Signs 66.79±1.31 53.44±1.14 60.79±1.27 64.37±1.27MSCOCO 43.74±1.12 43.84±1.00† 43.77±1.03 42.86±1.03 We ran experiments on Meta-Dataset (Triantaﬁllou et al., 2019), and compared the performance of support-based initialization, ﬁne-tuning and transductive ﬁne-tuning to the best results in Triantaﬁllou et al. (2019) for meta-training done on ImageNet-1k (ILSVRC) in Table 4. We observe that support- based initialization is better than or comparable to state-of-the-art on 8 out of 10 tasks. Additionally, transductive ﬁne-tuning is better, most times signiﬁcantly, than state-of-the-art on 8 out of 10 tasks; it trails the state-of-the-art closely on the remaining tasks. The few-shot episode sampling was done the same way as described in Triantaﬁllou et al. (2019); except for the few-shot class sampling for ImageNet-1k (ILSVRC) and Omniglot, which was done uniformly over all few-shot classes (Triantaﬁllou et al. (2019) use a hierarchical sampling technique to sample classes that are far from each other in the hierarchy, and hence easier to distinguish between). The hyper-parameters used for meta-training and few-shot ﬁne-tuning are kept the same as the ones in Section 4 and are not tuned for these experiments. Images of size 84 ×84 are used. Similar to the ImageNet-21k experiments, these experiments scale down the entropic term in (8) by log |Ct|and forego the ReLU in (6) and (7). 18Published as a conference paper at ICLR 2020 D F REQUENTLY ASKED QUESTIONS 1. Why has it not been noticed yet that this simple approach works so well? Non-transductive ﬁne-tuning as a baseline has been considered before (Vinyals et al., 2016; Chen et al., 2018). The fact that this is comparable to state-of-the-art has probably gone unnoticed because of the following reasons: • Given that there are only a few labeled support samples provided in the few-shot setting, initializing the classiﬁer becomes important. The support-based initialization (cf. Section 3.1) motivated from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Gidaris & Komodakis, 2018) classiﬁes support samples correctly (for a support shot of 1, this may not be true for higher support shots). This initialization, as opposed to initializing the weights of the classiﬁer randomly, was critical to performance in our experiments. • In our experience, existing meta-training methods, both gradient-based ones and metric- based ones, are difﬁcult to tune for larger architectures. We speculate that this is the reason a large part of the existing literature focuses on smaller backbone architectures. The few-shot learning literature has only recently started to move towards bigger backbone architectures (Oreshkin et al., 2018; Rusu et al., 2018). From Table 3 we see that non-tranductive ﬁne- tuning gets better with a deeper backbone architecture. A similar observation was made by (Chen et al., 2018). The observation that we can use “simple” well-understood training techniques from standard supervised learning that scale up to large backbone architectures for few-shot classiﬁcation is a key contribution of our paper. Transductive methods have recently started to become popular in the few-shot learning literature (Nichol et al., 2018; Liu et al., 2018a). Because of the scarcity of labeled support samples, it is crucial to make use of the unlabeled query samples in the few-shot regime. Our advocated baseline makes use of both a good initialization and transduction, relatively new in the few-shot learning literature, which makes this simplistic approach go unrecognized till now. 2. Transductive ﬁne-tuning works better than existing algorithms because of a big backbone architecture. One should compare on the same backbone architectures as the existing algo- rithms for a fair comparison. The current literature is in the spirit of increasingly sophisticated approaches for modest perfor- mance gains, often with different architectures (cf. Table 1). This is why we set out to establish a baseline. Our simple baseline is comparable or better than existing approaches. The backbone we have used is common in the recent few-shot learning literature (Rusu et al., 2018; Qiao et al., 2018) (cf. Table 1). Additionally, we have included results on smaller common backbone architec- tures, namely conv (64)×4 and ResNet-12 in Appendix C.4, and some additional experiments in Appendix C.2. These experiments suggest that transductive ﬁne-tuning is a sound baseline for a variety of different backbone architectures. This indicates that we should take results on existing benchmarks with a grain of salt. Also see the response to question 1 above. 3. There are missing entries in Tables 1 and 3. Is it still a fair comparison? Tables 1 and 3 show all relevant published results by the original authors. Re-implementing existing algorithms to ﬁll missing entries without access to original code is impractical and often yields results inferior to those published, which may be judged as unfair. The purpose of a benchmark is to enable others to test their method easily. This does not exist today due to myriad performance-critical design choices often not detailed in the papers. In fact, missing entries in the table indicate the inadequate state of the current literature. Our work enables benchmarking relative to a simple, systematic baseline. 4. Fine-tuning for few-shot learning is not novel. We do not claim novelty in this paper. Transductive ﬁne-tuning is our advocated baseline for few-shot classiﬁcation. It is a combination of different techniques that are not novel. Yet, it performs better than existing algorithms on all few-shot protocols with ﬁxed hyper-parameters. We emphasize that this indicates the need to re-interpret existing results on benchmarks and re-evaluate the status quo in the literature. 19Published as a conference paper at ICLR 2020 5. Transductive ﬁne-tuning has a very high latency at inference time, this is not practical. Our goal is to establish a systematic baseline for accuracy, which might help judge the accuracy of few-shot learning algorithms in the future. The question of test-time latency is indeed important but we have not focused on it in this paper. Appendix C.3 provides results using a smaller backbone where we see that the WRN-16-4 network is about 20-70x slower than metric-based approaches employing the same backbone while having signiﬁcantly better accuracy. The latencies with WRN-28-10 are larger (see the computational complexity section in Section 4.3) but with a bigger advantage in terms of accuracy. There are other transductive methods used for few-shot classiﬁcation (Nichol et al., 2018; Liu et al., 2018a), that are expected to be slow as well. 6. Transductive ﬁne-tuning does not make sense in the online setting when query samples are shown in a sequence. Transductive ﬁne-tuning can be performed even with a single test datum. Indeed, the network can specialize itself completely to classify this one datum. We explore a similar scenario in Section 4.3 and Fig. 2a, which discuss the performance of transductive ﬁne-tuning with a query shot of 1 (this means 5 query samples one from each class for 5-way evaluation). Note that the loss function in (8) leverages multiple query samples when available. It does not require that the query samples be balanced in terms of their ground-truth classes. In particular, the loss function in (8) is well-deﬁned even for a single test datum. For concerns about latency, see the question 5 above. 7. Having transductive approaches will incentivize hacking the query set. There are already published methods that use transductive methods (Nichol et al., 2018; Liu et al., 2018a), and it is a fundamental property of the transductive paradigm to be dependent on the query set, in addition to the support set. In order to prevent query set hacking, we will make the test episodes public which will enable consistent benchmarking, even for transductive methods. 8. Why is having the same hyper-parameters for different few-shot protocols so important? A practical few-shot learning algorithm should be able to handle any few-shot protocol. Having one model for each different scenario is unreasonable in the real-world, as the number of different scenarios is, in principle, inﬁnite. Current algorithms do not handle this well. A single model which can handle any few-shot scenario is thus desirable. 9. Is this over-ﬁtting to the test datum? No, label of the test datum is not used in the loss function. 10. Can you give some intuition about the hardness metric? How did you come up with the formula? The hardness metric is the clustering loss where the labeled support samples form the centers of the class-speciﬁc clusters. The special form, namely, E(x,y)∈Dq log 1−p(y|x) p(y|x) (cf. (9)) allows an interpretation of log-odds. We used this form because it is sensitive to the number of few-shot classes (cf. Fig. 3). Similar metrics, e.g., E(x,y)∈Dq [−log p(y|x)] can also be used but they come with a few caveats. Note that it is easier for p(y|x) to be large for small way because the normalization constant in softmax has fewer terms. For large way, p(y|x) could be smaller. This effect is better captured by our metric. 11. How does Fig. 3 look for algorithm X, Y, Z? We compared two algorithms in Fig. 3, namely transductive ﬁne-tuning and support-based initial- ization. Section 4.4 and the caption of Fig. 3 explains how the former algorithm is better. We will consider adding comparisons to other algorithms to this plot in the future. 20",
      "references": [],
      "meta_data": {
        "arxiv_id": "1909.02729v5",
        "authors": [
          "Guneet S. Dhillon",
          "Pratik Chaudhari",
          "Avinash Ravichandran",
          "Stefano Soatto"
        ],
        "published_date": "2019-09-06T06:14:03Z",
        "github_url": "https://github.com/gidariss/FewShotWithoutForgetting"
      },
      "llm_extracted_info": {
        "main_contributions": "A simple, scalable baseline for few-shot image classification: pre-train a backbone on a large meta-training set with standard cross-entropy, append a new classifier for few-shot classes and initialize it via weight imprinting, then apply transductive fine-tuning that uses unlabeled query samples with an entropy-regularization term. This baseline consistently outperforms many specialized meta-learning methods on standard benchmarks (Mini-ImageNet, Tiered-ImageNet, CIFAR-FS, FC-100) and scales to ImageNet-21k, highlighting that large pre-training can drive strong few-shot performance and that current benchmarks may understate difficulty. It also introduces a hardness metric for episodes to enable systematic comparison across protocols.",
        "methodology": "Pretrain a backbone (WRN-28-10) with standard augmentation, label smoothing, mixup, SGD. Append a new linear classifier on top of the backbone logits and initialize its weights by weight imprinting: for each class, set the classifier weight to the normalized prototype of the backbone logits for that class; apply a ReLU and L2 normalization between backbone and classifier. Fine-tune all parameters on each few-shot episode by minimizing the joint objective: cross-entropy on the labeled support set plus an entropy regularizer on the unlabeled query set (minimizing H(p(·|x))). Process multiple query samples jointly (transductive setting). For large Ct (ImageNet-21k) scale the entropy term by log|Ct| and remove the ReLU to stabilize high-way scenarios. Use Adam for fine-tuning; 25 epochs; fixed learning rate 5e-5; two parameter updates per epoch (CE on Ds, entropy on Dq). Pretraining and evaluation use fixed hyperparameters across datasets; 1,000 episodes per benchmark; 15 query shots by default.",
        "experimental_setup": "Datasets: Mini-ImageNet, Tiered-ImageNet, CIFAR-FS, FC-100; ImageNet-21k for large-scale evaluation. Meta-training splits: (train) or (train+val); evaluation on test classes. Backbone: WRN-28-10; ablations with conv(64)×4 and ResNet-12. Data augmentation: random flips, padding, brightness/contrast; mixup (α=0.25); label smoothing (ε=0.1); batch norm with no weight decay on BN params. Pretraining: SGD with momentum 0.9, batch size 256, two cycles of learning rate; 8-GPU mixed-precision training. Fine-tuning: one GPU, full precision, 25 epochs, Adam, no regularization. Evaluation protocol: 1-shot/5-way and 5-shot/5-way across datasets; 1,000 episodes for most datasets; 80 episodes for ImageNet-21k due to many ways; query shot set to 15 unless noted. They also report mean accuracy with 95% confidence intervals and analyze hardness of episodes with a proposed metric Omega_phi.",
        "limitations": "High inference-time latency due to online gradient-based fine-tuning (transductive updates), making it slow at test time compared to purely metric-based methods. Performance gains depend on backbone size, with large backbones yielding stronger results but higher compute. The entropy regularizer introduces hyperparameters and may require tuning, although the authors keep them fixed here. Results rely on transductive use of query data, which can be sensitive to the test set and could raise concerns about benchmark-specific artifacts. The approach is presented as a baseline, not a universal solution, and may not scale to real-time or online settings without optimization. Some experimental choices (e.g., removing ReLU for very large class counts) indicate instability across protocols.",
        "future_research_directions": "Develop faster transductive or semi-supervised alternatives to reduce inference latency; design benchmarks and reporting practices (e.g., hardness-based metrics and area under the curve across protocols) to compare few-shot methods more systematically; explore combinations with richer meta-learning objectives, transfer learning, and domain adaptation to leverage taxonomy/meronomy information; study the impact of pretraining with even larger or more diverse class sets and alternative feature-to-classifier input forms; investigate reduced-complexity initializations beyond weight imprinting and more robust entropy-based regularizers; extend evaluations to more datasets (Meta-Dataset, diverse domains) and modalities; propose online or streaming few-shot methods that adapt without full fine-tuning.",
        "experimental_code": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.autograd import Variable\nfrom pdb import set_trace as breakpoint\n\n\nclass LinearDiag(nn.Module):\n    def __init__(self, num_features, bias=False):\n        super(LinearDiag, self).__init__()\n        weight = torch.FloatTensor(num_features).fill_(1) # initialize to the identity transform\n        self.weight = nn.Parameter(weight, requires_grad=True)\n\n        if bias:\n            bias = torch.FloatTensor(num_features).fill_(0)\n            self.bias = nn.Parameter(bias, requires_grad=True)\n        else:\n            self.register_parameter('bias', None)\n\n    def forward(self, X):\n        assert(X.dim()==2 and X.size(1)==self.weight.size(0))\n        out = X * self.weight.expand_as(X)\n        if self.bias is not None:\n            out = out + self.bias.expand_as(out)\n        return out\n\n\nclass FeatExemplarAvgBlock(nn.Module):\n    def __init__(self, nFeat):\n        super(FeatExemplarAvgBlock, self).__init__()\n\n    def forward(self, features_train, labels_train):\n        labels_train_transposed = labels_train.transpose(1,2)\n        weight_novel = torch.bmm(labels_train_transposed, features_train)\n        weight_novel = weight_novel.div(\n            labels_train_transposed.sum(dim=2, keepdim=True).expand_as(weight_novel))\n        return weight_novel\n\n\nclass AttentionBasedBlock(nn.Module):\n    def __init__(self, nFeat, nK, scale_att=10.0):\n        super(AttentionBasedBlock, self).__init__()\n        self.nFeat = nFeat\n        self.queryLayer = nn.Linear(nFeat, nFeat)\n        self.queryLayer.weight.data.copy_(\n            torch.eye(nFeat, nFeat) + torch.randn(nFeat, nFeat)*0.001)\n        self.queryLayer.bias.data.zero_()\n\n        self.scale_att = nn.Parameter(\n            torch.FloatTensor(1).fill_(scale_att), requires_grad=True)\n        wkeys = torch.FloatTensor(nK, nFeat).normal_(0.0, np.sqrt(2.0/nFeat))\n        self.wkeys = nn.Parameter(wkeys, requires_grad=True)\n\n\n    def forward(self, features_train, labels_train, weight_base, Kbase):\n        batch_size, num_train_examples, num_features = features_train.size()\n        nKbase = weight_base.size(1) # [batch_size x nKbase x num_features]\n        labels_train_transposed = labels_train.transpose(1,2)\n        nKnovel = labels_train_transposed.size(1) # [batch_size x nKnovel x num_train_examples]\n\n        features_train = features_train.view(\n            batch_size*num_train_examples, num_features)\n        Qe = self.queryLayer(features_train)\n        Qe = Qe.view(batch_size, num_train_examples, self.nFeat)\n        Qe = F.normalize(Qe, p=2, dim=Qe.dim()-1, eps=1e-12)\n\n        wkeys = self.wkeys[Kbase.view(-1)] # the keys of the base categoreis\n        wkeys = F.normalize(wkeys, p=2, dim=wkeys.dim()-1, eps=1e-12)\n        # Transpose from [batch_size x nKbase x nFeat] to\n        # [batch_size x self.nFeat x nKbase]\n        wkeys = wkeys.view(batch_size, nKbase, self.nFeat).transpose(1,2)\n\n        # Compute the attention coeficients\n        # batch matrix multiplications: AttentionCoeficients = Qe * wkeys ==>\n        # [batch_size x num_train_examples x nKbase] =\n        #   [batch_size x num_train_examples x nFeat] * [batch_size x nFeat x nKbase]\n        AttentionCoeficients = self.scale_att * torch.bmm(Qe, wkeys)\n        AttentionCoeficients = F.softmax(\n            AttentionCoeficients.view(batch_size*num_train_examples, nKbase))\n        AttentionCoeficients = AttentionCoeficients.view(\n            batch_size, num_train_examples, nbase)\n\n        # batch matrix multiplications: weight_novel = AttentionCoeficients * weight_base ==>\n        # [batch_size x num_train_examples x num_features] =\n        #   [batch_size x num_train_examples x nKbase] * [batch_size x nKbase x num_features]\n        weight_novel = torch.bmm(AttentionCoeficients, weight_base)\n        # batch matrix multiplications: weight_novel = labels_train_transposed * weight_novel ==>\n        # [batch_size x nKnovel x num_features] =\n        #   [batch_size x nKnovel x num_train_examples] * [batch_size x num_train_examples x num_features]\n        weight_novel = torch.bmm(labels_train_transposed, weight_novel)\n        weight_novel = weight_novel.div(\n            labels_train_transposed.sum(dim=2, keepdim=True).expand_as(weight_novel))\n\n        return weight_novel\n\n\nclass Classifier(nn.Module):\n    def __init__(self, opt):\n        super(Classifier, self).__init__()\n        self.weight_generator_type = opt['weight_generator_type']\n        self.classifier_type = opt['classifier_type']\n        assert(self.classifier_type == 'cosine' or\n               self.classifier_type == 'dotproduct')\n\n        nKall = opt['nKall']\n        nFeat = opt['nFeat']\n        self.nFeat = nFeat\n        self.nKall = nKall\n\n        weight_base = torch.FloatTensor(nKall, nFeat).normal_(\n            0.0, np.sqrt(2.0/nFeat))\n        self.weight_base = nn.Parameter(weight_base, requires_grad=True)\n        self.bias = nn.Parameter(\n            torch.FloatTensor(1).fill_(0), requires_grad=True)\n        scale_cls = opt['scale_cls'] if ('scale_cls' in opt) else 10.0\n        self.scale_cls = nn.Parameter(\n            torch.FloatTensor(1).fill_(scale_cls),\n            requires_grad=True)\n\n        if self.weight_generator_type == 'none':\n            # If the weight generator type is `none` then feature averaging\n            # is being used. However, in this case the generator does not\n            # involve any learnable parameter and thus does not require\n            # training.\n            self.favgblock = FeatExemplarAvgBlock(nFeat)\n        elif self.weight_generator_type=='feature_averaging':\n            self.favgblock = FeatExemplarAvgBlock(nFeat)\n            self.wnLayerFavg = LinearDiag(nFeat)\n        elif self.weight_generator_type=='attention_based':\n            scale_att = opt['scale_att'] if ('scale_att' in opt) else 10.0\n            self.favgblock = FeatExemplarAvgBlock(nFeat)\n            self.attblock = AttentionBasedBlock(\n                nFeat, nKall, scale_att=scale_att)\n            self.wnLayerFavg = LinearDiag(nFeat)\n            self.wnLayerWatt = LinearDiag(nFeat)\n        else:\n            raise ValueError('Not supported/recognized type {0}'.format(\n                self.weight_generator_type))\n\n\n    def get_classification_weights(\n            self, Kbase_ids, features_train=None, labels_train=None):\n        \"\"\"Gets the classification weights of the base categories.\n\n        This routine returns the classification weight vectors for base and novel\n        categories depending on whether training data for novel categories is\n        provided.\n\n        Args:\n            Kbase_ids: A 2D tensor with shape [batch_size x nKbase]\n            features_train: A 3D tensor [batch_size x num_train_examples x num_channels]\n            labels_train: A 3D tensor [batch_size x num_train_examples x nKnovel]\n        Returns:\n            cls_weights: A 3D tensor [batch_size x nK x num_channels]\n        \"\"\"\n\n        #******** Get the classification weights for the base categories *******\n        batch_size, nKbase = Kbase_ids.size()\n        weight_base = self.weight_base[Kbase_ids.view(-1)]\n        weight_base = weight_base.view(batch_size, nKbase, -1)\n        #***********************************************************************\n\n        if features_train is None or labels_train is None:\n            # If training data for novel categories are not provided then return\n            # only the base category weights.\n            return weight_base\n\n        # generate weights for novel categories\n        _, num_train_examples, num_channels = features_train.size()\n        nKnovel = labels_train.size(2)\n        if self.classifier_type=='cosine':\n            features_train = F.normalize(\n                features_train, p=2, dim=features_train.dim()-1, eps=1e-12)\n        if self.weight_generator_type=='none':\n            weight_novel = self.favgblock(features_train, labels_train)\n            weight_novel = weight_novel.view(batch_size, nKnovel, num_channels)\n        elif self.weight_generator_type=='feature_averaging':\n            weight_novel_avg = self.favgblock(features_train, labels_train)\n            weight_novel = self.wnLayerFavg(\n                weight_novel_avg.view(batch_size * nKnovel, num_channels)\n            )\n            weight_novel = weight_novel.view(batch_size, nKnovel, num_channels)\n        elif self.weight_generator_type=='attention_based':\n            weight_novel_avg = self.favgblock(features_train, labels_train)\n            weight_novel_avg = self.wnLayerFavg(\n                weight_novel_avg.view(batch_size * nKnovel, num_channels)\n            )\n            if self.classifier_type=='cosine':\n                weight_base_tmp = F.normalize(\n                    weight_base, p=2, dim=weight_base.dim()-1, eps=1e-12)\n            else:\n                weight_base_tmp = weight_base\n\n            weight_novel_att = self.attblock(\n                features_train, labels_train, weight_base_tmp, Kbase_ids)\n            weight_novel_att = self.wnLayerWatt(\n                weight_novel_att.view(batch_size * nKnovel, num_channels)\n            )\n            weight_novel = weight_novel_avg + weight_novel_att\n            weight_novel = weight_novel.view(batch_size, nKnovel, num_channels)\n        else:\n            raise ValueError('Not supported / recognized type {0}'.format(\n                self.weight_generator_type))\n        # Concatenate base and novel weights\n        weight_both = torch.cat([weight_base, weight_novel], dim=1)\n        return weight_both\n\n\n    def apply_classification_weights(self, features, cls_weights):\n        \"\"\"Applies the classification weights to the feature vectors.\n\n        Args:\n            features: A 3D tensor [batch_size x num_test_examples x num_channels]\n            cls_weights: A 3D tensor [batch_size x nK x num_channels]\n        Returns:\n            cls_scores: A 3D tensor [batch_size x num_test_examples x nK]\n        \"\"\"\n        if self.classifier_type=='cosine':\n            features = F.normalize(\n                features, p=2, dim=features.dim()-1, eps=1e-12)\n            cls_weights = F.normalize(\n                cls_weights, p=2, dim=cls_weights.dim()-1, eps=1e-12)\n\n        cls_scores = self.scale_cls * torch.baddbmm(1.0,\n            self.bias.view(1, 1, 1), 1.0, features, cls_weights.transpose(1,2))\n        return cls_scores\n\n\n    def forward(self, features_test, Kbase_ids, features_train=None, labels_train=None):\n        \"\"\"Recognize on the test examples both base and novel categories.\n\n        Used during few-shot evaluation to classify test images using base and\n        novel category weights generated from the training exemplars.\n        \"\"\"\n        cls_weights = self.get_classification_weights(\n            Kbase_ids, features_train, labels_train)\n        cls_scores = self.apply_classification_weights(\n            features_test, cls_weights)\n        return cls_scores\n\n\ndef create_model(opt):\n    return Classifier(opt)\n",
        "experimental_info": "Experimental settings extracted from repository content (config files and architecture modules) related to the few-shot learning method implemented in this codebase:\n\n- Core method implemented: weight imprinting / generated-classifier for novel categories using a base-weight bank and exemplar-based training data, with optional attention-based enhancement. The main implementation is in architectures/ClassifierWithFewShotGenerationModule.py (and related components FeatExemplarAvgBlock, AttentionBasedBlock, LinearDiag, and Classifier).\n  - Key components:\n    - FeatExemplarAvgBlock: computes average feature weights per novel class from labeled exemplars.\n    - AttentionBasedBlock: computes weight vectors for novel classes via attention over base-class weights using a small query/key mechanism.\n    - Classifier: combines base-class weights with generated novel-class weights. Supports weight_generator_type in {none, feature_averaging, attention_based} and classifier_type in {cosine, dotproduct}.\n    - get_classification_weights: builds class weights for base and novel categories depending on whether novel-class training data (features_train, labels_train) are provided; returns a 3D tensor of weights per episode.\n    - apply_classification_weights: applies weights to test features to produce scores, with optional cosine normalization when using cosine classifier.\n\n- Data/model setup (as per repository config):\n  - Datasets used in experiments include MiniImageNet (with train/val/test phases) and ImageNet Low Shot variants; supported architectures include ConvNet, ResNet-based backbones (ResNet10, ResNetLike, ResNet-based variants) defined in architectures/ directory.\n  - Classifier backends are defined via config in config/miniImageNet_Conv128CosineClassifier*.py and config/imagenet_ResNet10CosineClassifier*.py, specifying:\n    - classifier_type: either cosine or dotproduct.\n    - weight_generator_type: 'none', 'feature_averaging', or 'attention_based'.\n    - nKall: total number of categories used per episode (base + novel as applicable).\n    - nFeat: dimensionality of feature vectors.\n    - scale_cls / scale_att: learnable scaling factors.\n  - The two-stage paradigm is supported: base training with a backbone (feat_model) and a classifier that can imprint/derive novel-class weights (classifier). Stage 2 corresponds to generating/augmenting novel-class weights via get_classification_weights using exemplars, consistent with weight imprinting-like behavior.\n\n- Experimental settings encoded in config files (examples from repository):\n  - MiniImageNet setup (e.g., miniImageNet_Conv128CosineClassifierGenWeightAttN1.py and related files):\n    - data_train_opt: nKnovel = 0, nKbase = 64, nExemplars = 1, nTestNovel = 15*5 = 75, nTestBase = 15*5 = 75, batch_size = 8, epoch_size = batch_size * 1000 (i.e., ~1000 episodes per epoch).\n    - data_test_opt: nKnovel = 5, nKbase = 64, nExemplars = 1, nTestNovel = 75, nTestBase = 75, batch_size = 1, epoch_size = 2000.\n    - max_num_epochs: various (60 or 100 depending on config); muliple experiments exist (e.g., 60 for some setups).\n    - feat_model backbones: ConvNet or ResNet-derived architectures, e.g., ConvNet configured with in_planes, out_planes, num_stages; optional dropout in some variants.\n    - classifier head: cosine classifier with weight generator type either 'none', 'feature_averaging', or 'attention_based' and scale_cls default 10.\n    - training uses SGD for feature extractor and classifier with stepwise learning rate schedules (LUT_lr) per config file.\n\n  - ImageNet-scale experiments (config/imagenet_*.py):\n    - data_train_opt: large nKnovel/nKbase settings, batch_size often 1, epoch_size large; typical max_num_epochs values around 100 or 6 depending on specific config (e.g., rapid experimentation variants).\n    - networks: feat_model options include ResNet10 or DumbFeat (depending on config); classifier uses cosine classifier with weight generator type 'attention_based' or 'feature_averaging' or 'none'.\n    - weight generation controls for novel classes are specified via net_optionsC: {'classifier_type': 'cosine', 'weight_generator_type': 'attention_based' or 'feature_averaging', 'nKall': 1000, 'nFeat': 512, 'scale_cls': 10, 'scale_att': 30.0}.\n\n- Data augmentation and preprocessing inferred from dataset loader definitions:\n  - MiniImageNet dataset uses training transforms including RandomCrop with padding, RandomHorizontalFlip, and normalization; test/val uses center cropping and normalization.\n  - ImageNet low-shot and ImageNet variants use standard transforms (scale, center crop, ToTensor, Normalize).\n\n- Transductive setup notes (per method description):\n  - The codebase models transductive usage by processing query samples jointly in the classifier's forward path: the features_test are processed in batch along with novel-base weight generation to produce per-episode scores. The explicit entropy-regularizer term H(p(.|x)) on unlabeled query samples, as described in the method, is not obviously implemented as a separate loss term in the provided code; the transductive setting is primarily realized by joint processing of query samples and weight calculation per episode in the forward path.\n\n- Evaluation and logging:\n  - The framework reports mean accuracy with 95% CIs and can evaluate across 1,000 episodes per benchmark, with 15 query shots by default for many configurations.\n\n- In short, experimental_code in this repository that directly implements the core method is contained primarily in architectures/ClassifierWithFewShotGenerationModule.py (weight imprinting / novel weight generation and application against features), and is wired through config files that specify backbones, cosine (or dot-product) classifiers, and weight generation strategies. The experimental_info summarizes the config-driven experimental setup (datasets, backbones, episode structure, and two-stage weight imprinting style) used to realize the method in practice across MiniImageNet and ImageNet low-shot experiments."
      }
    },
    {
      "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
      "full_text": "Frozen Feature Augmentation for Few-Shot Image Classification Andreas B¨ar1 2 * Neil Houlsby1 Mostafa Dehghani1 Manoj Kumar1 † 1Google DeepMind 2Technische Universit¨at Braunschweig andreas.baer@tu-braunschweig.de {neilhoulsby, dehghani, mechcoder}@google Abstract Vision foundation models are currently one of the main driving forces in computer vision research. Simply training a linear classifier or a lightweight model on top of model outputs or so-called ‘frozen features’ leads to impressive performance on a number of tasks. Currently, frozen fea- tures are not modified during training of such lightweight models. On the other hand, when networks are trained di- rectly on images, data augmentation is a standard recipe that improves performance with no additional overhead. In this paper, we conduct an extensive pilot study that ex- plores applying data augmentations in the frozen feature space for few-shot image classification. We dub this type of augmentation ‘frozen feature augmentation (FroFA)’. Our study demonstrates that adopting deceptively simple point- wise FroFAs, such as brightness, can improve few-shot per- formance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction A prevalent trend now is to pretrain vision models on large datasets and adapt them downstream [5, 41, 56]. Notable, even training a simple linear layer or a light-weight model on top of vision transformer (ViT) outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream tasks [13, 19, 43]. However, there is still an interest in training ViTs to achieve good performance on ImageNet-sized [36, 52] or smaller [31, 34] datasets. In this setting, a crucial ingre- dient is data augmentation — a predefined set of simple, stochastic input transformations. Simple but effective ex- amples for image augmentations include random cropping which extracts a fixed-sized region from an image of ar- bitrary resolution, or pixel-wise modifications that change brightness, saturation, or contrast. These are complemented by more advanced augmentation strategies such as mixup [58] or RandAugment [10]. *Work conducted as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0.0 2.5 5.0 top-1 acc. (abs. gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Few-shot results averaged across eight test sets, in- cluding ILSVRC-2012 [14, 44]. We use cached features from an L/16 model [16] pretrained on JFT-3B [56] (left) or WebLI [5] following a sigmoid language-image pretraining (SigLIP) [57] (right). Our method, i.e., a multi-head attention pooling [30] head trained with weight decay (MAPwd) and frozen feature augmenta- tion (FroFA), shows significant gains across all shots with respect to a weight-decayed MAP, i.e., MAPwd, or an L2-regularized lin- ear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques in a data-constrained, few-shot frozen feature setting. In particular, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmenta- tions on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eighteen augmentations applied to frozen features extracted from vision transformers pretrained on JFT-3B [56], ImageNet-21k [14, 44], or WebLI [5]. We train a small lightweight multi-head attention pooling (MAP) [30, 56] head using these augmented inputs and evaluate its performance across eight downstream image classification datasets, where we on average achieve signif- icant gains (see Fig. 1). Our major insights are as follows: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ImageNet. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady im- 1 arXiv:2403.10519v2  [cs.CV]  26 Jul 2024provements on 1-, 5-, and 10-shot settings. 2. Unlike traditional image augmentations that apply a sin- gle randomly sampled value across the entire image, we introduce per-channel stochasticity by sampling inde- pendent random values for each channel. For example, on the 5-shot setting, we improve accuracy over a well- tuned MAP and linear probe baseline by 0.5% absolute and 0.8% absolute, respectively. 3. While FroFA provides modest but significant improve- ments on ImageNet, it excels on smaller transfer datasets. Across seven downstream datasets, FroFA out- performs the mean accuracy of the MAP baseline in the 5 shot setting by 3.2% absolute and the linear probe base- line by 4.2% absolute. 2. Related Works Transfer learning on few-shot data : State-of-the-art vi- sion models [5, 13, 16, 56] are typically pretrained on large-scale datasets, e.g., ImageNet-21k [14, 44] or ver- sions of JFT [21, 56], before transferred to other middle- scale to small-scale ones,e.g., CIFAR10 [1], ILSVRC-2012 [14, 44], or SUN397 [53, 54]. Depending on the model size, efficient transfer learning becomes a challenge. Many meth- ods have been proposed for large language models (LLMs), e.g., adapters [22], low-rank adaptation (LoRA) [23], or prompt tuning [32], of which some have been successfully adapted to computer vision [4, 17, 24, 59]. CLIP-Adapter [17] builds on the power of contrastive language-image pre- training (CLIP) [43] and combines it with adapters [22]. A follow-up work [59] proposes TiP-Adapter which uses a query-key cache model [18, 42] instead of a gradient de- scent approach. Inspired by the success of prompt tuning in LLMs [32], Jia et al. propose visual prompt tuning at the model input [24]. On the other hand, AdaptFormer [4] uses additional intermediate trainable layers to finetune a frozen vision transformer [16]. In contrast, we do not introduce additional prompts [24] or intermediate parameters [4, 17] that require backprop- agating through the network. Instead, we train a small network on top of frozen features coming from a vision transformer. This aligns with linear probing [43] which is typically used to transfer vision models to other tasks [13, 19, 56] — our objective. In addition, we focus our experiments around transfer learning on few-shot data [29, 51]. Although not surprising, few-shot results obtained by Dehghani et al . [13] clearly show significant gaps between linear probing and full fine- tuning. We take these results as an incentive to improve upon linear probing. Data augmentation: One go-to method to improve per- formance while training in a low-data regime is data aug- mentation [46]. Some prominent candidates in computer vision are AutoAugment [9], AugMix [20], RandAugment [9], and TrivialAugment [39]. These methods typically combine low-level image augmentations together to aug- ment the input. Although some works propose augmen- tations in feature space [15, 28, 33, 37, 50], a large-scale empirical study on frozen features of single-modal vision models does not exist. To this end, we investigate frozen feature augmentation (FroFA) by reformulating eighteen image augmentations. In particular, we consider a subset used in AutoAugment [9], inception crop [48], mixup [50, 58], and the recently introduced patch dropout [35]. 3. Framework Overview In this section, we give an overview of our framework. 3.1. Notation Let x ∈ IH×W×3 be an RGB image of height H, width W, and I = [0, 1]. A classification model processes x and outputs class scores y ∈ [0, 1]S for each class in a pre- defined set of classes S, with S = |S|. Let L and D be the number of intermediate layers and the number of fea- tures of a multi-layer classification model, respectively. We describe the intermediate feature representations of x as f = f(ℓ) = (f(ℓ) d ) ∈ RD, with layer index ℓ ∈ {1, ..., L} and feature index d ∈ {1, ..., D}. In the vision trans- former [26] architecture, f = f(ℓ) = (f(ℓ) n,c) ∈ RN×C is a two-dimensional entity, where N and C are the number of patches and number of per-patch channels, respectively. In addition, we introduce the patch index n ∈ {1, ..., N} and the per-patch channel index c ∈ {1, ..., C}. 3.2. Training on Cached Features We investigate pretrained vision transformers [26] with L transformer blocks (TBs) followed by a multi-head atten- tion pooling (MAP) [30] and a classification layer (CL). Fig. 2a presents a simplified illustration. For simplicity, we neglect all operations before the first transformer block (e.g., patchifying, positional embedding, etc.). To cache intermediate feature representations, we pro- cess each image x from an image dataset Dx through the network up until transformer blockL. Next, we store the re- sulting features f. After processing Dx we obtain a (frozen) feature dataset Df , with f ∈ Df (Fig. 2b). Finally, we train a lightweight model using the cached (frozen) features. Fig. 2c shows an example where a single MAP layer followed by a classification layer is trained using the feature dataset Df . Since our focus is fast training, we defer a detailed analysis on larger models to future work. 3.3. Frozen Feature Augmentation (FroFA) Data augmentation is a common tool to improve general- ization and is typically applied on the input, or in our case: 2(Frozen) Pretrained Model TB TB TB MAP CL (a) Step 1: Select a (frozen) pretrained model and a layer for caching. (Frozen) Pretrained Model image dataset (frozen) feature dataset TB TB TB (b) Step 2: Process an image dataset and cache the (frozen) features. Lightweight Model (frozen) feature dataset MAP CL frozen feature augmentation (FroFA)  (c) Step 3: Train on (augmented) frozen features. Figure 2. Pipeline for caching and training on (frozen) fea- tures. (2a): Given a (frozen) pretrained vision transformer, withL Transformer blocks (TBs), a multi-head attention pooling (MAP) layer, and a classification layer (CL), we select its L-th Trans- former block for caching. (2b): Next, we feed images x ∈ Dx to cache (frozen) features f ∈ Df . (2c): Finally, we use Df to train a lightweight model on top. We investigate frozen feature augmentation (FroFA) af ∈ Af in this scenario. images. A natural question arises: How to map such image augmentations to intermediate feature representations? Recall that the feature representation f = (fn,c) ∈ RN×C (layer index ℓ omitted) is two-dimensional. We first reshape it to a three-dimensional representation, i.e., f∗ = (f∗ n1,n2,c) ∈ R √ N× √ N×C. (1) We further define f∗ c = f∗ :,:,c ∈ R √ N× √ N×1 (2) as a two-dimensional representation of the c-th channel. Images and feature representations differ in two funda- mental aspects: channel dimensionality and value range. Before adapting image augmentations to the feature space, it is crucial to handle these differences. Channel dimensionality: RGB images have just three channels while intermediate representations possess an ar- bitrary number of channels. To address this, we ignore im- age augmentations that rely on three color channels, e.g., color jitter, and consider augmentations which can have an arbitrary number of channels instead, denoted asCa, cover- ing a majority of commonly applied image augmentations. Value range: RGB values lie within a specific range I, e.g., I = [0, 1] or I = {0, ...,255} ⊂N0, while in theory features have no such constraints. Assuming H = √ N and W = √ N, we define an image augmentation as ax : I √ N× √ N×Ca → I √ N× √ N×Ca , ax ∈ Ax, (3) where Ax is the set of image augmentations andCa = C is an arbitrary number of channels. To also address the value range mismatch, we introduce a deterministic feature-to- image mapping tf→x : R √ N× √ N×Ct → I √ N× √ N×Ct (4) that maps each element of f∗ (1) from R to I. In our exper- iments, we use xf = tf→x(f∗) = f∗ − fmin fmax − fmin , (5) where fmin and fmax are the minimum and maximum value of f∗, respectively, with elements of xf now in I = [0, 1]. We further define an image-to-feature mapping tf←x : I √ N× √ N×Ct → R √ N× √ N×Ct (6) that maps xf back to the original feature value range, with Ct = C by default. In this case, we simply invert (4) and use f∗ = tf←x(xf ) =xf · (fmax − fmin) +fmin. (7) Combining (3), (4), and (6), we obtain a generic (frozen) feature augmentation (FroFA) as a function composition af = tf←x ◦ ax ◦ tf→x. (8) We use three variations of af : 1. (Default) FroFA: We applyaf (8) once across the entire feature representation. We set Ca = Ct = C and com- pute fmin and fmax in (5), (7) across all elements of f∗. Further, as normally done in pixel space,ax (3) samples a random augmentation value and changes all elements of xf using the same value. For example, employing random contrast in a FroFA fashion scales each element of xf by the exact same randomly sampled factor. 2. Channel FroFA (cFroFA) : For each channel in the mapped features xf (5), ax (3) samples a random aug- mentation value per channel and applies that value to all elements in that channel. By using cFroFA for our ran- dom contrast example, we obtain C independently sam- pled scaling factors, one for each channel. 3. Channel2 FroFA (c2FroFA): In addition to applying augmentations per channel as done in cFroFA,tf→x (4) and tx←f (6) also operate per channel. In this case,fmin and fmax are the per-channel maximum and minimum, respectively. In contrast, FroFA and cFroFA use the maximum and minimum across the entire feature. We 3denote this variant as c 2FroFA since both the mappings (4), (6) and the augmentation (3) are applied on a per- channel basis. Although not adding additional stochas- ticity, we found that for random brightness this variant gives more stable results across a range of augmentation hyper parameters. While an element-wise FroFA might seem like a natural next step, our initial experiments lead to significantly worse results. We hypothesize that per-element augmentations might lead to substantial changes in the feature appearance. 4. Experimental Setup In this section, we introduce our experimental setup. 4.1. Network Architectures We employ the following pretrained vision transformers from prior work: Ti/16 [49], B/16 [16], and L/16 [16]. Fur- ther, we follow [56] and employ a lightweight multi-head attention pooling (MAP) layer [30] before the final classifi- cation layer on top of the frozen features (cf . Sec. 3.3). 4.2. Datasets Pretraining: We consider three datasets: JFT-3B, ImageNet-21k, and WebLI. First introduced by Hinton et al. [21], JFT is now a widely used proprietary, large-scale dataset [5, 7, 11, 16, 26, 27, 47, 56]. For our investigations we use the JFT-3B version following Zhai et al . [56]. It consists of nearly 3 billion multi-labeled images following a class-hierarchy of 29,593 labels. We further use ImageNet- 21k [14, 44] which consists of 14,197,122 (multi)-labeled images and 21,841 distinct labels. We equally split the first 51,200 images into a validation and test set and use the remaining 14,145,922 images for training. As a third dataset, we use WebLI [5] which is a recently introduced web-scale multilingual image-text dataset. Please refer to the Appendix, Sec. A3.1, for more details. Few-shot transfer : After pretraining we use eight datasets for few-shot transfer: ILSVRC-2012 [14, 44], CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. ILSVRC-2012, also known as ImageNet-1k, is a slimmed version of ImageNet-21k and contains 1,281,167 training images of 1,000 classes. We use it as our main few-shot benchmark throughout the paper. We randomly sample 1-shot, 5-shot, 10-shot, and 25-shot versions from the first 10% of the training set. We further create addi- tional disjoint sets by using the next four 10% fractions of the training set. In addition, we follow previous works [3] and create a ‘minival’ set using the last 1% (12,811 images) of the ILSVRC-2012 training set. The ‘minival’ set is used for hyper parameter tuning and design decisions while the official ILSVRC-2012 validation set is used as a test set. In summary, our setup consists of 1,000, 5,000, 10,000, or 25,000 training images, 12,811 validation images (‘mini- val’), and 50,000 test images (‘validation’). For the other seven datasets, we also select a training, validation, and test split and create few-shot versions. More details on how these splits are created can be found in the Appendix, Sec. A3.1. We follow a similar procedure as with ILSVRC-2012 and use 10% of the training images to cre- ate 1-shot, 5-shot, 10-shot, and 25-shot versions of each dataset. We further use each validation set for hyper pa- rameter tuning and report final results on the respective test set. 4.3. Data Augmentation We reuse the set of augmentations first defined in AutoAug- ment [9] and adopted in later works, such as RandAugment [10] and TrivialAugment [39]. In addition, we also consider a few other image augmentations [35, 48, 58]. We select five geometric augmentations, i.e., rotate, shear-x, shear-y, translate-x, and translate-y; four crop & drop augmenta- tions, i.e., crop, resized crop, inception crop [48], and patch dropout [35]; seven stylistic augmentations, i.e., brightness, contrast, equalize, invert, posterize, sharpness, and solarize; and two other augmentations, i.e., JPEG and mixup [58]. In total, we end up with eighteen distinct augmentations . Note that all data augmentations incorporate random oper- ations, e.g., a random shift in x- and y-direction (translate- x and translate-y, respectively), a randomly selected set of patches (patch dropout), a random additive value to each feature (brightness), or a random mix of two features and their respective classes (mixup). Please refer to the Ap- pendix, Sec. A3.2, for more details. We focus on the following set of experiments: 1. We investigate FroFA for all eighteen augmentations. 2. For our top-performing FroFAs, namely, brightness, contrast, and posterize, we incorporate additional stochasticity using cFroFA and c 2FroFA variants ( cf . Sec. 3.3). 3. We investigate a sequential protocol where two of the best three (c/c 2)FroFAs are arranged sequentially, namely, brightness c 2FroFA, contrast FroFA, and pos- terize cFroFA. We test all six possible combinations. 4. Finally, we also apply variations of RandAugment [10] and TrivialAugment [39] directly on top of cached frozen features. More details and results can be found in the Appendix, Secs. A3.2 and A4, respectively. 4.4. Training & Evaluation Details We describe some base settings for pretraining, few- shot learning, and evaluation. Please refer to Appendix, Sec. A3.3 for more training details. Pretraining: We use the Big Vision code base for https://github.com/google-research/big_vision 4pretraining. We take the Ti/16, B/16, and L/16 models pre- trained on JFT-3B from Zhai et al. [56]. In addition, we pretrain Ti/16, B/16 and L/16 on ImageNet-21k following the settings of Steiner et al. [46]. To further explore trans- fer capabilities we also use an L/16 model with sigmoid language-image pretraining (SigLIP) [57] on WebLI [5]. Few-shot learning: We use the Scenic code base [12] for few-shot learning. We train the lightweight MAP-based head by sweeping across five batch sizes (32, 64, 128, 256, and 512), four learning rates (0.01, 0.03, 0.06, and 0.1), and five training step sizes (1,000; 2000; 4,000; 8,000; and 16,000), yielding 100 configurations for each shot. We use the respective validation set for early stopping and to find the best sweep setting. Our cached-feature setup fits on a single-host TPUv2 platform where our experiments run in the order of minutes. Evaluation: We report the top-1 accuracy across all our few-shot datasets. On ILSVRC-2012, we tune few-shot models exclusively on our validation set (our ILSVRC-2012 ‘minival’, cf . Sec. 4.2) and report results on our test set (of- ficial ILSVRC-2012 ‘validation’ set, cf . Sec. 4.2). 4.5. Baseline Models We establish two baselines: MAP and linear probe. MAP: We first cache theN×C-shaped (frozen) features from the last transformer block. Afterwards, we train a lightweight MAP head from scratch using the cached fea- tures followed by the final classification layer ( cf . Fig. 2). For simplicity, the MAP head follows the same architectural design as the underlying pretrained model. In some exper- iments, we additionally apply weight decay (wd), denoted as MAPwd. We sweep across [ADD V ALUES] and use the respective validation set for early stopping and to find the best sweep setting. Linear probe: We use cached1×C-shaped outputs from the pretrained MAP head to solve an L2-regularized regres- sion problem with a closed-form solution [56]. We sweep the L2 decay factor using exponents of 2 ranging from -20 up to 10. This setting is our auxiliary baseline. 5. Finding the Optimal FroFA Setup We focus our first investigations on an L/16 model pre- trained on JFT-3B, i.e., our largest model and largest im- age classification pretraining dataset, followed by few-shot learning on subsets of ILSVRC-2012 training set, i.e., our largest few-shot dataset. We will refer to this setup as our L/16 JFT-3B base setup. 5.1. Baseline Performance We first report the baseline performance in Tab. 1. We ob- serve a large gap between MAP and linear probe in the 1- https://github.com/google-research/scenic Method 1-shot 5-shot 10-shot 25-shot MAP 57.9 78.8 80.9 83.2 Linear probe 66.5 79.6 81.5 82.4 Table 1. Average top-1 accuracy for baseline settings on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5) and follow the respective baseline setting ( cf . Sec. 4.5). The best setting for each baseline is found using our ILSVRC- 2012 validation set. Further, each shot is sampled five times. The best result per shot is boldfaced. shot setting (-8.6% absolute) which significantly decreases in the 5-, 10-, and 25-shot settings to -0.8%, -0.6%, and +0.8% absolute, respectively. In the following, our main point of comparison is the MAP baseline. This might be counter-intuitive since the performance is worse than linear probe in most cases. How- ever, the higher input dimensionality in the MAP-based set- ting (cf . Sec. 4.5) gives us the option to reshape the input to three dimensions ( cf . Sec. 3.3) which opens up more room and variety for frozen feature augmentations (Fro- FAs). Later in Sec. 6.4, we compare the performance of our best augmentations to the linear probe baseline. 5.2. Default FroFA As a next step, we investigate the effect of adding a single FroFA to the MAP baseline setting. We first focus on the default FroFA formulation which uses a single randomly sampled value per input ( cf . Sec. 3.3). Results are shown in Tab. 2 where we report gains with respect to the MAP baseline using eighteen distinct FroFAs categorized into ge- ometric, crop & drop, stylistic, and other. Geometric: Interestingly, all geometric augmentations consistently lead to worse performance across all settings. Crop & drop: A simple crop or a resized crop yield a significant performance boost in the 1-shot setting of +3.0% and +1.9% absolute, respectively. Further, patch dropout provides modest gains in the 1-shot regime. Dropping patches is related to training efficiency, so we investigate this further. Fig. 3a shows the top-1 accuracy on 1- and 25- shot as a function of number of patches. More results can be found in Appendix, Sec. A4.1. Similar to observations by Liu et al. [35] we can randomly drop a large fraction of patches (>50%) without loosing performance. A key dif- ference is that Liu et al. only investigated the effect in the image space, while we provide evidence that patch dropout also transfers to the feature space. Finally, inception crop does not improve performance. Stylistic: The largest gains can be observed when em- ploying a stylistic FroFA, in particular brightness, contrast, and posterize. We identified brightness as the best perform- ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on 5-shot, and up to 0.6% on 10-shot. 5Geometric Crop & drop Stylistic Other Shots MAP rotate shear-x shear-y translate-x translate-y crop res. crop incept. crop patch drop. brightness contrast equalize invert posterize sharpness* solarize* JPEG* mixup 1 57.9 −1.3 −0.6 −0.8 −1.2 −1.4 +3.0 +1.9 +0.0 +0.4 +4.8 +2.8 +1.0 +2.7 +3.7 −0.1 +1.0 −0.1 −1.4 5 78.8 −0.3 −0.2 −0.2 −0.3 −0.3 +0.0 −0.2 +0.0 +0.0 +1.1 +0.8 +0.5 −0.3 +0.8 +0.1 −0.1 −0.3 −0.3 10 80.9 −0.2 −0.1 −0.1 −0.2 −0.2 +0.0 −0.2 +0.0 +0.0 +0.6 +0.6 +0.4 +0.0 +0.6 +0.1 +0.0 −0.1 +0.2 25 83.2 −0.2 −0.1 −0.2 −0.1 −0.2 +0.0 −0.1 −0.1 +0.0 +0.1 +0.1 +0.0 −0.2 +0.0 +0.0 +0.0 +0.0 +0.1 Table 2. (Average) top-1 accuracy for default FroFA on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). In total, we investigate eighteen FroFAs, categorized intogeometric, crop & drop, stylistic, and other. We sweep across a base sweep ( cf . Sec. 4.4) and the respective augmentation sweep (cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 validation set. Each shot is sampled five times, except for JPEG, sharpness, and solarize (marked with ‘*’). We highlight deterioration by shades of red and improvement by shades of green . Best three FroFAs are boldfaced. 1 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA (a) Patch dropout FroFA 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot + brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA Figure 3. Average top-1 accuracy for FroFA variantson our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each FroFA operation point (cf . Appendix, Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. Brightness Contrast Posterize Shots MAP c c 2 c c 1 57.9 +4.8 +5.9 +6.1 +2.8 +2.5 +3.7 +5.9 5 78.8 +1.1 +1.5 +1.6 +0.8 +0.0 +0.8 +0.8 10 80.9 +0.6 +1.1 +0.9 +0.6 +0.0 +0.6 +0.5 25 83.2 +0.1 +0.4 +0.3 +0.1 −0.1 +0.0 +0.0 Table 3. Average top-1 accuracy for a selection of default ( ) and channel (c/c 2) FroFA on our ILSVRC-2012 test set. Ab- solute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep (cf . Sec. 4.4) and the respective augmentation sweep ( cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 val- idation set. Each shot is sampled five times. The best results per shot and FroFA are boldfaced (multiple ones if close, i.e., ±0.2). Other: Neither JPEG nor mixup yield performance gains but rather more or less worsen the performance. 5.3. Channel FroFA Next, we investigate channel FroFA (cFroFA) for bright- ness, contrast, and posterize. Results are shown in Tab. 3, where we report absolute gains with respect to the MAP baseline. First, contrast cFroFA worsens performance across all shots. Second, posterize cFroFA improves perfor- mance on 1-shot from +3.7% to +5.9% while maintaining performance on all other shots. Lastly, brightness cFroFA significantly improves performance across all shots, i.e., from +4.8% to +5.9% on 1-shot, from +1.1% to +1.5% on 5-shot, from +0.6% to +1.1% on 10-shot, and from +0.1% to +0.4% on 25-shot. Giving the strong improvements for brightness cFroFA, we further test brightness c 2FroFA (see Tab. 3). On a first look, both variants perform equally well. In Fig. 3b, we further report the top-1 accuracy on 1-shot and 25-shot as a function of the brightness augmentation level. Results across other shots are similar and can be found in Appendix, Sec. A4.1. We clearly observe that brightness cFroFA is much more sensitive to the brightness level than brightness c2FroFA. Aross all shots, brightness cFroFA only works well for small brightness levels (0.1 to 0.5), while the c2FroFA variant performs better than the MAP baseline across the board. We attribute the better sensitivity prop- 6erties of brightness c2FroFA to the channel-wise mappings (5), (7) since this is the only change between cFroFA and c2FroFA. We did not a observe similar effect when switch- ing from cFroFA posterize to c2FroFA posterize. 5.4. Sequential FroFA Finally, out of our best three augmentations, i.e., bright- ness c 2FroFA (B-c 2), contrast FroFA (C), and posterize cFroFA (P-c), we combine two of them sequentially. We end up with a total of six combinations. Tab. 4 compares the performance of these six combinations against our prior best (B-c 2). On 1-shot, (B-c 2→P-c) significantly outper- forms (B-c2), improving absolute gains from 6.1% to 7.7%, while maintaining performance on other shots. We con- clude that advanced FroFA protocols may further improve performance. As an initial investigation, we applied varia- tions of RandAugment and TrivialAugment using our best three FroFAs ( cf . Tab. 3), however, with limited success. We include results in the Appendix, Sec. A4.2, and leave a deeper investigation to future works. 6. FroFA on More Datasets and Architectures How well does our best non-sequential augmentation strat- egy (brightness c 2FroFA) transfer across multiple dataset and architectures settings? In Secs. 6.1 to 6.3, we report results on seven other downstream few-shot datasets, two additional architectures, and two additional pretraining se- tups, respectively. This time, however, we also incorpo- rate weight decay in all MAP-based models . Further, in Secs. 6.2 and 6.3, we solely focus on the improvements over the MAP baseline and include a discussion on the improve- ments over the linear probe baseline in Secs. 6.1 and 6.4. 6.1. Transfer to Other Downstream Datasets In Tab. 5, we report results on seven additional transfer datasets, i.e., CIFAR10, CIFAR100, DMLab, DTD, Re- sisc45, SUN397, and SVHN. We compare the weight- decayed MAP and L2-regularized linear probe baseline to our approach, i.e., weight-decayed MAP combined with brightness c2FroFA (MAPwd + FroFA). We observe that across almost all shots and transfer datasets, MAP wd + FroFA shows the best results. Moreover, MAP wd + FroFA outperforms L2-regularized linear probe with only one exception, i.e., SUN397 (1-shot). With respect to the mean across all seven datasets, MAP wd + FroFA is signifi- cantly better than MAPwd, with improvements ranging from +4.4% absolute on 1-shot to +1.0% absolute on 25-shot. Fig. 1, left, displays the absolute accuracy gains averaged across all eight transfer datasets, including ILSVRC-2012. As before, our approach, i.e., MAPwd + FroFA, yields the best results across all shots. We further observe that the gains decrease with higher shots which aligns with our pre- vious observations. Shots MAP B-c 2 B-c2→C C→ B-c2 B-c2→P-c P-c→ B-c2 C→P-c P-c→C 1 57.9 +6.1 +4.0 +2.7 +7.7 +5.2 +5.0 +3.1 5 78.8 +1.6 +1.5 +0.2 +1.5 +0.4 +1.3 +0.0 10 80.9 +0.9 +1.2 +0.1 +1.0 +0.1 +0.9 +0.3 25 83.2 +0.3 +0.4 −0.7 +0.2 −0.5 +0.2 −0.4 Table 4. Average top-1 accuracy for a sequential FroFA pro- tocol on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We combine the best settings of brightness c 2FroFA (B- c2), contrast FroFA (C), and posterize cFroFA (P-c) sequentially (two at a time, order indicated by ‘ ↑’). We sweep across a base sweep (cf . Sec. 4.4) to first find the best setting on our ILSVRC- 2012 validation set. Each shot is sampled five times. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Trans. dataset Method 1-shot 5-shot 10-shot 25-shot CIFAR10 MAPwd 85.1 96.7 97.1 97.5 Linear probe 80.9 94.1 96.7 97.3 MAPwd + FroFA 93.8 97.6 97.8 97.8 CIFAR100 MAPwd 63.1 82.7 85.5 86.8 Linear probe 58.4 80.9 83.8 85.1 MAPwd + FroFA 67.8 84.0 86.2 87.1 DMLab MAPwd 24.4 30.3 30.2 36.5 Linear probe 24.0 26.3 25.6 30.9 MAPwd + FroFA 27.1 29.4 30.3 36.8 DTD MAPwd 49.2 68.2 74.1 80.8 Linear probe 46.9 65.9 71.3 77.3 MAPwd + FroFA 53.5 70.7 76.1 82.2 Resisc45 MAPwd 63.2 86.9 89.8 90.7 Linear probe 67.1 85.6 88.2 91.0 MAPwd + FroFA 67.6 87.2 89.7 91.5 SUN397 MAPwd 51.3 73.5 77.7 80.3 Linear probe 56.7 70.9 75.6 78.6 MAPwd + FroFA 56.2 75.9 78.9 81.2 SVHN MAPwd 20.7 23.9 30.2 47.4 Linear probe 11.8 15.0 18.7 21.5 MAPwd + FroFA 21.8 31.0 43.5 50.3 Mean MAPwd 51.0 66.0 69.2 74.3 Linear probe 49.1 62.7 65.7 68.8 MAPwd + FroFA 55.4 68.0 71.8 75.3 Table 5. Top-1 accuracy of our best FroFA for additional transfer datasets using a JFT-3B L/16 model. Results are re- ported on the respective test set ( cf . Sec. A3.1). We compare results to a weight-decayed MAP baseline, i.e., MAP wd, and an L2-regularized linear probe. Depending on the setting, we sweep across a base,cf . Sec. 4.4, a weight decay or L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on the respective validation set. Per shot and dataset, the best result is boldfaced while the second-best result is underlined (multiple ones if close, i.e., ±0.2). 7Ti/16 B/16 L/16 model −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model −0.5 0.0 0.5 5-shot Ti/16 B/16 L/16 model 0.00 0.25 0.50 0.75 1.00 1.25 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 5 25-shot MAPwd linear probe (a) JFT-3B Ti/16 B/16 L/16 model −20 −15 −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 5-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 25-shot (b) ImageNet21k Figure 4. Average top-1 accuracy of brightness c2FroFA for JFT-3B (a) and ImageNet-21k (b) models on our ILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline are reported. Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. 6.2. Transfer to Other Architectures We employ brightness c2FroFA on two other JFT-pretrained models, namely Ti/16 and B/16. In Fig. 4a, we report im- provements in top-1 accuracy with respect to the weight- decayed MAP baseline. Across all shots and model archi- tectures, incorporating FroFA either maintains or improves performance, except for B/16, 25-shot. Given that larger models tend to be more prone to overfitting in the 1-shot setting, we observe increasing improvements from FroFA when scaling the architecture. With a higher number of shots, the observed improvements over the baseline model become smaller. We attribute this to the strong baseline per- formance leaving lesser headroom for improvements. We refer to the Appendix, Sec. A4.3, for the exact values. 6.3. Transfer to Other Pretraining Setups ImageNet-21k: In Fig. 4b, we report improvements in top- 1 accuracy with respect to the weight-decayed MAP base- line for ImageNet-21k-pretrained Ti/16, B/16, and L/16. Consistent with our JFT-3B observations, across all shots and model architectures, incorporating FroFA either main- tains or improves performance. The improvements dimin- ish as the number of shots increases. This trend is likely due to the higher baseline accuracies at higher shot counts. We again refer to the Appendix, Sec. A4.3, for the exact values. WebLI and SigLIP : We also tested an L/16 model with sigmoid language-image pretraining (SigLIP), follow- ing [57]. We report the absolute accuracy gains averaged across eight datasets. The results are shown in Fig. 1, right. From the results we can conclude that our FroFA setting also transfers to language-image pretrained models further emphasizing its generalizability. 6.4. Linear Probe Comparison on ILSVRC-2012 We will now look at Figs. 4a and 4b, but discuss gains with respect to the L2-regularized linear probe baseline. We start with models pretrained on JFT-3B (cf . Fig. 4a). On 1-shot, we observe that we lack behind linear probe but can close the gap by scaling up the model size. On 5- to 25-shot, with the exception of Ti/16 on 5-shot, brightness c 2FroFA significantly outperforms the linear probe baseline. On ImageNet-21k (cf . Fig. 4b), we observe even larger gaps to linear probe on 1-shot (up to -20% absolute). How- ever, similar to results on JFT-3B, performance on 5- to 25-shot improves significantly over linear probe or at worst stays the same. 7. Conclusions We investigated eighteen frozen feature augmentations (FroFAs) along three axes: model size, pretraining and transfer few-shot dataset. We show that a training with Fro- FAs, in particular stylistic ones, gives large improvements upon a representative baseline across all shots. In addition, per-channel variants further improve performance, e.g., by 1.6% absolute in the ILSVRC-2012 5-shot setting. Finally, we were able to show that our results transfer. Averaged results across seven downstream tasks show that using a variant of brightness FroFA improves by 4.4% absolute upon the same representative baseline in the 1-shot setting. 8References [1] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009. 2, 4, 12 [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit- twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass- abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv, 1612.03801:1–11, 2016. 4, 12 [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet- ter Plain ViT Baselines for ImageNet-1k.arXiv, 2205.01580: 1–3, 2022. 4 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recogni- tion. In Proc. of NeurIPS, pages 16664–16678, New Orleans, LA, USA, 2022. 2 [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad- bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A Jointly-Scaled Multilingual Language- Image Model. InProc. of ICLR, pages 1–33, Kigali, Rwanda, 2023. 1, 2, 4, 5, 12 [6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens- ing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE, 105(10):1865–1883, 2017. 4, 12 [7] Franc ¸ois Chollet. Xception: Deep Learning With Depthwise Separable Convolutions. In Proc. of CVPR , pages 1063– 6919, Honolulu, HI, USA, 2017. 4 [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. of CVPR, pages 3606–3613, Columbus, OH, USA, 2014. 4, 12 [9] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va- sudevan, and Quoc V . Le. AutoAugment: Learning Aug- mentation Strategies From Data. In Proc. of CVPR , pages 113–123, Long Beach, CA, USA, 2019. 2, 4 [10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandAugment: Practical Automated Data Augmenta- tion with a Reduced Search Space. In Proc. of NeurIPS , pages 18613–18624, virtual, 2020. 1, 4, 13 [11] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan. CoAtNet: Marrying Convolution and Attention for All Data Sizes. In Proc. of NeurIPS, pages 3965–3977, virtual, 2021. 4 [12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A JAX Library for Computer Vision Research and Beyond. In Proc. of CVPR, pages 21393–21398, New Orleans, LA, USA, 2022. 5 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan- nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku- mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar- avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh- nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Proc. of ICML , pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 12 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, 2009. 1, 2, 4, 12 [15] Terrance DeVries and Graham W. Taylor. Dataset Augmen- tation in Feature Space. In Proc. of ICLR - Workshops, pages 1–12, Toulon, France, 2017. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proc. of ICLR, pages 1–21, virtual, 2021. 1, 2, 4 [17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP- Adapter: Better Vision-Language Models with Feature Adapters. Int. J. Comput. Vis., pages 1–15, 2023. 2 [18] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im- proving Neural Language Models with a Continuous Cache. In Proc. of ICLR, pages 1–9, Toulon, France, 2017. 2 [19] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-Efficient Model Adaptation for Vision Transformers. In Proc. of AAAI, pages 817–825, Washington, DC, USA, 2023. 1, 2 [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. In Proc. of ICLR, pages 1–15, Virtual, 2020. 2 [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling Knowledge in a Neural Network. In proc. of NIPS - Work- shops, pages 1–9, Montr´eal, QC, Canada, 2014. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2, 4 [22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proc. of ICML , pages 2790–2799, Long Beach, CA, USA, 2019. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR, pages 1–13, virtual, 2022. 2 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- 9sual Prompt Tuning. In Proc. of ECCV, pages 709–727, Tel Aviv, Israel, 2022. 2 [25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, pages 1–15, San Diego, CA, USA, 2015. 13 [26] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General Visual Representation Learning. In Proc. of ECCV, pages 491–507, virtual, 2020. 2, 4 [27] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Mod- els. arXiv, 2112.13492:1–32, 2023. 4 [28] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl- liam Campbell. A Closer Look At Feature Space Data Aug- mentation For Few-Shot Intent Classification. In Proc. of EMNLP - Workshops, pages 1–10, Hong Kong, China, 2019. 2 [29] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The Omniglot Challenge: a 3-year Progress Re- port. Curr. Opin. Behav. Sci., 29:97–104, 2019. 2 [30] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In Proc. of ICML , pages 3744–3753, Long Beach, CA, USA, 2019. 1, 2, 4 [31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision Transformer for Small-size Datasets. arXiv, 2112.13492:1–11, 2021. 1 [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proc. of EMNLP, pages 3045–3059, virtual, 2021. 2 [33] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In Proc. of ICPR , pages 728–733, Beijing, China, 2018. 2 [34] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient Training of Visual Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, 2021. 1 [35] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az- izpour, and Kevin Smith. PatchDropout: Economizing Vi- sion Transformers Using Patch Dropout. In Proc. of WACV, pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 5 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV, pages 10012–10022, virtual, 2021. 1 [37] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson. Learning Multimodal Data Augmentation in Feature Space. In Proc. of ICLR, pages 1–15, Kigali, Rwanda, 2023. 2 [38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proc. of ICLR, pages 1–18, New Orleans, LA, USA, 2019. 13 [39] Samuel G. M ¨uller and Frank Hutter. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proc. of ICCV, pages 774–782, virtual, 2021. 2, 4, 13 [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat- ural Images with Unsupervised Feature Learning. In proc. of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 4, 12 [41] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah- moud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je- gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi- otr Bojanowski. Dinov2: Learning Robust Visual Features Without Supervision. arXiv, 2304.07193:1–31, 2023. 1 [42] Emin Orhan. A Simple Cache Model for Image Recognition. In Proc. of NeurIPS, pages 10128–10137, Montr´eal, Canada, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, pages 8748–8763, virtual, 2021. 1, 2 [44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. Int. J. Comput. Vis., 115(3):211–252, 2015. 1, 2, 4, 12 [45] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proc. of ICML, pages 4596–4604, Stockholm, Sweden, 2018. 13 [46] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. Trans. Mach. Learn. Res., pages 1–16, 2022. 2, 5, 13 [47] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proc. of ICCV , pages 843–852, Venice, Italy, 2017. 4 [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Ar- chitecture for Computer Vision. In Proc. of CVPR , pages 2818–2826, Las Vegas, NV , USA, 2016. 2, 4 [49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training Data-Efficient Image Transformers & Distillation Through Attention. In Proc. of ICML , pages 10347–10357, virtual, 2021. 4 [50] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na- jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben- gio. Manifold Mixup: Better Representations by Interpo- lating Hidden States. In Proc. of ICML, pages 6438–6447, Long Beach, CA, USA, 2019. 2 10[51] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Proc. of NIPS , pages 3637–3645, Barcelona, Spain, 2016. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2 [52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. In Proc. of ICCV , pages 548–558, virtual, 2021. 1 [53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In Proc. of CVPR, pages 3485–3492, San Francisco, CA, USA, 2010. 2, 4, 12 [54] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN Database: Exploring a Large Collection of Scene Categories. Int. J. Comput. Vis., 119(1): 3–22, 2016. 2, 4, 12 [55] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andr´e Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A Large-scale Study of Representation Learn- ing with the Visual Task Adaptation Benchmark. arXiv, 1910.04867:1–33, 2020. 4, 12 [56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling Vision Transformers. In Proc. of CVPR, pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4, 5, 12, 13 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre- Training. In Proc. of ICCV , pages 11975–11986, Paris, France, 2023. 1, 5, 8, 13 [58] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Mini- mization. In Proc. of ICLR , pages 1–13, Vancouver, BC, Canada, 2018. 1, 2, 4 [59] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun- chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip- Adapter: Training-Free Adaption of CLIP for Few-Shot Classification. In Proc. of ECCV, pages 493–510, Tel Aviv, Israel, 2022. 2 11Frozen Feature Augmentation for Few-Shot Image Classification Supplementary Material A1. Introduction We give additional details and results to complement the main paper. All included citations refer to the main paper’s references. A2. Brightness We provide the code snippet for brightness c2 FroFa def transform_aug_reverse( x, augment, aug_min_val=0, aug_max_val=1.0, x_min_val=None, x_max_val=None, clip=True): \"\"\"Transform to (low, high)-space, perform augmentation, transform back.\"\"\" l = x_min_val if x_min_val is None: l = tf.reduce_min(x) h = x_max_val if x_max_val is None: h = tf.reduce_max(x) # [l, h] --> [0, 1] x = (x - l) / (h - l + 1e-8) # [0, 1] --> [low, high] x = x * (aug_max_val - aug_min_val) x = x + aug_min_val x = tf.cast(augment(x), tf.float32) if clip: tf.clip_by_value(x, aug_min_val, aug_max_val) # [low, high] --> [0, 1] x = (x - aug_min_val) x = x / (aug_max_val - aug_min_val) x = x * (h - l + 1e-8) + l # [0, 1] --> [l, h] return x def get_random_brightness(max_delta=0.1, clip=False): # A random value in [-max_delta, +max_delta] # is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. def _random_brightness(image): return tf.image.random_brightness( image, max_delta) def tar(x): return transform_aug_reverse( x, augment=_random_brightness, aug_min_val=0, aug_max_val=1.0, clip=clip) return tar def get_random_brightness_per_channel_v2( max_delta=0.1, clip=True): \"\"\"Applies channel-wise random brightness transformations.\"\"\" # A random value in [-max_delta, +max_delta] is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. random_brightness = get_random_brightness( max_delta, clip) def _random_brightness_pc(x): x = tf.expand_dims(x, axis=2) # (H, W, 1, C) x = tf.unstack(x, axis=-1) # C x (H, W, 1) x = [random_brightness( {\"image\": x_i})[\"image\"] for x_i in x] return tf.concat(x, axis=-1) return _random_brightness_pc A3. Detailed Experimental Setup In the following, we provide additional details to our exper- imental setup. A3.1. Datasets In this section, we focus on details regarding our pretraining and few-shot datasets. Pretraining: As stated in the main paper, Sec. 4.2, we pretrain our models by either using JFT-3B [56], ImageNet- 21k [14, 44], or WebLI [5]. In JFT-3B, the images are annotated with noisy labels by using a semi-automated pipeline. We follow common practice [13, 56] and ignore the hierarchical aspect of the labels. ImageNet-21k is a superset of the well known ILSVRC-2012 dataset, also known as “ImageNet-1k” or just “ImageNet”. WebLI is a recently introduced image- and-language dataset. It contains 10 billion images and tens of billions image-text pairs with over 100 languages. Few-shot transfer: As stated in the main paper, Sec. 4.2, our experiments concentrate around few-shot transfer on ILSVRC-2012 [14, 44]. We also provide results on CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. When official test and validation splits are available, we use them for eval- uation across all datasets. In general, we use the versions in TensorFlow Datasets. CIFAR10 contains 60,000 images of 10 equally dis- tributed classes split into 50,000 training images and 10,000 test images. We further split the official training dataset into 45,000 training images and 5,000 validation images. CIFAR100 is a superset of CIFAR10 with 100 equally distributed classes and 60,000 images. Similar to CIFAR10, we use 45,000 images for training, 5,000 images for valida- tion and 10,000 images for test. DMLab consists of frames collected from the DeepMind Lab environment. Each frame is annotated with one out https://www.tensorflow.org/datasets 12of six classes. We use 65,550 images for training, 22,628 images for validation, and 22,735 for test. DTD is a collection of 5,640 textural images categorized into 47 distinct classes. Each of the three splits, i.e., train- ing, validation, and test, has exactly 1,880 images. Resisc45 is a benchmark with 31,500 images for image scene classification in remote sensing scenarios. In total, 47 different catogries for scenes are defined. We use the first 23,000 images for training, the subsequent 2,000 images for validation and the last 6,300 images for test. SUN397 is a 397-category database of 108,753 images for scene understanding. We use 76,128 images for training, 10,875 images for validation, and 21,750 images for test. SVHN is a Google Street View dataset with a large col- lection of house number images. In total, 10 distinct classes exist. We use the cropped version with 73,257 images for training and 26,032 images for test. Further, we create a val- idation subset by only using the first 70,000 out of 73,257 training images for actual training and the remaining 3,257 images for validation. A3.2. Data Augmentation In this section, we provide additional details on the used data augmentation techniques and protocols. (c/c2)FroFA: In Tab. 6, we give detailed descriptions of each FroFA, cFroFA, and c 2FroFA setting. We mostly build upon an AutoAugment implementation from Big Vision. To keep it simple, we use v or v1, v2 as sweep parameter(s) for all augmentations. By default, we first re- shape the two-dimensional features f to three-dimensional features f∗ (1) of shape √ N × √ N × C, with N = 196 and C ∈ {192, 768, 1024} in all our experiments. Note that the value of C depends on the architecture. We further want to point out, while some augmentations heavily rely on the three-dimensional representation, e.g., all geometric ones, some others are also transferable to a two-dimensional rep- resentation, e.g., brightness or contrast. As pointed out in the main paper, Tab. 3, brightness c2FroFA, contrast FroFA, and posterize cFroFA are our best FroFAs. For all three, we list the best sweep settings in Tab. 7. Advanced protocols: As mentioned in the main paper, Sec. 4.3, besides our fixed sequential protocol ( cf . Tab. 4) we also tested variations of RandAugment [10] and Triv- ialAugment [39]. In all protocols, we sample from the best settings of brightness c2FroFA, contrast FroFA, and poster- ize cFroFA. In particular, we use v = 1.0 for brightness c2FroFA, v = 6.0 for contrast FroFA, and v1 = 1, v2 = 8 for posterize cFroFA ( cf . Tab. 6). We re-use the abbrevi- ations from Tab. 4 in the following, i.e., B-c 2, C, and P- c, respectively. For the RandAugment and TrivialAugment https://github.com/google- research/big_vision/ blob/main/big_vision/pp/autoaugment.py variations, we uniformly sample from either the best three FroFAs, i.e., Atop3 = {B-c2, C, P-c}, or the best two Fro- FAs, i.e., Atop2 = A3 \\ {C}. Further, our RandAugment variation randomly constructs a sequence of augmentations by uniformly sampling the integer sequence length from 1 to |A|, with A ∈ {Atop2, Atop3} depending on whether Atop2 or Atop3 is used. A3.3. Training Details Pretraining: In the JFT-3B setup, we use pretrained mod- els from Zhai et al. [56]. The models are pretrained using a sigmoid cross-entropy loss. The weights are optimized by Adafactor [45] in half-precision mode, β1 = 0.9, and β2 = 0.999. Further, (decoupled) weight decay [38] is applied with 3.0 on the head and 0.03 for the rest of the network weights. The learning rate is adapted by a recip- rocal square-root schedule for 4,000,000 steps with a lin- ear warm-up phase of 10,000 steps and a linear cool-down phase of 50,000 steps. The starting learning rate is 0.01 for Ti/16 and L/16 and 0.03 for B/16. The images are prepro- cessed by an224×224 inception crop and a random horizon- tal flip. We set the batch size to 4,096. To stabilize training, a global norm clipping of 1.0 is used. In the ImageNet-21k setup, we follow settings from Steiner et al. [46] and use a sigmoid cross-entropy loss for multi-label pretraining. We use the Adam optimizer [25] in half-precision mode and set β1 = 0.9 and β2 = 0.999. Fur- ther, we apply (decoupled) weight decay with either 0.03 for Ti/16 or 0.1 for B/16 and L/16. We adapt the learning rate using a cosine schedule for roughly 930,000 steps (300 epochs) with a linear warm-up phase of 10,000 steps. We set the starting learning rate to 0.001 for all models. During preprocessing, we crop the images to 224×224 following an inception-style crop and a random horizontal flip. While we don’t use any additional augmentation for Ti/16, we fol- low suggestions by Steiner et al. [46] and use the ‘light1’ and ‘medium2’ augmentation settings for B/16 and L/16, respectively. Finally, we use a batch size of 4,096 and sta- bilize training by using a global norm clipping of 1.0. In the WebLI setup, we take an L/16 model from [57]. In particular, we use [ADD DETAILS]. Few-shot learning: We first cache each few-shot dataset by processing each of them through a pretrained model and store the extracted features (cf . Fig. 2). We resize each im- age to 224×224 before feeding it to the model. We follow up with a training where we mostly use trans- fer learning settings from Steiner et al. [46]. We use a sig- moid cross-entropy loss. This might be non-intuitive given that all of our few-shot datasets are not multi-labeled. How- ever, we didn’t really observe any performance drops com- pared to using the more common softmax cross-entropy loss, so we stick to the sigmoid cross-entropy loss. We use stochastic gradient descent with momentum of 0.9. Simi- 13Augmentation Description Geometric rotate We rotate each of the C feature channels fc (2) by z ∼ U(−v, v). We sweep across v ∈ {15, 30, 45, 60, 75, 90} representing the maximum positive and negative rotation angle in degrees. shear-{x,y} We (horizontally/vertically) shear each of the C feature channels fc (2) by z ∼ U(0, v). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7} representing the maximum level of horizontal or vertical shearing. translate-{x,y} We (horizontally/vertically) translate each of theC feature channels fc (2) by uniformly samplingz from {0, 1, ..., v}. We sweep across integer values 1 ≤ v ≤ 7 representing the maximum horizontal or vertical translation. Crop & drop crop We randomly crop each of the C feature channels fc (2) to v×v at the same spatial position. We sweep across integer values 1 ≤ v ≤ 13 representing the square crop size. resized crop We resize each of the C feature channels fc (2) to v × v and then randomly crop each to 14 × 14 at the same spatial position. We sweep across v ∈ {16, 18, 20, 22, 24, 26, 28, 35, 42} representing the resized squared spatial resolution. inception crop We apply an inception crop with probability v. We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. patch dropout We randomly keep v out of N patches of f having shape N × C. Note that the patch ordering is also randomized. We sweep across v ∈ {1, 2, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 116, 132, 148, 164, 180}. Stylistic brightness We randomly add a value z ∼ U(−v, v) to each of the C feature channels fc (2). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. We test this method using all FroFA variants. In the default FroFA and the cFroFA variants, the features are scaled by (5) taking the minimumfmin and maximum fmax across all chan- nels into account. In the c 2FroFA variant, each channel fc (2) is shifted individually and uses the channel minimum and maximum instead. Further, in the cFroFA and c2FroFA variants we sample C values of z, one for each channel. contrast We randomly scale each of the C feature channels fc (2) by z ∼ U( 1 v , v). We sweep across v ∈ {1.25, 1.5, 2, 3, 4, 5, 6, 7, 9, 10}. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sample C values of z, one for each channel. equalize We first map the features from value range R to the integer subset I = {0, 1, ...,195}, i.e., executing (5) followed up by a discretization step. We choose this value range as preliminary results mapping from R to the more commonly used I = {0, 1, ...,255} instead didn’t show any effects. We continue by equalizing 196 bins and then transforming the results back to the original space using (7). We apply equalize with probability v. In particular, we sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. invert We change the sign of features f∗ with probability v. We sweep acrossv ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. posterize We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. In other words, we use an 8-bit representation for features f∗. Posterize performs a quantization by a bit-wise left and right shift. We uniformly sample the shift value z between integer values v1 and v2. In our sweep, we test a subset of all possible combinations. In particular, we first set v2 = 8and reduce v1 from 7 to 1. We then fix v1 = 1and increase v2 from 2 to 7 again. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sampleC values of z, one for each channel. sharpness We first apply a two-dimensional convolution on f∗ (1) using a 3×3 smoothing filter. Next, we mix the original features with the resulting “smoothed” features using a randomly sampled blending factor z ∼ U(0, v). We sweep across v ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0}. solarize We do not map features from R to I = [0, 1], but stay in R. We compute the minimum fmin and maximum fmax across features f∗. We conditionally subtract all values smaller than0.5·fmin from fmin or larger than0.5·fmax from fmax. We apply this method with a probabilityv and sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Other JPEG We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. We then perform a JPEG compression of each channel by randomly sampling a JPEG quality z ∼ U(v1, v2). We sweep across combinations of v1 ∈ {10, 25, 50, 75} and v2 ∈ {25, 50, 75, 100}, with v2 > v1. mixup We do not map features from R to [0, 1], but stay in R. We mix two features f∗ i , f∗ j according to z ·f∗ i + (1−z) ·f∗ j by sampling a random value z ∼ B(α, α), with Beta distribution B(α, α) parameterized by α = v. The labels are mixed using the same procedure. We sweep across v ∈ {0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Table 6. Details on our used set of augmentations. For simplicity, instead of introducing a new hyper parameter for each data augmenta- tion, we re-use v as a sweep parameter that is set during a sweep and differs for each augmentation. If not stated otherwise, each method is only applied as default FroFA and we first map featuresf (two-dimensional representation) or f∗ (three-dimensional representation) from value range R to I = [0, 1] using (5). By default, we assume a three-dimensional representation f∗ although some augmentations would work also in the two-dimensional representation f, i.e., a reshaping is not necessary. lar to the pretraining setup, we also store the internal state in half-precision. We do not apply any weight decay. The learning rate is adapted following a cosine schedule with a linear warm-up phase of 500 steps. In addition, we stabilize 14FroFA Shots Base learning rate Batch size Training steps v or v1, v2 B-c2 1 0.01 512 4,000 1.0 10 0.01 64 16,000 1.0 15 0.01 256 8,000 0.9 25 0.01 512 8,000 0.8 C 1 0.01 32 16,000 6.0 10 0.01 128 8,000 6.0 15 0.01 512 2,000 6.0 25 0.01 256 4,000 7.0 P-c 1 0.01 512 8,000 1, 8 10 0.03 512 8,000 1, 8 15 0.03 512 16,000 1, 8 25 0.03 64 16,000 2, 8 Table 7. Our best sweep settings for our best three FroFAs , namely, brightness cFroFA (B-c 2), contrast (C), and posterize cFroFA (P-c). We list the shots, base learning rate, batch size, number of training steps, and the augmentation parameter, denoted as v or v1, v2 (see Tab. 6 for a detailed explanation ofv and v1, v2). The best sweep settings are found using our ILSVRC-2012 vali- dation set. RA∗ TA∗ Shots MAP B-c 2 Atop2 Atop3 Atop2 Atop3 1 58.4 +6.0 +3.9 +2.4 +4.8 +4.3 5 79.1 +1.5 +1.0 +0.4 +1.4 +1.2 10 80.7 +1.3 +1.0 +0.6 +1.4 +1.4 25 83.0 +0.6 +0.4 +0.0 +0.5 +0.4 Table 8. Top-1 accuracy for advanced FroFA protocols on our ILSVRC-2012 test set. Absolute gains to the MAP baseline (ref- erence run) are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We compare brightness c 2FroFA (B-c2) with our variations of RandAugment (RA∗) and TrivialAugment (TA∗), cf . Sec. A3.2. For the latter, we either use the top-2 ( Atop2) or top- 3 ( Atop3) augmentations. We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 val- idation set. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). training by using a global norm clipping of 1.0. Further, we sweep across batch size, learning rate and number of steps yielding 100 combinations (cf . Sec. 4.4) for each shot. A4. Additional Experimental Results In this section, we show additional experimental results. A4.1. Patch Dropout and Brightness In Fig. 3, we only report results for 1-shot and 25- shot settings using patch dropout FroFA and brightness (c/c2)FroFA. We extend this by also reporting results for 5-shot and 10-shot settings in Figs. 5 and 6. We observe the same effects in the other settings as well. A4.2. Advanced FroFA Protocols In Tab. 8, we report results for our RandAugment (RA ∗) and TrivialAugment (TA∗) variations. We did not average across five runs and thus only report absolute gains with re- spect to a reference run. Therefore, numbers which are also reported in the main paper, e.g., Tab. 4, are slightly differ- ent. All in all, we observe that both RA ∗ and TA∗ do not improve upon the best single augmentation, i.e., brightness c2FroFA (B-c2). We also observe that increasing the set of augmentations from Atop2 to Atop3 rather worsens the per- formance for both RA∗ and TA∗. A4.3. Detailed FroFA Transfer Results In Tab. 9, we report exact numbers for Fig. 4, i.e., Ti/16, B/16, and L/16 pretrained on either ImageNet-21k or JFT- 3B and subsequently finetuned on few-shotted ILSVRC- 2012 training sets. Numbers for the two baselines, i.e., MAP ( with weight decay) and linear probe, and our best method, i.e., MAP ( with weight decay) combined with brightness c2FroFA (MAP + FroFA), are reported. In addi- tion, we report numbers, where we use MAPwithout weight decay in Tab. 10. As before, we observe that our method performs worse on all 1-shot settings, but is on par or sig- nificantly better than MAP and/or linear probe on most 5- to 25-shot settings. 151 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 72 74 76 78 5-shot 1 50 100 150 number of patches 76 77 78 79 80 81 10-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA Figure 5. Average top-1 accuracy for patch dropout FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each number of patches (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 74 76 78 80 5-shot 0.1 0.3 0.5 0.7 0.9 brightness level 79 80 81 82 10-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot MAP + brightness cFroFA + brightness c2FroFA Figure 6. Top-1 accuracy for channel variants (c/c2) of brightness FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each brightness level (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAPwd 20.5 53.6 59.7 64.9 19.1 46.4 53.6 60.2 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAPwd + FroFA 20.6 54.5 60.1 65.2 19.6 47.2 53.6 60.3 B/16 MAPwd 30.5 71.7 75.3 78.0 51.3 74.8 77.5 79.8 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAPwd + FroFA 30.6 73.3 76.0 78.1 52.5 75.1 77.6 79.5 L/16 MAPwd 38.7 75.9 78.6 80.6 62.0 79.9 81.5 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAPwd + FroFA 39.3 78.0 80.0 81.0 63.7 80.4 82.0 83.6 Table 9. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline, as well as our best FroFA-based approach, i.e., weight-decayed MAP combined with brightness c 2FroFA (MAPwd + FroFA). Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAPwd + FroFA, is on par or significantly better than MAPwd and/or linear probe on most 5- to 25-shot settings. 16ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAP 20.4 53.2 59.5 64.7 17.9 45.5 53.5 60.1 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAP + FroFA 22.1 54.9 60.1 65.0 20.3 47.2 53.6 60.1 B/16 MAP 31.3 70.3 75.1 78.1 48.9 73.4 76.5 79.4 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAP + FroFA 30.6 73.4 76.3 78.3 52.4 75.2 77.8 79.9 L/16 MAP 38.8 74.9 78.5 80.7 57.9 78.8 80.9 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAP + FroFA 39.3 78.0 80.0 81.2 63.9 80.3 82.0 83.6 Table 10. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the MAP and L2-regularized linear probe baseline, as well as our best FroFA-based approach,i.e., MAP combined with brightness c2FroFA (MAP + FroFA). Depending on the setting, we sweep across a base,cf . Sec. 4.4, an L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAP + FroFA, is on par or significantly better than MAP and linear probe on most 5- to 25-shot settings. 17",
      "references": [],
      "meta_data": {
        "arxiv_id": "2403.10519v2",
        "authors": [
          "Andreas Bär",
          "Neil Houlsby",
          "Mostafa Dehghani",
          "Manoj Kumar"
        ],
        "published_date": "2024-03-15T17:59:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Frozen Feature Augmentation (FroFA) introduces a simple yet effective way to augment frozen features from pretrained vision transformers for few-shot image classification. The main contributions are: (i) a comprehensive pilot study showing that per-channel, pointwise feature-space augmentations (notably brightness, contrast, and posterize) yield consistent performance gains across three ViT architectures, three large pretraining datasets, and eight transfer datasets; (ii) a principled mapping from image-space augmentations to frozen feature space via per-channel and per-channel-min/max normalization (FroFA, cFroFA, c2FroFA) and the exploration of 18 augmentations; (iii) identification that geometric augmentations in feature space are mostly detrimental on ImageNet, while stylistic augmentations provide robust gains, especially in low-data regimes; (iv) demonstration of significant improvements over MAP and linear probes, particularly on small transfer datasets, and the effectiveness of per-channel FroFA variants; (v) investigation of sequential FroFA protocols and transferability across architectures and pretraining regimes, including SigLIP/WebLI.",
        "methodology": "Cache intermediate frozen features from the last transformer block of pretrained vision transformers (ViTs: Ti/16, B/16, L/16) and train a lightweight multi-head attention pooling (MAP) head on top. FroFA maps image-space augmentations to feature space via tf→x (normalization to [0,1]), apply augmentation ax (global or per-channel), then tf←x back to feature space (8). Three FroFA variants: (i) FroFA (global, single augmentation per input across channels), (ii) cFroFA (per-channel augmentation values), (iii) c2FroFA (per-channel augmentation plus per-channel tf→x and tf←x mappings). Eighteen augmentations are considered, spanning geometric (rotate, shear, translate), crop/drop (crop, resized crop, inception crop, patch dropout), stylistic (brightness, contrast, equalize, invert, posterize, sharpness, solarize), and others (JPEG, mixup). Per-channel stochasticity is emphasized (especially for brightness and contrast). Training regimes involve a weight-decayed MAP baseline (MAPwd) or linear probes as baselines, with hyperparameter sweeps over batch sizes, learning rates, and training steps. The approach is validated by caching features from large pretrained backbones, assembling a small MAP head, and evaluating on top-1 accuracy across few-shot settings (1, 5, 10, 25 shots).",
        "experimental_setup": "Pretraining datasets: JFT-3B (3B noisy-labeled images), ImageNet-21k (14M+ images, 21,841 labels), and WebLI (multilingual, image-text data). Architectures: ViT-based Ti/16, B/16, L/16. Datasets for few-shot transfer: ILSVRC-2012 (ImageNet-1k), CIFAR10, CIFAR100, DMLab, DTD, Resisc45, SUN397, SVHN. Splits: 1/5/10/25-shot constructed from the first 10% (and additional 10% fractions) of training data; minival subset for hyperparameter tuning; test sets as official validation/test. Procedure: cache features from the L-th transformer block, train MAP head with FroFA-augmented features, compare against MAP (with and without weight decay) and linear probe baselines, report top-1 accuracy. Additional experiments extend FroFA to other architectures and pretrained regimes (ImageNet-21k-, JFT-3B-, WebLI/SigLIP-based models) and include sequential FroFA protocols and randomized augmentation techniques.",
        "limitations": "Limitations include: (1) geometric FroFA alterations consistently degrade ImageNet performance, indicating sensitivity to the feature-space geometry; (2) gains are larger on small transfer datasets and can diminish with higher shot counts as baselines improve; (3) reliance on caching frozen features and training a lightweight MAP head may limit applicability to tasks requiring fine-tuning or end-to-end training; (4) sensitivity to augmentation hyperparameters (e.g., brightness level) for per-channel FroFA variants, especially cFroFA vs c2FroFA; (5) need for extensive hyperparameter sweeps and potential overfitting to validation splits; (6) lack of deep theoretical understanding of why specific FroFAs help.",
        "future_research_directions": "Future work could explore: (i) broader and learned FroFA policies via meta-learning or neural architecture search to automatically select augmentations and per-channel settings; (ii) integration with parameter-efficient fine-tuning methods (adapters, LoRA) to combine feature-space augmentation with gradient-based adaptation; (iii) extending FroFA to other modalities (multimodal models) and tasks (segmentation, detection) and to mid/large-scale datasets; (iv) investigation of dynamic, dataset- or instance-dependent FroFA mappings and theoretical analyses of why stylistic augmentations help in frozen features; (v) efficiency optimizations for feature caching and deployment, and exploring auto-tuning of FroFA hyperparameters; (vi) deeper study into per-channel statistics and their relation to feature geometry and classifier heads.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Channel Importance Matters in Few-Shot Image Classification",
      "full_text": "Channel Importance Matters in Few-Shot Image Classiﬁcation Xu Luo 1 Jing Xu 2 Zenglin Xu 2 3 Abstract Few-Shot Learning (FSL) requires vision mod- els to quickly adapt to brand-new classiﬁcation tasks with a shift in task distribution. Understand- ing the difﬁculties posed by this task distribution shift is central to FSL. In this paper, we show that a simple channel-wise feature transformation may be the key to unraveling this secret from a channel perspective. When facing novel few-shot tasks in the test-time datasets, this transformation can greatly improve the generalization ability of learned image representations, while being ag- nostic to the choice of datasets and training al- gorithms. Through an in-depth analysis of this transformation, we ﬁnd that the difﬁculty of rep- resentation transfer in FSL stems from the severe channel bias problem of image representations: channels may have different importance in differ- ent tasks, while convolutional neural networks are likely to be insensitive, or respond incorrectly to such a shift. This points out a core problem of the generalization ability of modern vision systems which needs further attention in the future. Our code is available at https://github.com/ Frankluox/Channel_Importance_FSL. 1. Introduction Deep convolutional neural networks (Krizhevsky et al., 2012; He et al., 2016) have revolutionized computer vi- sion in the last decade, making it possible to automatically learn representations from a large number of images. The learned representations can generalize well to brand-new images. As a result, image classiﬁcation performance is close to humans on most benchmarks. However, in addi- tion to recognizing previously-seen categories, humans can quickly change their focus of image patterns in changing 1University of Electronic Science and Technology of China 2Harbin Institute of Technology Shenzhen 3Pengcheng Laboratory. Correspondence to: Xu Luo <frank.luox@outlook.com>, Zenglin Xu <xuzenglin@hit.edu.cn>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Animals in   miniImageNet Plants in   iNaturalist Plant Disease Images Discriminative Features Figure 1.Examples of task distribution shift. Different classiﬁ- cation tasks may focus on distinct discriminative information. Top: animals in miniImageNet with different plants as background. Mid- dle: plants as the main categories in iNaturalist. Bottom: Different types of plant diseases in the ﬁne-grained Plant Disease dataset. environments and recognize new categories given only a few observations. This fast learning capability, known as Few-Shot Learning (FSL), challenges current vision models on the ability to quickly adapt to novel classiﬁcation tasks that are different from those in training. This task distri- bution shift means that categories, domains of images or granularity of categories in new tasks deviate from those in the training tasks. Recent studies of few-shot image classiﬁcation have high- lighted the importance of the quality of learned image repre- sentations (Raghu et al., 2020; Doersch et al., 2020; Dhillon et al., 2020; Tian et al., 2020; Rizve et al., 2021), and also showed that representations learned by neural networks do not generalize well to novel few-shot classiﬁcation tasks when there is task distribution shift (Chen et al., 2021; Do- ersch et al., 2020; Agarwal et al., 2021). Thus it is crucial to understand how task distribution shift affects the generaliza- tion ability of image representations in few-shot learning. As shown in Figure 1, task distribution shift may lead to changes in discriminative image features that are critical to the classiﬁcation task at hand. For example, in the task of recognizing animals, a convolutional neural network trained on miniImageNet can successfully identify the discrimina- tive information related to animals. Although the repre- sentations learned by the network may encode some plant information (from image background), plants do not appear as a main category in miniImageNet and it may be insuf- ﬁcient for the network to distinguish various plants in a novel few-shot task sampled from the iNaturalist dataset. arXiv:2206.08126v2  [cs.CV]  20 Jun 2022Channel Importance Matters in Few-Shot Image Classiﬁcation Even when the network is well trained to recognize plants on iNaturalist, it is difﬁcult to be adapted to the novel task of identifying plant diseases due to the granularity shift, since the discriminative information now becomes the more ﬁne-grained lesion part of leaves. In this paper, we show that this difﬁculty encountered in few-shot learning leads to achannel bias problem in learned image representations (i.e., features). Speciﬁcally, in the layer after global pooling, different channels in the learned feature seek for different patterns (as veriﬁed in (Zhou et al., 2015; Bau et al., 2017)) during training, and the channels are weighted (in a biased way) based on their importance to the training task. However, when applied to novel few-shot classiﬁcation tasks, the learned image features usually do not change much or have inappropriately changed without adapting to categories in novel tasks. This bias towards training tasks may result in imprecise attention to image features in novel tasks. What leads to our discovery of the channel bias problem is a simple transformation function that we found in a mathe- matical analysis textbook. Applied to top of image represen- tations channel-wisely only at test time on the ﬂy, this trans- formation function can consistently and largely improve predictions for out-of-distribution few-shot classiﬁcation tasks, being agnostic to the choice of datasets and training algorithms (e.g., 0.5-7.5% average improvement over 5-way 5-shot tasks on 19 different test-time datasets, as shown in Table 1). Through analysis, we reveal the existence of channel bias problem, and show that this transformation rec- tiﬁes channel emphasis by adjusting the Mean Magnitude of Channels (MMC) of image representations over the tar- get task. Concretely, it serves as a smoothing function that suppresses channels of large MMC and largely ampliﬁes channels of small MMC. To further understand the channel bias problem, we derive an oracle adjustment on the MMC of image representations in binary classiﬁcation tasks. Such studies demonstrate that the channel bias problem exists in many different target tasks with various types of task distributions shift, and it becomes severe with the distribution shift expanding (as shown in Figure 6). In addition, through test-time shot analysis, we verify that the channel bias problem requires more attention in few-shot setting, while simple ﬁne-tuning can help address this problem in many-shot setting. 2. A Channel-wise Feature Transformation 2.1. Problem Setup In few-shot image classiﬁcation, a training set Dtrain is used at ﬁrst to train a neural network parametrized by θ, which will be evaluated on a series of few-shot classiﬁcation tasks constructed from the test-time dataset Dtest. Impor- tantly, there should be task distribution shift betweenDtrain and Dtest, which may include category shift, domain shift or granularity shift. Each evaluated N-way K-shot few- shot classiﬁcation task τ is constructed by ﬁrst sampling N classes from Dtest, and then sampling K and M images from each class to constitute a support setSτ and a query set Qτ, respectively. The support setSτ = {(xτ k,n,yτ k,n)}K,N k,n=1 consisting of K ×N images xτ k,n and corresponding la- bels yτ k,n from the N classes is used to construct a classi- ﬁer pθ(·|x,Sτ), which is further evaluated on the query set Qτ = {x∗τ m,n}M,N m,n=1. The evaluation metric is the average prediction accuracy on query set over all sampled few-shot classiﬁcation tasks. In order to evaluate on different types and degrees of task distribution shift, in the following experiments, we select a broad range of datasets for Dtrain and Dtest. For Dtrain, we choose (1) the train split of miniImageNet (Vinyals et al., 2016) that contains 38400 images from 64 classes; (2) the train split of ImageNet 1K (Russakovsky et al., 2015) containing more than 1M images from 1000 classes; (3) train+val split of iNaturalist 2018 (Horn et al., 2018), a ﬁne- grained dataset of plants and animals with a total of more than 450000 training images from 8142 classes. For Dtest, we choose the test split of miniImageNet, and all evaluation datasets of Meta-dataset (Triantaﬁllou et al., 2020), BSCD- FSL benchmark (Guo et al., 2020) and DomainNet (Peng et al., 2019), for a total of 19 datasets, to ensure adequate coverage of different categories, domains and task granular- ities. 2.2. Universal Performance Gains from a Test-time Simple Feature Transformation Let x∈RD denote an image and fθ(·) a feature extractor learned from the training set Dtrain. The l-th channel of the feature z = fθ(x) ∈Rd is deﬁned as the l-th dimension of z, i.e., {zi}d i=1 is the set of all dchannels. The simple transformation function φk : [0,+∞) →[0,+∞) that we consider is deﬁned as φk(λ) = { 1 lnk( 1 λ+1) , λ> 0 0, λ = 0 (1) where k >0 is a hyperparameter. At test time, we sim- ply use this function to transform each channel of image features, i.e., φk(z) = (φk(z1),...,φ k(zd)). (2) When applying this transformation, we transform all image features in the target classiﬁcation task regardless of whether they are in the support set or query set; any subsequent operation keeps unchanged. Note that this function can only be applied to features taking non-negative values, common in most convolutional neural networks using ReLU as theChannel Importance Matters in Few-Shot Image Classiﬁcation Table 1.Performance gains of the simple feature transformation on various training and testing datasets with a broad range of choices of network architectures and algorithms . The black values indicate the original accuracy, and the red values indicate the increase. Each running of evaluation contains 10000 5-way 5-shot tasks sampled using a ﬁxed seed, and the average accuracy is reported. The three groups of test-time datasets come from MetaDataset, BSCD-FSL benchmark and DomainNet, respectively. TrainData mini-train ImageNet iNaturalist Algorithm PN PN CE MetaB MetaOpt CE S2M2 PN CE MoCo-v2 CE Architecture Conv-4 Res-12 Res-12 Res-12 Res-12 SE-Res50 WRN Res-50 Res-50 Res-50 Res-50 Average mini-test 66.6+1.2 73.5+2.2 75.9+1.6 74.7+2.6 74.8+0.5 76.2+0.2 82.5+1.2 82.2-1.6 89.1-0.5 93.7+2.2 69.9+2.2 78.1+1.1 CUB 52.0+2.8 57.0+3.0 59.6+2.3 60.1+2.6 60.3+1.7 59.9+2.2 68.5+2.8 65.3+2.5 78.2+0.4 70.0+6.8 94.7+0.0 66.0+2.5 Textures 50.9+2.3 57.1+4.2 63.1+2.4 61.2+3.7 60.2+1.8 63.5+0.6 69.3+2.9 61.9+2.4 71.6+0.8 82.8+0.9 63.2+2.3 64.1+2.0 Trafﬁc Signs 52.6+2.1 64.8+2.2 65.6+1.4 67.3+1.5 67.1+4.9 62.2+2.9 69.6+3.1 64.0+2.2 67.2+3.5 68.4+8.8 60.5+4.0 64.4+3.3 Aircraft 32.1+0.9 31.3+1.6 34.7+1.9 34.7+2.3 35.6+2.4 38.2+2.0 40.5+4.7 38.4+1.7 46.6+2.5 34.5+8.8 42.1+2.5 34.0+2.9 Omniglot 61.0+10.0 77.6+7.8 86.9+3.7 81.6+7.9 78.0+9.9 89.9+2.3 85.9+7.4 76.4+2.9 88.6+5.3 74.5+15.8 83.8+9.0 80.4+7.5 VGG Flower 71.0+3.1 71.1+5.5 79.2+3.8 78.3+4.5 78.4+3.1 83.0+1.7 87.8+2.5 81.4+2.6 89.3+1.7 86.2+6.3 91.9+1.1 81.6+3.3 MSCOCO 52.0+1.2 58.2+1.1 59.0+0.7 58.0+1.6 58.4+0.1 57.1+0.5 63.5+0.1 61.3-0.5 64.3-0.4 71.4+1.4 50.4+1.9 59.4+0.7 Quick Draw 49.7+6.5 60.2+5.4 67.5+6.5 61.9+9.0 61.0+6.2 69.8+2.8 66.4+8.2 59.8+6.9 70.2+3.0 63.7+8.3 60.8+6.2 62.8+6.3 Fungi 48.5+1.5 49.0+3.7 52.2+3.3 51.5+4.0 54.6+1.9 55.2+0.5 61.6+3.8 58.5+1.3 65.1+1.1 60.2+9.2 70.0+1.8 56.9+2.9 Plant Disease 66.6+7.8 73.3+7.9 80.0+5.1 75.6+7.6 78.6+4.5 83.1+3.2 86.4+3.5 72.5+8.0 84.1+3.3 87.1+4.7 85.6+4.1 79.4+5.4 ISIC 38.5+1.6 36.8+2.9 40.4+1.0 38.8+1.7 39.5+2.3 37.7+3.9 40.5+5.5 39.5+4.0 37.8+3.6 43.2+2.8 39.0+4.3 39.2+3.1 EuroSAT 63.0+4.5 67.3+5.5 75.7+2.9 71.9+4.5 72.8+5.8 75.7+1.6 81.2+2.9 72.5+6.1 78.4+2.2 83.5+2.7 73.5+3.7 74.1+3.9 ChestX 22.9+0.2 23.0+0.5 24.1+0.3 23.5+0.5 24.5+0.4 23.6+0.2 24.2+0.9 23.2+0.3 24.2+0.8 25.4+0.9 23.9+0.1 23.9+0.5 Real 67.0+1.8 72.2+3.1 76.3+1.6 75.0+2.6 75.8+1.1 76.7+0.5 81.7+1.9 80.5+0.4 87.1-0.1 88.8+2.1 72.9+1.7 77.6+1.5 Sketch 42.6+2.9 45.3+5.0 51.1+2.6 50.2+3.4 50.6+2.0 50.9+2.4 56.8+4.1 53.1+1.5 63.2+2.5 63.9+5.8 51.9+1.4 52.7+3.1 Infograph 33.1+2.8 34.7+3.7 35.3+2.8 35.0+4.0 38.3+1.1 38.2+2.5 39.2+3.7 39.7+2.7 42.3+4.2 41.6+7.1 38.5+2.9 37.8+3.4 Painting 49.0+1.7 52.5+3.3 56.1+1.4 55.1+2.5 56.2+0.7 59.3+0.8 64.2+1.8 61.8-0.2 69.6+0.5 76.5+3.0 56.4+1.9 59.7+1.6 Clipart 47.5+3.6 49.7+4.8 55.5+3.1 54.9+4.3 56.4+2.6 60.4+2.3 63.0+4.3 60.9+1.8 72.7+1.5 67.4+7.0 58.4+2.2 58.8+3.4 activation function. We discuss one variant of the function dealing with features having negative values (e.g., networks with Leaky ReLU) in Appendix D. A plot of this function with various choices of kis shown in Figure 2. Table 1 shows the performance gains brought by this trans- formation on 5-way 5-shot FSL tasks. We test the transfor- mation on representations trained with different algorithms, including (1) the conventional training methods including cross-entropy (CE) and the S2M2 algorithm (Mangla et al., 2020), (2) meta-learning methods including ProtoNet (Snell et al., 2017) (PN), Meta-baseline (Chen et al., 2021) and MetaOpt (Lee et al., 2019), and (3) MoCo-v2 (He et al., 2020), a unsupervised contrastive learning method. We test these methods with various backbone networks: Conv- 4 (Vinyals et al., 2016) and four variants of ResNet (He et al., 2016) including ResNet-12 (Oreshkin et al., 2018), WRN- 28-10 (Zagoruyko & Komodakis, 2016), ResNet-50 and SE-ResNet50 (Hu et al., 2018). We replace Leaky ReLU with ReLU in ResNet-12 to obtain positive features (cause of performance degradation in Table 1). At test-time, we use the Nearest-Centroid Classiﬁer (Snell et al., 2017) for CE, linear probing for S2M2 and MoCo-v2, and for meta- learning algorithms we use their own test-time classiﬁer. Training and evaluation details can be found in Appendix B. The result shows how this simple feature transformation substantially improves few-shot learning across various al- gorithms, datasets and architectural choices, with a ﬁxed hyperparameter k= 1.3 (We show how performance varies with different choices of k in Appendix C). The only ex- ception happens when the test-time task distribution is very 0.00 0.05 0.10 0.15 0.20 0.25 x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8y k=0.5 k=1.3 k=2.0 k=5.0 Figure 2.The simple transformation function φk with various choices of k. similar to a subset of training distribution: training the su- pervised models on ImageNet and testing on miniImageNet, MSCOCO, Real or Painting1, or training on iNaturalist and testing on CUB. Is this transformation useful only if there exists task distribution shift between training and testing? To verify this, we train a CE model on each of ten datasets and test on 5-way 5-shot tasks sampled from each dataset. When testing on the training dataset, we evaluate on images not included in training. The results shown in Figure 3 clearly give evidence that the transformation is beneﬁcial only to few-shot classiﬁcation with task distribution shift—the per- formance is improved only when test-time task distribution deviates from training, and this distribution shift includes domain shift (e.g., from Sketch to QuickDraw), category 1There are a lot of painting-style images in ImageNet. Con- trastive learning (MoCo) can be seen as an inﬁnitely ﬁne-grained classiﬁcation task, thus having a relatively large different task distribution shift from training to testing, even on the same dataset.Channel Importance Matters in Few-Shot Image Classiﬁcation -0.4 +2.3 +2.4 +1.4 +5.1 +1.0 +2.9 +2.6 +6.5 +3.3  +0.5 -0.2 +1.0 +2.4 +2.4 +2.3 +0.8 +1.0 +4.3 +1.1  +0.2 +0.1 -0.2 +0.1 +0.3 +0.2 +0.3 +0.1 +0.1 +0.0  +2.2 +2.3 +1.6 +0.0 +3.5 +2.6 +2.5 +2.0 +4.5 +1.7  +2.0 +2.1 +1.0 +0.6 +0.0 +1.0 +3.7 +0.6 +7.6 +1.6  +0.1 +0.6 +0.3 -0.2 +2.0 -0.8 +1.0 +0.1 +0.8 +0.5  +0.5 +0.2 +0.7 +0.5 +4.0 +0.0 -0.1 +1.5 +4.4 +0.6  +2.4 +2.1 +2.6 +5.0 +4.2 +1.8 +6.9 -0.7 +2.0 +2.5  +2.2 +0.9 +0.2 +1.2 +5.3 +1.3 +1.4 +1.8 -0.5 +1.4  +3.1 +5.5 +3.6 +5.4 +3.6 +4.3 +6.4 +2.8 +5.7 -0.9  mini  CUB  T exture  T S  PlantD  ISIC  ES A T  Sk etch  QDr a w  Fungi  mini CUB T exture T S PlantD ISIC ES A T Sk etch QDr a w Fungi  T r ain  T est  Figure 3.In-distribution (diagonal) and out-of-distribution (off- diagonal) performance gains of the simple channel-wise transfor- mation on representations trained with CE. When the test-time dataset equals the training dataset (diagonal), the categories of images remain the same but test-time images are unseen during training (as in conventional classiﬁcation). shift (e.g., from Plant Disease to Fungi) and granularity shift (e.g., from iNaturalist to Plant Disease in Table 1). 3. The Channel Bias Problem In this section, we analyze the simple transformation, which leads us to discover the channel bias problem of visual rep- resentations. Given the transformation function described in Eq.(1), it can be ﬁrst noticed that φ′ k(λ) >0, lim λ→0+ φ′ k(λ) = +∞, ∃t> 0, s.t. ∀λ∈(0,t),φ′′ k(λ) <0, (3) where tis a large value for most k, relative to the magni- tudes of almost all channels (e.g., when k= 1.3, t≈0.344, while most channel values are less than 0.3). The positive- ness of the derivative ensures that the relative relationship between channels will not change, while the negative sec- ond derivative narrows their gaps; the inﬁnite derivative near zero pulls up small channels by a large margin, i.e., limλ→0+ φk(λ) λ = +∞. See Appendix E for the necessity of all these properties. A clear impact of these properties on features is to make channel distribution smooth: suppress channels with high magnitude, and largely amplify channels with low magnitude. This phenomenon is clearly shown in Figure 4, where we plot mean magnitudes of all 640 feature channels on miniImageNet and PlantDisease, with red ones being the original distribution, blue ones being the trans- formed distribution. The transformed distribution becomes more uniform. Intuitively, different channels have high responses to dif- ferent features, and a larger Mean Magnitude of a Channel 0 100 200 300 400 500 600 miniImageNet 0.00 0.02 0.04 0.06 0.08 0.10 0.12magnitude before after 0 100 200 300 400 500 600 Plant Disease 0.00 0.05 0.10 0.15 0.20 0.25magnitude before after Figure 4.Mean magnitudes of feature channels before and af- ter applying the simple transformation. The feature extractor is trained using PN on the training set ofminiImageNet. Left: test set of miniImageNet. Right: The Plant Disease dataset. The change of relative magnitude is due to different variances of channels. (MMC) implies that the model puts more emphasis on this channel, hoping that this channel is more important for the task at hand. Combining the analysis above with previous experiment results, we conjecture that the MMC of repre- sentations should change when testing on novel tasks with a shift in distribution. This meets our intuition that different tasks are likely to be characterized by distinct discriminative features, as shown in the examples of Figure 1. 3.1. Deriving the Oracle MMC of Any Binary Task We now wonder how much the MMC estimated by neu- ral networks in a task deviates from the best MMC or channel importance of that task. To achieve this goal, we ﬁrst derive the optimal MMC for any classiﬁcation task by multiplying a positive constant to each channel of fea- tures, given that we know the ﬁrst-order and second-order statistics of features. For convenience, we consider the binary classiﬁcation problem. Speciﬁcally, let D1, D2 de- note probability distributions of two classes over feature space Z ⊂[0,+∞)d, and z1 ∼ D1, z2 ∼ D2 denote samples of each class. Let µ1,µ2 and Σ1,Σ2 denote their means and covariance matrices, respectively. We assume that the channels of features are uncorrelated with each other, i.e., there exist σ1,σ2 ∈[0,+∞)d, s.t. Σ1 = diag(σ1), Σ2 = diag(σ2). The original MMC of the binary task is deﬁned as ωo = (µ1 + µ2)/2. We assume that the MMC after adjustment is ω∈[0,+∞)d. Let ˜z1, ˜z2 denote stan- dadized version of z1, z2 that have unit MMC, i.e., ˜z1,l = z1,l/ωo l,˜z2,l = z2,l/ωo l ⇒(˜µ1,l + ˜µ2,l)/2 = 1,∀l ∈[d] ([d] is equivalent to {1,2,...,d }). A simple approach to adjust MMC to ωis to transform features to ω⊙˜z1 and ω⊙˜z2 respectively, where⊙denotes the hadamard product. Here, we consider a metric-based classiﬁer. Speciﬁcally, a standardized feature ˜z is classiﬁed as the ﬁrst class if ||ω⊙(˜z−˜µ1)||2 <||ω⊙(˜z−˜µ2)||2 and otherwise the second class. This classiﬁer is actually the Nearest-Centroid Classiﬁer (NCC) (Snell et al., 2017) with accurate centroids. Assume that two classes of images are sampled equal times,Channel Importance Matters in Few-Shot Image Classiﬁcation 0.0 0.1 0.2 0.3 0.4 0.5 0.6 None 0.1 0.0 0.1 0.2 0.3 0.4 0.5 queries of class 1. ACC: 52.38% queries of class 2. ACC: 48.69% support 0.0 0.2 0.4 0.6 0.8 1.0 Simple 0.2 0.0 0.2 0.4 0.6 0.8 1.0 queries of class 1. ACC: 69.84% queries of class 2. ACC: 66.80% support 0.2  0.1  0.0 0.1 0.2 Oracle 0.0 0.1 0.2 0.3 0.4 0.5 queries of class 1. ACC: 88.88% queries of class 2. ACC: 80.08% support Figure 5.Visualization of two channels of image features in two classes of Plant Disease. The feature extractor is trained using PN on miniImageNet. We visualize a one-shot task with only two channels available for classiﬁcation. The plot with “None” shows the original channels. The plots with “Simple” and “Oracle” show channels adjusted by the simple and oracle transformation. The per-class accuracy is calculated as the proportion of samples correctly classiﬁed by the classiﬁcation boundary in each class. then the expected misclassiﬁcation rate of this classiﬁer is R= 1 2[Pz1∼D1 (||ω⊙(˜z1 −˜µ1)||2 >||ω⊙(˜z1 −˜µ2)||2) +Pz2∼D2 (||ω⊙(˜z2 −˜µ2)||2 >||ω⊙(˜z2 −˜µ1)||2)]. (4) The following theorem gives an upper bound of the mis- classiﬁcation rate and further gives the oracle MMC of any given task. Proposition 3.1. Assume that µ1,l ̸= µ2,l and σ1,l+σ2,l > 0 hold for any l∈[d], then we have R≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . (5) To minimize this upper bound, the adjusted oracle MMC of each channel ωl should satisfy: ωl ∝|µ1,l −µ2,l| σ1,l + σ2,l . (6) Proofs are given in Appendix A. We here use the word “oracle” because it is derived using the class statistics of the target dataset, which is not available in few-shot tasks. This derived MMC has an intuitive explanation: if the difference between the means of features from two classes is large but the variances of features from two classes are both small, the single channel can better distinguish the two classes and thus should be emphasized in the classiﬁcation task. In fact, if we further assume x1,l and x2,l are Gaussian-distributed and consider only using the l-th channel for classiﬁcation, then the misclassiﬁcation error for the i-th class (i= 1,2) is a strictly monotonically decreasing function of |µ1,l − µ2,l|/σi,l. Table 2 shows the performance improvement over the simple feature transformation when adjusting the MMC to derived oracle one in each of the real few-shot binary classiﬁcation tasks. For every sampled binary task in a dataset, we calcu- late the oracle adjustment based on Eq. (6); see Appendix F.1 for details. The oracle MMC improves performance on all datasets, and always by a large margin. Note that although the oracle MMC is derived using a metric-based classiﬁer, it can also help a linear classiﬁer to boost perfor- mance, which will be further discussed in Section 4. The large performance gains using the derived channel impor- tance indicate that the MMC of features on new test-time few-shot task indeed has a large mismatch with ground-truth channel importance. To obtain a better understanding, in Figure 5, we visualize image representations of two classes when transferred from miniImageNet to Plant Disease. The two exhibited classes are apples with Apple Scab and Black Rot diseases, respec- tively. We visualize 2 out of 640 channels in the features, shown as the x-axis and y-axis in the ﬁgure. We select these channels by ﬁrst selecting a channel that requires a large sup- pression of MMC (x-axis), and then a channel that requires a large increase (y-axis). As seen, the x-axis channel has a large intra-class variance ( the variances are 0.13 and 0.11 in two classes on the x-axis channel, compared to 0.03 and 0.08 on the y-axis channel) and a small class mean differ- ence (about 0.03, compared to 0.13 on the y-axis channel), so it is hard to distinguish two classes through this chan- nel. By adjusting the mean magnitude of this channel, the simple transformation and oracle adjustment decrease the intra-class variance of the x-axis channel, and so decrease its inﬂuence on classiﬁcation. Similarly, the y-axis channel can better distinguish two classes due to its relatively larger class mean difference and smaller intra-class variance, so the inﬂuence of the y-axis channel should be strengthened. 3.2. Analysis of Channel Importance Next, we take the derived oracle MMC as an approxima- tion of the ground-truth channel importance, and use it to observe how the simple transformation works, as well as how much the channel emphasis of neural networks de- viates from the ground-truth channel importance of tasks in each test-time dataset. We deﬁne MMC of a dataset D as the average l1-normalized MMCs over all possibleChannel Importance Matters in Few-Shot Image Classiﬁcation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Original Simple Oracle 0.00000.00250.00500.00750.01000.01250.01500.01750.0200 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle 0.000 0.002 0.004 0.006 0.008 0.010 0.012 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007MMC after transformation Original Simple Oracle 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010MMC after transformation Original Simple Oracle 0.00 0.01 0.02 0.03 0.04 0.05 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle miniImageNet CUB T exture Traffic Sign Plant Disease ISIC EuroSAT Sketch QuickDraw Fungi Figure 6.Visualization of MMC of ten datasets ωD before and after the use of simple and oracle transformation. In each plot, a point represents a channel, and the x-axis and y-axis represent the MMC before and after transformation respectively, averaged over all possible binary tasks in the corresponding dataset. For comparison, we also plot the line y= xrepresenting the “None” scenario where none of the transformations are applied to features. The feature extractor is trained using PN on miniImageNet. Table 2.The performance gains of the oracle MMC on 5-shot binary classiﬁcation tasks on various datasets. The derived MMC improves the few-shot performance of both metric and non-metric test-time methods: Nearest-Centroid Classiﬁer (NCC) and Linear Classiﬁer (LC). Algorithm Classiﬁer Transformation mini CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Avg PN NCC None 90.5 80.6 80.6 85.1 89.2 65.7 86.5 71.9 82.4 74.6 80.7 Simple 91.3 82.4 83.1 85.8 93.0 68.6 89.2 75.2 85.1 77.2 83.1 Oracle 93.1 88.7 87.2 92.4 95.6 69.1 91.5 81.2 89.4 88.4 87.7 S2M2 LC None 94.0 87.1 85.7 88.7 95.0 68.7 93.5 78.7 85.5 82.8 86.0 Simple 94.4 88.3 87.3 91.2 96.4 72.2 93.8 81.0 89.2 84.5 87.8 Oracle 96.3 94.0 90.7 96.1 98.3 72.6 95.2 87.0 93.0 93.3 91.7 binary tasks in that dataset. Speciﬁcally, suppose in one dataset Dthere are Cclasses, and let ωij denote the MMC in the binary task discriminating the i-th and j-th class. ωij = ωij/||ωij||1 normalizes the MMC, such that the l-th component of the vector ωij represents the percent- age of channel emphasis on the l-th channel. Then the MMC of D is deﬁned as ωD = ∑ 1≤i<j≤C ωij, which gives average percentages of channel emphasis over all bi- nary tasks. We visualize the oracle MMC, compared with MMC adjusted by the simple transformation and the orig- inal MMC of each dataset in Figure 6. A point in each ﬁgure represents a channel of the image features, with x and y axis being its MMC of that dataset before and af- ter transformation, respectively. To obtain a more precise understanding, we also want to quantitatively measure dif- ference between different MMCs or image features. To achieve this, given a distance measure d(·,·) (not necessar- ily a metric), we deﬁne three levels of distances: (1) dataset- level distance d(ωDa,ωDb) that measures the distance be- tween MMCs of two datasets (or the same dataset with different transformations); (2) in-dataset task-level distance C(C+1) 2 ∑ 1≤i<j≤C d(ωa ij,ωb ij) that measures average dis- tance between MMCs of all tasks from a dataset obtained by different feature transformations, and (3) image-level distance 1 |D| ∑|D| i=1 d(zia,zi b), a more ﬁne-grained one that measures average distance between all l1-normalized image features zia,zi b of dataset Dunder different feature transfor- mations. For dataset-level distance, we adopt the normalized mean square difference d(x,y) = 1 d ∑d l=1(xl −yl)2/x2 l, since it treats each channel equally w.r.t. to the scale and is sensitive to high deviation. However, for task-level and image-level distance, we choose the mean square differ- ence d(x,y) = 1 d ∑d l=1(xl −yl)2 instead to avoid high variations caused by a single task or image feature that has channels with very small magnitude; see Appendix F.2 for details. We calculate the distance (1) between the original MMC of the training set (mini-train) and each test set, to see how much neural networks change channel emphasis when faced with novel tasks, (2) between the original and oracle MMC to see how much the changed emphasis is biased on each dataset, and (3) between the simple and oracle MMC of each dataset to see how much the simple transformation alleviates the problem. The results are shown in Table 3. Neural networks are overconﬁdent in previously learned channel importance. Comparing the ﬁrst and sec- ond rows in Table 3, we can see that the adjustment of MMC that the network made on new tasks is far from enough: the distance of original MMCs between train and test set (the ﬁrst row) is much smaller than that between original and oracle MMCs on the test set. This suggests channels that areChannel Importance Matters in Few-Shot Image Classiﬁcation Table 3.Three levels of distance between different MMCs or l1-normalized image features. The ﬁrst row shows the dataset-level distance between the original MMC of the training set (mini-train) and each test set; the second row shows the dataset-level distance between the original and oracle MMCs on each dataset; rows 3-6 show the task-level and image-level distances (both ampliﬁed by 106 times) between MMCs obtained by simple and oracle transformation or between original MMCs (None) and the MMCs obtained by oracle transformation. The feature extractor is trained using PN on the training set of miniImageNet (mini-train). Test dataset Level Compared dataset Trans. mini-train mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Dataset Train v.s. Test None - 0.18 1.56 0.88 1.13 1.54 2.28 1.30 1.01 1.58 0.79 Test None v.s. Oracle 0.42 0.72 3.60 1.78 4.04 3.92 3.47 5.62 4.26 3.37 3.87 Task Test None v.s. Oracle 3.60 4.04 3.53 4.13 3.68 4.09 3.15 4.24 5.31 4.22 3.38 Simple v.s. Oracle 3.54 3.80 2.93 3.62 3.22 3.65 2.59 3.71 4.35 3.18 2.78 Image Test None v.s. Oracle 10.52 10.65 11.53 25.20 9.88 9.75 13.04 16.33 27.36 13.46 11.39 Simple v.s. Oracle 7.98 8.14 8.69 16.74 7.06 7.34 8.43 11.22 19.50 9.32 8.71 mixing bowlAfrican hunting dogmalamute electric guitarblack-footed ferret Query OriginalOracle Figure 7.Examples of Grad-Cam (Selvaraju et al., 2017) class activation maps of query samples using PN before and after the oracle adjustment of MMC on binary 5-shot tasks sampled from the test set of miniImageNet. important to previously learned tasks are still considered by the neural network to be important for distinguishing new tasks, but in fact, the discriminative channels are very likely to change on new tasks. This can be also observed from each plot in Figure 6, where the oracle MMC pushes up channels having small magnitudes and suppresses channels having large magnitudes. The magnitudes of a large number of small-valued channels are ampliﬁed 10×times or more by the oracle MMC, while large-valued channels are sup- pressed 5×times or more, and in most datasets originally large-valued channels eventually have similar channel im- portance to those of originally small-valued channels. The simple transformation, although not being perfect, also reg- ularizes channels due to its smoothing property discussed in Section 3. We call this problem the channel bias problem. The channel bias problem diminishes as task distribu- tion shift lessens. The channel patterns in Figure 6 on all datasets look similar, except for miniImageNet, whose over- all pattern is close to the liney= xrepresenting the original MMCs. There does not exist dominant channels when test- ing on miniImageNet (The maximum scale of channels is within 0.006), while on other datasets there are channels where the neural network assigns much higher but wrong MMCs which deviate far away from the y= xline. In the second row of Table 3, we can also see that the distance between the original and oracle MMCs on miniImageNet, especially on mini-train that the model trained on, is much smaller than that on other datasets2. Since mini-test has a similar task distribution with mini-train, we can infer that the channel bias is less serious on datasets that have similar task distribution. This explains why in Table 1 and Figure 3 the simple transformation gets a relatively low improve- ment when trained on mini-train and tested on mini-test, and even degrades performance when trained and tested on tasks sampled from the same task distribution. The channel bias problem distracts the neural network from new objects. In Figure 7, we compare some class activation maps before and after the oracle adjustment of MMC. We observe that adjusting channel importance helps the model adjust the attention to the objects responsible for classiﬁcation using a classiﬁer constructed by only a few support images. This matches observation in previous work (Zhou et al., 2015; Bau et al., 2017) that different chan- nels of image representations are responsible for detecting different objects. The task distribution shift makes mod- els confused about which object to focus on, and a proper adjustment of channel emphasis highlights the objects of interest. The simple transformation pushes MMCs towards the oracle ones. Observing Figure 6, it is evident that the sim- ple transformation pushes MMCs towards the oracle ones (compared with the line y = x), albeit not perfectly. This observation is further conﬁrmed by the None v.s. Oracle and Simple v.s. Oracle comparison of ﬁne-grained task- level and image-level distance shown from the third row to the last row of Table 3. On each of the test-time dataset, the distance between MMCs obtained by simple and oracle transformation is smaller than that bewteen original MMCs 2Unnormalized mean square difference ignores critical changes of small-valued channels. This is why we do not observe simi- lar phenomenon from the task and image-level difference; see Appendix F.2 for detailed explanations.Channel Importance Matters in Few-Shot Image Classiﬁcation 100 101 102 Number of Shots 60 70 80 90ACC(%) miniImageNet NCC LC Fine-tune 100 101 102 Number of Shots 6 4 2 0 2 (ACC)% miniImageNet NCC LC Fine-tune Figure 8.Shot analysis of miniImageNet. Left: performance of dif- ferent test-time methods. Right: performance gains of the simple transformation using different test-time methods. and the MMCs obtained by oracle transformation. 4. Analysis of the Number of Shots We have seen that the channel bias problem is one of the main reasons why image representations cannot generalize well to new few-shot classiﬁcation tasks. However, two questions remain to be answered: (1) we are still unclear whether this problem is only tied with few-shot image clas- siﬁcation. In all previous experiments, we tested on tasks where only 5 labeled images per class are given. What will happen if we have more training examples in the new task? (2) How much will different test-time methods be inﬂuenced by the channel bias problem? If we have the opportunity to ﬁne-tune the learned representations, will the proposed simple transformation still work? In order to give answers to these questions, we conduct shot analysis experiments on three representative test-time meth- ods that are adopted or are the basis of most mainstream few-shot classiﬁcation algorithms: (1) The metric-based method Nearest-Centroid Classiﬁer (NCC) presented in Pro- toNet, which ﬁrst average image features of each class in the support set to form class centroids and then assign query features to the class of the nearest centroid; (2) Linear Clas- siﬁer (LC), which trains a linear layer upon learned image features in the support set, and (3) Fine-tuning, which ﬁne- tunes the feature extractor together with the linear layer using images in the support set. The feature extractor is trained using the state-of-the-art S2M2 algorithm on the training set of miniImageNet, and we test it on the test set of miniImageNet using the above three test-time methods with different numbers of labeled images in each class of the support set. The results are shown in Figure 8. We show the original accuracy of all methods, as well as the impact of simple transformation on the performance. We ﬁrst take a look at the right plot, which shows the im- pact of the simple transformation on all the methods. The performance gains on NCC and LC stay at a relatively high value for all tested shots, which is up to 400 labeled images per class. This indicates that the channel bias problem is not only linked to few-shot settings, but also exists in many-shot settings. However, when we have abundant support images, we have an alternative choice of ﬁne-tuning the feature ex- tractor directly. Fine-tuning methods have the potential to fully resolve the channel bias problem by directly modifying the image representation and rectifying the channel distribu- tion. The right ﬁgure shows that the simple transformation does not improve ﬁne-tuning methods, so indeed the channel bias problem has been largely alleviated. In the left ﬁgure, the ﬁne-tuning method exhibits its advantages in many-shot setting, but falls short in few-shot settings. Therefore, we can infer that the channel bias problem exists only in the few-shot setting where freezing the feature extractor and building the classiﬁer on learned features becomes a better choice. We also have another notable observation. While the perfor- mance gain of simple transformation on NCC stays around a ﬁxed value, the performance gain on LC decreases with the increase of shots. Thus the channel bias problem is alle- viated to some extent in many-shot settings. This is because more labeled data tells the linear classiﬁer sufﬁcient infor- mation about intra-class variance of data, making it possible to adjust MMC by modifying the scale of each row of the linear transformation matrix. So Linear Classiﬁer can stably increase its performance when more labeled data comes in, until no more linear separation can be achieved, and also the time ﬁne-tuning should get into play to adjust the feature space directly. 5. Discussion and Related Work Task distribution shift. Task distribution shift may hap- pen when a model faces category shift, domain shift or granularity shift. Conventional benchmarks of FSL only consider category shift, i.e. the categories are disjoint for training and testing, such as miniImageNet (Vinyals et al., 2016) and CIFAR-FS (Bertinetto et al., 2019). In cross- domain few-shot learning (Chen et al., 2019), domain shift exists between train and test-time tasks, and several later benchmarks such as BSCD-FSL (Guo et al., 2020) and Meta-dataset (Triantaﬁllou et al., 2020) both target at such setting. Recently, the shift of granularities of categories has been considered as another type of task distribution shift, and is also called Coarse-to-Fine Few-Shot (C2FS) Learn- ing (Luo et al., 2021a; Bukchin et al., 2021; Yang et al., 2021a), which trains a model on coarse-labeled images and tests on few-shot tasks that aim at distinguishing between ﬁne-grained subclasses of training categories. Our work reveals that all three types of task distribution shift have a similar phenomenon of channel bias problem. The inﬂuence of task distribution shift on FSL has been ﬁrstly studied in (Doersch et al., 2020). They ﬁnd that the representation constructed by meta-learning algorithms can- not capture useful discriminative information outside of the training categories. They solve this problem by highlightingChannel Importance Matters in Few-Shot Image Classiﬁcation the crucial spatial information for classiﬁcation, using a cross-attention module between support and query features in new tasks. The algorithm COSOC (Luo et al., 2021b) also considers ﬁltering task-irrelevant spatial information, but it achieves it more directly. They identify image back- ground as harmful information in both training and testing and design a method to remove the background, in order to reduce the difﬁculty of category transfer. The perspective of our work is different, not focusing on discriminative spatial positions of features, but orthogonally taking inspections on discriminative channel information of features. Test-time methods for FSL. The presented three methods in Section 4 represent three types of mainstream algorithms in FSL. (1) Finetuning—Optimization-based algorithms, originated mainly from MAML (Finn et al., 2017), opti- mizes both the learned feature extractor and classiﬁer to- gether at test-time. Most work that fall into this type use meta-learning to train the network (Rusu et al., 2019; Ra- jeswaran et al., 2019; Zintgraf et al., 2019; Park & Oliva, 2019). When training adopts conventional supervised ap- proaches, the method turns to resemble transfer learning approaches, and is adopted in (Dhillon et al., 2020) and BiT (Kolesnikov et al., 2020). In the experiments of Section 4, we notice that in few-shot settings, although alleviating channel bias problem, ﬁne-tuning method performs gener- ally worse and may require very different hyperparameters for different test-time datasets to avoid overﬁtting, which is impossible to achieve in a realistic few-shot scenario, thus we believe ﬁnetuning would not be the best test-time choice. (2) NCC—metric-based algorithms (Vinyals et al., 2016; Snell et al., 2017; Zhang et al., 2020; Hou et al., 2019; Do- ersch et al., 2020) that aim at learning a well-shaped feature space equipped with a distance metric for comparing the sim- ilarity of images, on which the test-time prediction depends. Metric-based methods, as we have shown, beneﬁt from in- ductive bias given by the metric and thus are widely adopted in state-of-the-art algorithms. (3) LC—most conventionally trained methods adopt LC as the test-time methods (Chen et al., 2019; Mangla et al., 2020; Tian et al., 2020; Liu et al., 2020; Rizve et al., 2021), and two meta-learning algorithms MetaOpt (Lee et al., 2019) and ANIL (Raghu et al., 2020) use LC in both training and testing. The importance of a good quality of image representation is mainly ﬁgured out from this line of work. Other feature transformations in FSL. LFT (Tseng et al., 2020) introduces learnable channel-wise feature transforma- tions into training for cross-domain few-shot learning. The transformations are put inside backbone, instead of on top of representations, and are only used at train time, learned in a learning-to-learn fashion using multiple domains of datasets. Z-score transformation upon image representations is introduced in (Fei et al., 2021) for solving the hubness problem of image representations in FSL. CCF (Xu et al., 2021) proposes a variant of variational autoencoder to trans- form features, which utilizes category relationship between training and test-time classes to rectify the feature distribu- tions. Feature-wise linear modulation (FiLM) (Perez et al., 2018) that turns scaling and shifting coefﬁcients in batch normalization layer (seen as parameters of a linear feature transformation) into dataset- or task-dependent learnable parameters has been adopted in several FSL algorithms (Ore- shkin et al., 2018; Requeima et al., 2019; Triantaﬁllou et al., 2021; Li et al., 2022). The core idea of these methods is to only tune the FiLM modules at test time in order to reduce overﬁtting. Thus these methods in some sense belong to ﬁnetuning-based methods, and have the potential to perform better than vanilla ﬁnetuning in low-shot settings. Contrary to our work, all methods discussed above do not discover or target at the channel bias problem. The most relevant method to our paper may be ConFeSS (Das et al., 2022), a framework that masks task-irrelevant channels in image representations at test time for cross-domain few-shot learn- ing. Our work shows that the success of ConFeSS may be attributed to alleviating the channel bias problem by aban- doning overconﬁdent channels when transferred to novel tasks. 6. Conclusion In this paper, we reveal the channel bias problem in few-shot image classiﬁcation. The problem can be alleviated by a simple channel-wise feature transformation presented in this work. This transformation, used at test-time without adding any computation overhead, can be applied to most pre- trained convolutional neural networks and few-shot learning algorithms. We show it serves as prior knowledge that regu- larizes the channel distribution of features. Further analysis, including a derivation of the oracle MMC adjustment, ana- lyzes comprehensively the channel bias problem. We hope that the channel bias problem revealed in this work, along with analysis of different test-time methods, can provide the community with a better understanding of task distribution shift and representation transfer in few-shot classiﬁcation, which may in turn help produce better algorithms. Acknowledgments Special thanks to Qi Yong for providing indispensable spir- itual support for the work. We also would like to thank all reviewers for very constructive comments that help us improve the paper. This work was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204), and a key program of fun- damental research from Shenzhen Science and Technology Innovation Commission (No. JCYJ20200109113403826).Channel Importance Matters in Few-Shot Image Classiﬁcation References Agarwal, M., Yurochkin, M., and Sun, Y . On sensitivity of meta-learning to support data. In Advances in Neural Information Processing Systems, 2021. Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541–6549, 2017. Bertinetto, L., Henriques, J. F., Torr, P. H. S., and Vedaldi, A. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations, 2019. Bukchin, G., Schwartz, E., Saenko, K., Shahar, O., Feris, R., Giryes, R., and Karlinsky, L. Fine-grained angular contrastive learning with coarse labels. In IEEE Confer- ence on Computer Vision and Pattern Recognition , pp. 8730–8740, 2021. Cantelli, F. P. Sui conﬁni della probabilita. In Atti del Congresso Internazionale dei Matematici: Bologna del 3 al 10 de settembre di 1928, pp. 47–60, 1929. Chen, W., Liu, Y ., Kira, Z., Wang, Y . F., and Huang, J. A closer look at few-shot classiﬁcation. In International Conference on Learning Representations, 2019. Chen, Y ., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta- baseline: exploring simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9062–9071, 2021. Das, D., Yun, S., and Porikli, F. Confess: A framework for single source cross-domain few-shot learning. In International Conference on Learning Representations, 2022. Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto, S. A baseline for few-shot image classiﬁcation. In Inter- national Conference on Learning Representations, 2020. Doersch, C., Gupta, A., and Zisserman, A. Crosstransform- ers: spatially-aware few-shot transfer. In Advances in Neural Information Processing Systems, 2020. Fei, N., Gao, Y ., Lu, Z., and Xiang, T. Z-score normalization, hubness, and few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 142–151, 2021. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning, pp. 1126–1135, 2017. Guo, Y ., Codella, N., Karlinsky, L., Codella, J. V ., Smith, J. R., Saenko, K., Rosing, T., and Feris, R. A broader study of cross-domain few-shot learning. In European Conference on Computer Vision, pp. 124–141, 2020. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016. He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729– 9738, 2020. Horn, G. V ., Aodha, O. M., Song, Y ., Cui, Y ., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. J. The inaturalist species classiﬁcation and detection dataset. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8769–8778, 2018. Hou, R., Chang, H., Ma, B., Shan, S., and Chen, X. Cross attention network for few-shot classiﬁcation. In Advances in Neural Information Processing Systems , pp. 4005– 4016, 2019. Hu, J., Shen, L., and Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132–7141, 2018. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In European Conference on Computer Vision, pp. 491–507, 2020. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1106–1114, 2012. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta- learning with differentiable convex optimization. InIEEE Conference on Computer Vision and Pattern Recognition, pp. 10657–10665, 2019. Li, W.-H., Liu, X., and Bilen, H. Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7161–7170, 2022. Liu, B., Cao, Y ., Lin, Y ., Li, Q., Zhang, Z., Long, M., and Hu, H. Negative margin matters: Understanding margin in few-shot classiﬁcation. In European Conference on Computer Vision, pp. 438–455, 2020.Channel Importance Matters in Few-Shot Image Classiﬁcation Luo, X., Chen, Y ., Wen, L., Pan, L., and Xu, Z. Boosting few-shot classiﬁcation with view-learnable contrastive learning. In IEEE International Conference on Multime- dia and Expo (ICME), pp. 1–6, 2021a. Luo, X., Wei, L., Wen, L., Yang, J., Xie, L., Xu, Z., and Tian, Q. Rectifying the shortcut learning of background for few-shot learning. In Advances in Neural Information Processing Systems, 2021b. Mangla, P., Singh, M., Sinha, A., Kumari, N., Balasubra- manian, V . N., and Krishnamurthy, B. Charting the right manifold: Manifold mixup for few-shot learning. InIEEE Winter Conference on Applications of Computer Vision, pp. 2207–2216, 2020. Oreshkin, B. N., L ´opez, P. R., and Lacoste, A. TADAM: task dependent adaptive metric for improved few-shot learning. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 719–729, 2018. Park, E. and Oliva, J. B. Meta-curvature. In Advances in Neural Information Processing Systems, pp. 3309–3319, 2019. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V ., et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, pp. 2825–2830, 2011. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1406–1415, 2019. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. Film: Visual reasoning with a general con- ditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. In International Conference on Learning Representations, 2020. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems , pp. 113–124, 2019. Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. Fast and ﬂexible multi-task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019. Rizve, M. N., Khan, S. H., Khan, F. S., and Shah, M. Explor- ing complementary strengths of invariant and equivariant representations for few-shot learning. In IEEE Confer- ence on Computer Vision and Pattern Recognition , pp. 10836–10846, 2021. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M. S., Berg, A. C., and Li, F. Imagenet large scale visual recognition challenge. In IJCV, volume 115, pp. 211–252, 2015. Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. Meta-learning with latent embedding optimization. In International Confer- ence on Learning Representations, 2019. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explana- tions from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017. Snell, J., Swersky, K., and Zemel, R. S. Prototypical net- works for few-shot learning. In Advances in Neural In- formation Processing Systems, pp. 4077–4087, 2017. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. Rethinking few-shot image classiﬁcation: A good embedding is all you need? InEuropean Conference on Computer Vision, pp. 266–282, 2020. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man- zagol, P., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020. Triantaﬁllou, E., Larochelle, H., Zemel, R. S., and Du- moulin, V . Learning a universal template for few-shot dataset generalization. In Proceedings of the 38th Inter- national Conference on Machine Learning, pp. 10424– 10433, 2021. Tseng, H., Lee, H., Huang, J., and Yang, M. Cross-domain few-shot classiﬁcation via learned feature-wise transfor- mation. In International Conference on Learning Repre- sentations, 2020. Tukey, J. W. et al. Exploratory data analysis , volume 2. Reading, MA, 1977. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.Channel Importance Matters in Few-Shot Image Classiﬁcation Xu, J., Pan, X., Luo, X., Pei, W., and Xu, Z. Exploring category-correlated feature for few-shot image classiﬁca- tion. arXiv preprint arXiv:2112.07224, 2021. Yang, J., Yang, H., and Chen, L. Towards cross-granularity few-shot learning: Coarse-to-ﬁne pseudo-labeling with visual-semantic meta-embedding. In ACM Multimedia Conference, pp. 3005–3014, 2021a. Yang, S., Liu, L., and Xu, M. Free lunch for few-shot learn- ing: Distribution calibration. In International Conference on Learning Representations, 2021b. Ye, H. and Chao, W. How to train your maml to excel in few-shot classiﬁcation. In International Conference on Learning Representations, 2022. Zagoruyko, S. and Komodakis, N. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016. Zhang, C., Cai, Y ., Lin, G., and Shen, C. Deepemd: Few- shot image classiﬁcation with differentiable earth mover’s distance and structured classiﬁers. In IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pp. 12200–12210, 2020. Zhou, B., Khosla, A., Lapedriza, `A., Oliva, A., and Tor- ralba, A. Object detectors emerge in deep scene cnns. In International Conference on Learning Representations, 2015. Zintgraf, L. M., Shiarlis, K., Kurin, V ., Hofmann, K., and Whiteson, S. Fast context adaptation via meta-learning. In Proceedings of the 36th International Conference on Machine Learning, pp. 7693–7702, 2019.Channel Importance Matters in Few-Shot Image Classiﬁcation A. Proof of Proposition 3.1 Lemma A.1. (Cantelli’s inequality(Cantelli, 1929)) Let X be a random variable with ﬁnite expected value µand ﬁnite non-zero variance σ2. Then for any k> 0, P(X−µ≥kσ) ≤ 1 1 +k2 . (7) Lemma A.2. Let ai >0,bi >0,i = 1,...,D . Deﬁne f : [0,+∞)D/{0}→ R by f(x) = ∑D i=1 bix2 i (∑D i=1 aixi)2 , (8) then min x f(x) = 1 ∑D i=1 a2 i bi . (9) The mimimum value is reached when there exists a constant c> 0, such that ∀i∈[D],xi = aic bi . Proof. We show it by induction on dimension D. Denote the domain of f by U, i.e., U = [0,+∞)D/{0}. When D= 1, f(x) ≡b1 a2 1 is a constant, so the result holds. Assume that when D ≤ k, the result holds. We now prove that when D = k + 1, the result holds. It is obvious that ∀c >0,f(cx) = f(x), thus it sufﬁces to ﬁnd a minimum point in S = {x|a ≤||x||2 ≤b and x ∈U}for any chosen 0 < a < b. Since Sis a closed set and f is continuous, the minimum point exists. The minimum point either lies on the hyperspheres: ∂S = {x|||x||2 = a and x ∈ U}∪{ x|||x||2 = b and x ∈ U}or in between: S= {x|a< ||x||2 <b and x∈U}. If there exists a minimum point on one of the hyperspheres, say, the outer hypersphere {x|||x||2 = band x∈U}, then there exists another minimum point (a+b)x 2b ∈S (or (a+b)x 2a ∈S for the inner hypersphere). Thus it sufﬁces to ﬁnd the minimum point in S. Let S/i = {x|x∈S and xi = 0}and S>0 = {x|x∈S and xi >0 for alli∈[k+ 1]}. We have S= (∪k+1 i=1 S/i) ∪S>0. If x∈S/i, then f(x) = ∑i−1 j=1 bjx2 j + ∑k+1 j=i+1 bjx2 j (∑i−1 j=1 ajxj + ∑k+1 j=i+1 ajxj)2 , (10) which can be seen as a function with input dimension k. Thus from the induction, we have min x∈S/i f(x) = 1 ∑i−1 j=1 a2 j bj + ∑k+1 j=i+1 a2 j bj . (11) Next, we handle the setting when x∈S>0, i.e., ﬁnd all possible extreme points of f inside S>0. Note that ∂f ∂xi = 2(bixi ∑k+1 j=1 ajxj −ai ∑k+1 j=1 bjx2 j) (∑k+1 j=1 ajxj)3 , (12) then an extreme point xmust satisfy bixi k+1∑ j=1 ajxj −ai k+1∑ j=1 bjx2 j = 0,∀i∈[k+ 1], (13) which is equivalent to xi = ai ∑k+1 j=1 bjx2 j bi ∑k+1 j=1 ajxj = ( ∑k+1 j=1 bjx2 j ∑k+1 j=1 ajxj )ai bi ,∀i∈[k+ 1]. (14)Channel Importance Matters in Few-Shot Image Classiﬁcation Thus xi = cai bi , where c= ∑k+1 j=1 bjx2 j∑k+1 j=1 ajxj . Furthermore, it is easy to show that xi = cai bi satisﬁes Eq. (14) for any c> 0. Denote any of the points satisfying this property as x∗, then f(x∗) = 1 ∑k+1 i=1 a2 i bi . (15) Comparing Eq. (11) and Eq. (15), it can be seen that f(x∗) < min x∈S/i f(x),∀i∈[k+ 1]. (16) Finally, note that ∂S and {S/i}k+1 i=1 constitute the boundary of S>0, thus Eq. (16) and earlier discussion about ∂S indicate that x∗is the minimum point of f, as desired. Proposition 3.1. Assume that µ1,l ̸= µ2,l and σ1,l + σ2,l >0 hold for any l∈[d], then we have R≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . (17) To minimize this upper bound, the adjusted oracle MMC of each channelωl should satisfy: ωl ∝|µ1,l −µ2,l| σ1,l + σ2,l . (18) Proof. R= 1 2[Pz1∼D1 (||ω⊙(˜z1 −˜µ1)||2 >||ω⊙(˜z1 −˜µ2)||2) +Pz2∼D2 (||ω⊙(˜z2 −˜µ2)||2 >||ω⊙(˜z2 −˜µ1)||2)] = 1 2[Pz1∼D1 ( d∑ l=1 ω2 l[(˜z1,l −˜µ1,l)2 −(˜z1,l −˜µ2,l)2] >0) +Pz2∼D2 ( d∑ l=1 ω2 l[(˜z2,l −˜µ2,l)2 −(˜z2,l −˜µ1,l)2] >0)] = 1 2[Pz1∼D1 ( d∑ l=1 ω2 l(1 −˜z1,l)(˜µ1,l −˜µ2,l) >0) +Pz2∼D2 ( d∑ l=1 ω2 l(1 −˜z2,l)(˜µ2,l −˜µ1,l) >0)] ≤ 2 ∑d l=1 ω4 l(˜µ1,l −˜µ2,l)2(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [Applying Lemma A.1] ≤ 2 ∑d l=1 ω4 l(˜µ1,l + ˜µ2,l)2(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [˜µ1,l˜µ2,l ≥0] = 8 ∑d l=1 ω4 l(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [Standadization: (˜µ1,l + ˜µ2,l)/2 = 1] ≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . [˜σ1,l˜σ2,l ≥0] (19) Let xl = ω2 l,al = (˜µ1,l−˜µ2,l)2,bl = (˜σ1,l+ ˜σ2,l)2, then according to Lemma A.2, the minimum value of the upper bound (19) is reached when ω2 l ∝(˜µ1,l−˜µ2,l)2 (˜σ1,l+˜σ2,l)2 = (µ1,l−µ2,l)2 (σ1,l+σ2,l)2 , i.e., ωl ∝|µ1,l−µ2,l| σ1,l+σ2,l . B. Training and Evaluation Details For S2M2 and MoCo-v2 in Table 1, we directly use the ofﬁcial publicly-available pre-trained checkpoints. All other algorithms in Table 1 are trained using a learning rate 0.1 with cosine decay schedule without restart. SGD with momentum 0.9 is adopted as the optimizer. For all meta-learning algorithms, a total of 60000 5-way 5-shot tasks are sampled for training,Channel Importance Matters in Few-Shot Image Classiﬁcation 0.5 1.0 1.5 2.0 Value of k 0.5 1.0 1.5 2.0 2.5 (ACC)% miniImageNet 0.5 1.0 1.5 2.0 Value of k 0.0 0.5 1.0 1.5 2.0 2.5 3.0 (ACC)% CUB 0.5 1.0 1.5 2.0 Value of k 3.4 3.6 3.8 4.0 4.2 (ACC)% T extures 0.5 1.0 1.5 2.0 Value of k 0.75 1.00 1.25 1.50 1.75 2.00 2.25 (ACC)% Traffic Signs 0.5 1.0 1.5 2.0 Value of k 5.5 6.0 6.5 7.0 7.5 8.0 (ACC)% Plant Disease 0.5 1.0 1.5 2.0 Value of k 2.0 2.2 2.4 2.6 2.8 3.0 (ACC)% ISIC 0.5 1.0 1.5 2.0 Value of k 4.4 4.6 4.8 5.0 5.2 5.4 (ACC)% EuroSAT 0.5 1.0 1.5 2.0 Value of k 3.8 4.0 4.2 4.4 4.6 4.8 5.0 (ACC)% Sketch 0.5 1.0 1.5 2.0 Value of k 3.50 3.75 4.00 4.25 4.50 4.75 5.00 5.25 (ACC)% QuickDraw 0.5 1.0 1.5 2.0 Value of k 2.50 2.75 3.00 3.25 3.50 3.75 (ACC)% Fungi 0.5 1.0 1.5 2.0 Value of k 1.1 1.2 1.3 1.4 1.5 1.6 (ACC)% Aircraft 0.5 1.0 1.5 2.0 Value of k 6.0 6.5 7.0 7.5 8.0 (ACC)% Omniglot 0.5 1.0 1.5 2.0 Value of k 4.4 4.6 4.8 5.0 5.2 5.4 (ACC)% VGG Flower 0.5 1.0 1.5 2.0 Value of k 0.5 0.0 0.5 1.0 (ACC)% MSCOCO 0.5 1.0 1.5 2.0 Value of k 0.40 0.45 0.50 0.55 0.60 (ACC)% ChestX 0.5 1.0 1.5 2.0 Value of k 3.75 4.00 4.25 4.50 4.75 5.00 5.25 (ACC)% Clipart 0.5 1.0 1.5 2.0 Value of k 2.50 2.75 3.00 3.25 3.50 3.75 4.00 (ACC)% Infograph 0.5 1.0 1.5 2.0 Value of k 1.75 2.00 2.25 2.50 2.75 3.00 (ACC)% Real 0.5 1.0 1.5 2.0 Value of k 2.0 2.2 2.4 2.6 2.8 3.0 3.2 (ACC)% Painting Figure 9.The effect of the hyperparameter kin the simple transformation on each test-time dataset. The feature extractor is trained on miniImageNet using PN. All experiments are conducted on 10000 5-way 5-shot tasks sampled with a ﬁxed seed. each of which contains 15 query images per class. The batch size (number of sampled tasks of each iteration) is 4. All other hyperparameters of MetaOpt match the default settings in the original paper. All conventionally-trained algorithms are trained for 60 epochs, and the batch size is set to 128. For the training of the CE (Cross-Entropy) algorithm, we normalize the representation before the fully-connected layer. We ﬁnd that if we do not normalize the representation during the training of CE, the simple transformation does not work. We leave it for future work to investigate this phenomenon. For the test-time linear classiﬁcation method we implement for MoCo and S2M2 in Table 1 and Figure 8, we adopt the Logistic Regression implementation of scikit-learn (Pedregosa et al., 2011). C. The Effect of Hyperparameter k In Figure 9, we show how the hyperparameter k in Eq. (1) inﬂuences the few-shot classiﬁcation performance. On all datasets, As the k becomes larger, the accuracy ﬁrst increases and then decreases. The optimal value of k varies for different datasets, ranging from 0.6 to 1.8. That being said, the simple transformation gives a relatively stable performance improvement on all datasets when k∈[1,2]. Notably, datasets with larger task distribution shift often give a smaller optimal k. This phenomenon is reasonable because as seen from Figure 2, a smaller kleads to a larger smoothing effect, and the transformation can better rectify the channel distribution when the task distribution shift is also larger. D. Attempts at Handling Negative Output Values of Neural Networks As shown in Section 3, the MMC of image representations can represent the emphasis of neural network on different channels. However, things get complicated if the output of the neural network can take negative values. If the value of a channel represents the activation of a feature, it is difﬁcult to say whether a large negative value means a large or small activation. At this time, we consider negative value as a signal of feature activation as well, which leads to the followingChannel Importance Matters in Few-Shot Image Classiﬁcation Table 4.Performance gains when applying the extended version of the simple transformation to ResNet-12 with Leaky ReLU trained on mini-train. Algorithm mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Avg PN 76.0+0.5 59.3+0.6 62.0+0.7 66.3-0.2 78.2+2.8 38.1+1.3 75.1+0.8 52.7+0.3 66.5+2.9 55.4+0.0 63.0+1.0 CE 79.4+0.2 64.5+0.8 66.1-0.2 69.9+0.1 84.9+2.1 40.1+0.1 77.6+0.4 53.6+0.4 72.1+3.7 57.4+1.1 66.6+0.9 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Aircraft Original Simple Oracle 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008MMC after transformation Omniglot Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation VGG Flower Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation MSCOCO Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation ChestX Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Clipart Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation Infograph Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035 0.0040MMC after transformation Real Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation Painting Original Simple Oracle Figure 10.The Visualization of MMC of the other nine datasets before and after the use of simple and oracle transformation.All notations are the same as in Figure 6. simple extension of the transformation: φk(λ) = { sign(λ) lnk( 1 |λ| +1) , |λ|>0 0, λ = 0 (20) now this extended simple transformation can be applied directly to the standard ResNet-12 using leaky ReLU, and the results are shown in Table 4. While leaky ReLU improves the basic performance compared to vanilla ReLU, the improvement of the simple transformation becomes signiﬁcantly smaller. We conjecture that a large negative magnitude of a channel does not strictly mean that this channel is important. It is future work to investigate how to exactly measure channel importance in such circumstances. E. Necessary Ingredients for a Good Transformation One may ask that whether all of the three properties presented in Eq. (3) are necessary for a transformation to successfully improve few-shot learning performance, or whether there exist good transformations other than φk(λ) considered in the main article. To verify the necessity of all properties, we design several functions, each of which does not satisfy one of the properties. First, we consider the function p(λ) =ln(aλ+ 1), where a> 0. This function has positive derivative and negative second derivative, but does not have large enough derivative near zero (p′(0) =a). In the left plot of Figure 11, we see that the improvement brought by this function is smaller than φ1.3(λ), and that the gap becomes smaller when a increases. This validates the necessity of having a large enough derivative near zero. We then consider the piece-wise function q(λ) = { φk(λ), 0 ≤λ<λ 0 a2λ2 + a1λ+ a0, λ ≥λ0 (21) where the values of a2,a1,a0 ensure the smoothness of q(λ) at λ= λ0 up to ﬁrst derivative, and also control the position of the extreme point x0 = −a1 2a2 . This function does not have positive derivative when λ ≥x0. We set x0 = 0.05, and change the value of λ0. The results are shown in the second plot in Figure 11. As seen, introducing negative derivative into the transformation substantially degrades performance. Finally, the property of having negative second derivative can be naturally broken by increasing the value of kin φk(λ), and Figure 9 shows that doing this would degrade performance.Channel Importance Matters in Few-Shot Image Classiﬁcation 0 2000 4000 6000 8000 10000 Value of a 56 57 58 59ACC(%) p( ) 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 Value of 0 56 57 58 59ACC(%) q( ) 0.00 0.01 0.02 0.03 0.04 Value of r 56 57 58 59ACC(%) 1.3( )+r 0.2 0.4 0.6 0.8 1.0 Value of k 56 57 58 59ACC(%) g( ) Figure 11.Exploration of necessary ingredients for a good channel-wise transformation. The accuracies show the average 5-way 5-shot performance over all 19 datasets using PN trained on miniImageNet. The red dashed line shows the original performance; the green dashed line shows the performance when using the simple transformation φ1.3(λ). The leftmost plot shows the performance when using the function p(λ) =ln(aλ+ 1). The second plot shows the performance when using the piece-wise function q(λ). The third plot shows the performance when adding a constant rto the simple transformation φ1.3(λ). The rightmost plot shows the performance when using the power function g(λ) =xk. Since inactivate channels may represent absence of a feature in a task instead of having low emphasis, thus they are likely to have no importance. Therefore another property φk(0) = 0, not shown in Eq. (3), could also be important for a good transformation. To investigate this, we add a constant rto φk(λ) and see how the performance would change. The third plot in Figure 11 shows the opposite result: a small constant added to the transformation helps further improve the performance. As adding this constant has more inﬂuence on small-valued channels, we conjecture that this helps further alleviate the channel bias problem, and that some inactivate channels indeed should gain some focus. Apparently, φk(λ) is not the only function that satisﬁes all the three properties. We consider the power function g(λ) =λk, where k >0. Although being very simple, this function matches all desired properties. The rightmost plot in Figure 11 shows that this function can indeed improve the performance as well. Note that this function has been used in (Yang et al., 2021b), where it is called the Tukey’s Ladder of Powers transformation (Tukey et al., 1977), and is used to transform the feature distribution to be more like a Gaussian distribution. Here we show that mitigating the channel bias problem may be another reason for why it works. F. Details of MMC Calculation and Comparison F.1. Oracle Transformation To apply the oracle transformation, for every test-time dataset D, we ﬁrst calculate feature mean µc and variance σc of each class cin D. Then for every sampled binary classiﬁcation task τ = {Sτ,Qτ}that aims at discriminating two classes c1 and c2, we calculate the oracle MMC ωdirectly from Eq. (6). Next, we standardize each image feature zin Sτ and Qτ, and multiply it by ωto obtain the transformed feature z←ω⊙˜z. The transformed features can be already used for classiﬁcation, but we ﬁnd that for some channels with very small means µl, the corresponding value of oracle MMC ωl becomes too big, deviating from what we expect. To avoid generating such outliers, we additionally restrict that for every channel l, the ratio of transformed MMC to the original MMC should not surpass a threshold, i.e., ωl/ωo l ≤αfor some α∈R+. If a channel ldoes not meet this requirement, we simply set ωl = ωo l. In all of our experiments, we set α= 50. The optimal MMC visualizied in Figure 6 and Figure 10 is also computed using this strategy. We leave it for future work to investigate the reason behind such phenomenon. F.2. Choice of Distance Measure The normalized mean square difference d(x,y) =1 d ∑d l=1(xl −yl)2/x2 l has the advantage of having equal treatment for both channels with small and large values. However, it can be largely inﬂuenced by “outlier channel” with very small xl. Since dataset-level MMC ωD is averaged over MMCs of all possible tasks in D, it is more stable and can use such distance measure. The task-level MMC and image features have much higher variances acorss tasks/images, thus for these two ﬁne-grained settings we just use the mean square difference d(x,y) =1 d ∑d l=1(xl −yl)2. This, however, introduces another problem that critical changes of channels with small values are always ignored by such unnormalized distance. Let’s see a simple example. Let ω1 = (0.05,0.08,0.87) and ω2 = (0.4,0.3,0.3) be two 3-dimensional l1-normalized MMCs. Assume that after transformation, their l1-normalized values become ω′ 1 = (0.15,0.1,0.75) and ω′ 2 = (0.55,0.22,0.23). The value of the ﬁrst dimension of ω1 triples and surpasses that of the second dimension after transformation, thus theChannel Importance Matters in Few-Shot Image Classiﬁcation Table 5.Found best hyperparameters of the test-time ﬁnetuning method on miniImageNet. The feature extrator is ResNet-12, trained by S2M2 algorithm. Shot Batch size Number of epochs Learning rate 1 5 10 0.1 5 25 30 0.05 10 50 50 0.05 20 50 100 0.02 50 64 100 0.01 100 64 500 0.005 400 64 500 0.005 Table 6.The inﬂuence of using different seeds during training or testing. The feature extractor is trained by PN on mini-train. Average 5-way 5-shot performance gains brought by the simple transformation on 10 datasets with 95% conﬁdence interval (over 5 trials) are shown. Seed mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Test +2.36±0.06 +2.98±0.03 +4.26±0.10 +2.37±0.06 +8.04±0.14 +2.80±0.10 +5.50±0.09 +5.02±0.09 +5.46±0.14 +3.90±0.11 Train +2.08±0.33 +3.28±0.43 +3.78±0.77 +2.00±0.89 +8.21±0.80 +3.64±0.44 +4.03±1.37 +4.21±0.93 +7.01±2.62 +3.59±0.50 channel emphasis changes substantially, while the channel emphasis of ω2 does not change much. We expect that the distance measuring the change of ω1 should be much larger than that measuring the change of ω2. The normalized mean square differences between the MMC before and after transformation are 1.36 and 0.09 for ω1 and ω2 respectively, which is in line with our intuition. However, the mean square differences are0.008 and 0.011 for ω1 and ω2 respectively. Thus under such circumstances, the normalized mean square difference is a much better choice. Although being simple, ω2 and ω1 are a good analogy to the MMC pattern on miniImageNet and some other datasets in Figure 6, respectively. In Figure 6, we can see that most MMC values on miniImageNet are around mid-level, which resembles ω2; most MMC values on other datasets are either very small or large, which resembles ω1. This explains why in Table 3 the task-level and image-level differences on miniImageNet are not smaller than those on other datasets. We leave it for future work to ﬁnd a distance measure that could avoid unstable results, while being sensitive to small-valued channels. G. More Details on Fine-tuning Based Method For ﬁne-tuning methods in Figure 8, we grid search the best hyperparameters in each shot setting on the test set. All best conﬁgurations are shown in Table 5. As seen, the hyperparameters of ﬁne-tuning methods are very sensitive to the number of shots. In low-shot settings, care should be taken for controlling the total steps of ﬁnetuning and learning rate, in order to avoid overﬁtting. This phenomenon is also shown in (Ye & Chao, 2022), where the authors show that MAML (Finn et al., 2017), one of the most widely adopted ﬁnetuning-based methods, has a much higher optimal test-time ﬁne-tuning steps than expected. H. Error Bars All results regarding performance in the main paper are shown without error bars. In Table 6, we show how different seeds affect the improvement brought by the simple transformation φk(λ). There are two seeds that could inﬂuence the result, one for training, and one for testing. When considering test seed, we ﬁx the feature extractor and use different seeds to sample tasks; when considering train seed, we ﬁx the test seed (same tasks) and evaluate different feature extractors trained with different seeds. As seen, while varying the test seed hardly affect the performance, varying the train seed produces some ﬂuctuations. After considering the ﬂuctuations, the improvement given by the transformation can still be statistically guaranteed.",
      "references": [],
      "meta_data": {
        "arxiv_id": "2206.08126v2",
        "authors": [
          "Xu Luo",
          "Jing Xu",
          "Zenglin Xu"
        ],
        "published_date": "2022-06-16T12:38:45Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies a simple test-time channel-wise feature transformation that improves few-shot image classification under task distribution shift; reveals a channel bias problem where different tasks rely on different channels while CNNs treat channel importances as fixed; introduces Mean Magnitude of Channels (MMC) as a lens to study channel importance; derives an oracle MMC that optimally reweights channels for binary tasks; demonstrates that the test-time transformation smooths channel magnitudes, moves MMCs toward the oracle, and yields substantial generalization gains across datasets, architectures, and training algorithms (5-way 5-shot, across 19 test datasets, with k≈1.3); provides extensive analysis, ablations, and code.",
        "methodology": "Proposes a per-channel nonlinear transformation φ_k(·) applied to features after global pooling: φ_k(λ) = (ln(k(1/λ+1))) for λ>0 and 0 for λ=0, extended to negative-valued activations in an appendix; apply at test-time to all channels regardless of task; analyzes MMC and channel bias; derives oracle MMC ω_l ∝ |µ1,l−µ2,l|/(σ1,l+σ2,l) for binary tasks; evaluates with NCC, LC, and fine-tuning across supervised, meta-learning, and contrastive learned features; performs shot analysis; visualizes Grad-CAM; uses datasets Meta-dataset, BSCD-FSL, DomainNet; code at GitHub.",
        "experimental_setup": "Training data: miniImageNet train split, ImageNet-1K train, iNaturalist 2018 train+val; Test data: miniImageNet test split plus 19 datasets from Meta-dataset, BSCD-FSL, DomainNet; Task setup: 5-way, 5-shot tasks; evaluation: average accuracy over 10k tasks per dataset; baselines: PN, Meta-baseline, MetaOpt, S2M2, Prototypical Networks, MoCo-v2; backbones: Conv-4, ResNet-12, WRN-28-10, ResNet-50, SE-ResNet-50; test-time classifiers: NCC for CE, LC for S2M2 and MoCo-v2, each meta-learning method using its own tester; hyperparameter k fixed at 1.3; 5-way 5-shot across 19 datasets; additional analyses: shot counts, seeds, Leaky ReLU variant; code link.",
        "limitations": "Limitations: gains are most pronounced under test-time distribution shift and can be modest or negative in in-distribution tasks; relies on non-negative activations (ReLU) or extended handling for negatives; the oracle MMC is not available in practice and is used for analysis; performance depends on the hyperparameter k and choice of distance measure; normalization during CE training is required for some setups; applicability to very few-shot regimes and different tasks is demonstrated but not exhaustively proven; may not fully address background/spurious correlations in all domains.",
        "future_research_directions": "Investigate learning to estimate task-specific channel importance without oracle, e.g., meta-learning a lightweight per-task channel weighting or gating; extend channel-wise transformation to negative activations more robustly; combine with FiLM, cross-attention, or background filtering for improved cross-domain transfer; study dynamic or per-task adaptation of k; apply to other modalities (audio, text, multimodal tasks) and to unsupervised/self-supervised settings; analyze interaction with distribution calibration and background removal methods; develop training-time strategies to reduce reliance on test-time transformations and to stabilize cross-domain generalization.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization",
      "full_text": "Published as a conference paper at ICLR 2021 SALIENCY MIX: A S ALIENCY GUIDED DATA AUG- MENTATION STRATEGY FOR BETTER REGULARIZA - TION A. F. M. Shahab Uddin∗ uddin@khu.ac.kr Mst. Sirazam Monira∗ monira@khu.ac.kr Wheemyung Shin∗ wheemi@khu.ac.kr TaeChoong Chung∗† tcchung@khu.ac.kr Sung-Ho Bae∗† shbae@khu.ac.kr ABSTRACT Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. How- ever, such information removal is undesirable. On the other hand, recent strate- gies suggest to randomly cut and mix patches and their labels among train- ing images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selec- tion strategies of the patches may not necessarily represent sufﬁcient informa- tion about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature repre- sentation. Therefore, we propose SaliencyMix that carefully selects a repre- sentative image patch with the help of a saliency map and mixes this indica- tive patch with the target image, thus leading the model to learn more appro- priate feature representation. SaliencyMix achieves the best known top-1 error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on ImageNet classiﬁcation, respectively, and also improves the model robustness against ad- versarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix. 1 I NTRODUCTION Machine learning has achieved state-of-the-art (SOTA) performance in many ﬁelds, especially in computer vision tasks. This success can mainly be attributed to the deep architecture of convolu- tional neural networks (CNN) that typically have 10 to 100 millions of learnable parameters. Such a huge number of parameters enable the deep CNNs to solve complex problems. However, besides the powerful representation ability, a huge number of parameters increase the probability of overﬁtting when the number of training examples is insufﬁcient, which results in a poor generalization of the model. In order to improve the generalization ability of deep learning models, several data augmentation strategies have been studied. Random feature removal is one of the popular techniques that guides the CNNs not to focus on some small regions of input images or on a small set of internal activations, thereby improving the model robustness. Dropout (Nitish et al., 2014; Tompson et al., 2015) and regional dropout (Junsuk & Hyunjung, 2019; Terrance & Graham, 2017; Golnaz et al., 2018; Singh & Lee, 2017; Zhun et al., 2017) are two established training strategies where the former randomly turns off some internal activations and later removes and/or alters random regions of the input im- ages. Both of them force a model to learn the entire object region rather than focusing on the most ∗Department of Computer Science & Engineering, Kyung Hee University, South Korea. †Corresponding author. 1 arXiv:2006.01791v2  [cs.LG]  27 Jul 2021Published as a conference paper at ICLR 2021 Target Image Source Image Dog - 80% ? Cat - 20% ? Augmented Image  Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Target Image Source Image Dog - 80% ? Cat - 20% ? Augmented Image  Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Target ImageSource Image Dog - 80% ? Cat - 20% ? Augmented Image Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Augmented Image Target Image Mixed label for randomly mixed images Source Image Dog - 80% & Cat 20% ? Dog - 80% & Cat 20% ? 20% Figure 1: Problem of randomly selecting image patch and mixing labels according to it. When the selected source patch does not represent the source object, the interpolated label misleads the model to learn unexpected feature representation. important features and thereby improving the generalization of the model. Although dropout and regional dropout improve the classiﬁcation performance, this kind of feature removal is undesired since they discard a notable portion of informative pixels from the training images. Recently, Yun et al. (2019) proposed CutMix, that randomly replaces an image region with a patch from another training image and mixes their labels according to the ratio of mixed pixels. Unlike Cutout (Devries & Taylor, 2017), this method can enjoy the properties of regional dropout without having any blank image region. However, we argue that the random selection process may have some possibility to select a patch from the background region that is irrelevant to the target objects of the source image, by which an augmented image may not contain any information about the corresponding object as shown in Figure 1. The selected source patch (background) is highlighted with a black rectangle on the source image. Two possible augmented images are shown wherein both of the cases, there is no information about the source object (cat) in the augmented images despite their mixing location on the target image. However, their interpolated labels encourage the model to learn both objects’ features (dog and cat) from that training image. But we recognize that it is undesirable and misleads the CNN to learn unexpected feature representation. Because, CNNs are highly sensitive to textures (Geirhos et al., 2019) and since the interpolated label indicates the selected background patch as the source object, it may encourage the classiﬁer to learn the background as the representative feature for the source object class. We address the aforementioned problem by carefully selecting the source image patch with the help of some prior information. Speciﬁcally, we ﬁrst extract a saliency map of the source image that highlights important objects and then select a patch surrounding the peak salient region of the source image to assure that we select from the object part and then mix it with the target image. Now the selected patch contains relevant information about the source object that leads the model to learn more appropriate feature representation. This more effective data augmentation strategy is what we call, ”SaliencyMix”. We present extensive experiments on various standard CNN architectures, benchmark datasets, and multiple tasks, to evaluate the proposed method. In summary, SaliencyMix has obtained the new best known top-1 error of 2.76% and 16.56% for WideResNet (Zagoruyko & Komodakis, 2016) on CIFAR-10 and CIFAR-100 (Krizhevsky, 2012), respectively. Also, on Ima- geNet (Olga et al., 2015) classiﬁcation problem, SaliencyMix has achieved the best known top-1 and top-5 error of 21.26% and 5.76% for ResNet-50 and 20.09% and 5.15% for ResNet-101 (He et al., 2016). In object detection task, initializing the Faster RCNN (Shaoqing et al., 2015) with Salien- cyMix trained model and then ﬁne-tuning the detector has improved the detection performance on Pascal VOC (Everingham et al., 2010) dataset by +1.77 mean average precision (mAP). Moreover, SaliencyMix trained model has proved to be more robust against adversarial attack and improves the top-1 accuracy by 1.96% on adversarially perturbed ImageNet validation set. All of these re- sults clearly indicate the effectiveness of the proposed SaliencyMix data augmentation strategy to enhance the model performance and robustness. 2 R ELATED WORKS 2.1 D ATA AUGMENTATION The success of deep learning models can be accredited to the volume and diversity of data. But col- lecting labeled data is a cumbersome and time-consuming task. As a result, data augmentation has 2Published as a conference paper at ICLR 2021 been introduced that aims to increase the diversity of existing data by applying various transforma- tions e.g., rotation, ﬂip, etc. Since this simple and inexpensive technique signiﬁcantly improves the model performance and robustness, data augmentation has widely been used to train deep learning models. Lecun et al. (1998) applied data augmentation to train LeNet for hand written character recognition. They performed several afﬁne transformations such as translation, scaling, shearing, etc. For the same task, Bengio et al. (2011) applied more diverse transformation such as Gaussian noise, salt and pepper noise, Gaussian smoothing, motion blur, local elastic deformation, and various occlusions to the images. Krizhevsky et al. (2012) applied random image patch cropping, horizontal ﬂipping and random color intensity changing based on principal component analysis (PCA). In Deep Image (Wu et al., 2015), color casting, vignetting, and lens distortion are applied besides ﬂipping and cropping to improve the robustness of a very deep network. Besides these manually designed data augmentations, Lemley et al. (2017) proposed an end-to-end learnable augmentation process, called Smart Augmentation. They used two different networks where one is used to learn the suitable augmentation type and the other one is used to train the actual task. Devries & Taylor (2017) proposed Cutout that randomly removes square regions of the input training images to improve the robustness of the model. Zhang et al. (2017) proposed MixUp that blends two training images to some degree where the labels of the augmented image are assigned by the linear interpolation of those two images. But the augmented images look unnatural and locally ambiguous. Recently, Cubuk et al. (2019) proposed an effective data augmentation method called AutoAugment that deﬁnes a search space of various augmentation techniques and selects the best suitable one for each mini-batch. Kim et al. (2020) proposed PuzzleMix that jointly optimize two objectives i.e., selecting an optimal mask and an optimal mixing plan. The mask tries to reveal most salient data of two images and the optimal transport plan aims to maximize the saliency of the revealed portion of the data. Yun et al. (2019) proposed CutMix that randomly cuts and mixes image patches among training samples and mixes their labels proportionally to the size of those patches. However, due to the randomness in the source patch selection process, it may select a region that does not contain any informative pixel about the source object, and the label mixing according to those uninformative patches misleads the classiﬁer to learn unexpected feature representation. In this work, the careful selection of the source patch always helps to contain some information about the source object and thereby solves the class probability assignment problem and helps to improve the model performance and robustness. 2.2 L ABEL SMOOTHING In object classiﬁcation, the class labels are usually represented by one-hot code i.e., the true labels are expected to have the probability of exactly 1 while the others to have exactly 0. In other words, it suggests the model to be overconﬁdent which causes overﬁtting to training dataset. As a result, the models have low performance on unseen test dataset. To alleviate this problem, label smoothing allows to relax the model conﬁdence on the true label by setting the class probability to a slightly lower value e.g., lower than 1. As a result, it guides the model to be more adaptive instead of being over-conﬁdent and ultimately improves the model robustness and performance (Szegedy et al., 2016). Our method also mixes the class labels and enjoys the beneﬁt of label smoothing. 2.3 S ALIENCY DETECTION Saliency detection aims to simulate the natural attention mechanism of human visual system (HVS) and can be classiﬁed into two main categories. The ﬁrst one is a bottom-up approach (Cheng et al., 2014; Zhu et al., 2014; Li et al., 2015; Zhou et al., 2015; Achanta et al., 2009; Li et al., 2013; Hou & Zhang, 2007; Qin et al., 2015; Peng et al., 2016; Lei et al., 2016; Montabone & Soto, 2010) that focuses on exploring low-level vision features. Some visual priors that are inspired by the HVS properties are utilized to describe a salient object. Cheng et al. (2014) utilized a contrast prior and proposed a regional contrast based salient object detection algorithm. Zhu et al. (2014) introduced a robust background measure in an optimization framework to integrate multiple low level cues to obtain clean and uniform saliency maps. Li et al. (2015) optimized the image boundary selection by a boundary removal mechanism and then used random walks ranking to formulate pixel-wised saliency maps. Zhou et al. (2015) proposed a saliency detection model where the saliency infor- mation is propagated using a manifold ranking diffusion process on a graph. In addition, some 3Published as a conference paper at ICLR 2021 Saliency  Detection  Peak Salient Region Source Image Saliency  Detection Peak Salient  Region  Target ImageSource Patch  Augmented Image Source Image  Saliency  Map of  the Source Image Selecting the Peak  Salient Region of  the Saliency Map  Target Image Selecting the  Source Patch Based  on the Peak Salient  Region  Augmented Image Mixing the Source  Patch with the  Target Image Source Image  Saliency  Map of  the Source Image Selecting the Peak  Salient Region of  the Saliency Map  Target Image Selecting the  Source Patch Based  on the Peak Salient  Region  Augmented Image Mixing the Source  Patch with the  Target Image Figure 2: The proposed SaliencyMix data augmentation. We ﬁrst extract the saliency map of the source image that highlights the regions of interest. Then we select a patch around the peak salient pixel location and mix it with the target image. traditional techniques are also introduced to achieve image saliency detection, such as frequency domain analysis (Achanta et al., 2009), sparse representation (Li et al., 2013), log-spectrum (Hou & Zhang, 2007) cellular automata (Qin et al., 2015), low-rank recovery (Peng et al., 2016), and Bayesian theory (Lei et al., 2016). Hou & Zhang (2007) proposed a spectral residual method that focuses on the properties of background. Achanta et al. (2009) proposed a frequency tuned approach that preserves the boundary information by retaining sufﬁcient amount of high frequency contents. Montabone & Soto (2010) introduced a method that was originally designed for a fast human detec- tion in a scene by proposing novel features derived from a visual saliency mechanism. Later on this feature extraction mechanism was generalized for other forms of saliency detection. The second one is a top-down approach which is task-driven and utilizes supervised learning with labels. Several deep learning based methods have been proposed for saliency detection (Deng et al., 2018; Liu et al., 2018; Zhang et al., 2018a;b; Qin et al., 2019). Deng et al. (2018) proposed a recur- rent residual reﬁnement network (R3Net) equipped with residual reﬁnement blocks (RRBs) to more accurately detect salient regions. Contexts play an important role in the saliency detection task and based on that Liu et al. (2018) proposed a pixel-wise contextual attention network, called PiCANet, to learn selectively attending to informative context locations for each pixel. Zhang et al. (2018a) in- troduced multi-scale context-aware feature extraction module and proposed a bi-directional message passing model for salient object detection. Zhang et al. (2018b) focused on powerful feature extrac- tion and proposed an attention guided network which selectively integrates multi-level contextual information in a progressive manner. Recently, Qin et al. (2019) proposed a predict and reﬁne ar- chitecture for salient object detection called boundary-aware saliency detection network (BASNet). The author introduced a hybrid loss to train a densely supervised Encoder-Decoder network. However, despite the high performance of top-down approach, there is a lack of generalization for various applications since they are biased towards the training data and limited to the speciﬁc objects. In this study, we require a saliency model to focus on the important object/region in a given scene without knowing their labels. As a result, we rely on bottom-up approach which are unsupervised, scale-invariant and more robust for unseen data. It is worth noting that the training based saliency methods can also be applied where the quality and quantity of data for training the saliency methods may be correlated with the effectiveness of data augmentation. Section 3.3 further explains the effects of different saliency detection algorithm on the proposed data augmentation method. 3 P ROPOSED METHOD Similar to Yun et al. (2019), we cut a source patch and mix it to the target image and also mix their labels proportionally to the size of the mixed patches. But in order to prevent the model from learning any irrelevant feature representation, the proposed method enforces to select a source patch in a way so that it must contains information about the source object. It ﬁrst extracts a saliency map of the source image to highlight the objects of interest and then selects a patch surrounding the peak salient region to mix with the target image. Here we explain the process in detail. 3.1 S ELECTION OF THE SOURCE PATCH The goal of saliency detection is to ﬁnd out the pixels or regions that are attractive to the HVS and to assign them with higher intensity values (Cong et al., 2019). A saliency detection method produces the visual saliency map, a gray-scale image, that highlights the objects of interest and thereby mostly 4Published as a conference paper at ICLR 2021 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Saliency Detection Method 40 35 30 25 20 15 10 5 0 Saliency Detection Method 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Source and Target Patch Mixing Style 4.5 CIFAR10 Tiny ImageNet CIFAR10 Tiny ImageNet 40 35 30 25 20 15 10 5 0 Source and Target Patch Mixing Style 45 (a) (b) (c) (d) 5.0 4.5 Figure 3: The effect of using different saliency detection methods in the proposed data augmentation on (a) CIFAR10 and (b) Tiny-ImageNet classiﬁcation tasks. Different ways of selection and mixing of the source patch with the target image and their effects on (c) CIFAR 10 and (d) Tiny-ImageNet classiﬁcation tasks. Performance is reported from the average of ﬁve and three runs for CIFAR10 and Tiny-ImageNet, respectively. focuses on the foreground. Let Is ∈RW×H×C is a randomly selected training (source) image with label ys from which a patch will be cut. Then its saliency map detection can be represented as Ivs = f(Is), (1) where Ivs ∈RW×H represents the visual saliency map of the given source image Is as shown in Figure 2 where the objects of interest have higher intensity values and f(·) represents a saliency detection model. Then we search for a pixel Ii,j vs in the saliency map that has the maximum intensity value. The i,j represent the xand ycoordinates of the most salient pixel and can be found as i,j = argmax(Ivs), (2) Then we select a patch, either by centering on theIi,j vs −thpixel if possible, or keeping theIi,j vs −th pixel on the selected patch. It ensures that the patch is selected from the object region, not from the background. The size of the patch is determined based on a combination ratio λwhich is sampled from the uniform distribution (0,1) to decide the percentage of an image to be cropped. 3.2 M IXING THE PATCHES AND LABELS Let It ∈RW×H×C is another randomly selected training (target) image with label yt, to where the source patch will be mixed. SaliencyMix partially mixes It and Is to produce a new training sample Ia, the augmented image, with label ya. The mixing of two images can be deﬁned as Ia = M ⊙Is + M′⊙It, (3) where Ia denotes the augmented image, M ∈ {0,1}W×H represents a binary mask, M′ is the complement of M and ⊙represents element-wise multiplication. First, the source patch location is deﬁned by using the peak salient information and the value ofλand then the corresponding location of the mask M is set to 1 and others to 0. The element-wise multiplication of M with the source image results with an image that removes everything except the region decided to keep. In contrast, M′performs in an opposite way of M i.e., the element-wise multiplication of M′with the target image keeps all the regions except the selected patch. Finally, the addition of those two creates a new training sample that contains the target image with the selected source patch in it (See Figure 2). Besides mixing the images we also mix their labels based on the size of the mixed patches as ya = λyt + (1−λ)ys, (4) where ya denotes the label for the augmented sample and λis the combination ratio. Other ways of mixing are investigated in Section 3.4. 3.3 I MPACT OF DIFFERENT SALIENCY DETECTION METHODS Here we investigate the effect of incorporating various saliency detection methods in our Salien- cyMix data augmentation technique. We use four well-recognized saliency detection algorithms 5Published as a conference paper at ICLR 2021 Table 1: Classiﬁcation performance (average of ﬁve runs) of SOTA data augmentation methods on CIFAR-10 and CIFAR-100 datasets using popular standard architectures. An additional ”+” sign after the dataset name indicates that the traditional data augmentation techniques have also been used during training. METHOD TOP-1 ERROR(%)CIFAR-10 CIFAR-10+ CIFAR-100 CIFAR-100+ RESNET-18 (BASELINE) 10.63 ±0.26 4.72±0.21 36.68±0.57 22.46±0.31RESNET-18 + CUTOUT 9.31±0.18 3.99 ±0.13 34.98±0.29 21.96±0.24RESNET-18 + CUTMIX 9.44±0.34 3.78 ±0.12 34.42±0.27 19.42±0.23RESNET-18 + SALIENCYMIX 7.59±0.22 3.65±0.10 28.73±0.13 19.29±0.21 RESNET-50 (BASELINE) 12.14 ±0.95 4.98 ±0.14 36.48±0.50 21.58±0.43RESNET-50 + CUTOUT 8.84±0.77 3.86 ±0.25 32.97±0.74 21.38±0.69RESNET-50 + CUTMIX 9.16±0.38 3.61 ±0.13 31.65±0.61 18.72±0.23RESNET-50 + SALIENCYMIX 6.81±0.30 3.46±0.08 24.89±0.39 18.57±0.29 WIDERESNET-28-10 (BASELINE) 6.97 ±0.22 3.87 ±0.08 26.06±0.22 18.80±0.08WIDERESNET-28-10 + CUTOUT 5.54±0.08 3.08 ±0.16 23.94±0.15 18.41±0.27WIDERESNET-28-10 + AUTOAUGMENT - 2.60±0.10 - 17.10 ±0.30WIDERESNET-28-10 + PUZZLEMIX(200EPOCHS) - - - 16.23WIDERESNET-28-10 + CUTMIX 5.18±0.20 2.87 ±0.16 23.21±0.20 16.66±0.20WIDERESNET-28-10 + SALIENCYMIX 4.04±0.13 2.76±0.07 19.45±0.32 16.56±0.17 (Montabone & Soto, 2010; Hou & Zhang, 2007; Achanta et al., 2009; Qin et al., 2019), and perform experiments using ResNet-18 as a baseline model on CIFAR-10 dataset and ResNet-50 as a baseline model on Tiny-ImageNet dataset for 200 epochs and 100 epochs, respectively. Note that the statis- tical saliency models (Montabone & Soto, 2010; Hou & Zhang, 2007; Achanta et al., 2009) work on any size of images. But the learning based model i.e., Qin et al. (2019) are not scale invariant and it should resizes the input image to 224 ×224 and the resulting saliency map is scaled back to the original size. Figure 3(a-b) show that Montabone & Soto (2010) performs better on both the datasets and the effects are identical on CIFA10 and ImageNet datasets. As a result, Montabone & Soto (2010) is used to extract the saliency map in the proposed data augmentation technique. 3.4 D IFFERENT WAYS OF SELECTING AND MIXING THE SOURCE PATCH There are several ways to select the source patch and mix it with the target image. In this section, we explore those possible schemes and examine their effect on the proposed method. We use ResNet- 18 and ResNet-50 architectures with SaliencyMix data augmentation and perform experiments on CIFAR-10 and Tiny-ImageNet datasets, respectively. We consider ﬁve possible schemes:(i) Salient to Corresponding, that selects the source patch from the most salient region and mix it to the corre- sponding location of the target image; (ii) Salient to Salient, that selects the source patch from the most salient region and mix it to the salient region of the target image; (iii) Salient to Non-Salient, that selects the source patch from the most salient region but mix it to the non-salient region of the target image; (iv) Non-Salient to Salient, that selects the source patch from the non-salient region of the source image but mix it to the salient region of the target image; and (v) Non-Salient to Non- Salient, that selects the source patch from the non-salient region of the source image and also mix it to the non-salient region of the target image. To ﬁnd out the non-salient region, we use the least important pixel of an image. Figure 3(c-d) show the classiﬁcation performance of the proposed SaliencyMix data augmentation with the above mentioned selection and mixing schemes. Similar to the Section 3.3, the effects of the proposed method are similar for CIFAR10 and Tiny-ImageNet datasets. Both the Non-Salient to Salient and Non-Salient to Non-Salient select the source patch from the non-salient region of the source image that doesn’t contain any information about the source object and thereby produce large classiﬁcation error compared to the other three options where the patch is selected from the most salient region of the source image. It justiﬁes our SaliencyMix i.e., the source patch should be selected in such a way so that it must contain information about the source object. On the other hand, Salient to Salient covers the most signiﬁcant part of the target image that restricts the model from learning its most important feature andSalient to Non-Salient may not occlude the target object which is necessary to improve the regularization. But Salient to Corresponding keeps balance by changeably occluding the most important part and other based on the orientation of the source and target object. Consequently, it produces more variety of augmented data and thereby achieves the lowest classiﬁcation error. Also, it introduces less computational burden since only the source image saliency detection is required. Therefore, the proposed method uses Salient to Corresponding as the default selection and mixing scheme. 6Published as a conference paper at ICLR 2021 Table 2: Performance comparison (the best performance) of SOTA data augmentation strategies on ImageNet classiﬁcation with stan- dard model architectures. METHOD TOP-1 T OP-5 ERROR(%) E RROR(%) RESNET-50 (BASELINE) 23.68 7.05 RESNET-50 + CUTOUT 22.93 6.66 RESNET-50 + STOCHASTICDEPTH 22.46 6.27 RESNET-50 + MIXUP 22.58 6.40 RESNET-50 + MANIFOLDMIXUP 22.50 6.21 RESNET-50 + AUTOAUGMENT 22.40 6.20 RESNET-50 + DROPBLOCK 21.87 5.98 RESNET-50 + CUTMIX 21.40 5.92 RESNET-50 + PUZZLEMIX 21.24 5.71 RESNET-50 + SALIENCYMIX 21.26 5.76 RESNET-101 (BASELINE) 21.87 6.29 RESNET-101 + CUTOUT 20.72 5.51 RESNET-101 + MIXUP 20.52 5.28 RESNET-101 + CUTMIX 20.17 5.24 RESNET-101 + SALIENCYMIX 20.09 5.15 Table 3: Impact of SaliencyMix trained model on transfer learning to object detec- tion task. The results are reported from the average of three runs. BACKBONENETWORK IMAGENET DETECTION CLS. ERR. (F-RCNN) TOP-1 (%) ( MAP) RESNET-50 (BASELINE) 23.68 76.71 (+0.00) CUTOUT-TRAINED 22.93 77.17 (+0.46) MIXUP-TRAINED 22.58 77.98 (+1.27) CUTMIX-TRAINED 21.40 78.31 (+1.60) SALIENCYMIX-TRAINED 21.26 78.48 (+1.77) 4 E XPERIMENTS We verify the effectiveness of the proposed SaliencyMix data augmentation strategy on multiple tasks. We evaluate our method on image classiﬁcation by applying it on several benchmark image recognition datasets using popular SOTA architectures. We also use the SaliencyMix trained model and ﬁne-tune it for object detection task to verify its usefulness in enhancing the detection perfor- mance. Furthermore, we validate the robustness of the proposed method against adversarial attacks. All experiments were performed on PyTorch platform with four NVIDIA GeForce RTX 2080 Ti GPUs. 4.1 I MAGE CLASSIFICATION 4.1.1 CIFAR-10 AND CIFAR-100 There are 60,000 color images of size 32×32 pixels in both the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2012) where CIFAR-10 has 10 distinct classes and CIFAR-100 has 100 classes. The number of training and test images in each dataset is 50,000 and 10,000, respectively. We apply several standard architectures: a deep residual network (He et al., 2016) with a depth of18 (ResNet- 18) and 50 (ResNet-50), and a wide residual network (Zagoruyko & Komodakis, 2016) with a depth of 28, a widening factor of 10, and dropout with a drop probability of p= 0.3 in the convolutional layers (WideResNet-28-10). We train the networks for 200 epochs with a batch size of 256 using stochastic gradient descent (SGD), Nesterov momentum of 0.9, and weight decay of 5e−4. The initial learning rate was 0.1 and decreased by a factor of 0.2 after each of the 60,120, and 160 epochs. The images are normalized using per-channel mean and standard deviation. We perform experiments with and without a traditional data augmentation scheme where the traditional data augmentation includes zero-padding, random cropping, and horizontal ﬂipping. Table 1 presents the experimental results on CIFAR datasets where the results are reported on ﬁve runs average. It can be seen that for each of the architectures, the proposed SaliencyMix data aug- mentation strategy outperforms all other methods except PuzzleMix (Kim et al., 2020). It is worth noting that PuzzleMix (Kim et al., 2020) and AutoAugment (Cubuk et al., 2019) require additional optimization process to ﬁnd out the best augmentation criterion, thereby introduces computational burden. On the other hand, rest of the methods do not require such process. The proposed method achieves the best known top-1 error of2.76% and 16.56% for WideResNet-28-10 on CIFAR-10 and CIFAR-100 datasets, respectively. Moreover, SaliencyMix shows signiﬁcant performance improve- ment over CutMix (Yun et al., 2019) when applied without any traditional augmentation technique. It reduces the error rate by1.85%,2.35%,and 1.14% on CIFAR-10 dataset when applied with ResNet- 18, ResNet-50 and WideResNet-28-10 architectures, respectively. Using the same architectures, it reduces the error rate by 5.69%,6.76%,and 3.76% on CIFAR-100 dataset, respectively. 4.1.2 I MAGE NET ImageNet (Olga et al., 2015) contains 1.2 million training images and 50,000 validation images of 1000 classes. To perform experiments on ImageNet dataset, we apply the same settings as used in 7Published as a conference paper at ICLR 2021 Baseline (ResNet50) Cutout Mixup CutMix SaliencyMix Vizsla Mountain Tent Marmot (a) CAM visualizations on un-augmented images. The proposed data augmentation method guides the model to precisely focus on target object. CAM for   Golden  Retriever  CAM for   Tiger Cat  Augmented  Image Mixup Cutout Cutmix SaliencyMix Golden Retriever Tiger Cat Target ImageSource Image (b) CAM visualizations on augmented images. Left most column shows the original images, top row shows the input augmented images by different meth- ods, middle and bottom rows show the CAM for ’Golden Retriever’, and ’Tiger Cat’ class, respectively. The proposed method and Yun et al. (2019) improves the localization ability of the model. Figure 4: Class activation map (CAM) for models that are trained with various data augmentation techniques. The images are randomly taken from ImageNet (Olga et al., 2015) validation set. Yun et al. (2019) for a fair comparison. We have trained our SaliencyMix for 300 epochs with an initial learning rate of 0.1 and decayed by a factor of 0.1 at epochs 75,150,and 225, with a batch size of 256. Also, the traditional data augmentations such as resizing, cropping, ﬂipping, and jitters have been applied during the training process. Table 2 presents the ImageNet experimental results where the best performance of each method is reported. SaliencyMix outperforms all other methods in comparison and shows competitive results with PuzzleMix (Kim et al., 2020). It drops top-1 error for ResNet-50 by 1.66%,1.31%, and 0.14% over Cutout, Mixup and CutMix data augmentation, respectively. For ResNet-101 architecture, SaliencyMix achieves the new best result of 20.09% top-1 error and 5.15% top-5 error. 4.2 O BJECT DETECTION USING PRE-TRAINED SALIENCY MIX In this section, we use the SaliencyMix trained model to initialize the Faster RCNN (Shaoqing et al., 2015) that uses ResNet-50 as a backbone network and examine its effect on object detection task. The model is ﬁne-tuned on Pascal VOC 2007 and 2012 datasets and evaluated on VOC 2007 test data using the mAP metric. We follow the ﬁne-tuning strategy of the original method (Shaoqing et al., 2015). The batch size, learning rate, and training iterations are set to8,4e−3,and 41K, respectively and the learning rate is decayed by a factor of0.1 at 33Kiterations. The results are shown in Table 3. Pre-training with CutMix and SaliencyMix signiﬁcantly improves the performance of Faster RCNN. This is because in object detection, foreground information (positive data) is much more important than the background (Lin et al., 2017). Since SaliencyMix helps the augmented image to have more foreground or object part than the background, it leads to better detection performance. It can be seen that SaliencyMix trained model outperforms other methods and achieves a performance gain of +1.77 mAP. 4.3 C LASS ACTIVATION MAP (CAM) A NALYSIS Class Activation Map (CAM) (Zhou et al., 2016) ﬁnds out the regions of input image where the model focuses to recognize an object. To investigate this, we extract CAM of models that are trained with various data augmentation techniques. Here we use a vanilla ResNet-50 model equipped with various data augmentation techniques and trained on ImageNet (Olga et al., 2015). Then we extract CAM for un-augmented images as well as for augmented images. Figure 4 presents the experimental results. Figure 4a shows that the proposed data augmentation technique guides the model to precisely focus on the target object compared to others. Also, Figure 4b shows the similar effect when we search for a speciﬁc object in a scene with multiple objects. It can be seen that Mixup (Zhang et al., 2017) has a severe problem of being confused when trying to recognize an object because the pixels are mixed and it is not possible to extract class speciﬁc features. Also, Cutout (Devries & Taylor, 2017) suffers disadvantages due to the uninformative image region. On the other hand, both the CutMix (Yun et al., 2019) and SaliencyMix effectively focuses on the corresponding features and precisely localizes the two objects in the scene. 8Published as a conference paper at ICLR 2021 Table 4: Performance comparison on adversar- ial robustness. Top-1 accuracy (%) of various data augmentation techniques on adversarially perturbed ImageNet validation set. BASELINECUTOUTMIXUP CUTMIX SALIENCYMIX ACC. (%) 8.2 11.5 24.4 31.0 32.96 Table 5: Training time comparison of various data augmentation techniques using ResNet-18 architecture on CIFAR-10 dataset. BASELINECUTOUTMIXUP CUTMIX SALIENCYMIX TIME(HOUR) 0.83 0.84 0.87 0.89 0.91 4.4 R OBUSTNESS AGAINST ADVERSARIAL ATTACK Deep learning based models are vulnerable to adversarial examples i.e., they can be fooled by slightly modiﬁed examples even when the added perturbations are small and unrecognizable (Szegedy et al., 2014; Goodfellow et al., 2015; Madry et al., 2017). Data augmentation helps to increase the robustness against adversarial perturbations since it introduces many unseen image samples during the training (Madry et al., 2017). Here we verify the adversarial robustness of a model that is trained using various data augmentation techniques and compare their effectiveness. Fast Gradient Sign Method (FGSM) (Madry et al., 2017) is used to generate the adversarial examples and ImageNet pre-trained models of each data augmentation techniques with ResNet-50 architecture is used in this experiment. Table 4 reports top-1 accuracy of various augmentation techniques on adversarially attacked ImageNet validation set. Due to the appropriate feature representation learn- ing and focusing on the overall object rather than a small part, SaliencyMix signiﬁcantly improves the robustness against the adversarial attack and achieves1.96% performance improvement over the nearly comparable method CutMix (Yun et al., 2019). 4.5 C OMPUTATIONAL COMPLEXITY We investigate the computational complexity of the proposed method and compare it with other data augmentation techniques in terms of training time. All the models are trained on CIFAR-10 dataset using ResNet-18 architecture for 200 epochs. Table 5 presents the training time comparison. It can be seen that SaliencyMix requires a slightly longer training time compared to others, due to saliency map generation. But considering the performance improvement, it can be negligible. 5 C ONCLUSION We have introduced an effective data augmentation strategy, called SaliencyMix, that is carefully designed for training CNNs to improve their classiﬁcation performance and generalization ability. The proposed SaliencyMix guides the models to focus on the overall object regions rather than a small region of input images and also prevents the model from learning in-appropriate feature representation by carefully selecting the representative source patch. It introduces a little compu- tational burden due to saliency detection, while signiﬁcantly boosts up the model performance and strengthen the model robustness on various computer vision tasks. Applying SaliencyMix with WideResNet achieves the new best known top-1 error of 2.76% and 16.56% on CIFAR-10 and CIFAR-100, respectively. On ImageNet classiﬁcation, applying SaliencyMix with ResNet-50 and ResNet-101 obtains the new best known top-1 error of 21.26% and 20.09%, respectively. On ob- ject detection, using the SaliencyMix trained model to initialize the Faster RCNN (ResNet-50 as a backbone network) and ﬁne-tuning leads to a performance improvement by +1.77 mAP. Further- more, SaliencyMix trained model is found to be more robust against adversarial attacks and achieves 1.96% accuracy improvement on adversarially perturbed ImageNet validation set compared to the nearly comparable augmentation method. Considering more detailed and/or high level semantic information for data augmentation will be our future work. ACKNOWLEDGMENTS This research was supported by Basic Science Research Program through the National Re- search Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (2018R1C1B3008159) and Basic Science Research Program through the National Research Foun- dation of Korea (NRF) under Grant [NRF-020R1F1A1050014]. 9Published as a conference paper at ICLR 2021 REFERENCES R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1597–1604, June 2009. doi: 10.1109/CVPR.2009.5206596. Yoshua Bengio, Fr ´ed´eric Bastien, Arnaud Bergeron, Nicolas Boulanger–Lewandowski, Thomas Breuel, Youssouf Chherawala, Moustapha Cisse, Myriam C ˆot´e, Dumitru Erhan, Jeremy Eu- stache, Xavier Glorot, Xavier Muller, Sylvain Pannetier Lebeuf, Razvan Pascanu, Salah Rifai, Franc ¸ois Savard, and Guillaume Sicard. Deep learners beneﬁt more from out-of-distribution ex- amples. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research , pp. 164–172, Fort Laud- erdale, FL, USA, 11–13 Apr 2011. PMLR. URL http://proceedings.mlr.press/ v15/bengio11b.html. Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intelligence , 37(3):569–582, 2014. R. Cong, J. Lei, H. Fu, M. Cheng, W. Lin, and Q. Huang. Review of visual saliency detection with comprehensive information. IEEE Transactions on Circuits and Systems for Video Technology , 29(10):2941–2959, Oct 2019. ISSN 1558-2205. doi: 10.1109/TCSVT.2018.2870832. E. D. Cubuk, B. Zoph, D. Man´e, V . Vasudevan, and Q. V . Le. Autoaugment: Learning augmentation strategies from data. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 113–123, 2019. doi: 10.1109/CVPR.2019.00020. Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, and Pheng-Ann Heng. R3net: Recurrent residual reﬁnement network for saliency detection. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pp. 684–690. AAAI Press, 2018. Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. ArXiv, abs/1708.04552, 2017. Mark Everingham, Luc Van Gool, Christopher Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88:303– 338, 06 2010. doi: 10.1007/s11263-009-0275-4. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias im- proves accuracy and robustness. In International Conference on Learning Representations, 2019. Ghiasi Golnaz, Lin Tsung-Yi, and V . Le Quoc. Dropblock: A regularization method for convolu- tional networks. In Neural Information Processing Systems (NeurIPS), 2018. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint, 01 2015. URL https://arxiv.org/abs/1412.6572. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, June 2016. doi: 10.1109/CVPR.2016.90. X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8, June 2007. doi: 10.1109/CVPR.2007.383267. Choe Junsuk and Shim Hyunjung. Attention-based dropout layer for weakly supervised object localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2219–2228, 2019. Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012. 10Published as a conference paper at ICLR 2021 Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In Neural Information Processing Systems (NeurIPS), pp. 1097–1105, January 2012. doi: 10.1145/3065386. Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, Nov 1998. ISSN 1558-2256. doi: 10.1109/5.726791. Jianjun Lei, Bingren Wang, Yuming Fang, Weisi Lin, Patrick Le Callet, Nam Ling, and Chunping Hou. A universal framework for salient object detection. IEEE Transactions on Multimedia, 18 (9):1783–1795, 2016. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. Changyang Li, Yuchen Yuan, Weidong Cai, Yong Xia, and David Dagan Feng. Robust saliency detection via regularized random walks ranking. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pp. 2710–2717, 2015. Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via dense and sparse reconstruction. In Proceedings of the IEEE international conference on computer vision, pp. 2976–2983, 2013. T. Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar. Focal loss for dense object detection. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999–3007, Oct 2017. doi: 10.1109/ICCV .2017.324. Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3089–3098, 2018. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2017. Sebastian Montabone and Alvaro Soto. Human detection using a mobile platform and novel features derived from a visual saliency mechanism. Image and Vision Computing, 28(3):391–402, 2010. Srivastava Nitish, Hinton Geoffrey, Krizhevsky Alex, Sutskever Ilya, and Salakhutdinov Rus- lan. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Ma- chine Learning Research , 15:1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html. Russakovsky Olga, Deng Jia, Su Hao, Krause Jonathan, Satheesh Sanjeev, Ma Sean, Huang Zhi- heng, Karpathy Andrej, Khosla Aditya, Bernstein Michael, C. Berg Alexander, and Fei-Fei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115 (3):211–252, 2015. Houwen Peng, Bing Li, Haibin Ling, Weiming Hu, Weihua Xiong, and Stephen J Maybank. Salient object detection via structured matrix decomposition. IEEE transactions on pattern analysis and machine intelligence, 39(4):818–832, 2016. Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Yao Qin, Huchuan Lu, Yiqun Xu, and He Wang. Saliency detection via cellular automata. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 110–119, 2015. Ren Shaoqing, He Kaiming, Girshick Ross, and Sun Jian. Faster r-cnn: Towards real-time object detection with region proposal networks. In Neural Information Processing Systems (NeurIPS), 2015. 11Published as a conference paper at ICLR 2021 K. K. Singh and Y . J. Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3544–3553, 2017. C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel- low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199. DeVries Terrance and W Taylor Graham. Improved regularization of convolutional neural networks with cutout. arXiv preprint, 2017. URL https://arxiv.org/abs/1708.04552. Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann Lecun, and Christoph Bregler. Efﬁcient object localization using convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 648–656, 06 2015. doi: 10.1109/CVPR.2015.7298664. Ren Wu, Shengen Yan, Yi Shan, Qingqing Dang, and Gang Sun. Deep image: Scaling up image recognition. arXiv preprint, 01 2015. URL https://arxiv.org/abs/1501.02876. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Interna- tional Conference on Computer Vision (ICCV), 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British Ma- chine Vision Conference 2016, 2016. doi: 10.5244/c.30.87. URL http://dx.doi.org/10. 5244/C.30.87. Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em- pirical risk minimization. arXiv preprint, 2017. URL https://arxiv.org/abs/1710. 09412. Lu Zhang, Ju Dai, Huchuan Lu, You He, and Gang Wang. A bi-directional message passing model for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1741–1750, 2018a. Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu, and Gang Wang. Progressive attention guided recurrent network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 714–722, 2018b. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discrimina- tive localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2921–2929, June 2016. doi: 10.1109/CVPR.2016.319. Li Zhou, Zhaohui Yang, Qing Yuan, Zongtan Zhou, and Dewen Hu. Salient region detection via inte- grating diffusion-based compactness and local contrast. IEEE Transactions on Image Processing, 24(11):3308–3320, 2015. Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun. Saliency optimization from robust back- ground detection. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 2814–2821, 2014. Zhong Zhun, Zheng Liang, Kang Guoliang, Li Shaozi, and Yang Yi. Random erasing data augmen- tation. arXiv preprint, 2017. URL https://arxiv.org/abs/1708.04896. 12",
      "references": [],
      "meta_data": {
        "arxiv_id": "2006.01791v2",
        "authors": [
          "A. F. M. Shahab Uddin",
          "Mst. Sirazam Monira",
          "Wheemyung Shin",
          "TaeChoong Chung",
          "Sung-Ho Bae"
        ],
        "published_date": "2020-06-02T17:18:34Z",
        "github_url": "https://github.com/SaliencyMix/SaliencyMix"
      },
      "llm_extracted_info": {
        "main_contributions": "SaliencyMix: a saliency-guided data augmentation that selects a highly informative image patch around the peak salient region of a source image and mixes it into a target image, with labels blended proportionally. It addresses the randomness of patch selection in CutMix by ensuring the patch contains object-relevant information, improving feature learning, generalization, and robustness. Empirical results establish new state-of-the-art top-1 errors on CIFAR-10/100 (WideResNet-28-10: 2.76% and 16.56%), top-1/top-5 on ImageNet (ResNet-50: 21.26%/5.76%, ResNet-101: 20.09%/5.15%), plus gains in object detection ( Faster R-CNN on Pascal VOC +1.77 mAP ) and adversarial robustness (+1.96% on adversarial ImageNet). Source code released.",
        "methodology": "Patch selection uses a saliency map f(Is) to locate the peak salient pixel and center a patch around it; patch size determined by lambda ~ Uniform(0,1); augmentation Ia = M ⊙ Is + M' ⊙ It with binary mask M and label ya = lambda yt + (1−lambda) ys; experiments compare multiple saliency detectors and patch mixing schemes; default scheme is Salient to Corresponding. Saliency maps can be bottom-up unsupervised methods (Montabone & Soto 2010) or others; label smoothing effect is beneficial; aims to keep informative object region and reduce background interference.",
        "experimental_setup": "Datasets and models: CIFAR-10/100 with ResNet-18/50 and WideResNet-28-10; ImageNet with ResNet-50/101; Object detection using Faster R-CNN with ResNet-50 backbone on Pascal VOC 2007/2012; Adversarial robustness tested with FGSM on ImageNet; Training details: CIFAR: 200 epochs, batch 256, SGD with momentum 0.9, weight decay 5e-4, LR 0.1 decayed at 60/120/160; ImageNet: 300 epochs, standard augmentations; Evaluation: top-1/top-5 errors on ImageNet, mAP on VOC; Comparisons with Cutout, Mixup, CutMix, PuzzleMix; Training time: SaliencyMix marginally longer due to saliency computation.",
        "limitations": "Computational overhead due to saliency map computation; results depend on chosen saliency detector; potential sensitivity to patch size and mixing ratio; experiments limited to vision datasets and a subset of architectures; scalability to other domains or real-time augmentation not explored.",
        "future_research_directions": "Explore incorporating higher-level semantic information beyond peak saliency; test additional saliency methods and adaptive patch sizing; apply SaliencyMix to other modalities and tasks; combine with other augmentation strategies or NAS-based augmentation search; optimize saliency computation for real-time training.",
        "experimental_code": "def train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    current_LR = get_learning_rate(optimizer)[0]\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.cuda()\n        target = target.cuda()\n\n        r = np.random.rand(1)\n        if args.beta > 0 and r < args.salmix_prob:\n            # generate mixed sample\n            lam = np.random.beta(args.beta, args.beta)\n            rand_index = torch.randperm(input.size()[0]).cuda()\n            target_a = target\n            target_b = target[rand_index]\n            bbx1, bby1, bbx2, bby2 = saliency_bbox(input[rand_index[0]], lam)\n            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby2:bbx2]  # (see note below)\n            # adjust lambda to exactly match pixel ratio\n            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n            # compute output\n            input_var = torch.autograd.Variable(input, requires_grad=True)\n            target_a_var = torch.autograd.Variable(target_a)\n            target_b_var = torch.autograd.Variable(target_b)\n            output = model(input_var)\n            loss = criterion(output, target_a_var) * lam + criterion(output, target_b_var) * (1. - lam)\n        else:\n            # compute output\n            input_var = torch.autograd.Variable(input, requires_grad=True)\n            target_var = torch.autograd.Variable(target)\n            output = model(input_var)\n            loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n\n        losses.update(loss.item(), input.size(0))\n        top1.update(err1.item(), input.size(0))\n        top5.update(err5.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0 and args.verbose == True:\n            print('Epoch: [{0}/{1}]\\t'\n                  'LR: {LR:.6f}\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n                epoch, args.epochs, LR=current_LR, batch_time=batch_time,\n                data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Train Loss {loss.avg:.3f}'.format(\n        epoch, args.epochs, top1=top1, top5=top5, loss=losses))\n\n    return losses.avg\n\n\ndef saliency_bbox(img, lam):\n    size = img.size()\n    W = size[1]\n    H = size[2]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # initialize OpenCV's static fine grained saliency detector and compute the saliency map\n    temp_img = img.cpu().numpy().transpose(1, 2, 0)\n    saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n    (success, saliencyMap) = saliency.computeSaliency(temp_img)\n    saliencyMap = (saliencyMap * 255).astype(\"uint8\")\n\n    maximum_indices = np.unravel_index(np.argmax(saliencyMap, axis=None), saliencyMap.shape)\n    x = maximum_indices[0]\n    y = maximum_indices[1]\n\n    bbx1 = np.clip(x - cut_w // 2, 0, W)\n    bby1 = np.clip(y - cut_h // 2, 0, H)\n    bbx2 = np.clip(x + cut_w // 2, 0, W)\n    bby2 = np.clip(y + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n",
        "experimental_info": "Experimental settings (SaliencyMix on ImageNet in the repository):\n- Task: ImageNet-1k training with SaliencyMix, using either ResNet (and PyramidNet via provided modules) as the backbone; implemented in SaliencyMix-ImageNet/train.py and tested/validated in SaliencyMix-ImageNet/test.py.\n- Patch generation: patches are centered on the peak salient region determined by OpenCV's saliency detector. saliency_bbox(img, lam) computes a bounding box around the most salient point; patch size is determined by cut ratio cut_rat = sqrt(1 - lam) where lam is sampled from Beta(beta, beta).\n- Mixing logic: with probability salmix_prob (args.salmix_prob), a mixed sample is created by selecting a random permutation rand_index, taking the corresponding inputs as candidates for the patch, and replacing a patch in the original input with a patch from a randomly chosen image. The proportion lam is updated to match the exact pixel ratio of the patched area: lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (width * height)). The model then computes output and loss as loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1 - lam).\n- Saliency function and patch location: saliency_bbox uses cv2.saliency.StaticSaliencyFineGrained_create to compute a saliency map from a cropped image, then selects the maximum saliency location as the patch center.\n- Data handling and training settings: ImageNet data paths are set in traindir and valdir; training uses RandomResizedCrop(224), RandomHorizontalFlip, ToTensor, ColorJitter/L iluminação (as in CIFAR version), and Normalize (mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]).\n- Hyperparameters (via CLI): --beta controls the Beta distribution for lam; --salmix_prob controls the probability of applying SaliencyMix on a given batch. Other training parameters follow standard ImageNet-1k setup (batch size 128, data parallel across GPUs, cross-entropy loss)."
      }
    },
    {
      "title": "The Effects of Regularization and Data Augmentation are Class Dependent",
      "full_text": "The Eﬀects of Regularization and Data Augmentation are Class Dependent Randall Balestriero1, L´ eon Bottou1, and Yann LeCun1,2 1Meta AI Research, 2NYU {rbalestriero,leonb,ylecun}@fb.com Codes, full result tables, and pre-trained model weights will be released soon Abstract Regularization is a fundamental technique to prevent over-ﬁtting and to improve generalization performances by constraining a model’s complexity. Current Deep Networks heavily rely on regu- larizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demon- strate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the “barn spi- der” classiﬁcation test accuracy falls from 68% to 46% only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -averaged over all classes and samples- has left us with models and regularizers that silently sacriﬁce performances on some classes. This scenario can become dan- gerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from 70% to 30% on class #8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question. 1 Introduction Machine learning and deep learning aim at learning systems to solve as accurately as possible a given task at hand (LeCun et al., 1998; Bishop and Nasrabadi, 2006; Jordan and Mitchell, 2015). This process often takes the form of being given a ﬁnite training set and a performance measure, optimizing the system’s parameters e.g. from gradient updates, and assessing the system’s performance on test set samples, i.e. samples that were not used during the system optimization. As the training set is ﬁnite, and the optimal design of the system is unknown, it is common to employ regularization during the optimization phase to reduce over-ﬁtting (Tikhonov, 1943; Tihonov, 1963) i.e. to decrease the system’s performance gap between train set and test set samples (Simard et al., 1991; Chapelle et al., 2000; Bottou, 2012; Neyshabur et al., 2014). Data-Augmentation (DA) is a data-driven and informed regularization strategy that artiﬁcially increase the number of training samples (Shorten and Khoshgoftaar, 2019). As opposed to most explicit reg- ularizers e.g. Tikhonov regularization (Krogh and Hertz, 1991), also denoted as weight decay, DA’s 1 arXiv:2204.03632v2  [cs.LG]  8 Apr 2022Current Deep Learning test error model complexity best on average best 1 best 0 best 2 class 1 class 0 class 2 best average test error of δ with poor generalization on class 0 Ideal Complexity Measure test error model complexitybest average test error of γ ≪δ with similar inter-class generalization agreement on the optimal complexity class 1 class 0 class 2 Figure 1: Structural risk minimization minimizes the empirical risk of several models with varying complexity, and selects the one oﬀering the best compromise between under-ﬁtting and over-ﬁtting (Vapnik and Chervonenkis, 1974). In deep learning, controlling the model’s complexity is achieved by picking diﬀerent DN architectures and/or by applying diﬀerent levels and ﬂavors of regularization. The key observation of our study is that when the model complexity is calibrated by a regularizer such as DA (see ﬁgs. 2 to 4) or weight-decay (see ﬁg. 5), the class-conditional empirical risks do not align between classes i.e. cross-validation produces models that perform really well on the majority of classes but arbitrarily poorly on the others as depicted on the left-hand-side. In an ideal setting where the control of the model’s complexity is well aligned with the task and model, one would observe the right-hand-side scenario where the same model complexity is optimal for all classes. regularization is implicit as it is not a function of a model’s parameter, but a function of the training samples (Neyshabur et al., 2014; Hern´ andez-Garc´ ıa and K¨ onig, 2018; LeJeune et al., 2019); although some DA strategies can be turned into explicit regularizers Balestriero et al. (2022). Nevertheless, a key distinction between DA and weight decay is that DA requires more domain knowledge to be successful than weight decay. Most —if not all— of current state-of-the-art employ such regularizers (Huang et al., 2018; Chen et al., 2020b; Liu et al., 2021; Tan and Le, 2021; Liu et al., 2022). In this paper, we will demonstrate that when employing regularization such as DA or weight decay, a signiﬁcant bias is introduced into the trained model . In particular, the regularized model exhibits strong per-class favoritism i.e. while the average test accuracy over all classes improves when employing regularization, it is at the cost of the model becoming arbitrarily inaccurate on some speciﬁc classes as illustrated in ﬁg. 1. After a brief theoretical justiﬁcation on why and when DA can be the cause of bias (section 2.1), we propose a dedicated sensitivity analysis of the bias produced by diﬀerent amounts of DA in sections 2.2 and 2.3, which is followed by a similar study dedicated to weight decay in section 2.4 and transfer learning in section 2.5. We shall highlight that although we perform a class-level study, it is possible to reﬁne this entire analysis at the sample-level. For readers familiar with statistical estimation results e.g. the bias-variance trade-oﬀ (Kohavi et al., 1996; Von Luxburg and Sch¨ olkopf, 2011) or bayesian estimation e.g. Tikhonov regularization (Box and Tiao, 2011; Gruber, 2017), it should not be surprising that regularization produces bias. In fact, it is often beneﬁcial to introduce bias through regularization if it results in a signiﬁcant reduction of the estimator variance —when one aims to minimize the average empirical risk. This is one of the main reason behind the success of techniques such as ridge regression. However, what is potentially dangerous is that the bias introduced by regularization treats classes diﬀerently, including on transfer learning tasks as we will demonstrate in section 2.5. Those observations also support recent theoretical results tying a model’s performance to its robustness and to DA, as we discuss in appendix A. 22 Regularization Creates Class-Dependent Model Bias that can be Harmful even for Transfer Learning Tasks The ﬁrst part of our study focuses on DA, a technique that regularizes a model by introducing new training samples, derived from the observed ones. DA samples have been known to sometimes disregard the semantic information of the original samples (Krizhevsky et al., 2012). Nevertheless, DA remains applied universally, and fearlessly across tasks and datasets (Shorten and Khoshgoftaar, 2019) as it provides signiﬁcant performance improvements, even in semi-supervised and unsupervised settings Guo et al. (2018); Xie et al. (2020); Misra and Maaten (2020). We ﬁrst provide in sections 2.1 and 2.2 some intuition on why DA can be a source of bias regardless of the task, dataset and model at hand. We then quantify the amount of bias caused by DA in various realistic scenarios in section 2.3; and extend our analysis to weight decay in section 2.4. Finally, we conclude by demonstrating how the bias introduced by regularization transfers to downstream tasks e.g. when deploying an Imagenet ( source) trained model on the INaturalist ( target) dataset in section 2.5; that scenario is key as it demonstrates the potential harm of selecting the best performing model —on average— on the source dataset which could turn out to also be the most biased model on the target dataset class of interest. In fact, it is crucial to remember that regularization, or any other form of structural risk minimization, improves generalization performances by increasing the bias of the estimator so that the estimator’s variance is decreased by a greater amount. However, nothing guarantees the fairness of this bias i.e. for it to be equally distributed amongst the dataset classes. 2.1 When Data-Augmentation Creates Bias To provide a simple explanation on how DA causes bias in a trained model, we propose the following derivation that holds for any signal e.g. timeseries, images, videos. Without loss of generality (Hui and Belkin, 2020) we will consider here the ℓ2 loss, although the same derivation carries out with any desired metric. Dataset notations. Given a sample x ∈X with X⊂ RD, we consider y ≜ f∗(x) to be the ground-truth target value. Hence our hope is to learn an approximator fθ that is as close as possible to f∗everywhere in X, although we only observe a ﬁnite training set X ≜ {(x1,y1),..., (xN,yN)}. Data-Augmentation notations. Additionally, one employs a DA policy T : RD ×K↦→ RD such that given a transformation parameter α ∈K, Tα(x) produces the transformed view of x. Often, one also deﬁnes a density pon Kthat helps in sampling transformation parameters that are a priori known to be the most useful. Theorem 1. Whenever the transformations produced by Tα,∀α do not respect the level-set of f∗, and whenever the model has enough capacity to minimize the training loss, the DA will create irreducible bias in fθ as in ∑ (x,y)∈X Eα [ ∥y −f∗(Tα(x))∥2 2 ]    = 0 iﬀ the DAs of x are on the same level-set of f∗ >0 and ∑ (x,y)∈X Eα [ ∥y −fθ(Tα(x))∥2 2 ] = 0    zero training error =⇒ biased fθ. (1) The main idea of the proof, provided in appendix B, is to show that if a transformation does not move samples on the level-set of the true function (left-hand-side of eq. (1)), then fθ will learn a diﬀerent level (since it has 0 training error), and thus ∥f∗−fθ∥>0 i.e. fθ is biased regardless of the training set. Whenever the left-hand-side of eq. (1) is 0, the DA is denoted as label-preserving (Cui et al., 2015; Taylor and Nitschke, 2018). From the above, we see that unless the target y associated to Tα(x) is modiﬁed accordingly to encode the shift in the target function level-set produced by Tα, 3Figure 2: All results in this ﬁgure employ oﬃcial pretrained models from PyTorch with random crop DA. We present examples of an augmented image of class “bird” ( top) along with the average accuracy on the training set (dashed line) and test set (plain line) on Imagenet, using 6 popular architectures ( middle). The random crop DA seems to loose its label-preserving property when less than 30% of the image is kept in the crop. However, when looking at per-class performances we observe an entirely diﬀerent story where random crop DA can be label-preserving with only 8% of the original image for some classes, while for other classes the label information starts to reduce at around 50% as reported at the bottom along with 9 images of the corresponding classes. The CutOut and ColorJitter cases are presented in ﬁgs. 10 and 11 and exhibit the same per-class behaviors. any DA that is not label-preserving will introduce a bias . Some DAs propose to incorporate label transformation i.e. not only x but also y is augmented to better inform on the uncertainty that has been added into Tθ(x). This is for example the case for MixUp (Zhang et al., 2017), ManifoldMixUp (Verma et al., 2019), CutMix (Yun et al., 2019) and their extensions. Our goal in the next section 2.2 is to demonstrate how DAs such as random crop, color jittering, or CutOut are only label preserving for some values of αthat vary with the sample class. As a consequence, while the use of the DA improves the average test performance, it is at the cost of a signiﬁcant reduction in performance for some of the classes. 4random crop  CutOut  color jittering Figure 3: All results in this ﬁgure employ oﬃcial pretrained models from PyTorch. Reprise of the bottom left of ﬁg. 2 for three diﬀerent DAs ( each column ) and using the same 6 popular architectures (resnet50, resnet101, resnet152, densenet121, densenet201, resnext101-32x8d) ( diﬀerent lines). We observe that across DAs, diﬀerent architectures agree on the label-preserving regimes for Tα i.e. even an ensemble of model would not reduce the class-dependent bias of the ﬁnal prediction . 2.2 The Same Data-Augmentation can be Label-Preserving or Not Between Diﬀerent Classes In the previous section 2.1 we provided a general argument on the suﬃcient conditions for DA to produce a biased model. We hope in this section to provide a more concrete example that applies to current DN training. To that end, we will demonstrate that a DA can be label-preserving or not depending on the sample’s class, hence, since the same DA policy is employed for all classes, the augmented dataset will exhibit a class-imbalance in favor of the classes for which the DA is most label-preserving. To measure by how much a given DA, Tα, is label-preserving, we propose to take 6 popular architectures that are pre-trained on Imagenet (Deng et al., 2009) from the oﬃcial PyTorch (Paszke et al., 2019) repository, and to evaluate their accuracy performances for varying DA settings (top of ﬁg. 2). We observe that when considering the dataset as a whole, it is possible to identify a DA regime as a function of αfor which the amount of information present inTα(x) becomes insuﬃcient to predict the correct label on average. But more interestingly, we also take the per-class accuracy performance (bottom of ﬁg. 2) and observe that for some classes, any level of transformation α can produce augmented samples with enough information to be correctly classiﬁed, while other classes see their samples become unpredictable as soon as Tα moves away from the identity mapping. Note that we report test set performances ensuring that the observed performances are not due to ad-hoc memorization. To further ensure that the observed relation between label-preservation, sample class, and amount of transformation αis sound, we provide in ﬁg. 3 the per-class test accuracy on diﬀerent models, all exhibit the same trends. In short, we identify that when creating an augmented dataset by applying the same DA across classes, the number of per-class samples that actually contain enough information about their true labels will become largely imbalance between classes, even if the original dataset was balanced . Any model trained on the augmented dataset will thus focus on the classes for which the DA is the most label-preserving. We propose in the next section to precisely quantify the impact of DA on each dataset class. 2.3 Measuring the Average Treatment Eﬀect of Data-Augmentation on Mod- els’ Class-Dependent Bias This section aims at quantifying precisely the amount of downward or upward per-class performance shift that came as a result from using DA. We thus propose a sensitivity analysis by training a large collection 5Figure 4: All results in this ﬁgure are averaged over 20 runs and employ the oﬃcial resnet50 implementation from PyTorch that we trained on Imagenet with horizontal ﬂip and varying lower bound for the random crop DA. During training, the lower bound on the amount of the original image kept in the random crops (x-axis) varies from 100% to 8% (commonly employed value), the upper bound is always 100%, and the test images are obtained from center crops. We observe that training the same architecture but with varying random crop parameter (lower bound) provides greater average test accuracy (blue) but makes the per-class performance fall for some of the classes . Samples for each class are provided in ﬁg. 8, in the appendix. of models with varying DA policies to precisely assess the relation between DA and class-dependent model bias. First, we propose in ﬁg. 4 a sensitivity analysis by training the same architecture on Imagenet with varying DA policies. In particular, we consider a given DA (random crop in this case) and we vary the support of the parameter αwhich represents how much of the original image is kept in the crop. We train DNs using α ∈[100,τ] with τ varying from 100 to 8 and for each case, we report our metrics averaged over 20 trained models. We observe a clear relation between increase in the strength of the DA, increase in the average test accuracy overall classes, and decrease in some per-class test accuracies. For example, on a resnet50 Imagenet setting, the accuracy on the “academic gown” class goes from 62% to 40% steadily as τ decreases. We defer the same experiment but using weight decay in the next section 2.4. To further convey our claim, we now propose a formal statistical test (Neyman and Pearson, 1933; Fisher, 1955) on the hypothesis that the per-class accuracy is signiﬁcantly lower when DA is applied for those classes. A test of signiﬁcance is a formal procedure for comparing observed data with a claim or hypothesis. In our case we aim to test if the mean accuracy on class y of a model trained without DA is greater than the one obtained with DA. Due to the stochastic optimization process, this accuracy is a random variable even when the dataset is not changed. Hence we deﬁne that random variable by Accuy(X) and our null hypothesis by H0 = E[Accuy(X)] < E[Accuy(XDA)] with X the original dataset and XDA the DA dataset using random crop with α ∈[100,8]. A one-sided t-test is used and the form that does not assume equal variances is known as Welch’s t-test (Welch, 1947) with statistic t= ˆµ(X1) −ˆµ(X2)√ ˆσ2(X1) −ˆσ2(X2) , and with degrees of freedom ν = ( ˆσ2(X1) N1 + ˆσ2(X2) N2 )2 ( ˆσ4(X1) N2 1 (N2−1) + ˆσ4(X2) N2 2 (N1−1) )−1 . We obtain that there is enough evidence to reject H0 with 95% conﬁdence for 4 .4% of all the classes, and with 99% conﬁdence for 2.1% of all the classes. Hence there is suﬃcient evidence to say that the per-class test accuracies is not increased when introducing DA for 4.4% of the 1000 Imagenet classes. Note that one could formulate this problem in term of average treatment eﬀect, where the treatment is the application of DA and the outcome is the accuracy on class y of the trained model. Doing so, one could measure the per-sample bias from the Conditional Treatment Eﬀect, however we limit ourselves to a per-class study and leave 6Figure 5: All results in this ﬁgure are averaged over 10 runs and employ the oﬃcial resnet50 implementation from PyTorch that we trained on Imagenet with varying weight decay parameter. A very surprising result that we report here is that the class-dependent bias that we observed from DA also occurs with one of the most fundamental and uninformed regularizer: weight decay . We report the per-class performance when only employed horizontal ﬂip as DA and a varying weight decay parameter, and we observe clear distinct behaviors between diﬀerent classes. Samples for each class are provided in ﬁg. 9, in the appendix. We provide the same ﬁgure for DenseNet121 model in ﬁg. 12. such ﬁne-grained analysis for future work. The next section 2.4 proposes to reproduce those experiments but considering weight decay as a regularizer instead of DA. 2.4 Uninformed Weight Decay Also Creates Class-Dependent Model Bias We quantiﬁed in section 2.3 how much per-class bias was produced by DA. As per the arguments given in sections 2.1 and 2.2, it would be natural to assume that what makes DA responsible for creating class-dependent bias in DNs is our misfortune in deﬁning correct augmentation policies, and thus, that other regularization techniques that are uninformed e.g. weight decay would behave diﬀerently. The goal of this section is to demonstrate that weight decay also suﬀers from the same class-dependent behavior indicating that designing a regularizer for DNs that is fair across classes might require novel innovative solutions. The basic formulation of weight-decay consists in having any loss to be minimized Land add to it an additional term denoted as γ∥θ∥2 2 where θ collects the model’s parameters and γ is the strength of the regularization (proportional to the restriction on the trained model complexity). In general, we do not incorporate the bias term(s) within θ as this would directly remove the ability of the model to learn the natural class prior (Hastie et al., 2009). As a result, and throughout this study, we consider θ to incorporate all the DN parameters except for the ones of batch-normalization layers, as commonly done in deep learning (Leclerc et al., 2022). We report in ﬁg. 5 the per-class performance of a resnet50 trained on Imagenet with varying weight decay coeﬃcient γ (as was done for DA in ﬁg. 4) and we observe that diﬀerent classes have diﬀerent test accuracy sensitivities to variations in γ. Some will see their generalization performance increase, while others will have decreasing generalization performances. That is, even for uninformative regularizers such as weight decay a per-class bias is introduced, reducing performances for some of the classes . The next section 2.5 proposes to quantifying how much bias transfers to diﬀerent downstream tasks. 2.5 The Class-Dependent Bias Transfers to Other Downstream Tasks The last experiment we propose is to quantify the amount of class-dependent bias that transfers to other downstream tasks, a common situation in transfer learning and in system deployment to the real world 7Figure 6: All results in this ﬁgure are averaged over 10 runs and employ the oﬃcial resnet50 implementation from PyTorch that we pre-trained on Imagenet with varying random crop lower bound and then transferred to INaturalist with frozen backbone weights (only the linear classiﬁer is tuned). We observe that even when the DA is applied on a diﬀerent dataset during a pre-training phase, the trained model incorporates an inherent bias that transfers to downstream tasks i.e. when deploying a model in a transfer learning task, selecting the one with best average test accuracy on the source dataset might result in deploying a model with the most biased against the classes of interest in the target dataset . Samples for each class are provided in ﬁg. 13, in the appendix. (Pan and Yang, 2009). We thus want to measure how regularization applied during the pre-training phase on a source dataset impacts the per-class accuracy of that model on the target dataset. In order to keep the setting similar to section 2.3, we adopt a resnet50 model with random crop DA. That model is pre-trained on Imagenet dataset (source) with varying value of τ (random crop lower bound) and then, the trained model is transferred to the INaturalist dataset (Van Horn et al., 2018) (target) that consists of 10,000 classes. When transferring the model to INaturalist, the parameters are kept frozen, and only a linear classiﬁer is trained on top of it. We report in ﬁg. 6 the performance of the trained models with varying τ on diﬀerent INaturalist classes. We observe once again that the best resnet50 —on average— is not necessarily the one that should be deployed as there exists a strong per-class bias that varies with τ. As a result, picking the best performing model from a source dataset to a target dataset, might leave the pipeline to perform poorly since that model might also be the one that is the most biased against the class of interest in the target dataset. This result should motivate the design of novel regularizers that do not reduce performances between classes at diﬀerent regimes. Additionally, due to the cost of training multiple models with varying regularization settings, one might wonder on the possible alternative solutions to detect trends such as shown in ﬁg. 6 only when given a single pre-trained model. 3 Conclusion We proposed in this study to understand the impact of regularization, in particular data-augmentation and weight decay, into the ﬁnal performances of a deep network. We obtained that the use of regu- larization increases the average test performances at the cost of signiﬁcant performance drops on some speciﬁc classes. By focusing on maximizing aggregate performance statistics we have produced learning mechanisms that can be potentially harmful, especially in transfer learning tasks. In fact, we have also observed that varying the amount of regularization employed during pre-training of a speciﬁc dataset impacts the per-class performances of that pre-trained model on diﬀerent downstream tasks e.g. going from Imagenet to INaturalist. 8References Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint arXiv:1907.02893. Balestriero, R., Misra, I., and LeCun, Y. (2022). A data-augmentation is worth a thousand samples: Exact quantiﬁcation from analytical augmented sample moments. arXiv preprint arXiv:2202.08325 . Bertsekas, D. P. (2014). Constrained optimization and Lagrange multiplier methods . Academic press. Bishop, C. M. and Nasrabadi, N. M. (2006). Pattern recognition and machine learning , volume 4. Springer. Bottou, L. (2012). Stochastic gradient descent tricks. In Neural networks: Tricks of the trade , pages 421–436. Springer. Box, G. E. and Tiao, G. C. (2011). Bayesian inference in statistical analysis , volume 40. John Wiley & Sons. Chapelle, O., Weston, J., Bottou, L., and Vapnik, V. (2000). Vicinal risk minimization. Advances in neural information processing systems, 13. Chen, S., Dobriban, E., and Lee, J. (2020a). A group-theoretic framework for data augmentation. Advances in Neural Information Processing Systems , 33:21321–21333. Chen, X., Fan, H., Girshick, R., and He, K. (2020b). Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 . Cui, X., Goel, V., and Kingsbury, B. (2015). Data augmentation for deep neural network acoustic modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 23(9):1469–1477. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee. Fawzi, A., Fawzi, O., and Frossard, P. (2018). Analysis of classiﬁers’ robustness to adversarial perturba- tions. Machine learning, 107(3):481–508. Fisher, R. (1955). Statistical methods and scientiﬁc induction. Journal of the Royal Statistical Society: Series B (Methodological), 17(1):69–78. Gruber, M. H. (2017). Improving eﬃciency by shrinkage: The James-Stein and ridge regression estima- tors. Routledge. Guo, X., Zhu, E., Liu, X., and Yin, J. (2018). Deep embedded clustering with data augmentation. In Asian conference on machine learning , pages 550–565. PMLR. Hastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction , volume 2. Springer. Hern´ andez-Garc´ ıa, A. and K¨ onig, P. (2018). Data augmentation instead of explicit regularization.arXiv preprint arXiv:1806.03852. Hu, M. and Li, J. (2019). Exploring bias in gan-based data augmentation for small samples. arXiv preprint arXiv:1905.08495. Huang, G., Liu, S., Van der Maaten, L., and Weinberger, K. Q. (2018). Condensenet: An eﬃcient densenet using learned group convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2752–2761. 9Hui, L. and Belkin, M. (2020). Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. arXiv preprint arXiv:2006.07322 . Ilse, M., Tomczak, J. M., and Forr´ e, P. (2021). Selecting data augmentation for simulating interventions. In International Conference on Machine Learning , pages 4555–4562. PMLR. Iosiﬁdis, V. and Ntoutsi, E. (2018). Dealing with bias via data augmentation in supervised learning scenarios. Jo Bates Paul D. Clough Robert J¨ aschke, 24. Jaipuria, N., Zhang, X., Bhasin, R., Arafa, M., Chakravarty, P., Shrivastava, S., Manglani, S., and Murali, V. N. (2020). Deﬂating dataset bias using synthetic data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops , pages 772–773. Jordan, M. I. and Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260. Kloft, M., Brefeld, U., Laskov, P., M¨ uller, K.-R., Zien, A., and Sonnenburg, S. (2009). Eﬃcient and accurate lp-norm multiple kernel learning. In Bengio, Y., Schuurmans, D., Laﬀerty, J., Williams, C., and Culotta, A., editors, Advances in Neural Information Processing Systems , volume 22. Curran Associates, Inc. Kohavi, R., Wolpert, D. H., et al. (1996). Bias plus variance decomposition for zero-one loss functions. In ICML, volume 96, pages 275–83. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems , 25. Krogh, A. and Hertz, J. (1991). A simple weight decay can improve generalization. Advances in neural information processing systems, 4. Leclerc, G., Ilyas, A., Engstrom, L., Park, S. M., Salman, H., and Madry, A. (2022). ﬀcv. https: //github.com/libffcv/ffcv/. LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324. LeJeune, D., Balestriero, R., Javadi, H., and Baraniuk, R. G. (2019). Implicit rugosity regularization via data augmentation. arXiv preprint arXiv:1905.11639 . Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 10012–10022. Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). A convnet for the 2020s. arXiv preprint arXiv:2201.03545 . McLaughlin, N., Del Rincon, J. M., and Miller, P. (2015). Data-augmentation for reducing dataset bias in person re-identiﬁcation. In 2015 12th IEEE International conference on advanced video and signal based surveillance (AVSS), pages 1–6. IEEE. Min, Y., Chen, L., and Karbasi, A. (2021). The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In Uncertainty in Artiﬁcial Intelligence , pages 129–139. PMLR. Misra, I. and Maaten, L. v. d. (2020). Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6707– 6717. 10Neyman, J. and Pearson, E. S. (1933). Ix. on the problem of the most eﬃcient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character , 231(694-706):289–337. Neyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614 . Pan, S. J. and Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32. Phaisangittisagul, E. (2016). An analysis of the regularization between l2 and dropout in single hidden layer neural network. In 2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS), pages 174–179. IEEE. Raghunathan, A., Xie, S. M., Yang, F., Duchi, J., and Liang, P. (2020). Understanding and mitigating the tradeoﬀ between robustness and accuracy. arXiv preprint arXiv:2002.10716 . Shorten, C. and Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal of big data , 6(1):1–48. Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1991). Tangent prop-a formalism for specifying selected invariances in an adaptive network. Advances in neural information processing systems , 4. Tan, M. and Le, Q. (2021). Eﬃcientnetv2: Smaller models and faster training. In International Confer- ence on Machine Learning , pages 10096–10106. PMLR. Taylor, L. and Nitschke, G. (2018). Improving deep learning with generic data augmentation. In 2018 IEEE Symposium Series on Computational Intelligence (SSCI) , pages 1542–1547. IEEE. Tihonov, A. N. (1963). Solution of incorrectly formulated problems and the regularization method.Soviet Math., 4:1035–1038. Tikhonov, A. N. (1943). On the stability of inverse problems. In Dokl. Akad. Nauk SSSR , volume 39, pages 195–198. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. (2018). Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152 . Valentini, G. and Dietterich, T. G. (2004). Bias-variance analysis of support vector machines for the development of svm-based ensemble methods. Journal of Machine Learning Research, 5(Jul):725–775. Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. (2018). The inaturalist species classiﬁcation and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 8769–8778. Vapnik, V. and Chervonenkis, A. Y. (1974). The method of ordered risk minimization, i. Avtomatika i Telemekhanika, 8:21–30. Vapnik, V. N. and Chervonenkis, A. Y. (2015). On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity, pages 11–30. Springer. Verma, V., Lamb, A., Beckham, C., Najaﬁ, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y. (2019). Manifold mixup: Better representations by interpolating hidden states. In International Conference on Machine Learning, pages 6438–6447. PMLR. 11Von Luxburg, U. and Sch¨ olkopf, B. (2011). Statistical learning theory: Models, concepts, and results. In Handbook of the History of Logic , volume 10, pages 651–706. Elsevier. Welch, B. L. (1947). The generalization of ‘student’s’problem when several diﬀerent population varlances are involved. Biometrika, 34(1-2):28–35. Xie, Q., Dai, Z., Hovy, E., Luong, T., and Le, Q. (2020). Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems , 33:6256–6268. Xu, Y., Noy, A., Lin, M., Qian, Q., Li, H., and Jin, R. (2020). Wemix: How to better utilize data augmentation. arXiv preprint arXiv:2010.01267 . Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. (2019). Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032. Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2017). mixup: Beyond empirical risk mini- mization. arXiv preprint arXiv:1710.09412 . Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and Jordan, M. (2019). Theoretically principled trade-oﬀ between robustness and accuracy. In International conference on machine learning , pages 7472–7482. PMLR. 12Supplementary Materials A Theoretical Relations Between Data-Augmentation, Gener- alization, and a Model’s Bias and Robustness A.1 Data-Augmentation, Regularization and Structural Risk Minimization Bias-Variance Decomposition. The expected error of a trained model can always be decomposed into two terms that can be tune by altering the considered model class and regularization, and a third term which is the inherent measurement noise, that can not be reduced. In fact, the true labels are obtained from y = f(x) + ϵ with f the true, unknown, data model, and ϵ some irreducible error i.e. coming from measurements or data compression. For the Mean-Squared Error (MSE) we obtain this decomposition as Eϵ,X,D [( y −ˆf(x; D) )2] =Eϵ,X,D [ ϵ2 + ϵ ( f(x) −ˆf(x; D) ) + ( f(x) −ˆf(x; D) )2] =Var(ϵ)2 + EX [( f(x) −ED [ ˆf(x; D) ])2 −ED [ ˆf(x; D) ]2 + ED [ ˆf(x; D)2 ]] =Var(ϵ)2 + EX [( f(x) −ED [ ˆf(x; D) ])2    bias + VarD ( ˆf(x; D) )    variance ] . Although we only derive here the MSE case, the same can be obtained in the multivariate case and in classiﬁcation settings (Valentini and Dietterich, 2004; Hastie et al., 2009). When the model is with low complexity e.g. the model is under-parametrized, or the regularization is applied aggressively, the variance term becomes small and the bias term increases. Conversely, when the model is not regularized and is overparametrized, the variance term increases and the bias term reduces. One strategy to ﬁnd the best model is oﬀered by structural risk minimization. Structural Risk Minimization (SRM). As introduced by Vapnik and Chervonenkis (1974), SRM proposes a search strategy to obtain the best model. One ﬁrst chooses a class of function that ˆf must live in, hopefully informed from a priori knowledge e.g. polynomials of degree k, resnet50 DN architecture. Then, one ﬁnds a hierarchical construction of nested functional spaces that relate to the function com- plexity. For example, it could be considering polynomials of increasing degree, up to k, or considering resnet50s with weights being bounded by an increasing constant. Finally, one minimizes the empirical risk (training error) for each model and picks the one with best valid set performances. The valid set performance can be estimated empirically from a subset of the training set that has been set apart before ﬁtting, or from other measures such as the VC-dimension of the trained model (Vapnik and Chervonenkis, 2015). For example, cross-validating the weight decay parameter of a model corresponds to SRM. We make this connection precise below. From Data-Augmentation and Weight Decay to Nested Functional Spaces. Regularization is commonly employed during training to prevent overﬁtting i.e. reduce the model complexity. One might argue that regularization does not strictly restrict the model functional space, it simply favors simpler models to be used. Yet, it turns out that regularization can be cast as explicitly restricting the parameter space of the model i.e. restricting the functional space of the model. 13To see this, let’s consider the following loss function to be minimized ℓ(θ,γ) that depends of the pa- rameters θ and the dataset D. We deﬁne the following two optimization problems, one with Tikhonov regularization with amplitude γ and one with a constraint on the space of θ min θ∈RK ℓ(θ,D) + γ∥θ∥2 2, (P1) min θ∈RK:∥θ∥2 2≤β ℓ(θ,D). (P2) Let’s also denote by ˆθ(γ) the parameter value that is a global minimum of (P1); if multiple global minimum exist, pick the one with greatest ℓ2 norm. We now have to solve for θ,λ the following system that results from the Lagrangian, with λ the Lagrange multiplier (Bertsekas, 2014) and s2 the slack variable of the inequality ∇θℓ(θ,D) + 2λθ= 0 and ∥θ∥2 2 −β−s2 = 0 and 2λs= 0, setting λ←γ, θ←ˆθ(γ), and β ←∥ˆθ(γ)∥2 2,s ←0 solves the system. As a result, solving the constrained optimization problem with β = ∥ˆθ(γ)∥2 2 and solving the Tikhonov regularized problem with γ coeﬃcient is equivalent. Since we also have that ∥ˆθ(γ1)∥2 2 ≤∥ˆθ(γ2)∥2 2, we obtain that starting from a high value of Tikhonov regularization and gradually reducing it produces nested functional spaces where the model live in. More general results can be obtained in diﬀerent regularization settings in Kloft et al. (2009); Hastie et al. (2009). Moving to the case of DA, the same procedure applies. In fact, one can use the results of Phaisangittisagul (2016); LeJeune et al. (2019); Balestriero et al. (2022) to cast DAs such as dropout, and image perturbations as explicit regularizers ` a la Tikhonov and repeat the above procedure. In fact, DA can be seen as producing a model with lower variance and a bias towards the augmented dataset. Hence, if the augmented dataset is not aligned with the true underlying data distribution, DA will increase the model’s bias. For results studying DA in the context of structural risk minimization, we refer the reader to Chapelle et al. (2000); Arjovsky et al. (2019); Chen et al. (2020a); Ilse et al. (2021). In short, it is clear from the above that DA is yet another form of regularization that can be applied for structural risk minimization. Yet, even if DA eﬀectively restrict the functional space of the model, it can be at the cost of producing strong biases, as we empirically observed in section 2. Provable Bias Induced by Data-Augmentation. Especially relevant to our results is a recent result of Xu et al. (2020). In this work, it was theorized what when the underlying dataset contains inherent biases, training on the original data is more eﬀective than employing an i.i.d. DA policy, i.e. applying the same random augmentation to all samples/classes, to produce an unbiased model. In short, the DA policy exacerbates the already present biases and makes the trained model further away from the unbiased optimum. As per our experiments from section 2, we observe that bias can take many form. For example, having classes with diﬀerent statistics e.g. objects that appear at very speciﬁc scale in the image. One strategy proposed by Xu et al. (2020), assuming that the bias of a model can be measured, was to adapt the DA policy to the dataset at hand to actively correct the present bias, as done for example in McLaughlin et al. (2015); Iosiﬁdis and Ntoutsi (2018); Jaipuria et al. (2020). In fact, when the bias is measurable, many recent studies have shown that DAs could be designed to reduce the bias present in a dataset (McLaughlin et al., 2015; Jaipuria et al., 2020). A.2 Known Tradeoﬀ Between Regularization, Generalization and Robust- ness There exist a few previous work in that direction, especially in the ﬁeld of robust and overparametrized machine learning. In Raghunathan et al. (2020) a surprising result demonstrated that the minimum norm interpolant of the original + DA could have a larger standard error than that of the original data’s minimum norm 14interpolant. Even more surprising, this result was obtained using consistent DA i.e. transformations that do not alter the label information of the samples as in py|x = py|Tα(x), or f∗(x) = f∗(Tα(x)) as discussed in eq. (1). This possible degradation of performance occurs as long as the model remains over-parametrized, even when considering the original+DA dataset. In addition to the implication of DA into bias, there exists an intertwined relationship between DA and model robustness. In fact, even assuming the use of perfectly adapted DAs, there exists an inherent tradeoﬀ between accuracy and robustness that holds even in the inﬁnite data limit (Tsipras et al., 2018; Fawzi et al., 2018; Zhang et al., 2019). For example, Min et al. (2021) proved in the robust linear classiﬁcation regime that (i) more data improves generalization in a weak adversary regime, (ii) more data can improve generalization up to a point where additional data starts to hurt generalization in a medium adversary regime, and that (iii) more data immediately decreases generalization error in a strong adversary regime. Lastly, a few studies have started to study the bias the could be cause from some DAs, although those studies focused on learned DAs e.g. from Generative Adversarial Networks (Hu and Li, 2019). In that scenario, the predicament is that the GAN in itself is biased, and thus any GAN-generated DA will inherit those biases. B Proof of theorem 1 Proof. To streamline our derivations and without loss of generality, we ﬁrst impose that T−α inverts the action of Tα, that T0 acts as the identity mapping and that Tα ◦Tβ = Tα+β. In short, we have a group structure that allows us to deﬁne the following equivalence class ∼that is positive iﬀ two images are related by a transformation, formally deﬁned as u ∼v ⇐⇒ ∃α: Tα(u) = v, (2) which is reﬂexive (u ∼u with α = 0), symmetric (u ∼v with α implies that v ∼u with −α), and transitive (u ∼v with α and v ∼w with β implies that u ∼w with α+ β). Lastly, given a set of samples X, we deﬁne the following a mapping that will decompose X into a set generators and corresponding DA parameters that allow to recover those subsets when applied to the subset generator. Formally, we have g(X) ↦→{(µ1,A1),(µ2,A2),... } s.t. ∪(µ,A)∈g(X) {Tα(µ),∀α∈A}= X. (3) Note that there is a bijection between the pairs given by g(X) and the set of equivalent samples deﬁned by X/∼. Using this, we obtain the following decomposition of the empirical error L= EX [∥f∗(x) −fθ(x)∥] = ∑ (µ,A)∈g(X) ∑ α∈A ∥f∗(Tα(µ)) −fθ(Tα(µ))∥p(Tα(µ)). Given the above, we now simply split a dataset X into two, one that contains training samples, and one that contains everything else. Without loss of generality we assume that p(Tα(µ)) is a constant and thus omit it. Using the fact that the model has 0 training error, we obtain that the approximation error between f∗and fθ can not be 0 unless the DA is perfectly aligned with the level-sets of f∗as in L= ∑ (µ,A)∈g(X\\Xtrain) ∑ α∈A ∥f∗(Tα(µ)) −fθ(Tα(µ))∥2 2 + ∑ (µ,A)∈g(Xtrain) ∑ α∈A ∥f∗(Tα(µ)) −fθ(Tα(µ))∥2 2 ≥ ∑ (µ,A)∈g(Xtrain) ∑ α∈A ∥f∗(Tα(µ)) −fθ(Tα(µ))∥2 2 ≥ ∑ (µ,A)∈g(Xtrain) ∑ α∈A ∥f∗(Tα(µ)) −const∥2 2, 15Figure 7: Per-class accuracy evolution when the pro- portion of the image being cropped ( θ) is progressively increased from 8 to 100. While the overall trends mostly applies across classes, some extreme cases can present diﬀerent sensitivity to aggressive cropping, lead- ing to some classes being more successful as being label-preserved after application of the cropping. The weighted average of those lines (based on the proportion of samples coming from each class) gives back ﬁg. 2). where the last equality assumes that the model has 0 training error, and will thus predict the same constant for all DA version of the same sample. And since that last equation is always positive if the DA does not respect the true level-sets of the true function f∗ we obtain the desired result. The same derivation can be carried out with any desired loss e.g. for classiﬁcation tasks. C Additional Figures Figure 8: Random samples from the validation set of Imagenet that correspond to ﬁg. 4. 16Figure 9: Random samples from the validation set of Imagenet that correspond to ﬁg. 5. 17Figure 10: Reprise of ﬁg. 2 but using color jittering DA. The same conclusions are reached, diﬀerent classes have diﬀerent label-preserving regimes under this DA. 18Figure 11: Reprise of ﬁg. 2 but using color jittering DA. The same conclusions are reached, diﬀerent classes have diﬀerent label-preserving regimes under this DA. 19Figure 12: All results in this ﬁgure employ oﬃcial model implementations from PyTorch that we have trained on Imagenet with varying weight decay parameter. Reprise of ﬁg. 5 but now with a diﬀerent architecture (DenseNet121) instead of the ResNet50. 20Figure 13: Random samples from the mini training set of INaturalist that correspond to ﬁg. 6 21",
      "references": [],
      "meta_data": {
        "arxiv_id": "2204.03632v2",
        "authors": [
          "Randall Balestriero",
          "Leon Bottou",
          "Yann LeCun"
        ],
        "published_date": "2022-04-07T17:57:29Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper shows that common regularization techniques in deep learning—data augmentation (DA) and weight decay—induce class-dependent bias. While these methods improve average accuracy, they can drastically degrade performance on specific classes, and such biases persist across architectures and transfer learning. The authors provide theoretical intuition (Theorem 1) explaining when DA causes irreducible bias (when augmented samples move off the true label level-sets) and quantify per-class sensitivity to DA, demonstrating that the same DA policy is label-preserving for some classes and not for others. They show that this class-wise bias transfers to downstream tasks (e.g., ImageNet pre-training affecting INaturalist performance) and argue that regularizers should be designed to avoid class-dependent bias rather than merely optimize average metrics.",
        "methodology": "The study combines theory and large-scale experiments. Theoretically, it derives conditions under which data augmentation introduces bias (Theorem 1) by analyzing transformations Tα that do not preserve the true label level-sets. Empirically, it trains standard CNNs on ImageNet with varying DA strength (e.g., random crop with lower bound α from 100% down to 8%), and assesses per-class accuracy across multiple architectures (ResNet-50/101/152, DenseNet-121/201, X-101-32x8d). It also repeats experiments with label-preserving vs non-label-preserving augmentations (CutOut, color jitter). Statistical tests (Welch’s t-tests) confirm per-class declines under DA for a subset of classes. The study also analyzes weight decay as an uninformed regularizer, showing class-dependent effects, and evaluates transfer learning by freezing ImageNet-pretrained backbones and training a linear head on INaturalist to quantify bias transfer. Supplementary materials include broader architectures and additional visualizations.",
        "experimental_setup": "Datasets: ImageNet (1k classes) for source training/evaluation; INaturalist (10k classes) for target transfer. Architectures: ResNet-50/101/152, DenseNet-121/201, Wide/Group variants (e.g., ResNext-101-32x8d). Regularizers/augmentations: random-crop DA with varying lower bound (α from 100 to 8), CutOut, ColorJitter; horizontal flip; weight decay γ with varying strength. Experimental protocol includes training with multiple seeds (e.g., 20 runs for DA sensitivity, 10 runs for weight decay), evaluation of per-class accuracy, and cross-architecture agreement checks (figure-level analyses). Statistical testing uses Welch’s t-test to assess per-class improvements with/without DA. Transfer setup trains a frozen ImageNet backbone and a linear classifier on INaturalist, reporting per-class transfer performance across τ-parameter regimes.",
        "limitations": "The analysis is largely empirical and dataset-specific (ImageNet and INaturalist) and mostly focused on vision; theoretical results rely on assumptions like 0 training error and group-structured augmentations. Per-class bias is demonstrated but the exact causal mechanisms can be dataset- and architecture-dependent. The study does not propose a concrete, generalizable regularizer that eliminates class-dependent bias, and it does not explore a wide range of regularizers beyond DA and weight decay. Transfer results, while indicative, may not generalize to all downstream tasks or domains.",
        "future_research_directions": "Develop adaptive or class-aware regularization that preserves label information across all classes; design DA policies that minimize inter-class bias or incorporate per-class transformations and targets; create new regularizers that trade off bias-variance without harming underrepresented classes; extend bias analysis to other modalities (NLP, audio) and more domains; study automated, data-driven methods to select regularization regimes with class-aware objectives; investigate fairness-oriented metrics for class-wise performance in pre-trained models and in deployment scenarios.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Negative Data Augmentation ",
      "full_text": "Published as a conference paper at ICLR 2021 NEGATIVE DATA AUGMENTATION Abhishek Sinha1∗ Kumar Ayush1∗ Jiaming Song1∗ Burak Uzkent1 Hongxia Jin2 Stefano Ermon1 Department of Computer Science1 Stanford University {a7b23, kayush, tsong, buzkent, ermon}@stanford.edu Samsung Research America2 ABSTRACT Data augmentation is often used to enlarge datasets with synthetic samples gen- erated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distri- bution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable con- ditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved con- ditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image clas- siﬁcation, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks. 1 I NTRODUCTION Data augmentation strategies for synthesizing new data in a way that is consistent with an underlying task are extremely effective in both supervised and unsupervised learning (Oord et al., 2018; Zhang et al., 2016; Noroozi & Favaro, 2016; Asano et al., 2019). Because they operate at the level of samples, they can be combined with most learning algorithms. They allow for the incorporation of prior knowledge (inductive bias) about properties of typical samples from the underlying data distribution (Jaiswal et al., 2018; Antoniou et al., 2017), e.g., by leveraging invariances to produce additional “positive” examples of how a task should be solved. To enable users to specify an even wider range of inductive biases, we propose to leverage an alter- native and complementary source of prior knowledge that speciﬁes how a task shouldnot be solved. We formalize this intuition by assuming access to a way of generating samples that are guaranteed to be out-of-support for the data distribution, which we call a Negative Data Augmentation (NDA). Intuitively, negative out-of-distribution (OOD) samples can be leveraged as a useful inductive bias because they provide information about the support of the data distribution to be learned by the model. For example, in a density estimation problem we can bias the model to avoid putting any probability mass in regions which we know a-priori should have zero probability. This can be an effective prior if the negative samples cover a sufﬁciently large area. The best NDA candidates are ones that expose common pitfalls of existing models, such as prioritizing local structure over global ∗Equal Contribution 1 arXiv:2102.05113v1  [cs.CV]  9 Feb 2021Published as a conference paper at ICLR 2021 structure (Geirhos et al., 2018); this motivates us to consider known transformations from the litera- ture that intentionally destroy the spatial coherence of an image (Noroozi & Favaro, 2016; DeVries & Taylor, 2017; Yun et al., 2019), such as Jigsaw transforms. Building on this intuition, we introduce a new GAN training objective where we use NDA as an additional source of fake data for the discriminator as shown in Fig. 1. Theoretically, we can show that if the NDA assumption is valid, optimizing this objective will still recover the data distribution in the limit of inﬁnite data. However, in the ﬁnite data regime, there is a need to generalize beyond the empirical distribution (Zhao et al., 2018). By explicitly providing the discriminator with samples we want to avoid, we are able to bias the generator towards avoiding undesirable samples thus improving generation quality. Figure 1: Negative Data Aug- mentation for GANs. Furthermore, we propose a way of leveraging NDA for unsu- pervised representation learning. We propose a new contrastive predictive coding (He et al., 2019; Han et al., 2019) (CPC) ob- jective that encourages the distribution of representations cor- responding to in-support data to become disjoint from that of NDA data. Empirically, we show that applying NDA with our proposed transformations (e.g., forcing the representation of nor- mal and jigsaw images to be disjoint) improves performance in downstream tasks. With appropriately chosen NDA strategies, we obtain superior empirical performance on a variety of tasks, with almost no cost in computation. For generative modeling, models trained with NDA achieve better image generation, image translation and anomaly detection performance compared with the same model trained without NDA. Similar gains are observed on representa- tion learning for images and videos over downstream tasks such as image classiﬁcation, object detection and action recognition. These results suggest that NDA has much potential to improve a variety of self-supervised learning techniques. 2 N EGATIVE DATA AUGMENTATION The input to most learning algorithms is a dataset of samples from an underlying data distribution pdata. While pdata is unknown, learning algorithms always rely on prior knowledge about its prop- erties (inductive biases (Wolpert & Macready, 1997)), e.g., by using speciﬁc functional forms such as neural networks. Similarly, data augmentation strategies exploit known invariances ofpdata, such as the conditional label distribution being invariant to semantic-preserving transformations. While typical data augmentation strategies exploit prior knowledge about what is in support ofpdata, in this paper, we propose to exploit prior knowledge about what is not in the support of pdata. This information is often available for common data modalities (e.g., natural images and videos) and is under-exploited by existing approaches. Speciﬁcally, we assume: (1) there exists an alternative distribution psuch that its support is disjoint from that of pdata; and (2) access to a procedure to efﬁciently sample from p. We emphasize pneed not be explicitly deﬁned (e.g., through an explicit density) – it may be implicitly deﬁned by a dataset or by a procedure that transforms samples from pdata into ones from pby suitably altering their structure. Figure 2: Negative augmentations produce out-of-distribution samples lacking the typical structure of natural images; these negative samples can be used to inform a model on what it shouldnot learn. 2Published as a conference paper at ICLR 2021 Analogous to typical data augmentations, NDA strategies are by deﬁnition domain and task speciﬁc. In this paper, we focus on natural images and videos, and leave the application to other domains (such as natural language processing) as future work. How do we select a good NDA strategy? Ac- cording to the manifold hypothesis (Fefferman et al., 2016), natural images lie on low-dimensional manifolds: pdata is supported on a low-dimensional manifold of the ambient (pixel) space. This suggests that many negative data augmentation strategies exist. Indeed, sampling random noise is in most cases a valid NDA. However, while this prior is generic, it is not very informative, and this NDA will likely be ineffective for most learning problems. Intuitively, NDA is informative if its support is close (in a suitable metric) to that of pdata, while being disjoint. These negative samples will provide information on the “boundary” of the support of pdata, which we will show is help- ful in several learning problems. In most of our tasks, the images are processed by convolutional neural networks (CNNs) that are good at processing local features but not necessarily global fea- tures (Geirhos et al., 2018). Therefore, we may consider NDA examples to be ones that preserve local features (“informative”) and break global features, so that it forces the CNNs to learn global features (by realizing NDAs are different from real data). Leveraging this intuition, we show several image transformations from the literature that can be viewed as generic NDAs over natural images in Figure 2, that we will use for generative modeling and representation learning in the following sections. Details about these transformations can be found in Appendix B. 3 NDA FOR GENERATIVE ADVERSARIAL NETWORKS Figure 3: Schematic overview of our NDA framework. Left: In the absence of NDA, the support of a generative model Pθ (blue oval) learned from samples (green dots) may “over-generalize” and include samples from P1 or P2. Right: With NDA, the learned distribution Pθ becomes disjoint from NDA distributions P1 and P2, thus pushing Pθ closer to the true data distribution pdata (green oval). As long as the prior is consistent, i.e. the supports of P1 and P2 are truly disjoint from pdata, the best ﬁt distribution in the inﬁnite data regime does not change. In GANs, we are interested in learning a generative model Gθ from samples drawn from some data distribution pdata (Goodfellow et al., 2014). GANs use a binary classiﬁer, the so-called discriminator Dφ, to distinguish real data from generated (fake) samples. The generator Gθ is trained via the following mini-max objective that performs variational Jensen-Shannon divergence minimization: min Gθ∈P(X) max Dφ LJS(Gθ,Dφ) where (1) LJS(Gθ,Dφ) = Ex∼pdata [log(Dφ(x))] + Ex∼Gθ [log(1 −Dφ(x))] (2) This is a special case to the more general variationalf-divergence minimization objective (Nowozin et al., 2016). The optimal Dφ for any Gθ is (pdata/Gθ)/(1 + pdata/Gθ), so the discriminator can serve as a density ratio estimator between pdata and Gθ. With sufﬁciently expressive models and inﬁnite capacity,Gθ will match pdata. In practice, however, we have access to ﬁnite datasets and limited model capacity. This means that the generator needs to generalize beyond the empirical distribution, which is challenging because the number of possi- ble discrete distributions scale doubly exponentially w.r.t. to the data dimension. Hence, as studied in (Zhao et al., 2018), the role of the inductive bias is critical. For example, Zhao et al. (2018) report 3Published as a conference paper at ICLR 2021 that when trained on images containing 2 objects only, GANs and other generative models can some- times “generalize” by generating images with 1 or 3 objects (which were never seen in the training set). The generalization behavior – which may or may not be desirable – is determined by factors such as network architectures, hyperparameters, etc., and is difﬁcult to characterize analytically. Here we propose to bias the learning process by directly specifying what the generator should not generate through NDA. We consider an adversarial game based on the following objective: min Gθ∈P(X) max Dφ LJS(λGθ + (1 −λ)P,Dφ) (3) where the negative samples are generated from a mixture of Gθ (the generator distribution) and P (the NDA distribution); the mixture weights are controlled by the hyperparameterλ. Intuitively, this can help addresses the above “over-generalization” issue, as we can directly provide supervision on what should not be generated and thus guide the support of Gθ (see Figure 3) . For instance, in the object count example above, we can empirically prevent the model from generating images with an undesired number of objects (see Appendix Section A for experimental results on this task). In addition, the introduction of NDA samples will not affect the solution of the original GAN ob- jective in the limit. In the following theorem, we show that given inﬁnite training data and inﬁnite capacity discriminators and generators, using NDA will not affect the optimal solution to the gener- ator, i.e. the generator will still recover the true data distribution. Theorem 1. Let P ∈P(X) be any distribution over Xwith disjoint support than pdata, i.e., such that supp(pdata) ∩supp(P) = ∅. Let Dφ : X →R be the set of all discriminators over X, f : R≥0 →R be a convex, semi-continuous function such thatf(1) = 0, f⋆be the convex conjugate of f, f′its derivative, and Gθ be a distribution with sample space X. Then ∀λ∈(0,1], we have: arg min Gθ∈P(X) max Dφ:X→R Lf(Gθ,Dφ) = arg min Gθ∈P(X) max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = pdata (4) where Lf(Q,Dφ) = Ex∼pdata [Dφ(x)] −Ex∼Q[f⋆(Dφ(x))] is the objective for f-GAN (Nowozin et al., 2016). However, the optimal discriminators are different for the two objectives: arg max Dφ:X→R Lf(Gθ,Dφ) = f′(pdata/Gθ) (5) arg max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = f′(pdata/(λGθ + (1 −λ)P)) (6) Proof. See Appendix C. The above theorem shows that in the limit of inﬁnite data and computation, adding NDA changes the optimal discriminator solution but not the optimal generator. In practice, when dealing with ﬁnite data, existing regularization techniques such as weight decay and spectral normalization (Miyato et al., 2018) allow potentially many solutions that achieve the same objective value. The introduc- tion of NDA samples allows us to ﬁlter out certain solutions by providing additional inductive bias through OOD samples. In fact, the optimal discriminator will reﬂect the density ratio between pdata and λGθ + (1−λ)P (see Eq.(6)), and its values will be higher for samples from pdata compared to those from P. As we will show in Section 5, a discriminator trained with this objective and suitable NDA performs better than relevant baselines for other downstream tasks such as anomaly detection. 4 NDA FOR CONSTRASTIVE REPRESENTATION LEARNING Using a classiﬁer to estimate a density ratio is useful not only for estimating f-divergences (as in the previous section) but also for estimating mutual information between two random variables. In representation learning, mutual information (MI) maximization is often employed to learn compact yet useful representations of the data, allowing one to perform downstream tasks efﬁciently (Tishby & Zaslavsky, 2015; Nguyen et al., 2008; Poole et al., 2019b; Oord et al., 2018). Here, we show that NDA samples are also beneﬁcial for representation learning. In contrastive representation learning (such as CPC (Oord et al., 2018)), the goal is to learn a map- ping hθ(x) : X→P (Z) that maps a datapoint x to some distribution over the representation space 4Published as a conference paper at ICLR 2021 Z; once the network hθ is learned, representations are obtained by sampling from z ∼hθ(x). CPC maximizes the following objective: ICPC(hθ,gφ) := Ex∼pdata(x),z∼hθ(x),ˆzi∼pθ(z) [ log ngφ(x,z) gφ(x,z) + ∑n−1 j=1 gφ(x, ˆzj) ] (7) where pθ(z) = ∫ hθ(z|x)pdata(x)dx is the marginal distribution of the representations associated with pdata. Intuitively, the CPC objective involves an n-class classiﬁcation problem where gφ at- tempts to identify a matching pair (i.e. (x,z)) sampled from the joint distribution from the (n−1) non-matching pairs (i.e. (x,ˆzj)) sampled from the product of marginals distribution. Note that gφ plays the role of a discriminator/critic, and is implicitly estimating a density ratio. As n →∞, the optimal gφ corresponds to an un-normalized density ratio between the joint distribution and the product of marginals, and the CPC objective matches its upper bound which is the mutual informa- tion between X and Z (Poole et al., 2019a; Song & Ermon, 2019). However, this objective is no longer able to control the representations for data that are out of support of pdata, so there is a risk that the representations are similar between pdata samples and out-of-distribution ones. To mitigate this issue, we propose to use NDA in the CPC objective, where we additionally introduce a batch of NDA samples, for each positive sample: ICPC(hθ,gφ) := E [ log (n+ m)gφ(x,z) gφ(x,z) + ∑n−1 j=1 gφ(x, ˆzj) + ∑m k=1 gφ(x,zk) ] (8) where the expectation is taken over x ∼pdata(x),z ∼hθ(x), ˆzi ∼pθ(z), xk ∼p (NDA dis- tribution), zk ∼ hθ(xk) for all k ∈ [m]. Here, the behavior of hθ(x) when x is NDA is op- timized explicitly, allowing us to impose additional constraints to the NDA representations. This corresponds to a more challenging classiﬁcation problem (compared to basic CPC) that encourages learning more informative representations. In the following theorem, we show that the proposed ob- jective encourages the representations for NDA samples to become disjoint from the representations for pdata samples, i.e. NDA samples and pdata samples do not map to the same representation. Theorem 2. (Informal) The optimal solution to hθ in the NDA-CPC objective maps the representa- tions of data samples and NDA samples to disjoint regions. Proof. See Appendix D for a detailed statement and proof. 5 NDA-GAN E XPERIMENTS In this section we report experiments with different types of NDA for image generation. Additional details about the network architectures and hyperparameters can be found in Appendix J. Figure 4: Histogram of differ- ence in the discriminator out- put for a real image and it’s Jigsaw version. Unconditional Image Generation. We conduct experiments on various datasets using the BigGAN architecture (Brock et al., 2018) for unconditional image generation 1. We ﬁrst explore various im- age transformations from the literature to evaluate which ones are effective as NDA. For each transformation, we evaluate its perfor- mance as NDA (training as in Eq. 3) and as a traditional data aug- mentation strategy, where we enlarge the training set by applying the transformation to real images (denoted PDA for positive data augmentation). Table 1 shows the FID scores for different types of transformations as PDA/NDA. The results suggest that transforma- tions that spatially corrupt the image are strong NDA candidates. It can be seen that Random Horizontal Flip is not effective as an NDA; this is because ﬂipping does not spatially corrupt the image but is rather a semantic preserving transformation, hence the NDA distribution P is not disjoint from pdata. On the contrary, it is rea- sonable to assume that if an image is likely under pdata, its ﬂipped variant should also be likely. This is conﬁrmed by the effectiveness of this strategy as PDA. 1We feed a single label to all images to make the architecture suitable for unconditional generation. 5Published as a conference paper at ICLR 2021 Table 1: FID scores over CIFAR-10 using different transformations as PDA and NDA in BigGAN. The results indicate that some transformations yield better results when used as NDA. The common feature of such transformations is they all spatially corrupt the images. w/o Aug.Jigsaw Cutout Stitch Mixup Cutmix Random Crop Random Flip Gaussian PDA NDAPDA NDAPDA NDAPDA NDAPDA NDAPDA NDAPDA NDAPDA NDA 18.64 98.09 12.6179.72 14.69108.69 13.9770.64 17.2990.81 15.0120.02 15.0516.65 124.3244.41 18.72 Table 2: Comparison of FID scores of different types of NDA for unconditional image generation on various datasets. The numbers in bracket represent the corresponding image resolution in pixels. Jigsaw consistently achieves the best or second best result. BigGAN Jigsaw Stitching Mixup Cutout Cutmix CR-BigGAN CIFAR-10 (32) 18.64 12.61 13.97 17.29 14.69 15.01 14.56 CIFAR-100 (32)22.19 19.72 20.99 22.21 22.08 20.78 – CelebA (64) 38.14 37.24 37.17 37.51 37.39 37.46 – STL10 (32) 26.80 23.94 26.08 24.45 24.91 25.34 – We believe spatially corrupted negatives perform well as NDA in that they push the discriminator to focus on global features instead of local ones (e.g., texture). We conﬁrm this by plotting the histogram of differences in the discriminator output for a real image and it’s Jigsaw version as shown in Fig. 4. We show that the difference is (a) centered close to zero for normal BigGAN (so without NDA training, the discriminator cannot distinguish real and Jigsaw samples well ), and (b) centered at a positive number (logit 10) for our method (NDA-BigGAN). Following our ﬁndings, in our remaining experiments we use Jigsaw, Cutout, Stitch, Mixup and Cutmix as they achieve signiﬁcant improvements when used as NDA for unconditional image generation on CIFAR-10. Table 2 shows the FID scores for BigGAN when trained with ﬁve types of negative data augmenta- tion on four different benchmarks. Almost all the NDA augmentations improve the baseline across datasets. For all the datasets except CIFAR-100, λ = 0.25, whereas for CIFAR-100 it is 0.5. We show the effect ofλon CIFAR-10 performance in Appendix G. We additionally performed an exper- iment using a mixture of augmentation policy. The results (FID 16.24) were better than the baseline method (18.64) but not as good as using a single strategy. Conditional Image Generation. We also investigate the beneﬁts of NDA in conditional image generation using BigGAN. The results are shown in Table 3. In this setting as well, NDA gives a signiﬁcant boost over the baseline model. We again use λ = 0 .25 for CIFAR-10 and λ = 0 .5 for CIFAR-100. For both unconditional and conditional setups we ﬁnd the Jigsaw and Stitching augmentations to achieve a better FID score than the other augmentations. Table 3: FID scores for conditional image generation using different NDAs.2 BigGAN Jigsaw Stitching Mixup Cutout Cutmix CR-BigGAN C-10 11.51 9.42 9.47 13.87 10.52 10.3 11.48 C-100 15.04 14.12 13.90 15.27 14.21 13.99 – Image Translation.Next, we apply the NDA method to image translation. In particular, we use the Pix2Pix model (Isola et al., 2017) that can perform image-to-image translation using GANs provided paired training data. Here, the generator is conditioned on an imageI, and the discriminator takes as input the concatenation of generated/real image and I. We use Pix2Pix for semantic segmentation on Cityscapes dataset (Cordts et al., 2016) (i.e. photos →labels). Table 4 shows the quantitative gains obtained by using Jigsaw NDA 3 while Figure 7 in Appendix E highlights the qualitative im- provements. The NDA-Pix2Pix model avoids noisy segmentation on objects including buildings and trees. 2We use a PyTorch code for BigGAN. The number reported in Brock et al. (2018) for C-10 is 14.73. 3We use the ofﬁcial PyTorch implementation and show the best results. 6Published as a conference paper at ICLR 2021 Table 4: Results on CityScapes, using per pixel accuracy (Pp.), per class accuracy (Pc.) and mean Intersection over Union (mIOU). We compare Pix2Pix and its NDA version. Metric Pp. Pc. mIOU Pix2Pix (cGAN) 0.80 0.24 0.27 NDA (cGAN) 0.84 0.34 0.28 Pix2Pix (L1+cGAN) 0.72 0.23 0.18 NDA (L1+cGAN) 0.75 0.28 0.22 Table 5: AUROC scores for different OOD datasets. OOD-1 contains different datasets, while OOD-2 contains the set of 19 different corruptions in CIFAR-10-C (Hendrycks & Dietterich, 2018) (the average score is reported). BigGAN Jigsaw EBM OOD-1 DTD 0.70 0.69 0.48 SVHN 0.75 0.61 0.63 Places-365 0.35 0.58 0.68 TinyImageNet0.40 0.62 0.67 CIFAR-100 0.63 0.64 0.50 Average 0.57 0.63 0.59 OOD-2 CIFAR-10-C 0.56 0.63 0.60 Anomaly Detection. As another added beneﬁt of NDA for GANs, we utilize the output scores of the BigGAN discriminator for anomaly detection. We experiment with 2 different types of OOD datasets. The ﬁrst set consists of SVHN (Netzer et al., 2011), DTD (Cimpoi et al., 2014), Places- 365 (Zhou et al., 2017), TinyImageNet, and CIFAR-100 as the OOD datapoints following the pro- tocol in (Du & Mordatch, 2019; Hendrycks et al., 2018). We train BigGAN w/ and w/o Jigsaw NDA on the train set of CIFAR-10 and then use the output value of discriminator to classify the test set of CIFAR-10 (not anomalous) and different OOD datapoints (anomalous) as anomalous or not. We use the AUROC metric as proposed in (Hendrycks & Gimpel, 2016) to evaluate the anomaly detection performance. Table 5 compares the performance of NDA with a likelihood based model (Energy Based Models (EBM (Du & Mordatch, 2019)). Results show that Jigsaw NDA performs much better than baseline BigGAN and other generative models. We did not include other NDAs as Jigsaw achieved the best results. We consider the extreme corruptions in CIFAR-10-C (Hendrycks & Dietterich, 2018) as the sec- ond set of OOD datasets. It consists of 19 different corruptions, each having 5 different levels of severity. We only consider the corruption of highest severity for our experiment, as these constitute a signiﬁcant shift from the true data distribution. Averaged over all the 19 different corruptions, the AUROC score for the normal BigGAN is 0.56, whereas the BigGAN trained with Jigsaw NDA achieves 0.63. The histogram of difference in discriminator’s output for clean and OOD samples are shown in Figure 8 in the appendix. High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN. 6 R EPRESENTATION LEARNING USING CONTRASTIVE LOSS AND NDA Unsupervised Learning on Images.In this section, we perform experiments on three benchmarks: (a) CIFAR10 (C10), (b) CIFAR100 (C100), and (c) ImageNet-100 (Deng et al., 2009) to show the beneﬁts of NDA on representation learning with the contrastive loss function. In our experiments, we use the momentum contrast method (He et al., 2019),MoCo-V2, as it is currently the state-of-the- art model on unsupervised learning on ImageNet. For C10 and C100, we train the MoCo-V2 model for unsupervised learning (w/ and w/o NDA) for 1000 epochs. On the other hand, for ImageNet-100, we train the MoCo-V2 model (w/ and w/o NDA) for 200 epochs. Additional hyperparameter details can be found in the appendix. To evaluate the representations, we train a linear classiﬁer on the representations on the same dataset with labels. Table 6 shows the top-1 accuracy of the classiﬁer. We ﬁnd that across all the three datasets, different NDA approaches outperform MoCo-V2. While Cutout NDA performs the best for C10, the best performing NDA for C100 and ImageNet-100 are Jigsaw and Mixup respectively. Figure 9 compares the cosine distance of the representations learned w/ and w/o NDA (jigsaw) and shows that jigsaw and normal images are projected far apart from each other when trained using NDA whereas with original MoCo-v2 they are projected close to each other. Transfer Learning for Object Detection.We transfer the network pre-trained over ImageNet-100 for the task of Pascal-VOC object detection using a Faster R-CNN detector (C4 backbone) Ren et al. (2015). We ﬁne-tune the network on Pascal VOC 2007+2012 trainval set and test it on the 2007 test 7Published as a conference paper at ICLR 2021 Table 6: Top-1 accuracy results on image recognition w/ and w/o NDA on MoCo-V2. MoCo-V2 Jigsaw Stitching Cutout Cutmix Mixup CIFAR-10 91.20 91.66 91.59 92.26 91.51 91.36 CIFAR-100 69.63 70.17 69.21 69.81 69.83 69.99 ImageNet-100 69.41 69.95 69.54 69.77 69.61 70.01 set. The baseline MoCo achieves 38.47 AP, 65.99 AP50, 38.81 AP75 whereas the MoCo trained with mixup NDA gets 38.72 AP, 66.23 AP50, 39.16 AP75 (an improvement of ≈0.3). Unsupervised Learning on Videos.In this section, we investigate the beneﬁts of NDA in self- supervised learning of spatio-temporal embeddings from video, suitable for human action recogni- tion. We apply NDA to Dense Predictive Coding (Han et al., 2019), which is a single stream (RGB only) method for self-supervised representation learning on videos. For videos, we create NDA samples by performing the same transformation on all frames of the video (e.g. the same jigsaw permutation is applied to all the frames of a video). We evaluate the approach by ﬁrst training the DPC model with NDA on a large-scale dataset (UCF101), and then evaluate the representations by training a supervised action classiﬁer on UCF101 and HMDB51 datasets. As shown in Table 7, Jigsaw and Cutmix NDA improve downstream task accuracy on UCF-101 and HMDB-51, achiev- ing new state-of-the-art performance among single stream (RGB only) methods for self-supervised representation learning (when pre-trained using UCF-101). Table 7: Top-1 accuracy results on action recognition in videos w/ and w/o NDA in DPC. DPC Jigsaw Stitching Cutout Cutmix Mixup UCF-101 (Pre-trained on UCF-101) 61.35 64.54 66.07 64.52 63.52 63.65 HMDB51 (Pre-trained on UCF-101) 45.31 46.88 45.31 45.31 48.43 43.75 7 R ELATED WORK In several machine learning settings, negative samples are produced from a statistical generative model. Sung et al. (2019) aim to generate negative data using GANs for semi-supervised learning and novelty detection while we are concerned with efﬁciently creating negative data to improve generative models and self-supervised representation learning. Hanneke et al. (2018) also propose an alternative theoretical framework that relies on access to an oracle which classiﬁes a sample as valid or not, but do not provide any practical implementation. Bose et al. (2018) use adversarial training to generate hard negatives that fool the discriminator for NLP tasks whereas we obtain NDA data from positive data to improve image generation and representation learning. Hou et al. (2018) use a GAN to learn the negative data distribution with the aim of classifying positive-unlabeled (PU) data whereas we do not have access to a mixture data but rather generate negatives by transforming the positive data. In contrastive unsupervised learning, common negative examples are ones that are assumed to be further than the positive samples semantically. Word2Vec (Mikolov et al., 2013) considers negative samples to be ones from a different context and CPC-based methods (Oord et al., 2018) such as momentum contrast (He et al., 2019), the negative samples are data augmentations from a different image. Our work considers a new aspect of “negative samples” that are neither generated from some model, nor samples from the data distribution. Instead, by applying negative data augmentation (NDA) to existing samples, we are able to incorporate useful inductive biases that might be difﬁcult to capture otherwise (Zhao et al., 2018). 8 C ONCLUSION We proposed negative data augmentation as a method to incorporate prior knowledge through out-of- distribution (OOD) samples. NDAs are complementary to traditional data augmentation strategies, 8Published as a conference paper at ICLR 2021 which are typically focused on in-distribution samples. Using the NDA framework, we interpret existing image transformations (e.g., jigsaw) as producing OOD samples and develop new learning algorithms to leverage them. Owing to rigorous mathematical characterization of the NDA assump- tion, we are able to theoretically analyze their properties. As an example, we bias the generator of a GAN to avoid the support of negative samples, improving results on conditional/unconditional im- age generation tasks. Finally, we leverage NDA for unsupervised representation learning in images and videos. By integrating NDA into MoCo-v2 and DPC, we improve results on image and action recognition on CIFAR10, CIFAR100, ImageNet-100, UCF-101, and HMDB-51 datasets. Future work include exploring other augmentation strategies as well as NDAs for other modalities. REFERENCES Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. A critical analysis of self-supervision, or what we can learn from a single image. arXiv preprint arXiv:1904.13132, 2019. Avishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1021–1032, 2018. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3213–3223, 2016. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019. Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. Journal of the American Mathematical Society, 29(4):983–1049, 2016. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias im- proves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pp. 2672–2680, 2014. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im- proved training of wasserstein gans. In Advances in neural information processing systems , pp. 5767–5777, 2017. Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In Proceedings of the IEEE International Conference on Computer Vision Workshops , pp. 0–0, 2019. 9Published as a conference paper at ICLR 2021 Steve Hanneke, Adam Tauman Kalai, Gautam Kamath, and Christos Tzamos. Actively avoiding nonsense in generative models. In Conference On Learning Theory, pp. 209–227, 2018. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. arXiv preprint arXiv:1807.01697, 2018. Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. Ming Hou, Brahim Chaib-Draa, Chao Li, and Qibin Zhao. Generative adversarial positive-unlabeled learning. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pp. 2255–2261, 2018. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125–1134, 2017. Ayush Jaiswal, Rex Yue Wu, Wael Abd-Almageed, and Prem Natarajan. Unsupervised adversarial invariance. In Advances in Neural Information Processing Systems, pp. 5092–5102, 2018. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen- tations of words and phrases and their compositionality. In Advances in neural information pro- cessing systems, pp. 3111–3119, 2013. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. Xuanlong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization.arXiv preprint arXiv:0809.0853, (11):5847– 5861, September 2008. doi: 10.1109/TIT.2010.2068870. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. InAdvances in neural information processing systems, pp. 271–279, 2016. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic- tive coding. arXiv preprint arXiv:1807.03748, 2018. Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia- tional bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019a. Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia- tional bounds of mutual information. arXiv preprint arXiv:1905.06922, May 2019b. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91–99, 2015. Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. arXiv preprint arXiv:1910.06222, October 2019. 10Published as a conference paper at ICLR 2021 Yi Lin Sung, Sung-Hsien Hsieh, Soo-Chang Pei, and Chun-Shien Lu. Difference-seeking gener- ative adversarial network–unseen sample generation. In International Conference on Learning Representations, 2019. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. arXiv preprint arXiv:1503.02406, March 2015. David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE trans- actions on evolutionary computation, 1(1):67–82, 1997. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 6023–6032, 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European confer- ence on computer vision, pp. 649–666. Springer, 2016. Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, and Stefano Ermon. Bias and generalization in deep generative models: An empirical study. In Advances in Neural Information Processing Systems, pp. 10792–10801, 2018. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition.IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017. A N UMEROSITY CONTAINMENT Zhao et al. (2018) systematically investigate generalization in deep generative models using two different datasets: (a) a toy dataset where there are knon-overlapping dots (with random color and location) in the image (see Figure 5a), and (b) the CLEVR dataset where ther are k objects (with random shape, color, location, and size) in the images (see Figure 5b). They train a GAN model (WGAN-GP Gulrajani et al. (2017)) with (either) dataset and observe that the learned distribution does not produce the same number of objects as in the dataset it was trained on. The distribution of the numerosity in the generated images is centered at the numerosity from the dataset, with a slight- bias towards over-estimation. For, example when trained on images with six dots, the generated images contain anywhere from two to eight dots (see Figure 6a). The observation is similar when trained on images with two CLEVR objects. The generated images contain anywhere from one to three dots (see Figure 6b). In order to remove samples with numerosity different from the train dataset, we use such samples as negative data during training. For example, while training on images with six dots we use images with four, ﬁve and seven dots as negative data for the GAN. The resulting distribution of the nu- merosity in the generated images is constrained to six. We observe similar behaviour when training a GAN with images containing two CLEVR objects as positive data and images with one or three objects as negative data. B I MAGE TRANSFORMATIONS Given an image of size H ×W, the different image transformations that we used are described below. Jigsaw-K (Noroozi & Favaro, 2016) We partition the image into a grid of K ×K patches of size (H/K) ×(W/K), indexed by [1,...,K ×K]. Then we shufﬂe the image patches according to a random permutation (different from the original order) to produce the NDA image. Empirically, we ﬁnd K = 2 to work the best for Jigsaw-KNDA. Stitching We stitch two equal-sized patches of two different images, either horizontally (H/2×W) or vertically (H×W/2), chosen uniformly at random, to produce the NDA image. 11Published as a conference paper at ICLR 2021 (a) Dots (b) CLEVR Figure 5: Toy Datasets used in Numerosity experiments. (a)  (b) Figure 6: Left: Distribution over number of dots. The arrows are the number of dots the learning algorithm is trained on, and the solid line is the distribution over the number of dots the model generates. Right: Distribution over number of CLEVR objects the model generates. Generating CLEVR is harder so we explore only one, but the behaviour with NDA is similar to dots. Cutout / CutmixWe select a random patch in the image with its height and width lying between one-third and one-half of the image height and width respectively. To construct NDA im- ages, this patch is replaced with the mean pixel value of the patch (like cutout (DeVries & Taylor, 2017) with the only difference that they use zero-masking), or the pixel values of another image at the same location (cutmix (Yun et al., 2019)). Mixup-α NDA image is constructed from a linear interpolation between two images x and y (Zhang et al., 2017), γx + (1 −γ)y; γ ∼Beta(α,α). α is chosen so that the dis- tribution has high density at 0.5. Other classes NDA images are sampled from other classes in the same dataset. See Appendix A. C NDA FOR GAN S Theorem 1. Let P ∈P(X) be any distribution over Xwith disjoint support than pdata, i.e., such that supp(pdata) ∩supp(P) = ∅. Let Dφ : X →R be the set of all discriminators over X, f : R≥0 →R be a convex, semi-continuous function such thatf(1) = 0, f⋆be the convex conjugate of f, f′its derivative, and Gθ be a distribution with sample space X. Then ∀λ∈(0,1], we have: arg min Gθ∈P(X) max Dφ:X→R Lf(Gθ,Dφ) = arg min Gθ∈P(X) max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = pdata (4) 12Published as a conference paper at ICLR 2021 where Lf(Q,Dφ) = Ex∼pdata [Dφ(x)] −Ex∼Q[f⋆(Dφ(x))] is the objective for f-GAN (Nowozin et al., 2016). However, the optimal discriminators are different for the two objectives: arg max Dφ:X→R Lf(Gθ,Dφ) = f′(pdata/Gθ) (5) arg max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = f′(pdata/(λGθ + (1 −λ)P)) (6) Proof. Let us use p(x),p(x),q(x) to denote the density functions of pdata,P and Gθ respectively (and P, P, Q for the respective distributions). First, from Lemma 1 in Nguyen et al. (2008), we have that max Dφ:X→R Lf(Gθ,Dφ) = Df(P∥Gθ) (9) max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = Df(P∥λQ+ (1 −λ)P) (10) where Df refers to the f-divergence. Then, we have Df(P||λQ+ (1 −λ)P) = ∫ X (λq(x) + (1−λ)p(x)) f ( p(x) λq(x) + (1−λ)p(x) ) = ∫ X λq(x)f ( p(x) λq(x) + (1−λ)p(x) ) + (1 −λ)f(0) ≥λf (∫ X q(x) p(x) λq(x) + (1−λ)p(x) ) + (1 −λ)f(0) (11) =λf (1 λ ∫ X λq(x) p(x) λq(x) + (1−λ)p(x) ) + (1 −λ)f(0) =λf (1 λ ∫ X (λq(x) + (1−λ)p(x) −(1 −λ)p(x)) p(x) λq(x) + (1−λ)p(x) ) + (1 −λ)f(0) =λf (1 λ − ∫ X ((1 −λ)p(x)) p(x) λq(x) + (1−λ)p(x) ) + (1 −λ)f(0) =λf (1 λ ) + (1 −λ)f(0) (12) where we use the fact that f is convex with Jensen’s inequality in Eq.(11) and the fact that p(x)p(x) = 0,∀x∈X in Eq.(12) since P and P has disjoint support. We also have Df(P||λP + (1 −λ)P) = ∫ X (λp(x) + (1−λ)p(x)) f ( p(x) λp(x) + (1−λ)p(x) ) = ∫ X (λp(x)) f ( p(x) λp(x) + (1−λ)p(x) ) + (1 −λ)f(0) = ∫ X (λp(x)) f ( p(x) λp(x) + 0 ) + (1 −λ)f(0) = λf (1 λ ) + (1 −λ)f(0) Therefore, in order for the inequality in Equation 11 to be an equality, we must have that q(x) = p(x) for all x ∈ X. Therefore, the generator distribution recovers the data distribution at the equlibrium posed by the NDA-GAN objective, which is also the case for the original GAN objective. Moreover, from Lemma 1 in Nguyen et al. (2008), we have that: arg max Dφ Lf(Q,Dφ) = f′(pdata/Q) (13) 13Published as a conference paper at ICLR 2021 Therefore, by replacing Qwith Gθ and (λGθ + (1 −λ)P), we have: arg max Dφ:X→R Lf(Gθ,Dφ) = f′(pdata/Gθ) (14) arg max Dφ:X→R Lf(λGθ + (1 −λ)P,Dφ) = f′(pdata/(λGθ + (1 −λ)P)) (15) which shows that the optimal discriminators are indeed different for the two objectives. D NDA FOR CONTRASTIVE REPRESENTATION LEARNING We describe the detailed statement of Theorem 2 and proof as follows. Theorem 3. For some distribution pover Xsuch that supp(p) ∩supp(pdata) = ∅, and for any maximizer of the NDA-CPC objective ˆh∈arg max hθ max gφ ICPC(hθ,gφ) the representations of negative samples are disjoint from that of positive samples for ˆh; i.e., ∀x ∈ supp(pdata),¯x ∈supp(p), supp(ˆh(¯x)) ∩supp(ˆh(x)) = ∅ Proof. We use a contradiction argument to establish the proof. For any representation mapping that maximizes the NDA-CPC objective, ˆh∈arg max hθ max gφ ICPC(hθ,gφ) suppose that the positive and NDA samples share some support, i.e., ∃x ∈ supp(pdata),¯x ∈ supp(p), supp(ˆh(¯x)) ∩supp(ˆh(x)) ̸= ∅ We can always constructˆh′that shares the same representation withˆhfor pdata but have disjoint rep- resentations for NDA samples; i.e., ∀x ∈supp(pdata),¯x ∈supp(p), the following two statements are true: 1. ˆh(x) = ˆh′(x); 2. supp(ˆh′(¯x)) ∩supp(ˆh′(x)) = ∅. Our goal is to prove that: max gφ ICPC(ˆh′,gφ) >max gφ ICPC(ˆh,gφ) (16) which shows a contradiction. For ease of exposition, let us allow zero values for the output ofg, and deﬁne 0/0 = 0 (in this case, if gassigns zero to positive values, then the CPC objective becomes−∞, so it cannot be a maximizer to the objective). Let ˆg∈arg maxICPC(ˆh,gφ) be an optimal critic to the representation model ˆhθ . We then deﬁne a following critic function: ˆg′(x,z) = { ˆg(x,z) if ∃x ∈supp(pdata) s.t. z ∈supp(ˆh′(x)) 0 otherwise (17) In other words, the critic assigns the same value for data-representation pairs over the support of pdata and zero otherwise. From the assumption over ˆh, ∃x ∈ supp(pdata),¯x ∈ supp(p), and z ∈supp(ˆh(¯x)), z ∈supp(ˆh(x)) 14Published as a conference paper at ICLR 2021 so (x,z) can be sampled as a positive pair and ˆg(x,z) >0. Therefore, max gφ ICPC(ˆh′,gφ) ≥ICPC(ˆh′,ˆg′) (18) = E [ log (n+ m)ˆg′(x,z) ˆg′(x,z) + ∑n−1 j=1 ˆg′(x, ˆzj) + ∑m k=1 ˆg′(x,zk)   =0 ] (plug in deﬁnition for NDA-CPC) ≥E [ log (n+ m)ˆg(x,z) ˆg(x,z) + ∑n−1 j=1 ˆg(x, ˆzj) + ∑m k=1 ˆg(x,zk) ] (existence of someˆg(x,z) >0 ) = max gφ ICPC(ˆh,gφ) (Assumption that ˆgis optimal critic) which proves the theorem via contradiction. E P IX2PIX Figure 7 highlights the qualitative improvements when we apply the NDA method to Pix2Pix model (Isola et al., 2017). Figure 7: Qualitative results on Cityscapes. F A NOMALY DETECTION Here, we show the histogram of difference in discriminator’s output for clean and OOD samples in Figure 8. High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN. G E FFECT OF HYPERPARAMETER ON UNCONDITIONAL IMAGE GENERATION Here, we show the effect of λfor unconditional image generation on CIFAR-10 dataset. Table 8: Effect ofλon the FID score for unconditional image generation on CIFAR-10 using Jigsaw as NDA. λ 1.0 0.75 0.5 0.25 0.15 FID 18.64 16.61 14.95 12.61 13.01 15Published as a conference paper at ICLR 2021 (a) Gaussian Noise  (b) Speckle Noise  (c) JPEG Compression Figure 8: Histogram of D(clean) - D(corrupt) for 3 different corruptions. H U NSUPERVISED LEARNING ON IMAGES 0.65 0.45 0.45 0.38 0.57 0.43 0.41 0.36 0.54 0.41 0.50 0.37  Figure 9: Comparing the cosine distance of the representations learned with Jigsaw NDA and Moco- V2 (shaded blue), and original Moco-V2 ( white). With NDA, we project normal and its jigsaw image representations further away from each other than the one without NDA. I D ATASET PREPARATION FOR FID EVALUATION For dataset preparation, we follow the the following procedures: (a) CIFAR-10 contains 60K 32×32 images with 10 labels, out of which 50K are used for training and 10K are used for testing, (b) CIFAR-100 contains 60K 32 ×32 images with 100 labels, out of which 50K are used for training and 10K are used for testing, (c) CelebA contains 162,770 train images and 19,962 test images (we resize the images to 64×64px), (d) STL-10 contains 100K (unlabeled) train images and 8K (labeled) test images (we resize the images to 32×32px). In our experiments the FID is calculated on the test dataset. In particular, we use 10K generated images vs. 10K test images for CIFAR-10, 10K vs. 10K for CIFAR-100, 19,962 vs. 19,962 for CelebA, and 8K vs 8K for STL-10. J H YPERPARAMETERS AND NETWORK ARCHITECTURE Generative Modeling. We use the same network architecture in BigGAN Brock et al. (2018) for our experiments. The code used for our experiments is based over the author’s PyTorch code. For CIFAR-10, CIFAR-100, and CelebA we train for 500 epochs whereas for STL-10 we train for 300 epochs. For all the datasets we use the following hyperparameters: batch-size = 64, generator learning rate = 2e-4, discriminator learning rate = 2e-4, discriminator update steps per generator update step = 4. The best model was selected on the basis of FID scores on the test set (as explained above). 16Published as a conference paper at ICLR 2021 Momentum Contrastive Learning. We use the ofﬁcial PyTorch implementation for our experi- ments. For CIFAR-10 and CIFAR-100, we perform unsupervised pre-training for 1000 epochs and supervised training (linear classiﬁer) for 100 epochs. For Imagenet-100, we perform unsupervised pre-training for 200 epochs and supervised training (linear classiﬁer) for 100 epochs. For CIFAR- 10 and CIFAR-100, we use the following hyperparameters during pre-training: batch-size = 256, learning-date = 0.3, temperature = 0.07, feature dimensionality = 2048. For ImageNet-100 pre- training we have the following: batch-size = 128, learning-date = 0.015, temperature = 0.2, feature dimensionality = 128. During linear classiﬁcation we use a batch size of 256 for all the datasets and learning rate of 10 for CIFAR-10, CIFAR-100, whereas for ImageNet-100 we use learning rate of 30. Dense Predictive Coding. We use the same network architecture and hyper-parameters in DPC Han et al. (2019) for our experiments and use the ofﬁcial PyTorch implementation. We perform self-supervised training on UCF-101 for 200 epochs and supervised training (action classiﬁer) for 200 epochs on both UCF-101 and HMDB51 datasets. K C ODE The code to reproduce our experiments is given here. 17",
      "references": [],
      "meta_data": {
        "arxiv_id": "2102.05113v1",
        "authors": [
          "Abhishek Sinha",
          "Kumar Ayush",
          "Jiaming Song",
          "Burak Uzkent",
          "Hongxia Jin",
          "Stefano Ermon"
        ],
        "published_date": "2021-02-09T20:28:35Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "A new Negative Data Augmentation (NDA) framework that leverages out-of-support (negative) samples to bias learning for GANs and contrastive representation learning; shows that NDA can recover the true data distribution in theory and improve finite-sample performance; demonstrates empirical gains on image generation, image translation, anomaly detection, and self-supervised representation learning for images and videos.",
        "methodology": "Introduce NDA into GAN training via a modified objective that mixes Gθ with an NDA distribution P and uses NDA samples to constrain the discriminator; provide theoretical guarantees (Theorem 1) that in infinite data/generative capacity, the generator recovers pdata; Propose NDA-CPC objective to encourage NDA representations to be disjoint from pdata representations; apply NDA across tasks with various NDA transforms (Jigsaw, Stitching, Cutout, Cutmix, Mixup).",
        "experimental_setup": "Unconditional and conditional image generation on CIFAR-10/100, CelebA, STL-10 using BigGAN; Image translation with Pix2Pix on Cityscapes; Anomaly detection using CIFAR-10 with OOD datasets and CIFAR-10-C; Representation learning experiments on CIFAR-10/100 and ImageNet-100 with MoCo-V2; Action recognition on UCF-101 and HMDB-51 with DPC; Object detection transfer via Faster R-CNN pre-trained on ImageNet-100; Datasets and evaluation metrics (FID, AUROC, Pp/Pc/mIoU, AP).",
        "limitations": "Theoretical results rely on infinite data/capacity; finite-data regime requires regularization; selection of NDA strategies and hyperparameters (λ) is empirical; NDA assumes access to an NDA distribution disjoint from pdata; generalization to other modalities beyond images/videos not fully explored; transformations must be informative NDA candidates.",
        "future_research_directions": "Explore more NDA transformations and domains; study finite-sample theory and optimization dynamics; combine NDA with other self-supervised methods; extend NDA to NLP or audio modalities; investigate automated discovery of NDA strategies; improve anomaly detection and OOD robustness using NDA.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks",
      "full_text": "Published as a conference paper at ICLR 2020 MIXUP INFERENCE : B ETTER EXPLOITING MIXUP TO DEFEND ADVERSARIAL ATTACKS Tianyu Pang∗, Kun Xu∗, Jun Zhu† Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua University; RealAI {pty17,xu-k16}@mails.tsinghua.edu.cn, dcszj@tsinghua.edu.cn ABSTRACT It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally unreasonable behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efﬁcient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants. 1 I NTRODUCTION Deep neural networks (DNNs) have achieved state-of-the-art performance on various tasks (Good- fellow et al., 2016). However, counter-intuitive adversarial examples generally exist in different domains, including computer vision (Szegedy et al., 2014), natural language processing (Jin et al., 2019), reinforcement learning (Huang et al., 2017), speech (Carlini & Wagner, 2018) and graph data (Dai et al., 2018). As DNNs are being widely deployed, it is imperative to improve model robustness and defend adversarial attacks, especially in safety-critical cases. Previous work shows that adversarial examples mainly root from the locally unstable behavior of classiﬁers on the data manifolds (Goodfellow et al., 2015; Fawzi et al., 2016; 2018; Pang et al., 2018b), where a small adversarial perturbation in the input space can lead to an unreasonable shift in the feature space. On the one hand, many previous methods try to solve this problem in the inference phase, by introducing transformations on the input images. These attempts include performing local linear transformation like adding Gaussian noise (Tabacof & Valle, 2016), where the processed inputs are kept nearby the original ones, such that the classiﬁers can maintain high performance on the clean inputs. However, as shown in Fig. 1(a), the equivalent perturbation, i.e., the crafted adversarial perturbation, is still δand this strategy is easy to be adaptively evaded since the randomness ofx0 w.r.t x0 is local (Athalye et al., 2018). Another category of these attempts is to apply various non-linear transformations, e.g., different operations of image processing (Guo et al., 2018; Xie et al., 2018; Raff et al., 2019). They are usually off-the-shelf for different classiﬁers, and generally aim to disturb the adversarial perturbations, as shown in Fig. 1(b). Yet these methods are not quite reliable since there is no illustration or guarantee on to what extent they can work. On the other hand, many efforts have been devoted to improving adversarial robustness in the training phase. For examples, the adversarial training (AT) methods (Madry et al., 2018; Zhang et al., 2019; Shafahi et al., 2019) induce locally stable behavior via data augmentation on adversarial examples. However, AT methods are usually computationally expensive, and will often degenerate model ∗Equal contribution. †Corresponding author. 1 arXiv:1909.11515v2  [cs.LG]  20 Feb 2020Published as a conference paper at ICLR 2020 !\"!\" #−\" %&% %' %(&%(Mixup Inference(Global Linear Transformation) !% %()%!)%(Local Linear Transformation !% %( %∗ !∗ %(∗ Non-Linear TransformationObserved inputsVirtual inputsThe processedinputs fed into classifiers (a) (b) (c) Figure 1: Intuitive mechanisms in the input space of different input-processing based defenses. xis the crafted adversarial example, x0 is the original clean example, which is virtual and unknown for the classiﬁers. δis the adversarial perturbation. performance on the clean inputs or under general-purpose transformations like rotation (Engstrom et al., 2019). In contrast, the mixup training method (Zhang et al., 2018) introduces globally linear behavior in-between the data manifolds, which can also improve adversarial robustness (Zhang et al., 2018; Verma et al., 2019a). Although this improvement is usually less signiﬁcant than it resulted by AT methods, mixup-trained models can keep state-of-the-art performance on the clean inputs; meanwhile, the mixup training is computationally more efﬁcient than AT. The interpolated AT method (Lamb et al., 2019) also shows that the mixup mechanism can further beneﬁt the AT methods. However, most of the previous work only focuses on embedding the mixup mechanism in the training phase, while the induced global linearity of the model predictions is not well exploited in the inference phase. Compared to passive defense by directly classifying the inputs (Zhang et al., 2018; Lamb et al., 2019), it would be more effective to actively defend adversarial attacks by breaking their locality via the globally linear behavior of the mixup-trained models. In this paper, we develop an inference principle for mixup-trained models, named mixup inference (MI). In each execution, MI performs a global linear transformation on the inputs, which mixups the input xwith a sampled clean example xs, i.e., ˜x= λx+ (1 −λ)xs (detailed in Alg. 1), and feed ˜xinto the classiﬁer as the processed input. There are two basic mechanisms for robustness improving under the MI operation (detailed in Sec. 3.2.1), which can be illustrated by simple geometric intuition in Fig. 1(c). One is perturbation shrinkage: if the input is adversarial, i.e., x= x0 + δ, the perturbation δwill shrink by a factor λafter performing MI, which is exactly the mixup ratio of MI according to the similarity between triangles. Another one is input transfer: after the MI operation, the reduced perturbation λδacts on random ˜x0. Comparing to the spatially or semantically local randomness introduced by Gaussian noise or image processing, ˜x0 introduces spatially global and semantically diverse randomness w.r.tx0. This makes it less effective to perform adaptive attacks against MI (Athalye et al., 2018). Furthermore, the global linearity of the mixup-trained models ensures that the information of x0 remained in ˜x0 is proportional to λ, such that the identity of x0 can be recovered from the statistics of ˜x0. In experiments, we evaluate MI on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) under the oblivious attacks (Carlini & Wagner, 2017) and the adaptive attacks (Athalye et al., 2018). The results demonstrate that our MI method is efﬁcient in defending adversarial attacks in inference, and is also compatible with other variants of mixup, e.g., the interpolated AT method (Lamb et al., 2019). Note that Shimada et al. (2019) also propose to mixup the input points in the test phase, but they do not consider their method from the aspect of adversarial robustness. 2 P RELIMINARIES In this section, we ﬁrst introduce the notations applied in this paper, then we provide the formula of mixup in training. We introduce the adversarial attacks and threat models in Appendix A.1. 2.1 N OTATIONS Given an input-label pair (x,y), a classiﬁer F returns the softmax prediction vector F(x) and the predicted label ˆy = arg maxj∈[L] Fj(x), where Lis the number of classes and [L] = {1,··· ,L}. The classiﬁer F makes a correct prediction on xif y= ˆy. In the adversarial setting, we augment the 2Published as a conference paper at ICLR 2020 data pair (x,y) to a triplet (x,y,z ) with an extra binary variable z, i.e., z= {1, if xis adversarial, 0, if xis clean. (1) The variable z is usually considered as hidden in the inference phase, so an input x(either clean or adversarially corrupted) can be generally denoted as x = x0 + δ·1z=1. Here x0 is a clean sample from the data manifold p(x) with label y0, 1z=1 is the indicator function, and δis a potential perturbation crafted by adversaries. It is worthy to note that the perturbation δshould not change the true label of the input, i.e., y= y0. For ℓp-norm adversarial attacks (Kurakin et al., 2017; Madry et al., 2018), we have ∥δ∥p ≤ϵ, where ϵis a preset threshold. Based on the assumption that adversarial examples are off the data manifolds, we formally have x0 + δ /∈supp(p(x)) (Pang et al., 2018a). 2.2 M IXUP IN TRAINING In supervised learning, the most commonly used training mechanism is the empirical risk minimiza- tion (ERM) principle (Vapnik, 2013), which minimizes 1 n ∑n i=1 L(F(xi),yi) on the training dataset D= {(xi,yi)}n i=1 with the loss function L. While computationally efﬁcient, ERM could lead to memorization of data (Zhang et al., 2017) and weak adversarial robustness (Szegedy et al., 2014). As an alternative, Zhang et al. (2018) introduce the mixup training mechanism, which minimizes 1 m ∑m j=1 L(F(˜xj),˜yj). Here ˜xj = λxj0 + (1−λ)xj1; ˜yj = λyj0 + (1−λ)yj1, the input-label pairs (xj0,yj0) and (xj1,yj1) are randomly sampled from the training dataset, λ∼Beta(α,α) and αis a hyperparameter. Training by mixup will induce globally linear behavior of models in-between data manifolds, which can empirically improve generalization performance and adversarial robustness (Zhang et al., 2018; Tokozume et al., 2018a;b; Verma et al., 2019a;b). Compared to the adversarial training (AT) methods (Goodfellow et al., 2015; Madry et al., 2018), trained by mixup requires much less computation and can keep state-of-the-art performance on the clean inputs. 3 M ETHODOLOGY Although the mixup mechanism has been widely shown to be effective in different domains (Berthelot et al., 2019; Beckham et al., 2019; Verma et al., 2019a;b), most of the previous work only focuses on embedding the mixup mechanism in the training phase, while in the inference phase the global linearity of the trained model is not well exploited. Compared to passively defending adversarial examples by directly classifying them, it would be more effective to actively utilize the globality of mixup-trained models in the inference phase to break the locality of adversarial perturbations. 3.1 M IXUP INFERENCE The above insight inspires us to propose the mixup inference (MI)method, which is a specialized inference principle for the mixup-trained models. In the following, we apply colored y, ˆyand ys to visually distinguish different notations. Consider an input triplet (x,y,z ), where zis unknown in advance. When directly feeding xinto the classiﬁer F, we can obtain the predicted label ˆy. In the adversarial setting, we are only interested in the cases where xis correctly classiﬁed by F if it is clean, or wrongly classiﬁed if it is adversarial (Kurakin et al., 2018). This can be formally denoted as 1y̸=ˆy = 1z=1. (2) The general mechanism of MI works as follows. Every time we execute MI, we ﬁrst sample a label ys ∼ps(y), then we sample xs from ps(x|ys) and mixup it with xas ˜x= λx+ (1 −λ)xs. ps(x,y) denotes the sample distribution, which is constrained to be on the data manifold, i.e., supp(ps(x)) ⊂ supp(p(x)). In practice, we execute MI for N times and average the output predictions to obtain FMI(x), as described in Alg. 1. Here we ﬁx the mixup ratio λin MI as a hyperparameter, while similar properties hold if λcomes from certain distribution. 3.2 T HEORETICAL ANALYSES Theoretically, with unlimited capability and sufﬁcient clean samples, a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples (Hornik et al., 1989; Guo et al., 2019), i.e., ∀xi,xj ∼p(x) and λ∈[0,1], there is H(λxi + (1 −λ)xj) = λH(xi) + (1−λ)H(xj). (3) 3Published as a conference paper at ICLR 2020 Algorithm 1Mixup Inference (MI) Input: The mixup-trained classiﬁer F; the input x. Hyperparameters: The sample distribution ps; the mixup ratio λ; the number of execution N. Initialize FMI(x) = 0; for k= 1 to N do Sample ys,k ∼ps(ys), xs,k ∼ps(xs|ys,k); Mixup xwith xs,k as ˜xk = λx+ (1 −λ)xs,k; Update FMI(x) = FMI(x) + 1 NF(˜xk); end for Return: The prediction FMI(x) of input x. Specially, we consider the case where the training objective Lis the cross-entropy loss, then H(xi) should predict the one-hot vector of label yi, i.e., Hy(xi) = 1y=yi. If the input x = x0 + δ is adversarial, then there should be an extra non-linear part G(δ; x0) of F, since x is off the data manifolds. Thus for any input x, the prediction vector can be compactly denoted as F(x) = F(x0 + δ·1z=1) = H(x0) + G(δ; x0) ·1z=1. (4) According to Eq. (3) and Eq. (4), the output of ˜xin MI is given by: F(˜x) = H(˜x0) + G(λδ; ˜x0) ·1z=1 = λH(x0) + (1−λ)H(xs) + G(λδ; ˜x0) ·1z=1, (5) where ˜x0 = λx0 + (1 −λ)xs is a virtual unperturbed counterpart of ˜xas shown in Fig. 1(c). Note that FMI(x) in Alg. 1 is a Monte Carlo approximation of Eps[F(˜x)] as FMI(x) = 1 N N∑ i=1 F(˜xi) ∞ −→Eps[F(˜x)], (6) where ∞ −→represents the limitation when the execution timesN →∞. Now we separately investigate the y-th and ˆy-th (could be the same one) components of F(˜x) according to Eq. (5), and see how these two components differ from those of F(x). These two components are critical because they decide whether we can correctly classify or detect adversarial examples (Goodfellow et al., 2016). Note that there is Hy(x0) = 1 and Hys(xs) = 1, thus we have the y-th components as Fy(x) = 1 + Gy(δ; x0) ·1z=1; Fy(˜x) = λ+ (1 −λ) ·1y=ys + Gy(λδ; ˜x0) ·1z=1. (7) Furthermore, according to Eq. (2), there is 1y=ˆy = 1z=0. We can represent the ˆy-th components as Fˆy(x) = 1z=0 + Gˆy(δ; x0) ·1z=1; Fˆy(˜x) = λ·1z=0 + (1 −λ) ·1ˆy=ys + Gˆy(λδ; ˜x0) ·1z=1. (8) From the above formulas we can ﬁnd that, except for the hidden variable z, the sampling label ys is another variable which controls the MI output F(˜x) for each execution. Different distributions of sampling ys result in different versions of MI. Here we consider two easy-to-implement cases: MI with predicted label (MI-PL): In this case, the sampling label ys is the same as the predicted label ˆy, i.e., ps(y) = 1y=ˆy is a Dirac distribution on ˆy. MI with other labels (MI-OL): In this case, the label ys is uniformly sampled from the labels other than ˆy, i.e., ps(y) = Uˆy(y) is a discrete uniform distribution on the set {y∈[L]|y̸= ˆy}. We list the simpliﬁed formulas of Eq. (7) and Eq. (8) under different cases in Table 1 for clear representation. With the above formulas, we can evaluate how the model performance changes with and without MI by focusing on the formula of ∆F(x; ps) = FMI(x) −F(x) ∞ −→Eps[F(˜x)] −F(x). (9) Speciﬁcally, in the general-purpose setting where we aim to correctly classify adversarial exam- ples (Madry et al., 2018), we claim that the MI method improves the robustness if the prediction 4Published as a conference paper at ICLR 2020 Table 1: The the simpliﬁed formulas of Eq. (7) and Eq. (8) in different versions of MI. Here MI-PL indicates mixup inference with predicted label; MI-OL indicates mixup inference with other labels. MI-PL MI-OL z= 0 z= 1 z= 0 z= 1 Fy(x) 1 1 +Gy(δ; x0) 1 1 +Gy(δ; x0) Fy(˜x) 1 λ+ Gy(λδ; ˜x0) λ λ+ (1−λ) ·1y=ys + Gy(λδ; ˜x0) Fˆy(x) 1 Gˆy(δ; x0) 1 Gˆy(δ; x0) Fˆy(˜x) 1 (1 −λ) +Gˆy(λδ; ˜x0) λ Gˆy(λδ; ˜x0) value on the true label yincreases while it on the adversarial label ˆydecreases after performing MI when the input is adversarial (z= 1). This can be formally denoted as ∆Fy(x; ps)|z=1 >0; ∆Fˆy(x; ps)|z=1 <0. (10) We refer to this condition in Eq. (10) as robustness improving condition (RIC). Further, in the detection-purpose setting where we want to detect the hidden variable zand ﬁlter out adversarial inputs, we can take the gap of the ˆy-th component of predictions before and after the MI operation, i.e., ∆Fˆy(x; ps) as the detection metric (Pang et al., 2018a). To formally measure the detection ability on z, we use the detection gap (DG), denoted as DG = ∆Fˆy(x; ps)|z=1 −∆Fˆy(x; ps)|z=0. (11) A higher value of DG indicates that ∆Fˆy(x; ps) is better as a detection metric. In the following sections, we speciﬁcally analyze the properties of different versions of MI according to Table 1, and we will see that the MI methods can be used and beneﬁt in different defense strategies. 3.2.1 M IXUP INFERENCE WITH PREDICTED LABEL In the MI-PL case, when the input is clean (i.e., z = 0 ), there is F(x) = F(˜x), which means ideally the MI-PL operation does not inﬂuence the predictions on the clean inputs. When the input is adversarial (i.e., z= 1), MI-PL can be applied as a general-purpose defense or a detection-purpose defense, as we separately introduce below: General-purpose defense:If MI-PL can improve the general-purpose robustness, it should satisfy RIC in Eq. (10). By simple derivation and the results of Table 1, this means that Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(λδ; ˜x0)] {>1 −λ, if k= ˆy, <λ −1, if k= y. (12) Since an adversarial perturbation usually suppress the predicted conﬁdence on the true label and pro- mote it on the target label (Goodfellow et al., 2015), there should beGˆy(δ; ˜x0) >0 and Gy(δ; ˜x0) <0. Note that the left part of Eq. (12) can be decomposed into Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(δ; ˜x0)]   input transfer + Exs∼ps(x|ˆy) [Gk(δ; ˜x0) −Gk(λδ; ˜x0)]   perturbation shrinkage . (13) Here Eq. (13) indicates the two basic mechanisms of the MI operations defending adversarial attacks, as shown in Fig. 1(c). The ﬁrst mechanism is input transfer, i.e., the clean input that the adversarial perturbation acts on transfers from the deterministic x0 to stochastic ˜x0. Compared to the Gaussian noise or different image processing methods which introduce spatially or semantically local randomness, the stochastic ˜x0 induces spatially global and semantically diverse randomness. This will make it harder to perform an adaptive attack in the white-box setting (Athalye et al., 2018). The second mechanism isperturbation shrinkage, where the original perturbationδshrinks by a factor λ. This equivalently shrinks the perturbation threshold since ∥λδ∥p = λ∥δ∥p ≤λϵ, which means that MI generally imposes a tighter upper bound on the potential attack ability for a crafted perturbation. Besides, empirical results in previous work also show that a smaller perturbation threshold largely weakens the effect of attacks (Kurakin et al., 2018). Therefore, if an adversarial attack defended by these two mechanisms leads to a prediction degradation as in Eq. (12), then applying MI-PL would improve the robustness against this adversarial attack. Similar properties also hold for MI-OL as described in Sec. 3.2.2. In Fig. 2, we empirically demonstrate that most of the existing adversarial attacks, e.g., the PGD attack (Madry et al., 2018) satisﬁes these properties. 5Published as a conference paper at ICLR 2020 MI-OL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 !\"#$ !\"%#$ !\"%# !\"# −'\"(;#$*−1,−1−'\"*(;%#$ * !\" 1,−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# 1,−1 '-\"(;#$ !-\"%#$ !-\"%#*−1,−1+'-\"*(;%#$ MI-PL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !\" !\"#$ !\"# −'\"(;#$ !\"%#$ !\"%# 1−*−'\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# '-\"(;#$ !-\"%#$ !-\"%# 1−*+'-\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' (1−*)(,−2),−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' 1−* Clean inputsAdversarial inputs∆'\"='\"*(;%#$−'\"(;#$ ∆'-\"='-\"(;#$−'-\"*(;%#$ Figure 2: The results are averaged on 100 randomly test clean samples of CIFAR-10. The adversarial attack is untargeted PGD-10. Note that the ∆Gy calculated here is the minus value of it in Eq. (12) and Eq. (15). Detection-purpose defense:According to Eq. (11), the formula of DG for MI-PL is DGMI-PL = Exs∼ps(x|ˆy)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (14) By comparing Eq. (12) and Eq. (14), we can ﬁnd that they are consistent with each other, which means that for a given adversarial attack, if MI-PL can better defend it in general-purpose, then ideally MI-PL can also better detect the crafted adversarial examples. 3.2.2 M IXUP INFERENCE WITH OTHER LABELS As to MI-OL, when the input is clean (z= 0), there would be a degeneration on the optimal clean prediction as Fy(˜x) = Fˆy(˜x) = λ, since the sampled xs does not come from the true label y. As compensation, MI-OL can better improve robustness compared to MI-PL when the input is adversarial (z= 1), since the sampled xs also does not come from the adversarial label ˆyin this case. General-purpose defense:Note that in the MI-OL formulas of Table 1, there is a term of 1y=ys. Since we uniformly select ys from the set [L] \\{ˆy}, there is E(1y=ys) = 1 L−1 . According to the RIC, MI-OL can improve robustness against the adversarial attacks if there satisﬁes Eys∼Uˆy(y)Exs∼ps(x|ys) [Gk(δ; x0) −Gk(λδ; ˜x0)] { >0, if k= ˆy, < (λ−1)(L−2) L−1 , if k= y. (15) Note that the conditions in Eq. (15) is strictly looser than Eq. (12), which means MI-OL can defend broader range of attacks than MI-PL, as veriﬁed in Fig. 2. Detection-purpose defense:According to Eq. (11) and Table 1, the DG for MI-OL is DGMI-OL = Eys∼Uˆy(y)Exs∼ps(x|ys)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (16) It is interesting to note thatDGMI-PL = DGMI-OL, thus the two variants of MI have the same theoretical performance in the detection-purpose defenses. However, in practice we ﬁnd that MI-PL performs better than MI-OL in detection, since empirically mixup-trained models cannot induce ideal global linearity (cf. Fig. 2 in Zhang et al. (2018)). Besides, according to Eq. (6), to statistically make sure that the clean inputs will be correctly classiﬁed after MI-OL, there should be ∀k∈[L] \\{y}, Eys∼Uˆy(y)Exs∼ps(x|ys)[Fy −Fk] >0 =⇒λ>L −1. (17) 4 E XPERIMENTS In this section, we provide the experimental results on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) to demonstrate the effectiveness of our MI methods on defending adversarial attacks. Our codes are available at https://github.com/P2333/Mixup-Inference. 6Published as a conference paper at ICLR 2020 Table 2: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-10. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 4. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 93.8 3.6 3.2 3.1 ≤1 ≤1 ≤1 Mixup + Gaussian noise 84.4 13.5 9.6 8.8 37.7 28.6 27.9 Mixup + Random rotation 82.0 21.8 18.7 18.2 38.9 32.5 26.5 Mixup + Xie et al. (2018) 82.1 23.0 19.6 19.1 38.4 31.1 25.2 Mixup + Guo et al. (2018) 83.3 31.2 28.8 28.3 57.8 49.1 48.9 ERM + MI-OL (ablation study) 81.6 7.4 6.4 6.1 33.0 26.7 23.2 Mixup + MI-OL 83.9 26.1 18.8 18.3 55.6 51.2 50.8 Mixup + MI-Combined 82.9 33.7 31.0 30.7 56.1 49.7 49.4 Interpolated AT 89.7 46.7 43.5 42.5 65.6 62.5 61.9 Interpolated AT + Gaussian noise 84.7 55.6 53.7 53.5 70.1 69.1 69.0 Interpolated AT + Random rotation 83.4 57.8 56.7 55.9 69.8 68.2 67.4 Interpolated AT + Xie et al. (2018) 82.1 59.7 58.4 57.9 71.1 69.7 69.3 Interpolated AT + Guo et al. (2018) 83.9 60.9 60.7 60.3 73.2 72.1 71.6 AT + MI-OL (ablation study) 81.2 56.2 55.8 55.1 67.7 67.2 66.4 Interpolated AT + MI-OL 84.2 64.5 63.8 63.3 75.3 74.7 74.5 0.23 0.78 0.08 0.72 0.05 0.88 0.01 0.84 00.10.20.30.40.50.60.70.80.91 MixupMixup + MI-PLPGD-10 (untargeted)PGD-50 (untargeted)PGD-10 (targeted)PGD-50 (targeted) Random guess (a) AUC scores 0 10 20 30 40 50 60 70 80 90 100 Accuracy on clean examples (%) 0 5 10 15 20 25 30 35 40 45 50Accuracy on adversarial examples (%) mixup + Gaussian noise mixup + Rotation mixup + Xie et al. (2018) mixup + Guo et al. (2018) ERM + MI-OL mixup + MI-OL mixup + MI-Combined (b) Adversarial accuracy w.r.t clean accuracy Figure 3: Results on CIFAR-10. (a) AUC scores on 1,000 randomly selected test clean samples and 1,000 adversarial counterparts crafted on these clean samples. (b) The adversarial accuracy w.r.t clean accuracy on 1,000 randomly selected test samples. The adversarial attack is untargeted PGD-10, with ϵ= 8/255 and step size 2/255. Each point for a certain method corresponds to a set of hyperparameters. 4.1 S ETUP In training, we use ResNet-50 (He et al., 2016) and apply the momentum SGD optimizer (Qian, 1999) on both CIFAR-10 and CIFAR-100. We run the training for200 epochs with the batch size of 64. The initial learning rate is 0.01 for ERM, mixup and AT; 0.1 for interpolated AT (Lamb et al., 2019). The learning rate decays with a factor of 0.1 at 100 and 150 epochs. The attack method for AT and interpolated AT is untargeted PGD-10 with ϵ = 8/255 and step size 2/255 (Madry et al., 2018), and the ratio of the clean examples and the adversarial ones in each mini-batch is 1 : 1 (Lamb et al., 2019). The hyperparameter αfor mixup and interpolated AT is1.0 (Zhang et al., 2018). All defenses with randomness are executed 30 times to obtain the averaged predictions (Xie et al., 2018). 4.2 E MPIRICAL VERIFICATION OF THEORETICAL ANALYSES To verify and illustrate our theoretical analyses in Sec. 3, we provide the empirical relationship between the output predictions of MI and the hyperparameter λin Fig. 2. The notations and formulas annotated in Fig. 2 correspond to those introduced in Sec. 3. We can see that the results follow our theoretical conclusions under the assumption of ideal global linearity. Besides, both MI-PL and MI-OL empirically satisfy RIC in this case, which indicates that they can improve robustness under the untargeted PGD-10 attack on CIFAR-10, as quantitatively demonstrated in the following sections. 7Published as a conference paper at ICLR 2020 Table 3: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-100. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 5. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 74.2 5.5 5.3 5.2 ≤1 ≤1 ≤1 Mixup + Gaussian noise 65.0 5.5 5.3 5.3 10.0 4.3 4.1 Mixup + Random rotation 66.2 7.8 6.7 6.3 21.4 15.5 15.2 Mixup + Xie et al. (2018) 66.3 9.6 7.6 7.4 30.2 22.5 22.3 Mixup + Guo et al. (2018) 66.1 13.1 10.8 10.5 33.3 26.3 26.1 Mixup + MI-OL 68.8 12.6 9.4 9.1 37.0 29.0 28.7 Mixup + MI-Combined 67.0 14.8 11.7 11.3 31.4 26.9 26.7 Interpolated AT 64.7 26.6 24.1 24.0 52.0 50.1 49.8 Interpolated AT + Gaussian noise 60.4 32.6 31.6 31.4 50.1 50.0 49.6 Interpolated AT + Random rotation 62.6 34.5 32.4 32.1 51.0 49.9 49.7 Interpolated AT + Xie et al. (2018) 62.1 42.2 41.5 41.3 57.1 56.3 55.8 Interpolated AT + Guo et al. (2018) 61.5 36.2 33.7 33.3 53.8 52.4 52.2 Interpolated AT + MI-OL 62.0 43.8 42.8 42.5 58.1 56.7 56.5 4.3 P ERFORMANCE UNDER OBLIVIOUS ATTACKS In this subsection, we evaluate the performance of our method under the oblivious-box attacks (Carlini & Wagner, 2017). The oblivious threat model assumes that the adversary is not aware of the existence of the defense mechanism, e.g., MI, and generate adversarial examples based on the unsecured classiﬁcation model. We separately apply the model trained by mixup and interpolated AT as the classiﬁcation model. The AUC scores for the detection-purpose defense are given in Fig. 3(a). The results show that applying MI-PL in inference can better detect adversarial attacks, while directly detecting by the returned conﬁdence without MI-PL performs even worse than a random guess. We also compare MI with previous general-purpose defenses applied in the inference phase, e.g., adding Gaussian noise or random rotation (Tabacof & Valle, 2016); performing random padding or resizing after random cropping (Guo et al., 2018; Xie et al., 2018). The performance of our method and baselines on CIFAR-10 and CIFAR-100 are reported in Table 2 and Table 3, respectively. Since for each defense method, there is a trade-off between the accuracy on clean samples and adversarial samples depending on the hyperparameters, e.g., the standard deviation for Gaussian noise, we carefully select the hyperparameters to ensure both our method and baselineskeep a similar performance on clean data for fair comparisons. The hyperparameters used in our method and baselines are reported in Table 4 and Table 5. In Fig. 3(b), we further explore this trade-off by grid searching the hyperparameter space for each defense to demonstrate the superiority of our method. As shown in these results, our MI method can signiﬁcantly improve the robustness for the trained mod- els with induced global linearity, and is compatible with training-phase defenses like the interpolated AT method. As a practical strategy, we also evaluate a variant of MI, calledMI-Combined, which applies MI-OL if the input is detected as adversarial by MI-PL with a default detection threshold; otherwise returns the prediction on the original input. We also perform ablation studies of ERM / AT + MI-OL in Table 2, where no global linearity is induced. The results verify that our MI methods indeed exploit the global linearity of the mixup-trained models, rather than simply introduce randomness. 4.4 P ERFORMANCE UNDER WHITE -BOX ADAPTIVE ATTACKS Following Athalye et al. (2018), we test our method under the white-box adaptive attacks (detailed in Appendix B.2). Since we mainly adopt the PGD attack framework, which synthesizes adversarial examples iteratively, the adversarial noise will be clipped to make the input image stay within the valid range. It results in the fact that with mixup on different training examples, the adversarial perturbation will be clipped differently. To address this issue, we average the generated perturbations over the adaptive samples as the ﬁnal perturbation. The results of the adversarial accuracy w.r.t the number of adaptive samples are shown in Fig. 4. We can see that even under a strong adaptive attack, equipped with MI can still improve the robustness for the classiﬁcation models. 8Published as a conference paper at ICLR 2020 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Mixup + MI-OL Interpolated AT + MI-OL Adversarial accuracyNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samples Untargetedmode UntargetedmodeTargetedmode 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Targetedmode Adaptive PGD-10Adaptive PGD-50 Adaptive PGD-200 Figure 4: Classiﬁcation accuracy under the adaptive PGD attacks on CIFAR-10. The number of adaptive samples refers to the execution times of sampling xs in each iteration step of adaptive PGD. The dash lines are the accuracy of trained models without MI-OL under PGD attacks. 5 C ONCLUSION In this paper, we propose the MI method, which is specialized for the trained models with globally linear behaviors induced by, e.g., mixup or interpolated AT. As analyzed in Sec. 3, MI can exploit this induced global linearity in the inference phase to shrink and transfer the adversarial perturbation, which breaks the locality of adversarial attacks and alleviate their aggressivity. In experiments, we empirically verify that applying MI can return more reliable predictions under different threat models. ACKNOWLEDGEMENTS This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, U19B2034, U1811461), Beijing NSF Project (No. L172037), Beijing Academy of Artiﬁcial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program and the NVIDIA NV AIL Program with GPU/DGX Acceleration. REFERENCES Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning (ICML), 2018. Christopher Beckham, Sina Honari, Alex Lamb, Vikas Verma, Farnoosh Ghadiri, R Devon Hjelm, and Christopher Pal. Adversarial mixup resynthesizers. In Advances in Neural Information Processing Systems (NeurIPS), 2019. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artiﬁcial Intelligence and Security (AISec), 2017. Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 1–7. IEEE, 2018. Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In International Conference on Machine Learning (ICML), 2018. Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 9Published as a conference paper at ICLR 2020 Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation sufﬁce: Fooling cnns with simple transformations. In International Conference on Machine Learning (ICML), 2019. Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classi- ﬁers: from adversarial to random noise. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1632–1640, 2016. Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015. Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations (ICLR), 2018. Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regular- ization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 33, pp. 3714–3722, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pp. 630–645. Springer, 2016. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989. Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Di Jin, Zhijing Jin, Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language attack on text classiﬁcation and entailment. arXiv preprint arXiv:1907.11932, 2019. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In The International Conference on Learning Representations (ICLR) Workshops, 2017. Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, et al. Adversarial attacks and defences competition. arXiv preprint arXiv:1804.00097, 2018. Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving robust neural networks without sacriﬁcing accuracy. arXiv preprint arXiv:1906.06784, 2019. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 427–436, 2015. Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS) , pp. 4579–4589, 2018a. 10Published as a conference paper at ICLR 2020 Tianyu Pang, Chao Du, and Jun Zhu. Max-mahalanobis linear discriminant analysis networks. In International Conference on Machine Learning (ICML), 2018b. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145–151, 1999. Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for adversarially robust defense. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6528–6537, 2019. Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information Processing Systems (NeurIPS), 2019. Takuya Shimada, Shoichiro Yamaguchi, Kohei Hayashi, and Sosuke Kobayashi. Data interpolating prediction: Alternative interpretation of mixup. arXiv preprint arXiv:1906.08412, 2019. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In 2016 International Joint Conference on Neural Networks (IJCNN), pp. 426–433. IEEE, 2016. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Learning from between-class examples for deep sound recognition. International Conference on Learning Representations (ICLR), 2018a. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classi- ﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5486–5494, 2018b. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. In International Conference on Machine Learning (ICML), 2019a. Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019b. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In International Conference on Learning Representations (ICLR), 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning (ICML), 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018. 11Published as a conference paper at ICLR 2020 A M ORE BACKGROUNDS In this section, we provide more backgrounds which are related to our work in the main text. A.1 A DVERSARIAL ATTACKS AND THREAT MODELS Adversarial attacks.Although deep learning methods have achieved substantial success in different domains (Goodfellow et al., 2016), human imperceptible adversarial perturbations can be easily crafted to fool high-performance models, e.g., deep neural networks (DNNs) (Nguyen et al., 2015). One of the most commonly studied adversarial attack is the projected gradient descent (PGD) method (Madry et al., 2018). Let rbe the number of iteration steps, x0 be the original clean example, then PGD iteratively crafts the adversarial example as x∗ i = clipx,ϵ(x∗ i−1 + ϵi ·sign(∇x∗ i−1L(x∗ i−1,y))), (18) where clipx,ϵ(·) is the clipping function. Here x∗ 0 is a randomly perturbed image in the neighborhood of x0, i.e., ˚U(x0,ϵ), and the ﬁnally returned adversarial example is x= x∗ r = x0 + δ, following our notations in the main text. Threat models.Here we introduce different threat models in the adversarial setting. As suggested in Carlini et al. (2019), a threat model includes a set of assumptions about the adversarys goals, capabilities, and knowledge. Adversary’s goals could be simply fooling the classiﬁers to misclassify, which is referred to as untargeted mode. Alternatively, the goals can be more speciﬁc to make the model misclassify certain examples from a source class into a target class, which is referred to as targeted mode. In our experiments, we evaluate under both modes, as shown in Table 2 and Table 3. Adversary’s capabilities describe the constraints imposed on the attackers. Adversarial examples require the perturbation δto be bounded by a small threshold ϵunder ℓp-norm, i.e., ∥δ∥p ≤ϵ. For example, in the PGD attack, we consider under the ℓ∞-norm. Adversary’s knowledge describes what knowledge the adversary is assumed to have. Typically, there are three settings when evaluating a defense method: •Oblivious adversaries are not aware of the existence of the defense Dand generate adver- sarial examples based on the unsecured classiﬁcation model F (Carlini & Wagner, 2017). •White-box adversaries know the scheme and parameters of D, and can design adaptive methods to attack both the model F and the defense Dsimultaneously (Athalye et al., 2018). •Black-box adversaries have no access to the parameters of the defense Dor the model F with varying degrees of black-box access (Dong et al., 2018). In our experiments, we mainly test under the oblivious setting (Sec. 4.3) and white-box setting (Sec. 4.4), since previous work has already demonstrated that randomness itself is efﬁcient on defending black-box attacks (Guo et al., 2018; Xie et al., 2018). A.2 I NTERPOLATED ADVERSARIAL TRAINING To date, the most widely applied framework for adversarial training (AT) methods is the saddle point framework introduced in Madry et al. (2018): min θ ρ(θ), where ρ(θ) = E(x,y)∼p[max δ∈S L(x+ δ,y; θ)]. (19) Here θrepresents the trainable parameters in the classiﬁer F, and Sis a set of allowed perturbations. In implementation, the inner maximization problem for each input-label pair (x,y) is approximately solved by, e.g., the PGD method with different random initialization (Madry et al., 2018). As a variant of the AT method, Lamb et al. (2019) propose the interpolated AT method, which combines AT with mixup. Interpolated AT trains on interpolations of adversarial examples along with interpolations of unperturbed examples (cf. Alg. 1 in Lamb et al. (2019)). Previous empirical results demonstrate that interpolated AT can obtain higher accuracy on the clean inputs compared to the AT method without mixup, while keeping the similar performance of robustness. 12Published as a conference paper at ICLR 2020 Table 4: The parameter settings for the methods in Table 2. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.04 Mixup + Random rotation Rotation degree range [−40◦,40◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [16,24] Mixup + Guo et al. (2018) The random crop size is randomly selected from [22,30] ERM + MI-OL (ablation study) The λOL = 0.6 Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.075 Interpolated AT + Random rotation Rotation degree range [−30◦,30◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [20,28] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [20,28] AT + MI-OL (ablation study) The λOL = 0.8 Interpolated AT + MI-OL The λOL = 0.6 B T ECHNICAL DETAILS We provide more technical details about our method and the implementation of the experiments. B.1 M ORE DISCUSSION ON THE MI METHOD Generality. According to Sec. 3, except for the mixup-trained models, the MI method is generally compatible with any trained model with induced global linearity. These models could be trained by other methods, e.g., manifold mixup (Verma et al., 2019a; Inoue, 2018; Lamb et al., 2019). Besides, to better defend white-box adaptive attacks, the mixup ratio λin MI could also be sampled from certain distribution to put in additional randomness. Empirical gap.As demonstrated in Fig. 2, there is a gap between the empirical results and the theo- retical formulas in Table 1. This is because that the mixup mechanism mainly acts as a regularization in training, which means the induced global linearity may not satisfy the expected behaviors. To improve the performance of MI, a stronger regularization can be imposed, e.g., training with mixup for more epochs, or applying matched λboth in training and inference. B.2 A DAPTIVE ATTACKS FOR MIXUP INFERENCE Following Athalye et al. (2018), we design the adaptive attacks for our MI method. Speciﬁcally, according to Eq. (6), the expected model prediction returned by MI is: FMI(x) = Eps[F(λx+ (1 −λ)xs)]. (20) Note that generally the λ in MI comes from certain distribution. For simplicity, we ﬁx λ as a hyperparameter in our implementation. Therefore, the gradients of the prediction w.r.t. the input xis: ∂FMI(x) ∂x = Eps [∂F(λx+ (1 −λ)xs) ∂x ] (21) = Eps [∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs ·∂λx+ (1 −λ)xs ∂x ] (22) = λEps [∂F(u) ∂u |u=λx+(1−λ)xs ] . (23) 13Published as a conference paper at ICLR 2020 Table 5: The parameter settings for the methods in Table 3. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.025 Mixup + Random rotation Rotation degree range [−20◦,20◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [18,26] Mixup + Guo et al. (2018) The random crop size is randomly selected from [24,32] Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.06 Interpolated AT + Random rotation Rotation degree range [−20◦,20◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [22,30] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [24,32] Interpolated AT + MI-OL The λOL = 0.6 Clean Adversarial Figure 5: Adversarial examples crafted by adaptive attacks with ϵ= 16/255 on CIFAR-10, against the defense of Interpolated AT + MI-OL. In the implementation of adaptive PGD attacks, we ﬁrst sample a series of examples {xs,k}NA k=1, where NA is the number of adaptive samples in Fig. 3. Then according to Eq. (18), the sign of gradients used in adaptive PGD can be approximated by sign (∂FMI(x) ∂x ) ≈sign (NA∑ k=1 ∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs,k ) . (24) B.3 H YPERPARAMETER SETTINGS The hyperparameter settings of the experiments shown in Table 2 and Table 3 are provided in Table 4 and Table 5, respectively. Since the original methods in Xie et al. (2018) and Guo et al. (2018) are both designed for the models on ImageNet, we adapt them for CIFAR-10 and CIFAR-100. Most of our experiments are conducted on the NVIDIA DGX-1 server with eight Tesla P100 GPUs. 14",
      "references": [],
      "meta_data": {
        "arxiv_id": "1909.11515v2",
        "authors": [
          "Tianyu Pang",
          "Kun Xu",
          "Jun Zhu"
        ],
        "published_date": "2019-09-25T14:21:55Z",
        "github_url": "https://github.com/P2333/Mixup-Inference"
      },
      "llm_extracted_info": {
        "main_contributions": "We introduce mixup inference (MI), an inference-time procedure designed for mixup-trained models to actively exploit the global linearity induced by mixup in order to defend adversarial attacks. MI augments each input by performing a mixup with randomly sampled clean examples, effectively shrinking and transferring adversarial perturbations and producing more robust predictions. Two MI variants are analyzed: MI with the predicted label (MI-PL) and MI with other labels (MI-OL). A detection-oriented variant uses the prediction gap to flag adversarial inputs. The paper provides theoretical conditions (robustness improving conditions, RIC) showing how MI leverages perturbation shrinkage and input transfer, and empirically demonstrates improved adversarial robustness on CIFAR-10/100 for mixup-trained models, including compatibility with interpolated adversarial training (Interpolated AT).",
        "methodology": "The methodology combines (1) mixup training to enforce global linearity among convex combinations of training samples, (2) a principled mixup inference (MI) procedure: for input x, sample ys ~ p_s(y) and xs ~ p_s(x|ys), form x_tilde = λ x + (1−λ) xs, and feed F(x_tilde); repeat N times and average predictions FMI(x). Two variants: MI-PL (ys equals predicted label ˆy) and MI-OL (ys uniformly drawn from labels ≠ ˆy). The authors derive F(x) ≈ H(x0) + G(δ; x0) 1_{z=1} under cross-entropy training, and F(x_tilde) ≈ H(x̃0) + G(λ δ; x̃0) 1_{z=1}, with x̃0 = λ x0 + (1−λ) xs. They define robustness improving condition ∆F on true label increases and ∆F on adversarial label decreases under MI, decomposing into input transfer and perturbation shrinkage components. They also define a detection gap (DG) for MI-PL and MI-OL to quantify adversarial detectability. Empirical validation includes AUC detection metrics and adversarial accuracy under untargeted PGD attacks, as well as integration with MI-Combined strategy and Interpolated AT.",
        "experimental_setup": "Datasets: CIFAR-10 and CIFAR-100. Model: ResNet-50 trained with SGD momentum, 200 epochs, batch size 64. Training settings include learning rates (0.01 for ERM, Mixup, AT; 0.1 for Interpolated AT) with stepwise decay at 100 and 150 epochs; α = 1.0 for mixup. Attacks: untargeted PGD with ϵ = 8/255 and step size 2/255; evaluation under oblivious attacks and white-box adaptive attacks. Defenses compared: Mixup; Mixup + Gaussian noise; Mixup + Random rotation; Mixup + Xie et al. 2018; Mixup + Guo et al. 2018; ERM + MI-OL (ablation); MI-OL; MI-PL; MI-Combined; Interpolated AT; Interpolated AT + Gaussian noise/Rotation/Xie/Guo; Interpolated AT + MI-OL. Evaluation metrics include clean accuracy and adversarial accuracy across PGD10/50/200, as well as AUC for detection under oblivious settings. The experiments include extensive ablations and an adaptive white-box attack study, with results reported in Tables 2–5 and figures showing MI gains and detection performance.",
        "limitations": "Assumes that the model exhibits near-ideal global linearity due to mixup; empirical results show gaps between theory and practice because mixup acts as regularization rather than perfect linearity. The effectiveness of MI depends on quality of mixup-induced linearity and the sampling strategy (λ fixed or varied). There is additional computational cost due to N repeated inferences during MI. The evaluation is mainly on CIFAR-10/100 with ResNet-50; generalization to larger datasets (e.g., ImageNet) or other domains not shown. Under strong white-box adaptive attacks, while MI improves robustness, it does not render defenses invulnerable. The approach relies on sampling from a data-manifold-preserving distribution ps(x|ys).",
        "future_research_directions": "Explore stronger or more explicit regularization to enforce global linearity in training; experiment with different MI sampling schemes, including λ drawn from distributions; apply MI to other mixup-based methods and domains (e.g., ImageNet, NLP, audio); evaluate against more sophisticated white-box adaptive attacks and develop stronger adaptive strategies; combine MI with additional detection mechanisms and robustness techniques; optimize computational efficiency (reducing N or using amortized inference); provide theoretical guarantees for robustness bounds under MI.",
        "experimental_code": "# MI-PL implementation excerpt (attack_resnet_mixuptest_PL.py)\n# Build mixup pool by predicted label\nfor i, data_batch in enumerate(dataloader_train):\n    img_batch, label_batch = data_batch\n    img_batch = img_batch.to(device)\n    _, label_ind = torch.max(label_batch.data, 1)\n    mixup_pool_PL[label_ind.numpy()[0]].append(img_batch)\n    if i >= (num_pool - 1):\n        break\nprint('Finish constructing mixup_pool_PL')\n\n# MI-PL inference\npred_cle_mixup_all_PL = 0\npred_adv_mixup_all_PL = 0\n\nfor k in range(num_sample_MIPL):\n    # CLEAN\n    len_cle = np.random.randint(len(mixup_pool_PL[predicted_cle]))\n    mixup_img_cle = (1 - FLAGS.lamdaPL) * mixup_pool_PL[predicted_cle][len_cle] + FLAGS.lamdaPL * img_batch\n    pred_cle_mixup = classifier(mixup_img_cle)\n    pred_cle_mixup_all_PL = pred_cle_mixup_all_PL + soft_max(pred_cle_mixup.data)\n\n    # ADVERSARIAL\n    len_adv = np.random.randint(len(mixup_pool_PL[predicted_adv]))\n    mixup_img_adv = (1 - FLAGS.lamdaPL) * mixup_pool_PL[predicted_adv][len_adv] + FLAGS.lamdaPL * adv_x\n    pred_adv_mixup = classifier(mixup_img_adv)\n    pred_adv_mixup_all_PL = pred_adv_mixup_all_PL + soft_max(pred_adv_mixup.data)\n\npred_cle_mixup_all_PL = pred_cle_mixup_all_PL / num_sample_MIPL\npred_adv_mixup_all_PL = pred_adv_mixup_all_PL / num_sample_MIPL\n\n# post-process: choose between PL and OL\ngap_cle = cle_con.type(torch.float).item() - cle_con_mixup_PL.type(torch.float).item()\ngap_adv = adv_con.type(torch.float).item() - adv_con_mixup_PL.type(torch.float).item()\nif gap_cle < FLAGS.threshold:\n    pred_cle_mixup_all = pred_cle_mixup_all_PL\nelse:\n    pred_cle_mixup_all = pred_cle_mixup_all_OL\n\nif gap_adv < FLAGS.threshold:\n    pred_adv_mixup_all = pred_adv_mixup_all_PL\nelse:\n    pred_adv_mixup_all = pred_adv_mixup_all_OL\n",
        "experimental_info": "MI variant implementations (as used in the repository):\n\nMI-PL (mixup inference with predicted label)\n- Dataset: CIFAR-10; 10 classes (and CIFAR-100 in some scripts)\n- Data: 32x32 color images; uses cifar_transform() for data augmentation\n- Pool construction: Build mixup_pool_PL from training data, grouped by the predicted label of the sample\n- Parameters:\n  - num_test = 1000\n  - num_pool = 10000\n  - num_sample_MIPL = FLAGS.num_sample_MIPL (default 5)\n  - lamdaPL = FLAGS.lamdaPL (default 0.5)\n  - threshold = FLAGS.threshold (default 0.2)\n- Inference: loop k in range(num_sample_MIPL), sample a pool image with the same predicted label, mix with input x using lamdaPL, classify, accumulate probabilities\n- Decision rule: after averaging, compute gap between the maximum probability of the clean/prediction and the PL-mixed prediction (gap_cle and gap_adv). If gap < threshold, use MI-PL results; otherwise use MI-OL results\n\nMI-OL (mixup inference with other labels)\n- Pool construction: Build mixup_pool_OL from training data, but samples are drawn from a different label (xs_label) not equal to the predicted label\n- Parameters:\n  - num_sample_MIOL = FLAGS.num_sample_MIOL (default 15)\n  - lamdaOL = FLAGS.lamdaOL (default 0.5)\n- Inference: for k in range(num_sample_MIOL), pick a random label xs_label != predicted label, pick an image from its pool, mix with lamdaOL, feed classifier, accumulate probabilities\n- Optional data augmentations controlled by booleans (combine_with_Raff, combine_with_JPEG, combine_with_Grayscale)\n- Decision: gating between PL and OL results using the gap value (computed from MI-PL) as above\n\nOther notes:\n- Attacker baseline: PGD with norm inf; eps, eps_iter, and nb_iter derived from FLAGS and clipping bounds\n- Data transforms for CIFAR, and evaluation uses the softmax outputs to compute accuracies and gaps\n- The scripts also include optional advanced augmentations and flexible pooling to compare MI variants in white-box tests."
      }
    },
    {
      "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity",
      "full_text": "Published as a conference paper at ICLR 2021 Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity Jang-Hyun Kim, Wonho Choo, Hosan Jeong, Hyun Oh Song Department of Computer Science and Engineering, Seoul National University Neural Processing Research Center {janghyun,wonho.choo,grazinglion,hyunoh}@mllab.snu.ac.kr Abstract While deep neural networks show great performance on ﬁtting to the training distribution, improving the networks’ generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the diﬀerence between submodular functions. We also propose an eﬃcient modular approximation based iterative submodular minimization algorithm for eﬃcient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly super- vised localization results compared to other mixup methods. The source code is available athttps://github.com/snu-mllab/Co-Mixup . 1 Introduction Deep neural networks have been applied to a wide range of artiﬁcial intelligence tasks such as computer vision, natural language processing, and signal processing with remarkable performance (Ren et al., 2015; Devlin et al., 2018; Oord et al., 2016). However, it has been shown that neural networks have excessive representation capability and can even ﬁt random data (Zhang et al., 2016). Due to these characteristics, the neural networks can easily overﬁt to training data and show a large generalization gap when tested on previously unseen data. To improve the generalization performance of the neural networks, a body of research has been proposed to develop regularizers based on priors or to augment the training data with task-dependent transforms (Bishop, 2006; Cubuk et al., 2019). Recently, a new task- independent data augmentation technique, calledmixup, has been proposed (Zhang et al., 2018). The original mixup, calledInput Mixup, linearly interpolates a given pair of input data and can be easily applied to various data and tasks, improving the generalization performance and robustness of neural networks. Other mixup methods, such asmanifold mixup(Verma et al., 2019) orCutMix (Yun et al., 2019), have also been proposed addressing diﬀerent ways to mix a given pair of input data.Puzzle Mix (Kim et al., 2020) utilizes saliency information and local statistics to ensure mixup data to have rich supervisory signals. However, these approaches only consider mixing a given random pair of input data and do not fully utilize the rich informative supervisory signal in training data including collection of object saliency, relative arrangement, etc. In this work, we simultaneously consider mix- matching diﬀerent salient regions among all input data so that each generated mixup example accumulates as many salient regions from multiple input data as possible while ensuring Correspondence to: Hyun Oh Song. 1 arXiv:2102.03065v1  [cs.LG]  5 Feb 2021Published as a conference paper at ICLR 2021 Input batch Input Mixup CutMix Puzzle Mix Co-Mixup Weasel (0.8) Deer (0.2) Bird (0.4)  Fish (0.6) Bird (0.4)  Dog (0.6) Shark (1.0)Weasel (0.6) Deer (0.4) Weasel (0.5) Deer (0.5) Random sampled pair Bird (0.2) Dog (0.3) Fish (0.5) Figure 1: Example comparison of existing mixup methods and the proposed Co-Mixup. We provide more samples in Appendix H. diversity among the generated mixup examples. To this end, we propose a novel optimization problem that maximizes the saliency measure of each individual mixup example while encouraging diversity among them collectively. This formulation results in a novel discrete submodular-supermodular objective. We also propose a practical modular approximation method for the supermodular term and present an eﬃcient iterative submodular minimization algorithm suitable for minibatch-based mixup for neural network training. As illustrated in the Figure 1, while the proposed method,Co-Mixup, mix-matches the collection of salient regions utilizing inter-arrangements among input data, the existing methods do not consider the saliency information (Input Mixup & CutMix) or disassemble salient parts (Puzzle Mix). We verify the performance of the proposed method by training classiﬁers on CIFAR-100, Tiny-ImageNet, ImageNet, and the Google commands dataset (Krizhevsky et al., 2009; Chrabaszcz et al., 2017; Deng et al., 2009; Warden, 2017). Our experiments show the models trained with Co-Mixup achieve the state of the performance compared to other mixup baselines. In addition to the generalization experiment, we conduct weakly-supervised object localization and robustness tasks and conﬁrm Co-Mixup outperforms other mixup baselines. 2 Related works Mixup Data augmentation has been widely used to prevent deep neural networks from over-ﬁtting to the training data (Bishop, 1995). The majority of conventional augmentation methods generate new data by applying transformations depending on the data type or the target task (Cubuk et al., 2019). Zhang et al. (2018) proposedmixup, which can be independently applied to various data types and tasks, and improves generalization and robustness of deep neural networks.Input mixup(Zhang et al., 2018) linearly interpolates between two input data and utilizes the mixed data with the corresponding soft label for training. Following this work,manifold mixup (Verma et al., 2019) applies the mixup in the hidden feature space, andCutMix (Yun et al., 2019) suggests a spatial copy and paste based mixup strategy on images. Guo et al. (2019) trains an additional neural network to optimize a mixing ratio.Puzzle Mix (Kim et al., 2020) proposes a mixup method based on saliency and local statistics of the given data. In this paper, we propose a discrete optimization-based mixup method simultaneously ﬁnding the best combination of collections of salient regions among all input data while encouraging diversity among the generated mixup examples. Saliency The seminal work from Simonyan et al. (2013) generates a saliency map using a pre-trained neural network classiﬁer without any additional training of the network. Following the work, measuring the saliency of data using neural networks has been studied to obtain a more precise saliency map (Zhao et al., 2015; Wang et al., 2015) or to reduce the saliency computation cost (Zhou et al., 2016; Selvaraju et al., 2017). The saliency information is widely applied to the tasks in various domains, such as object segmentation or speech recognition (Jung and Kim, 2011; Kalinli and Narayanan, 2007). 2Published as a conference paper at ICLR 2021 Submodular-Supermodular optimization A submodular (supermodular) function is a set function with diminishing (increasing) returns property (Narasimhan and Bilmes, 2005). It is known that any set function can be expressed as the sum of a submodular and supermodular function (Lovász, 1983), called BP function. Various problems in machine learning can be naturally formulated as BP functions (Fujishige, 2005), but it is known to be NP-hard (Lovász, 1983). Therefore, approximate algorithms based on modular approximations of submodular or supermodular terms have been developed (Iyer and Bilmes, 2012). Our formulation falls into a category of BP function consisting of smoothness function within a mixed output (submodular) and a diversity function among the mixup outputs (supermodular). 3 Preliminary Existing mixup methods return {h(x1,xi(1)),...,h (xm,xi(m))} for given input data {x1,...,x m}, where h : X×X →X is a mixup function and(i(1),...,i (m)) is a ran- dom permutation of the data indices. In the case of input mixup,h(x,x′) is λx+ (1 −λ)x′, where λ∈[0,1] is a random mixing ratio. Manifold mixup applies input mixup in the hidden feature space, and CutMix usesh(x,x′) = 1B ⊙x+ (1 −1B) ⊙x′, where1B is a binary rectangular-shape mask for an imagex and ⊙represents the element-wise product. Puzzle Mix deﬁnesh(x,x′) as z⊙Π⊺x+ (1 −z) ⊙Π′⊺x′, whereΠ is a transport plan andz is a discrete mask. In detail, forx∈Rn, Π ∈{0,1}n and z∈Ln for L= {l L |l= 0,1,...,L }. In this work, we extend the existing mixup functions ash : Xm →X m′ which performs mixup on a collection of input data and returns another collection. LetxB ∈Rm×n denote the batch of input data in matrix form. Then, our proposed mixup function is h(xB) = ( g(z1 ⊙xB),...,g (zm′⊙xB) ) , where zj ∈Lm×n for j = 1,...,m ′ with L= {l L |l = 0,1,...,L }and g : Rm×n →Rn returns a column-wise sum of a given matrix. Note that, thekth column ofzj, denoted as zj,k ∈Lm, can be interpreted as the mixing ratio amongm inputs at thekth location. Also, we enforce∥zj,k∥1 = 1 to maintain the overall statistics of the given input batch. Given the one-hot target labelsyB ∈{0,1}m×C of the input data withC classes, we generate soft target labels for mixup data asy⊺ B˜oj for j = 1,...,m ′, where ˜oj = 1 n ∑n k=1 zj,k ∈[0,1]m represents the input source ratio of thejth mixup data. We train models to estimate the soft target labels by minimizing the cross-entropy loss. 4 Method 4.1 Objective Saliency Our main objective is to maximize the saliency measure of mixup data while maintaining the local smoothness of data,i.e., spatially nearby patches in a natural image look similar, temporally adjacent signals have similar spectrum in speech, etc. (Kim et al., 2020). As we can see from CutMix in Figure 1, disregarding saliency can give a misleading supervisory signal by generating mixup data that does not match with the target soft label. While the existing mixup methods only consider the mixup between two inputs, we generalize the number of inputsm to any positive integer. Note, eachkth location of outputs hasm candidate sources from the inputs. We model the unary labeling cost as the negative value of the saliency, and denote the cost vector at thekth location asck ∈Rm. For the saliency measure, we calculate the gradient values of training loss with respect to the input and measure ℓ2 norm of the gradient values across input channels (Simonyan et al., 2013; Kim et al., 2020). Note that this method does not require any additional architecture dependent modules for saliency calculation. In addition to the unary cost, we encourage adjacent locations to have similar labels for the smoothness of each mixup data. In summary, the objective can be formulated as follows: m′ ∑ j=1 n∑ k=1 c⊺ kzj,k + β m′ ∑ j=1 ∑ (k,k′)∈N (1 −z⊺ j,kzj,k′) −η m′ ∑ j=1 n∑ k=1 log p(zj,k), 3Published as a conference paper at ICLR 2021 0.5 1.0 1.5 2.0 sum supermodular unary Diverse z∗ Salient but not salient but not diverseObjective value (a) 1 2 3 4 5 6 0 20 40 60 80 Number of mixed inputs Counts (b) small τ large τ 1.0 1.1 1.2 1.3 1.4 1.5 1 1 1 1.22 1.36 no-mix CutMix Co-Mixup Input PuzzleMix Batch saliency (c) 0 0.2 0.4 0.6 0.8 1.0 0.4 0.6 0.8 1.0 baselines τ Diversity (d) Figure 2: (a) Analysis of our BP optimization problem. The x-axis is a one-dimensional arrangement of solutions: The mixed output is more salient but not diverse towards the right and less salient but diverse on the left. The unary term (red) decreases towards the right side of the axis, while the supermodular term (green) increases. By optimizing the sum of the two terms (brown), we obtain the balanced outputz∗. (b) A histogram of the number of inputs mixed for each output given a batch of 100 examples from the ImageNet dataset. As τ increases, more inputs are used to create each output on average. (c) Mean batch saliency measurement of a batch of mixup data using the ImageNet dataset. We normalize the saliency measure of each input to sum up to 1. (d) Diversity measurement of a batch of mixup data. We calculate the diversity as1 −∑ j ∑ j′̸=j ˜o⊺ j˜oj′/m, where˜oj = oj/∥oj∥1. We can control the diversity among Co-Mixup data (red) and ﬁnd the optimum by controllingτ. where the prior p is given by zj,k ∼ 1 LMulti(L,λ) with λ = ( λ1,...,λ m) ∼ Dirichlet(α,...,α ), which is a generalization of the mixing ratio distribution of Zhang et al. (2018), andNdenotes a set of adjacent locations (i.e., neighboring image patches in vision, subsequent spectrums in speech, etc.). Diversity Note that the naive generalization above leads to the identical outputs because the objective is separable and identical for each output. In order to obtain diverse mixup outputs, we model a similarity penalty between outputs. First, we represent the input source information of thejth output by aggregating assigned labels as∑n k=1 zj,k. For simplicity, let us denote∑n k=1 zj,k as oj. Then, we measure the similarity betweenoj’s by using the inner-product onRm. In addition to the input source similarity between outputs, we model the compatibility between input sources, represented as a symmetric matrixAc ∈Rm×m + . Speciﬁcally, Ac[i1,i2] quantiﬁes the degree to which inputi1 and i2 are suitable to be mixed together. In summary, we use inner-product onA= (1 −ω)I+ ωAc for ω ∈[0,1], resulting in a supermodular penalty term. Note that, by minimizing⟨oj,oj′⟩A = o⊺ jAoj′, ∀j ̸= j′, we penalize output mixup examples with similar input sources and encourage each individual mixup examples to have high compatibility within. In this work, we measure the distance between locations of salient objects in each input and use the distance matrix Ac[i,j] = ∥argmaxksi[k] −argmaxksj[k]∥1, wheresi is the saliency map of theith input and k is a location index (e.g., k is a 2-D index for image data). From now on, we denote this inner-product term as thecompatibility term. Over-penalization The conventional mixup methods perform mixup as many as the number of examples in a given mini-batch. In our setting, this is the case whenm= m′. However, the compatibility penalty between outputs is inﬂuenced by the pigeonhole principle. For example, suppose the ﬁrst output consists of two inputs. Then, the inputs must be used again for the remainingm′−1 outputs, or onlym−2 inputs can be used. In the latter case, the number of available inputs (m−2) is less than the outputs (m′−1), and thus, the same input must be used more than twice. Empirically, we found that the remaining compatibility term above over-penalizes the optimization so that a substantial portion of outputs are returned as singletons without any mixup. To mitigate the over-penalization issue, we apply clipping to the compatibility penalty term. Speciﬁcally, we model the objective so that no extra penalty would occur when the compatibility among outputs is below a certain level. 4Published as a conference paper at ICLR 2021 Now we present our main objective as following: z∗= argmin zj,k∈Lm, ∥zj,k∥1=1 f(z), where f(z) := m′ ∑ j=1 n∑ k=1 c⊺ kzj,k + β m′ ∑ j=1 ∑ (k,k′)∈N (1 −z⊺ j,kzj,k′) (1) + γmax   τ, m′ ∑ j=1 m′ ∑ j′̸=j ( n∑ k=1 zj,k )⊺ A ( n∑ k=1 zj′,k )      =fc(z) −η m′ ∑ j=1 n∑ k=1 log p(zj,k). In Figure 2, we describe the properties of the BP optimization problem of Equation (1) and statistics of the resulting mixup data. Next, we verify the supermodularity of the compatibility term. We ﬁrst extend the deﬁnition of the submodularity of a multi-label function as follows (Windheuser et al., 2012). Deﬁnition 1. For a given label setL, a functions: Lm ×Lm →R is pairwise submodular, if ∀x,x′∈Lm, s(x,x) +s(x′,x′) ≤s(x,x′) +s(x′,x). A functionsis pairwise supermodular, if −s is pairwise submodular. Proposition 1. The compatibility termfc in Equation (1) is pairwise supermodular for every pair of(zj1,k,zj2,k) if A is positive semi-deﬁnite. Proof. See Appendix B.1. Finally note that, A = (1 −ω)I + ωAc, where Ac is a symmetric matrix. By using spectral decomposition, Ac can be represented asUDU⊺, where D is a diagonal matrix and U⊺U = UU⊺ = I. Then, A= U((1 −ω)I+ ωD)U⊺, and thus for smallω >0, we can guarantee A to be positive semi-deﬁnite. 4.2 Algorithm Our main objective consists of modular (unary, prior), submodular (smoothness), and super- modular (compatibility) terms. To optimize the main objective, we employ the submodular- supermodular procedure by iteratively approximating the supermodular term as a modular function (Narasimhan and Bilmes, 2005). Note thatzj represents the labeling of thejth out- put andoj represents the aggregated input source information of thejth output, ∑n k=1 zj,k. Before introducing our algorithm, we ﬁrst inspect the simpler case without clipping. Proposition 2. The compatibility termfc without clipping is modular with respect tozj. Proof. Note, A is a positive symmetric matrix by the deﬁnition. Then, for an index j0, we can represent fc without clipping in terms of oj0 as ∑m′ j=1 ∑m′ j′=1,j′̸=jo⊺ jAoj′ = 2 ∑m′ j=1,j̸=j0 o⊺ jAoj0 +∑m′ j=1,j̸=j0 ∑m′ j′=1,j′/∈{j0,j}o⊺ jAoj′ = (2 ∑m′ j=1,j̸=j0 Aoj)⊺oj0 +c= v⊺ -j0 oj0 + c, where v-j0 ∈Rm and c ∈R are values independent withoj0 . Finally, v⊺ -j0 oj0 + c =∑n k=1 v⊺ -j0 zj0,k + c is a modular function ofzj0 . By Proposition 2, we can apply a submodular minimization algorithm to optimize the objective with respect tozj when there is no clipping. Thus, we can optimize the main objective without clipping in coordinate descent fashion (Wright, 2015). For the case with clipping, we modularize the supermodular compatibility term under the following criteria: 1. The modularized function value should increase as the compatibility across outputs increases. 2. The modularized function should not apply an extra penalty for the compatibility below a certain level. 5Published as a conference paper at ICLR 2021 Input batch Mixed output batch 3. Solve modularized Equation (1) for zj 1. Compute modularapproximateddiversityforzj 2. Perform modularization of Equation (1) with respect tozj Figure 3: Visualization of the proposed mixup procedure. For a given batch of input data (left), a batch of mixup data (right) is generated, which mix-matches diﬀerent salient regions among the input data while preserving the diversity among the mixup examples. The histograms on the right represent the input source information of each mixup data (oj). Borrowing the notation from the proof in Proposition 2, for an indexj, fc(z) = max{τ,v⊺ -joj+ c}= max{τ −c,v⊺ -joj}+ c. Note, oj = ∑n k=1 zj,k represents the input source information of the jth output andv-j = 2 ∑m′ j′=1,j′̸=jAoj′ encodes the status of the other outputs. Thus, we can interpret the supermodular term as a penalization of each label ofoj in proportion to the correspondingv-j value (criterion 1), but not for the compatibility belowτ−c(criterion 2). As a modular function which satisﬁes the criteria above, we use the following function: fc(z) ≈max{τ′,v-j}⊺oj for ∃τ′∈R. (2) Note that, by satisfying the criteria above, the modular function reﬂects the diversity and over-penalization desiderata described in Section 4.1. We illustrate the proposed mixup procedure with the modularized diversity penalty in Figure 3. Proposition 3. The modularization given by Equation(2) satisﬁes the criteria above. Proof. See Appendix B.2. Algorithm 1Iterative submodular minimiza- tion Initialize z as z(0). Let z(t) denote a solution of thetth step. Φ: modularization operator based on Equa- tion (2). for t= 1,...,T do for j = 1,...,m ′do f(t) j (zj) := f(zj; z(t) 1:j−1,z(t−1) j+1:m′). ˜f(t) j = Φ(f(t) j ). Solve z(t) j = argmin ˜f(t) j (zj). end for end for return z(T) By applying the modular approximation de- scribed in Equation (2) tofc in Equation (1), we can iteratively apply a submodular min- imization algorithm to obtain the ﬁnal so- lution as described in Algorithm 1. In de- tail, each step can be performed as follows: 1) Conditioning the main objective f on the current values except zj, denoted as fj(zj) = f(zj; z1:j−1,zj+1:m′). 2) Modu- larization of the compatibility term offj as Equation (2), resulting in a submodular function ˜fj. We denote the modularization operator asΦ, i.e., ˜fj = Φ(fj). 3) Applying a submodular minimization algorithm to˜fj. Please refer to Appendix C for implementa- tion details. Analysis Narasimhan and Bilmes (2005) proposed a modularization strategy for general supermodular set functions, and apply a submodular minimization algorithm that can monotonically decrease the original BP objective. However, the proposed Algorithm 1 based on Equation (2) is much more suitable for minibatch based mixup for neural network training than the set modularization proposed by Narasimhan and Bilmes (2005) in terms of complexity and modularization variance due to randomness. For simplicity, let us assume 6Published as a conference paper at ICLR 2021 each zj,k is anm-dimensional one-hot vector. Then, our problem is to optimizem′n one-hot m-dimensional vectors. To apply the set modularization method, we need to assign each possible value ofzj,k as an element of{1,2,...,m }. Then the supermodular term in Equation (1) can be interpreted as a set function withm′nmelements, and to apply the set modularization,O(m′nm) sequential evaluations of the supermodular term are required. In contrast, Algorithm 1 calculatesv-j in Equation (2) in onlyO(m′) time per each iteration. In addition, each modularization step of the set modularization method requires a random permutation of them′nm elements. In this case, the optimization can be strongly aﬀected by the randomness from the permutation step. As a result, the optimal labeling of eachzj,k from the compatibility term is strongly inﬂuenced by the random ordering undermining the interpretability of the algorithm. Please refer to Appendix D for empirical comparison between Algorithm 1 and the method by Narasimhan and Bilmes (2005). 5 Experiments We evaluate our proposed mixup method on generalization, weakly supervised object local- ization, calibration, and robustness tasks. First, we compare the generalization performance of the proposed method against baselines by training classiﬁers on CIFAR-100 (Krizhevsky et al., 2009), Tiny-ImageNet (Chrabaszcz et al., 2017), ImageNet (Deng et al., 2009), and the Google commands speech dataset (Warden, 2017). Next, we test the localization performance of classiﬁers following the evaluation protocol of Qin and Kim (2019). We also measure calibration error (Guo et al., 2017) of classiﬁers to verify Co-Mixup successfully alleviates the over-conﬁdence issue by Zhang et al. (2018). In Section 5.4, we evaluate the robustness of the classiﬁers on the test dataset with background corruption in response to the recent problem raised by Lee et al. (2020) that deep neural network agents often fail to generalize to unseen environments. Finally, we perform a sensitivity analysis of Co-Mixup and provide the results in Appendix F.3. 5.1 Classification We ﬁrst train PreActResNet18 (He et al., 2016), WRN16-8 (Zagoruyko and Komodakis, 2016), and ResNeXt29-4-24 (Xie et al., 2017) on CIFAR-100 for 300 epochs. We use stochastic gradient descent with an initial learning rate of 0.2 decayed by factor 0.1 at epochs 100 and 200. We set the momentum as 0.9 and add a weight decay of 0.0001. With this setup, we train a vanilla classiﬁer and reproduce the mixup baselines (Zhang et al., 2018; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020), which we denote asVanilla, Input, Manifold, CutMix, Puzzle Mixin the experiment tables. Note that we use identical hyperparameters regarding Co-Mixup over all of the experiments with diﬀerent models and datasets, which are provided in Appendix E. Table 1 shows Co-Mixup signiﬁcantly outperforms all other baselines in Top-1 error rate. Co-Mixup achieves 19.87% in Top-1 error rate with PreActResNet18, outperforming the best baseline by 0.75%. We further test Co-Mixup on diﬀerent models (WRN16-8 & ResNeXt29- 4-24) and verify Co-Mixup improves Top-1 error rate over the best performing baseline. Dataset (Model) Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup CIFAR-100 (PreActResNet18) 23.59 22.43 21.64 21.29 20.62 19.87 CIFAR-100 (WRN16-8) 21.70 20.08 20.55 20.14 19.24 19.15 CIFAR-100 (ResNeXt29-4-24) 21.79 21.70 22.28 21.86 21.12 19.78 Tiny-ImageNet (PreActResNet18) 43.40 43.48 40.76 43.11 36.52 35.85 ImageNet (ResNet-50, 100 epochs) 24.03 22.97 23.30 22.92 22.49 22.39 Google commands (VGG-11) 4.84 3.91 3.67 3.76 3.70 3.54 Table 1: Top-1 error rate on various datasets and models. For CIFAR-100, we train each model with three diﬀerent random seeds and report the mean error. We further test Co-Mixup on other datasets; Tiny-ImageNet, ImageNet, and the Google commands dataset (Table 1). For Tiny-ImageNet, we train PreActResNet18 for 1200 epochs 7Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 4: Conﬁdence-Accuracy plots for classiﬁers on CIFAR-100. From the ﬁgure, the Vanilla network shows over-conﬁdent predictions, whereas other mixup baselines tend to have under-conﬁdent predictions. We can ﬁnd that Co-Mixup has best-calibrated predictions. following the training protocol of Kim et al. (2020). As a result, Co-Mixup consistently improves Top-1 error rate over baselines by 0.67%. In the ImageNet experiment, we follow the experimental protocol provided in Puzzle Mix (Kim et al., 2020), which trains ResNet-50 (He et al., 2015) for 100 epochs. As a result, Co-Mixup outperforms all of the baselines in Top-1 error rate. We further test Co-Mixup on the speech domain with the Google commands dataset and VGG-11 (Simonyan and Zisserman, 2014). We provide a detailed experimental setting and dataset description in Appendix F.1. From Table 1, we conﬁrm that Co-Mixup is the most eﬀective in the speech domain as well. 5.2 Localization We compare weakly supervised object localization (WSOL) performance of classiﬁers trained on ImageNet (in Table 1) to demonstrate that our mixup method better guides a classiﬁer to focus on salient regions. We test the localization performance using CAM (Zhou et al., 2016), a WSOL method using a pre-trained classiﬁer. We evaluate localization performance following the evaluation protocol in Qin and Kim (2019), with binarization threshold 0.25 in CAM. Table 2 summarizes the WSOL performance of various mixup methods, which shows that our proposed mixup method outperforms other baselines. 5.3 Calibration We evaluate the expected calibration error (ECE) (Guo et al., 2017) of classiﬁers trained on CIFAR-100. Note, ECE is calculated by the weighted average of the absolute diﬀerence between the conﬁdence and accuracy of a classiﬁer. As shown in Table 2, the Co-Mixup classiﬁer has the lowest calibration error among baselines. From Figure 4, we ﬁnd that other mixup baselines tend to haveunder-conﬁdent predictions resulting in higher ECE values even thanVanilla network (also pointed out by Wen et al. (2020)), whereas Co-Mixup has best-calibrated predictions resulting in relatively 48% less ECE value. We provide more ﬁgures and results with other datasets in Appendix F.2. Task Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup Localization (Acc. %) (↑) 54.36 55.07 54.86 54.91 55.22 55.32 Calibration (ECE %) (↓) 3.9 17.7 13.1 5.6 7.5 1.9 Table 2: WSOL results on ImageNet and ECE (%) measurements of CIFAR-100 classiﬁers. 5.4 Robustness In response to the recent problem raised by Lee et al. (2020) that deep neural network agents often fail to generalize to unseen environments, we consider the situation where the statistics of the foreground object, such as color or shape, is unchanged, but with the corrupted (or replaced) background. In detail, we consider the following operations: 1) replacement with another image and 2) adding Gaussian noise. We use ground-truth bounding boxes to separate the foreground from the background, and then apply the previous operations independently to obtain test datasets. We provide a detailed description of datasets in Appendix G. 8Published as a conference paper at ICLR 2021 With the test datasets described above, we evaluate the robustness of the pre-trained classiﬁers. As shown in Table 3, Co-Mixup shows signiﬁcant performance gains at various background corruption tests compared to the other mixup baselines. For each corruption case, the classiﬁer trained with Co-Mixup outperforms the others in Top-1 error rate with the performance margins of 2.86% and 3.33% over the Vanilla model. Corruption type Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup Random replacement 41.63 39.41 39.72 46.20 39.23 38.77 (+17.62) ( +16.47) ( +16.47) ( +23.16) ( +16.69) ( +16.38) Gaussian noise 29.22 26.29 26.79 27.13 26.11 25.89 (+5.21) ( +3.35) ( +3.54) ( +4.09) ( +3.57) ( +3.49) Table 3: Top-1 error rates of various mixup methods for background corrupted ImageNet validation set. The values in the parentheses indicate the error rate increment by corrupted inputs compared to clean inputs. 5.5 Baselines with multiple inputs To further investigate the eﬀect of the number of inputs for the mixup in isolation, we conduct an ablation study on baselines using multiple mixing inputs. For fair comparison, we use Dirichlet(α,...,α ) prior for the mixing ratio distribution and select the best performing α in {0.2,1.0,2.0}. Note that we overlay multiple boxes in the case of CutMix. Table 4 reports the classiﬁcation test errors on CIFAR-100 with PreActResNet18. From the table, we ﬁnd that mixing multiple inputs decreases the performance gains of each mixup baseline. These results demonstrate that mixing multiple inputs could lead to possible degradation of the performance and support the necessity of considering saliency information and diversity as in Co-Mixup. # inputs for mixup Input Manifold CutMix Co-Mixup # inputs= 2 22.43 21.64 21.29 # inputs= 3 23.03 22.13 22.01 19.87 # inputs= 4 23.12 22.07 22.20 Table 4: Top-1 error rates of mixup baselines with multiple mixing inputs on CIFAR-100 and PreActResNet18. We report the mean values of three diﬀerent random seeds. Note that Co-Mixup optimally determines the number of inputs for each output by solving the optimization problem. 6 Conclusion We presented Co-Mixup for optimal construction of a batch of mixup examples by ﬁnding the best combination of salient regions among a collection of input data while encouraging diversity among the generated mixup examples. This leads to a discrete optimization problem minimizing a novel submodular-supermodular objective. In this respect, we present a practical modular approximation and iterative submodular optimization algorithm suitable for minibatch based neural network training. Our experiments on generalization, weakly supervised object localization, and robustness against background corruption show Co-Mixup achieves the state of the art performance compared to other mixup baseline methods. The proposed generalized mixup framework tackles the important question of ‘what to mix?’ while the existing methods only consider ‘how to mix?’. We believe this work can be applied to new applications where the existing mixup methods have not been applied, such as multi-label classiﬁcation, multi-object detection, or source separation. 9Published as a conference paper at ICLR 2021 Acknowledgements This research was supported in part by Samsung Advanced Institute of Technology, Samsung Electronics Co., Ltd, Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00882, (SW STAR LAB) Development of deployable learning intelligence via self-sustainable and trustworthy machine learning), and Research Resettlement Fund for the new faculty of Seoul National University. Hyun Oh Song is the corresponding author. References C.M.Bishop. Trainingwithnoiseisequivalenttotikhonovregularization. Neural computation, 7(1):108–116, 1995. C. M. Bishop.Pattern recognition and machine learning. springer, 2006. P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets.arXiv preprint arXiv:1707.08819, 2017. E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation strategies from data.In CVPR, 2019. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F. F. Li. Imagenet: a large-scale hierarchical image database. In CVPR, 2009. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018. S. Fujishige.Submodular functions and optimization. Elsevier, 2005. C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In ICML, 2017. H. Guo, Y. Mao, and R. Zhang. Mixup as locally linear out-of-manifold regularization.In AAAI, 2019. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.In CVPR, 2015. K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks.In ECCV, 2016. T. Horel and Y. Singer. Maximization of approximately submodular functions.In NeurIPS, 2016. R. Iyer and J. Bilmes. Algorithms for approximate minimization of the diﬀerence between submodular functions, with applications.arXiv preprint arXiv:1207.0560, 2012. C. Jung and C. Kim. A uniﬁed spectral-domain approach for saliency detection and its application to automatic object segmentation.IEEE Transactions on Image Processing, 21(3):1272–1283, 2011. O. Kalinli and S. S. Narayanan. A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech. InEighth Annual Conference of the International Speech Communication Association, 2007. J.-H. Kim, W. Choo, and H. O. Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup.In ICML, 2020. A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Citeseer, 2009. Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 10Published as a conference paper at ICLR 2021 K. Lee, K. Lee, J. Shin, and H. Lee. Network randomization: A simple technique for generalization in deep reinforcement learning.In ICLR, 2020. L. Lovász. Submodular functions and convexity. InMathematical Programming The State of the Art, pages 235–257. Springer, 1983. T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning.IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018. M. Narasimhan and J. A. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning.UAI, 2005. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio.arXiv preprint arXiv:1609.03499, 2016. Z. Qin and D. Kim. Rethinking softmax with cross-entropy: Neural network classiﬁer as mutual information estimator.arXiv preprint arXiv:1911.10688, 2019. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.In NeurIPS, 2015. R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization.In ICCV, 2017. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps.arXiv preprint arXiv:1312.6034, 2013. V. Verma, A. Lamb, C. Beckham, A. Najaﬁ, I. Mitliagkas, A. Courville, D. Lopez-Paz, and Y. Bengio. Manifold mixup: Better representations by interpolating hidden states.In ICML, 2019. L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks for saliency detection via local estimation and global search.In CVPR, 2015. P. Warden. URL https://research.googleblog.com/2017/08/launching-speech-commands- dataset.html., 2017. Y. Wen, G. Jerfel, R. Muller, M. W. Dusenberry, J. Snoek, B. Lakshminarayanan, and D. Tran. Improving calibration of batchensemble with data augmentation.In ICML Workshop on Uncertainty and Robustness in Deep Learning, 2020. T. Windheuser, H. Ishikawa, and D. Cremers. Generalized roof duality for multi-label optimization: Optimal lower bounds and persistency.In ECCV, 2012. S. J. Wright. Coordinate descent algorithms.Mathematical Programming, 151(1):3–34, 2015. S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. pages 1492–1500, 2017. S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features.In ICCV, 2019. S. Zagoruyko and N. Komodakis. Wide residual networks.arXiv preprint arXiv:1605.07146, 2016. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization.arXiv preprint arXiv:1611.03530, 2016. H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 11Published as a conference paper at ICLR 2021 R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection by multi-context deep learning. In CVPR, 2015. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In CVPR, 2016. A Supplementary notes for objective A.1 Notations In Table 5, we provide a summary of notations in the main text. Notation Meaning m, m′, n # inputs, # outputs, dimension of data ck ∈Rm (1 ≤k≤n) labeling cost for m input sources at thekth location zj,k ∈Lm (1 ≤j ≤m′, 1 ≤k≤n) input source ratio at the kth location of thejth output zj ∈Lm×n labeling of thejth output oj ∈Rm aggregation of the labeling of thejth output A∈Rm×m compatibility between inputs Table 5: A summary of notations. A.2 Interpretation of compatibility In our main objective Equation (1), we introduce a compatibility matrixA= (1 −ω)I+ ωAc between inputs. By minimizing⟨oj,oj′⟩A for j ̸= j′, we encourage each individual mixup examples to have high compatibility within. Figure 5 explains how the compatibility term works by comparing simple cases. Note that our framework can reﬂect any compatibility measures for the optimal mixup. 1 3 2 1 2 3 $%& $'& $&' $%' Output\t1Output\t2Output\t1Output\t2 Figure 5: Let us consider Co-Mixup with three inputs and two outputs. The ﬁgure represents two Co-Mixup results. Each input is denoted as a number and color-coded. Let us assume that input 1 and input 2 are more compatible,i.e., A12 ≫A23 and A12 ≫A13. Then, the left Co-Mixup result has a larger inner-product value⟨o1,o2⟩A than the right. Thus the mixup result on the right has higher compatibility than the result on the left within each output example. B Proofs B.1 Proof of proposition 1 Lemma 1.For a positive semi-deﬁnite matrixA∈Rm×m + and x,x′∈Rm, s(x,x′) = x⊺Ax′ is pairwise supermodular. Proof. s(x,x) +s(x′,x′) −s(x,x′) −s(x′,x) = x⊺Ax+ x⊺Ax−2x⊺Ax′= (x−x′)⊺A(x−x′), and becauseA is positive semi-deﬁnite,(x−x′)⊺A(x−x′) ≥0. 12Published as a conference paper at ICLR 2021 Proposition 1. The compatibility termfc in Equation (1) is pairwise supermodular for every pair of(zj1,k,zj2,k) if A is positive semi-deﬁnite. Proof. For j1 and j2, s.t., j1 ̸= j2, max { τ,∑m′ j=1 ∑m′ j′=1,j′̸=j(∑n k=1 zj,k)⊺A(∑n k=1 zj′,k) } = max{τ,c + 2z⊺ j1,kAzj2,k} = −min{−τ,−c −2z⊺ j1,kAzj2,k}, for ∃c ∈ R. By Lemma 1, −z⊺ j1,kAzj2,k is pairwise submodular, and because a budget additive function preserves submodularity (Horel and Singer, 2016),min{−τ,−c−2z⊺ j1,kAzj2,k}is pairwise submodular with respect to(zj1,k,zj2,k). B.2 Proof of proposition 3 Proposition 3. The modularization given by Equation (2) satisﬁes the criteria. Proof. Note, by the deﬁnition in Equation (1), the compatibility between thejth and j′th outputs is o⊺ j′Aoj, and thus, v⊺ -joj represents the compatibility between thejth output and the others. In addition,∥oj∥1 = ∥∑n k=1 zj,k∥1 = ∑n k=1 ∥zj,k∥1 = n. In a local view, for the given oj, let us deﬁne a vectoro′ j as o′ j[i1] = oj[i1] + α and o′ j[i2] = oj[i2] −α for α > 0. Without loss of generality, let us assume v-j is sorted in ascending order. Then, v⊺ -joj ≤v⊺ -jo′ j implies i1 > i2, and because the max function preserves the ordering, max{τ′,v-j}⊺oj ≤max{τ′,v-j}⊺o′ j. Thus, the criterion 1 is locally satisﬁed. Next, for τ′ > 0, ∥max{τ′,v-j}⊺oj∥1 ≥τ′∥oj∥1 = τ′n. Let ∃i0 s.t. for i < i0,v-j[i] < τ′, and for i≥i0,v-j[i] ≥τ′. Then, foroj containing positive elements only in indices smaller thani0, max{τ′,v-j}⊺oj = τ′n which means there is no extra penalty from the compatibility. In this respect, the proposed modularization satisﬁes the criterion 2 as well. C Implementation details We perform the optimization after down-sampling the given inputs and saliency maps to the speciﬁed size (4 ×4). After the optimization, we up-sample the optimal labeling to match the size of the inputs and then mix inputs according to the up-sampled labeling. For the saliency measure, we calculate the gradient values of training loss with respect to the input data and measure ℓ2 norm of the gradient values across input channels (Simonyan et al., 2013). In classiﬁcation experiments, we retain the gradient information of network weights obtained from the saliency calculation for regularization. For the distance in the compatibility term, we measureℓ1-distance between the most salient regions. For the initialization in Algorithm 1, we usei.i.d. samples from a categorical distribution with equal probabilities. We use alpha-beta swap algorithm from pyGCO1 to solve the minimization step in Algorithm 1, which can ﬁnd local-minima of a multi-label submodular function. However, the worst-case complexity ofalpha-beta swap algorithm with|L|= 2 is O(m2n), and in the case of mini-batch with 100 examples, iteratively applying the algorithm canbecomeabottleneckduringthenetworktraining. Tomitigatethecomputationaloverhead, we partition the mini-batch (each of size 20) and then apply Algorithm 1 independently per each partition. The worst-case complexity theoretic of the naive implementation of Algorithm 1 increases exponentially as|L|increases. Speciﬁcally, the worst-case theoretic complexity of thealpha- beta swap algorithm is proportional to the square of the number of possible states ofzj,k, which is proportional tom|L|−1. To reduce the number of possible states in a multi-label case, we solve the problem for binary labels (|L|= 2) at the ﬁrst inner-cycle and then extend to multi labels (|L|= 3) only for the currently assigned indices of each output in the subsequent cycles. This reduces the number of possible states toO(m+ ¯m|L|−1) where ¯m≪m. Here, ¯m means the number of currently assigned indices for each output. 1https://github.com/Borda/pyGCO 13Published as a conference paper at ICLR 2021 Based on the above implementation, we train models with Co-Mixup in a feasible time. For example, in the case of ImageNet training with 16 Intel I9-9980XE CPU cores and 4 NVIDIA RTX 2080Ti GPUs, Co-Mixup training requires 0.964s per batch, whereas the vanilla training without mixup requires 0.374s per batch. Note that Co-Mixup requires saliency computation, and when we compare the algorithm with Puzzle Mix, which performs the same saliency computation, Co-Mixup is only slower about 1.04 times. Besides, as we down-sample the data to the ﬁxed size regardless of the data dimension, the additional computation cost of Co-Mixup relatively decreases as the data dimension increases. Finally, we present the empirical time complexity of Algorithm 1 in Figure 6. As shown in the ﬁgure, Algorithm 1 has linear time complexity over|L|empirically. Note that we use|L|= 3 in all of our main experiments, including a classiﬁcation task. 2 3 4 1.1 1.2 1.3 |L| Average Execution Time (ms) 2 1020 50 100 0 2 4 6 8 10 m Average Execution Time (ms) Figure 6: Mean execution time (ms) of Algorithm 1 per each batch of data over 100 trials. The left ﬁgure shows the time complexity of the algorithm over|L|and the right ﬁgure shows the time complexity over the number of inputsm. Note that the other parameters are ﬁxed equal to the classiﬁcation experiments setting,m= m′= 20, n= 16, and|L|= 3. D Algorithm Analysis In this section, we perform comparison experiments to analyze the proposed Algorithm 1. First, we compare our algorithm with the exact brute force search algorithm to inspect the optimality of the algorithm. Next, we compare our algorithm with the BP algorithm proposed by Narasimhan and Bilmes (2005). D.1 Comparison with Brute Force To inspect the optimality of the proposed algorithm, we compare the function values of the solutions of Algorithm 1, brute force search algorithm, and random guess. Due to the exponential time complexity of the brute force search, we compare the algorithms on small scale experiment settings. Speciﬁcally, we test algorithms on settings of(m= m′= 2, n= 4), (m= m′= 2, n= 9), and(m= m′= 3, n= 4) varying the number of inputs and outputs (m, m′) and the dimension of datan. We generate unary cost matrix in the objectivef by sampling data from uniform distribution. We perform experiments with 100 diﬀerent random seeds and summarize the results on Table 6. From the table, we ﬁnd that the proposed algorithm achieves near optimal solutions over various settings. We also measure relative errors between ours and random guess, (f(zours) −f(zbrute))/(f(zrandom) −f(zbrute)). As a result, our algorithm achieves relative error less than0.01. D.2 Comparison with another BP algorithm We compare the proposed algorithm and the BP algorithm proposed by Narasimhan and Bilmes (2005). We evaluate function values of solutions by each method using a random 14Published as a conference paper at ICLR 2021 Conﬁguration Ours Brute force (optimal) Random guess Rel. error (m= m′= 2, n= 4) 1.91 1.90 3.54 0.004 (m= m′= 2, n= 9) 1.93 1.91 3.66 0.01 (m= m′= 3, n= 4) 2.89 2.85 22.02 0.002 Table 6: Mean function values of the solutions over 100 diﬀerent random seeds. Rel. error means the relative error between ours and random guess. unary cost matrix from a uniform distribution. We compare methods over various scales by controlling the number of mixing inputsm. Table 7 shows the averaged function values with standard deviations in the parenthesis. As we can see from the table, the proposed algorithm achieves much lower function values and deviations than the method by Narasimhan and Bilmes (2005) over various settings. Note that the method by Narasimhan and Bilmes (2005) has high variance due to randomization in the algorithm. We further compare the algorithm convergence time in Table 8. The experiments verify that the proposed algorithm is much faster and eﬀective than the method by Narasimhan and Bilmes (2005). Algorithm m= 5 m= 10 m= 20 m= 50 m= 100 Ours 3.1 (1.7) 15 (6.6) 54 (15) 205 (26) 469 (31) Narasimhan 269 (58) 1071 (174) 4344 (701) 24955 (4439) 85782 (14337) Random 809 (22) 7269 (92) 60964 (413) 980973 (2462) 7925650 (10381) Table 7: Mean function values of the solutions over 100 diﬀerent random seeds. We report the standard deviations in the parenthesis. Random represents the random guess algorithm. Algorithm m= 5 m= 10 m= 20 m= 50 m= 100 Ours 0.02 0.04 0.11 0.54 2.71 Narasimhan 0.06 0.09 0.27 1.27 7.08 Table 8: Convergence time (s) of the algorithms. E Hyperparameter settings We perform Co-Mixup after down-sampling the given inputs and saliency maps to the pre-deﬁned resolutions regardless of the size of the input data. In addition, we normalize the saliency of each input to sum up to 1 and deﬁne unary cost using the normalized saliency. As a result, we use an identical hyperparameter setting for various datasets; CIFAR-100, Tiny-ImageNet, and ImageNet. In details, we use(β,γ,η,τ ) = (0.32,1.0,0.05,0.83) for all of experiments. Note thatτ is normalized according to the size of inputs (n) and the ratio of the number of inputs and outputs (m/m′), and we use an isotropic Dirichlet distribution with α= 2 for priorp. For a compatibility matrix, we useω= 0.001. For baselines, we tune the mixing ratio hyperparameter,i.e., the beta distribution parameter (Zhang et al., 2018), among{0.2,1.0,2.0}for all of the experiments if the speciﬁc setting is not provided in the original papers. F Additional Experimental Results F.1 Another Domain: Speech In addition to the image domain, we conduct experiments on the speech domain, verifying Co-Mixup works on various domains. Following (Zhang et al., 2018), we train LeNet (LeCun 15Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 7: Conﬁdence-Accuracy plots for classiﬁers on CIFAR-100. Note, ECE is calculated by the mean absolute diﬀerence between the two values. et al., 1998) and VGG-11 (Simonyan and Zisserman, 2014) on the Google commands dataset (Warden, 2017). The dataset consists of 65,000 utterances, and each utterance is about one-second-long belonging to one out of 30 classes. We train each classiﬁer for 30 epochs with the same training setting and data pre-processing of Zhang et al. (2018). In more detail, we use160 ×100 normalized spectrograms of utterances for training. As shown in Table 9, we verify that Co-Mixup is still eﬀective in the speech domain. Model Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup LeNet 11.24 10.83 12.33 12.80 10.89 10.67 VGG-11 4.84 3.91 3.67 3.76 3.70 3.57 Table 9: Top-1 classiﬁcation test error on the Google commands dataset. We stop training if validation accuracy does not increase for 5 consecutive epochs. F.2 Calibration In this section, we summarize the expected calibration error (ECE) (Guo et al., 2017) of classiﬁers trained with various mixup methods. For evaluation, we use the oﬃcial code provided by the TensorFlow-Probability library2 and set the number of bins as 10. As shown in Table 10, Co-Mixup classiﬁers have the lowest calibration error on CIFAR-100 and Tiny-ImageNet. As pointed by Guo et al. (2017), the Vanilla networks have over- conﬁdent predictions, but however, we ﬁnd that mixup classiﬁers tend to have under-conﬁdent predictions (Figure 7; Figure 8). As shown in the ﬁgures, Co-Mixup successfully alleviates the over-conﬁdence issue and does not suﬀer from under-conﬁdence predictions. Dataset Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup CIFAR-100 3.9 17.7 13.1 5.6 7.5 1.9 Tiny-ImageNet 4.5 6.2 6.8 12.0 5.6 2.5 ImageNet 5.9 1.2 1.7 4.3 2.1 2.1 Table 10: Expected calibration error (%) of classiﬁers trained with various mixup methods on CIFAR-100, Tiny-ImageNet and ImageNet. Note that, at all of three datasets, Co-Mixup outperforms all of the baselines in Top-1 accuracy. F.3 Sensitivity analysis We measure the Top-1 error rate of the model by sweeping the hyperparameter to show the sensitivity using PreActResNet18 on CIFAR-100 dataset. We sweep the label smoothness coeﬃcient β ∈{0,0.16,0.32,0.48,0.64}, compatibility coeﬃcientγ ∈{0.6,0.8,1.0,1.2,1.4}, clipping levelτ ∈{0.79,0.81,0.83,0.85,0.87}, compatibility matrix parameterω ∈{0,5 · 10−4,10−3,5 ·10−3,10−2}, and the size of partitionm∈{2,4,10,20,50}. Table 11 shows that Co-Mixup outperforms the best baseline (PuzzleMix, 20.62%) with a large pool of 2https://www.tensorﬂow.org/probability/api_docs/python/tfp/stats/expected_calibration_error 16Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 8: Conﬁdence-Accuracy plots for classiﬁers on Tiny-ImageNet. 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 9: Conﬁdence-Accuracy plots for classiﬁers on ImageNet. hyperparameters. We also ﬁnd that Top-1 error rate increases as the partition batch sizem increases untilm= 20. Smoothness coeﬃcient, β = 0 β = 0.16 β = 0.32 β = 0.48 β = 0.64 β 20.29 20.18 19.87 20.35 21.24 Compatibility coeﬃcient, γ = 0.6 γ = 0.8 γ = 1.0 γ = 1.2 γ = 1.4 γ 20.3 19.99 19.87 20.09 20.13 Clipping parameter, τ = 0.79 τ = 0.81 τ = 0.83 τ = 0.85 τ = 0.87 τ 20.45 20.14 19.87 20.15 20.23 Compatibility matrix ω= 0 ω= 5·10−4 ω= 10−3 ω= 5·10−3 ω= 10−2 parameter, ω 20.51 20.42 19.87 20.18 20.14 Partition size, m= 2 m= 4 m= 10 m= 20 m= 50 m 20.3 20.22 20.15 19.87 19.96 Table 11: Hyperparameter sensitivity results (Top-1 error rates) on CIFAR-100 with PreAc- tResNet18. We report the mean values of three diﬀerent random seeds. F.4 Comparison with non-mixup baselines We compare the generalization performance of Co-Mixup with non-mixup baselines, verifying the proposed method achieves the state of the art generalization performance not only for the mixup-based methods but for other general regularization based methods. One of the regularization methods called VAT (Miyato et al., 2018) uses virtual adversarial loss, which is deﬁned as the KL-divergence of predictions between input data against local perturbation. We perform the experiment with VAT regularization on CIFAR-100 with PreActResNet18 for 300 epochs in the supervised setting. We tuneα (coeﬃcient of VAT regularization term) in {0.001, 0.01, 0.1},ϵ(radius ofℓ-inf ball) in {1, 2}, and the number of noise update steps in {0, 1}. Table 12 shows that Co-Mixup, which achieves Top-1 error rate of 19.87%, outperforms the VAT regularization method. G Detailed description for background corruption We build the background corrupted test datasets based on ImageNet validation dataset to compare the robustness of the pre-trained classiﬁers against the background corruption. 17Published as a conference paper at ICLR 2021 # update=0 # update=1 VAT loss coeﬃcient ϵ= 1 ϵ= 2 ϵ= 1 ϵ= 2 α= 0.001 23.38 23.62 24.76 26.22 α= 0.01 23.14 23.67 28.33 31.95 α= 0.1 23.65 23.88 34.75 39.82 Table 12: Top-1 error rates of VAT on CIFAR-100 dataset with PreActResNet18. ImageNet consists of images{x1,...,x M}, labels{y1,...,y M}, and the corresponding ground- truth bounding boxes{b1,...,b M}. We use the ground-truth bounding boxes to separate the foreground from the background. Letzj be a binary mask of imagexj, which has value1 inside of the ground-truth bounding boxbj. Then, we generate two types of background corrupted sample˜xj by considering the following operations: 1. Replacement with another image as ˜xj = xj ⊙zj + xi(j) ⊙(1 −zj) for a random permutation {i(1),...,i (M)}. 2. Adding Gaussian noise as˜xj = xj ⊙zj + ϵ⊙(1 −zj), whereϵ∼N(0,0.12). We clip pixel values of˜xj to [0, 1]. Figure 10 visualizes subsets of the background corruption test datasets. (a)  (b) Figure 10: Each subﬁgure shows background corrupted samples used in the robustness experiment. (a) Replacement with another image in ImageNet. (b) Adding Gaussian noise. The red boxes on the images represent ground-truth bounding boxes. H Co-Mixup generated samples In Figure 12, we present Co-Mixup generated image samples by using images from ImageNet. We use an input batch consisting of 24 images, which is visualized in Figure 11. As can be seen from Figure 12, Co-Mixup eﬃciently mix-matches salient regions of the given inputs maximizing saliency and creates diverse outputs. In Figure 12, inputs with the target objects on the left side are mixed with the objects on the right side, and objects on the top side are mixed with the objects on the bottom side. In Figure 13, we present Co-Mixup generated image samples with largerτ using the same input batch. By increasingτ, we can encourage Co-Mixup to use more inputs to mix per each output. 18Published as a conference paper at ICLR 2021 Figure 11: Input batch. 19Published as a conference paper at ICLR 2021 Figure 12: Mixed output batch. 20Published as a conference paper at ICLR 2021 Figure 13: Another mixed output batch with largerτ. 21",
      "references": [],
      "meta_data": {
        "arxiv_id": "2102.03065v1",
        "authors": [
          "Jang-Hyun Kim",
          "Wonho Choo",
          "Hosan Jeong",
          "Hyun Oh Song"
        ],
        "published_date": "2021-02-05T09:12:02Z",
        "github_url": "https://github.com/Borda/pyGCO"
      },
      "llm_extracted_info": {
        "main_contributions": "We propose Co-Mixup, a saliency-guided batch mixup method that jointly selects multiple salient regions across a batch of inputs to create diverse mixup samples, formulated as a BP (submodular–supermodular) optimization. It maximizes per-sample saliency while encouraging diversity among outputs via a modular-approximation iterative submodular minimization, enabling efficient minibatch training. It achieves state-of-the-art generalization, calibration, and weakly supervised localization compared with prior mixup methods.",
        "methodology": "Define labels zj,k indicating how much each input contributes to the k-th location of the j-th mixup output; saliency costs c^T z_j,k; smoothness term using neighborhood N; diversity via compatibility matrix A combining input similarity Ac; objective f(z) = unary + smoothness + diversity, with clipping to avoid over-penalization; demonstrate fc term is pairwise supermodular; use iterative modular approximation Φ to replace supermodular term by modular function; coordinate descent style algorithm 1 to minimize f; per-iteration compute v_-j etc; practical implementation: down-sample inputs and saliency to 4x4 grid; use alpha-beta swap (pyGCO) to optimize; partition mini-batches to manage complexity; hyperparameters: (β, γ, η, τ) etc.",
        "experimental_setup": "Datasets: CIFAR-100 (PreActResNet18, WRN16-8, ResNeXt29-4-24), Tiny-ImageNet, ImageNet (ResNet-50), Google commands (VGG-11); training details follow standard mixup baselines; baselines: Vanilla, Input, Manifold, CutMix, Puzzle Mix; evaluation: Top-1 error, WSOL using CAM on ImageNet following Qin & Kim; calibration: expected calibration error (ECE); robustness to background corruption on ImageNet (random replacement, Gaussian noise); ablations: varying number of mixing inputs m; sensitivity analyses; implementation notes in Appendix; reported hyperparameters: (β, γ, η, τ) = (0.32, 1.0, 0.05, 0.83); Dirichlet prior α=2; compatibility ω=0.001; results show Co-Mixup achieves state-of-the-art performance across tasks.",
        "limitations": "Computational overhead due to saliency computation and iterative submodular optimization, especially on large batches; reliance on gradient-based saliency maps which may be imperfect; modular approximation introduces suboptimality; hyperparameter sensitivity and need for partitioning minibatches; down-sampling saliency to 4x4 may lose spatial detail; performance dependent on quality of saliency estimation; applicability primarily demonstrated on image and speech classification tasks; not explicitly theoretical guarantees of global optimum.",
        "future_research_directions": "Extend to multi-label classification, multi-object detection, or source separation; explore end-to-end differentiable or jointly learned saliency and mixing mechanisms; investigate alternative BP decompositions or more efficient modular approximations; improve scalability with larger batch sizes and higher-resolution saliency; study different compatibility measures Ac and their interpretation; apply Co-Mixup to other modalities and tasks (text, audio, video) and unsupervised or semi-supervised settings.",
        "experimental_code": "No experimental_code directly implementing the Co-Mixup method is present in the Repository Content. The repository contains a Python wrapper and tests for a graph-cut optimization library (pyGCO) unrelated to Co-Mixup.",
        "experimental_info": "No experimental settings for Co-Mixup found in Repository Content. The content describes the Co-Mixup method in the Paper Summary, but the repository focuses on pyGCO-related graph-cut experiments and standard graph-cut usage. Therefore, there are no experimental configurations (datasets, hyperparameters, ablations) for Co-Mixup in the provided Repository Content."
      }
    },
    {
      "title": "Augmentation-Aware Self-Supervision for Data-Efficient GAN Training",
      "full_text": "Augmentation-Aware Self-Supervision for Data-Efficient GAN Training Liang Hou1,3,4, Qi Cao1, Yige Yuan1,3, Songtao Zhao4, Chongyang Ma4, Siyuan Pan4, Pengfei Wan4, Zhongyuan Wang4, Huawei Shen1,3, Xueqi Cheng2,3 1CAS Key Laboratory of AI Safety and Security, Institute of Computing Technology, Chinese Academy of Sciences 2CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences 3University of Chinese Academy of Sciences 4Kuaishou Technology lianghou96@gmail.com Abstract Training generative adversarial networks (GANs) with limited data is challenging because the discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversarially learn from the self-supervised discriminator by generating augmentation-predictable real and not fake data. This formulation connects the learning objective of the generator and the arithmetic − harmonic mean divergence under certain assumptions. We compare our method with state- of-the-art (SOTA) methods using the class-conditional BigGAN and unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100, FFHQ, LSUN- Cat, and five low-shot datasets. Experimental results demonstrate significant improvements of our method over SOTA methods in training data-efficient GANs.1 1 Introduction Generative adversarial networks (GANs) [10] have achieved great progress in synthesizing diverse and high-quality images in recent years [ 2, 17, 19, 20, 13]. However, the generation quality of GANs depends heavily on the amount of training data [47, 18]. In general, the decrease of training samples usually yields a sharp decline in both fidelity and diversity of the generated images [39, 48]. This issue hinders the wide application of GANs due to the fact of insufficient data in real-world applications. For instance, it is valuable to imitate the style of an artist whose paintings are limited. GANs typically consist of a generator that is designed to generate new data and a discriminator that guides the generator to recover the real data distribution. The major challenge of training GANs under limited data is that the discriminator is prone to overfitting [47, 18], and therefore lacks generalization to teach the generator to learn the underlying real data distribution. 1Our code is available at https://github.com/liang-hou/augself-gan. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2205.15677v5  [cs.LG]  28 Dec 2023In order to alleviate the overfitting issue, recent researches have suggested a variety of approaches, mainly from the perspectives of training data [18], loss functions [40], and network architectures [28]. Among them, data augmentation-based methods have gained widespread attention due to its simplicity and extensibility. Specifically, DiffAugment [47] introduced differentiable augmentation techniques for GANs, in which both real and generated data are augmented to supplement the training set of the discriminator. However, this straightforward augmentation method overlooks augmentation-related semantic information, as it solely augments the domain of the discriminator while neglecting the range. Such a practice might introduces an inductive bias that potentially forces the discriminator to remain invariant to different augmentations [24], which could limit the representation learning of the discriminator and subsequently affect the generation performance of the generator [12]. In this paper, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of augmented data with the original data as reference to address the above problem. Meanwhile, the self-supervised discriminator is required to be distinguished between the real data and the generated data since their distributions are different during training, especially in the early stage. The proposed discriminator can benefit the generator in two ways, implicitly and explicitly. On one hand, the self-supervised discriminator can transfer the learned augmentation-aware knowledge to the original discriminator through parameter sharing. On the other hand, we allow the generator to learn adversarially from the self-supervised discriminator by generating augmentation- predictable real and not fake data (Equation (6)). We also theoretically analyzed the connection between this objective function and the minimization of a robust f-divergence divergence (the arithmetic − harmonic mean divergence [37]). In experiments, we show that the proposed method compares favorably to the data augmentation counterparts and other state-of-the-art (SOTA) methods on common data-limited benchmarks (CIFAR-10 [21], CIFAR-100 [21], FFHQ [17], LSUN-Cat [45], and five low-shot image generation datasets [36]) based on the class-conditional BigGAN [2] and unconditional StyleGAN2 [ 19] architectures. In addition, we carried out extensive experiments to demonstrate the effectiveness of the objective function design, the adaptability to stronger data augmentations, and the robustness of hyper-parameter selection in our method. 2 Related Work In this section, we provide an overview of existing work related to training GANs in data-limited scenarios. We also discuss methodologies incorporating self-supervised learning techniques. 2.1 GANs under Limited Training Data Recently, researchers have become interested in freeing training GANs from the need to collect large amounts of data for adaptability in real-world scenarios. Previous studies typically fall into two main categories. The first one involves adopting a pre-trained GAN model to the target domain by fine-tuning partial parameters [41, 31, 42, 30]. However, it requires external training data, and the adoption performance depends heavily on the correlation between the source and target domains. The other one focuses on training GANs from scratch with elaborated data-efficient training strategies. DiffAugment [47] utilized differentiable augmentation to supplement the training set to prevent discriminator from overfitting in limited data regimes. Concurrently, ADA [18] introduced adaptive data augmentation with a richer set of augmentation categories. APA [16] adaptively augmented the real data with the most plausible generated data. LeCam-GAN [40] proposed adaptive regularization for the discriminator and showed a connection to the Le Cam divergence [ 23]. [ 3] discovered that sparse sub-network (lottery tickets) [5] and feature-level adversarial augmentation could offer orthogonal gains to data augmentation methods. InsGen [ 43] improved the data efficiency of training GANs by incorporating instance discrimination tasks to the discriminator. MaskedGAN [14] employed masking in the spatial and spectral domains to alleviate the discriminator overfitting issue. GenCo [7] discriminated samples from multiple views with weight-discrepancy and data-discrepancy mechanisms. FreGAN [ 44] focused on discriminating between real and fake samples in the high- frequency domain. DigGAN [8] constrains the discriminator gradient gap between real and generated data. FastGAN [28] designed a lightweight generator architecture and observed that a self-supervised discriminator could enhance low-shot generation performance. Our method falls into the second category, supplementing data augmentation-based GANs and can be also applied to other methods. 2Original data  ωcolor = (0.2, 0.3, 0.5)  ωtranslation = (0.1, 0.8)  ωcutout = (0.4, 0.6) Figure 1: Examples of images with different kinds of differentiable augmentation (including the original unaugmented one) and their re-scaled corresponding augmentation parameters ω ∈ [0, 1]d. 2.2 GANs with Self-Supervised Learning Self-supervised learning techniques excel at learning meaningful representations without human- annotated labels by solving pretext tasks. Transformation-based self-supervised learning methods such as rotation recognition [9] have been incorporated into GANs to address catastrophic forgetting in discriminators [4, 38, 12]. Various other self-supervised tasks have also been explored, including jigsaw puzzle solving [1], latent transformation detection [33], and mutual information maximiza- tion [25]. Moreover, ContraD [15] decouples the representation learning and discrimination of the discriminator, utilizing contrastive learning for representation learning and a discriminator head for distinguishing real from fake upon the contrastive representations. In contrast to ours, CR-GAN [46] and ICR-GAN proposed consistency regularization for the discriminator, which corresponds to an explicit augmentation-invariant of the discriminator. both our proposed method and SSGAN-LA [12] belong to adversarial self-supervised learning, they differ in the type of self-supervised signals and model inputs. SSGAN-LA is limited to categorical self-supervision [ 9], which is incompati- ble with popular augmentation-based GANs like DiffAugment [47]. Our method is applicable for continuous self-supervision and integrates seamlessly with DiffAugment. Furthermore, continuous self-supervision have a magnitude relationship and thus can provide more refined gradient feedback for the model to overcome overfitting in data-limited scenarios. Additionally, unlike SSGAN-LA, our method does not constrain the invertibility of data transformations (Theorem 1) because it additionally take the original sample as input for the self-supervised discriminator (Equation (5)). 3 Preliminaries In this section, we introduce the necessary concepts and preliminaries for completeness of the paper. 3.1 Generative Adversarial Networks Generative adversarial networks (GANs) [10] typically contain a generator G : Z → Xthat maps a low-dimensional latent code z ∈ Zendowed with a tractable prior p(z), e.g., multivariate normal distribution N(0, I), to a high-dimensional data point x ∈ X, which induces a generated data distribution (density) pG(x) = R Z p(z)δ(x −G(z))dz with the Dirac delta distribution δ(·), and also contain a discriminator D : X →R that is required to distinguish between the real data sampled from the underlying data distribution (density) pdata(x) and the generated ones. The generator attempts to fool the discriminator to eventually recover the real data distribution, i.e., pG(x) = pdata(x). Formally, the loss functions for the discriminator and the generator can be formulated as follows: LD = Ex∼pdata(x)[f(D(x))] + Ez∼p(z)[h(D(G(z)))], (1) LG = Ez∼p(z)[g(D(G(z)))]. (2) Different real-valued functionsf, h, and g correspond to different variants of GANs [32]. For example, the minimax GAN [10] can be constructed by setting f(x) = −log(σ(x)) and h(x) = −g(x) = −log(1 − σ(x)) with the sigmoid function σ(x) = 1/(1 + exp(−x)). In this study, we follow the practices of DiffAugment [47] to adopt the hinge loss [ 26], i.e., f(x) = h(−x) = max(0, 1 − x) and g(x) = −x, for experiments based on BigGAN [2] and the log loss [10], i.e., f(x) = g(x) = −log(σ(x)) and h(x) = −log(1 − σ(x)), for experiments based on StyleGAN2 [19]. 3100% data 20% data 10% data 0.7 0.75 0.8 Accuracy CIFAR-10 100% data 20% data 10% data 0.45 0.5 0.55 CIFAR-100 BigGAN+DiffAug AugSelf-BigGAN Figure 2: Comparison of representation learning ability of discriminator between BigGAN + Dif- fAugment and our AugSelf-BigGAN on CIFAR-10 and CIFAR-100 using linear logistic regression. 3.2 Differentiable Augmentation for GANs DiffAugment [47] introduces differentiable augmentation T : X ×Ω → ˆX parameterized by a randomly-sampled parameter ω ∈ Ω with a prior p(ω) for data-efficient GAN training. The parameter ω determines exactly how to transfer a sample x to an augmented one ˆx ∈ ˆX for the discriminator. After manually re-scaling (for ω ∈ [0, 1]d), the parameters of all three kinds of differentiable augmentation used in DiffAugment for 2D images can be expressed as follows: • color: ωcolor = (λbrightness, λsaturation, λcontrast) ∈ [0, 1]3; • translation: ωtranslation = (xtranslation, ytranslation) ∈ [0, 1]2; • cutout: ωcutout = (xoffset, yoffset) ∈ [0, 1]2. Figure 1 illustrates the augmentation operations and their parameters. Formally, the loss functions for the discriminator and the generator of GANs with DiffAugment are defined as follows: Lda D = Ex∼pdata(x),ω∼p(ω)[f(D(T(x; ω)))] + Ez∼p(z),ω∼p(ω)[h(D(T(G(z); ω)))], (3) Lda G = Ez∼p(z),ω∼p(ω)[g(D(T(G(z); ω)))], (4) where ω can represent any combination of these parameters. We choose all augmentations by default, which means augmentation color, translation, and cutout are adopted for each image sequentially. 4 Method Data augmentation for GANs allows the discriminator to distinguish a single sample from multiple perspectives by transforming it into various augmented samples according to different augmentation parameters. However, it overlooks the differences in augmentation intensity, such as color contrast and translation magnitude, leading the discriminator to implicitly maintain invariance to these varying intensities. The invariance may limit the representation learning ability of the discriminator because it loses augmentation-related information (e.g., color and position) [24]. Figure 2 confirms the impact of this point on the discriminator representation learning task [4]. We argue that a discriminator that captures comprehensive representations contributes to better convergence of the generator [35, 22]. Moreover, data augmentation may lead to augmentation leaking in generated data, when using specific data augmentations such as random 90-degree rotations [18, 12]. Therefore, our goal is to eliminate the unnecessary potential inductive bias (invariance to augmentations) for the discriminator while preserving the benefits of data augmentation for training data-efficient GANs. To achieve this goal, we propose a novel augmentation-aware self-supervised discriminator ˆD : ˆX × X →Ω+ ∪ Ω− that predicts the augmentation parameter and authenticity of the augmented data given the original data as reference. Distinguishing between the real data and the generated data with different self-supervision is because they are different during training, especially in the early stage. Specifically, the predictive targets of real data and generated data are represented as ω+ ∈ Ω+ and ω− ∈ Ω−, respectively. They are constructed from the augmentation parameterω with different transformations, i.e., ω+ = −ω− = ω. Since the augmentation parameter is a continuous 4z G G(z) x T(G(z); ω) T(x; ω) ϕ ϕ ψ φ +/− ω+/ω− Figure 3: Diagram of AugSelf-GAN. The original augmentation-based discriminator is D(T(·)) = ψ(ϕ(T(·))). The augmentation-aware self-supervised discriminator is ˆD(T(·), ·) = φ(ϕ(T(·)) − ϕ(·)), where φ is our newly introduced linear layer with negligible additional parameters. vector, we use mean squared error loss to regress it. The proposed method combines continuous self-supervised signals with real-vs-fake discrimination signals, thus can be considered as soft-label augmentation [12]. Comparison with self-supervision that does not distinguish between real and fake is referred to Table 6 in Appendix C. Notice that the predictive targets (augmentations) can be a subset of performed augmentations (see Table 7 in Appendix C for comparison). Mathematically, the loss function for the augmentation-aware self-supervised discriminator is formulated as the following: Lss ˆD = Ex,ω h ∥ ˆD(T(x; ω), x) − ω+∥2 2 i + Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω−∥2 2 i . (5) In our implementations, the proposed self-supervised discriminator ˆD = φ◦ϕ shares the backbone ϕ : X →Rd with the original discriminator D = ψ◦ϕ except the output linear layerφ : Rd → Ω+ ∪Ω−. This parameter-sharing design not only improves the representation learning ability of the original discriminator but also saves the number of parameters in our model compared to the base model, e.g., 0.04% more parameters in BigGAN and 0.01% in StyleGAN2. More specifically, the self-supervised discriminator predicts the target based on the difference between learned representations of the augmented data and the original data, i.e., ˆD(T(x; ω), x) = φ(ϕ(T(x; ω)) − ϕ(x)) (see Table 8 in Appendix C for comparison with other architectures). The philosophy behind our design is that the backbone ϕ should capture rich (which necessitates the design of a simple head φ) and linear (inspiring us to perform subtraction on the features) representations. In order for the generator to directly benefit from the self-supervision of data augmentation, we establish a novel adversarial game between the augmentation-aware self-supervised discriminator and the generator with the objective function for the generator defined as follows: Lss G = Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω+∥2 2 i − Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω−∥2 2 i . (6) The objective function is actually the combination of the non-saturating loss (regarding the generated data as real, minG Ez,ω[∥ ˆD(T(G(z); ω), G(z)) − ω+∥2 2]) and the saturating loss (reversely optimiz- ing the objective function of the discriminator, maxG Ez,ω[∥ ˆD(T(G(z); ω), G(z)) − ω−∥2 2]) (see Table 9 in Appendix C for ablation). Intuitively, the non-saturating loss encourages the generator to produce augmentation-predictable data, facilitating fidelity but reducing diversity. Conversely, the saturating loss strives for the generator to avoid generating augmentation-predictable data, pro- moting diversity at the cost of fidelity. We will elucidate in Section 5 how this formalization assists the generator in matching the fidelity and diversity of real data, ultimately leading to an accurate approximation of the target data distribution. The total objective functions for the original discriminator, the augmentation-aware self-supervised discriminator, and the generator of our proposed method, named AugSelf-GAN, are given by: min D, ˆD Lda D + λd · Lss ˆD, (7) min G Lda G + λg · Lss G, (8) 50 1 2 3 4 5−2 0 2 4 6 8 p(x)/q(x) f(p(x)/q(x)) KL rKL JS LC AHM Figure 4: Comparison of the func- tion f in different f-divergences. The f-divergence between two prob- ability distributions p(x) and q(x) is defined as Df (p(x)∥q(x)) =R X q(x)f(p(x)/q(x))dx with a con- vex function f : R≥0 → R satisfying f(1) = 0 . The x- and y-axis denote the input and the value of the function f in the f-divergence. The function f of the AHM divergence yields the most robust value for large inputs. where the hyper-parameters are set as λd = λg = 1 in experiments by default unless otherwise speci- fied (see Figure 6 for empirical studies). Details of objective functions are referred to Appendix B. 5 Theoretical Analysis In this section, we analyze the connection between the theoretical learning objective of AugSelf-GAN and the arithmetic − harmonic mean (AHM) divergence [37] under certain assumptions. Proposition 1. For any generator G and given unlimited capacity in the function space, the optimal augmentation-aware self-supervised discriminator ˆD∗ has the form of: ˆD∗(ˆx, x) = R pdata(x, ω, ˆx)ω+dω + R pG(x, ω, ˆx)ω−dω pdata(x, ˆx) + pG(x, ˆx) (9) The proofs of all theoretical results (including the following ones) are deferred in Appendix A. Theorem 1. Assume that ω+ = −ω− = c is constant, under the optimal self-supervised discrimina- tor ˆD∗, optimizing the self-supervised task for the generator G is equivalent to: min G 4c · MAH(pdata(x, ˆx)∥pG(x, ˆx)), (10) where c = ∥c∥2 2 is constant and MAH is the arithmetic − harmonic mean divergence [37], of which the minimum is achieved if and only if pG(x, ˆx) = pdata(x, ˆx) ⇒ pG(x) = pdata(x). Theorem 1 reveals that the generator of AugSelf-GAN theoretically still satisfies generative modeling, i.e., accurately learning the target data distribution, under certain assumptions. Although AugSelf- GAN does not obey the strict assumption, we note that this is not rare in the literature.2 Under this assumption, AugSelf-GAN can be regarded as a multi-dimensional extension of LS-GAN [29] in terms of the loss function, while excluding that of the generator. Additionally, our analysis offers an alternative theoretically grounded generator loss function for the LS-GAN family.3 Corollary 1. The following equality and inequality hold for the AHM divergence: • MAH(pdata(x, ˆx)∥pG(x, ˆx)) + MAH(pG(x, ˆx)∥pdata(x, ˆx)) = ∆(pdata(x, ˆx)∥pG(x, ˆx)) • MAH(pdata(x, ˆx)∥pG(x, ˆx)) = 1 − W(pdata(x, ˆx)∥pG(x, ˆx)) ≤ 1 where ∆ is the Le Cam (LC) divergence [23], and W is the harmonic mean divergence [37]. Corollary 1 reveals an inequality MAH(pdata(x, ˆx)∥pG(x, ˆx)) ≤ ∆(pdata(x, ˆx)∥pG(x, ˆx)) because of non-negativity of AHM divergence MAH(pG(x, ˆx)∥pdata(x, ˆx)) ≥ 0. Figure 4 plots the function f in AHM divergence and other common f-divergences in the GAN literature. The AHM divergence shows better robustness of the function f than others for extremely large inputs p(x)/q(x) = D∗(x)/(1 − D∗(x)), which is likely for the optimal discriminator D∗ in data-limited scenarios. 2Goodfellow et al. [10] analyzed that saturated GAN optimizes the Jensen Shannon (JS) divergence, but in fact it uses non-saturated loss. LeCam-GAN [40] showed a connection between the Le Cam (LC) divergence [23] and its objective function based on fixed regularization, but in practice it uses exponential moving average. 3The theoretical analysis of LS-GAN is actually inconsistent with its generator loss function. 6Table 1: IS and FID comparisons of AugSelf-BigGAN with state-of-the-art methods on CIFAR-10 and CIFAR-100 with full and limited data. The best result is bold and the second best is underlined. Method 100% training data 20% training data 10% training data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) CIFAR-10 BigGAN [2] 9.07 9.59 8.52 21.58 7.09 39.78 DiffAugment [47] 9.16 8.70 8.65 14.04 8.09 22.40 CR-GAN [46] 9.17 8.49 8.61 12.84 8.49 18.70 LeCam-GAN [40] 9.43 8.28 8.83 12.56 8.57 17.68 DigGAN [8] 9.28 8.49 8.89 13.01 8.32 17.87 Tickets [3] - 8.19 - 12.83 - 16.74 MaskedGAN [14] - 8.41 ±.03 - 12.51 ±.09 - 15.89 ±.12 GenCo [7] - 7.98 ±.02 - 12.61 ±.05 - 18.10 ±.06 AugSelf-BigGAN 9.43±.14 7.68±.06 8.98±.09 10.97±.09 8.76±.05 15.68±.26 AugSelf-BigGAN+ 9.27 ±.05 7.54±.04 9.08±.04 9.95±.17 8.79±.04 12.76±.14 CIFAR-100 BigGAN [2] 10.71 12.87 8.58 33.11 6.74 66.71 DiffAugment [47] 10.66 12.00 9.47 22.14 8.38 33.70 CR-GAN [46] 10.81 11.25 9.12 20.28 8.70 26.90 LeCam-GAN [40] 11.05 11.20 9.81 18.03 9.27 27.63 DigGAN [8] 11.45 11.63 9.54 19.79 8.98 24.59 Tickets [3] - 10.73 - 17.43 - 23.80 MaskedGAN [14] - 11.65 ±.03 - 18.33 ±.09 - 24.02 ±.12 GenCo [7] - 10.92 ±.02 - 18.44 ±.04 - 25.22 ±.06 AugSelf-BigGAN 11.19 ±.09 9.88±.07 10.25±.06 16.11±.25 9.78±.08 21.30±.15 AugSelf-BigGAN+ 11.12 ±.10 10.09±.05 10.14±.11 15.33±.20 9.93±.06 18.64±.09 Table 2: FID comparison of AugSelf-StyleGAN2 with competing methods on FFHQ and LSUN-Cat with limited training samples. The best result is highlighted in bold and the second best is underlined. Method FFHQ LSUN-Cat 30K 10K 5K 1K 30K 10K 5K 1K StyleGAN2 [19] 6.16 14.75 26.60 62.16 10.12 17.93 34.69 182.85 + ADA [18] 5.46 8.13 10.96 21.29 10.50 13.13 16.95 43.25 + DiffAugment [47] 5.05 7.86 10.45 25.66 9.68 12.07 16.11 42.26 AugSelf-StyleGAN2 4.95 6.98 9.69 23.38 9.22 11.98 14.86 36.76 AugSelf-StyleGAN2+ 5.82 6.65 9.15 20.39 9.43 12.00 14.12 26.52 6 Experiments We implement AugSelf-GAN based on DiffAugment [ 47], keeping the backbones and settings unchanged for fair comparisons with prior work under two evaluation metrics, IS [34] and FID [11]. The mean and standard deviation (if reported) are obtained with five evaluation runs at the best FID checkpoint. Each of experiments in this work was conducted on an 32GB NVIDIA V100 GPU. 6.1 Comparison with State-of-the-Art Methods CIFAR-10 and CIFAR-100. Table 1 reports the results on CIFAR-10 and CIFAR-100 [21]. These experiments are based on the BigGAN architecture [2]. Our method significantly outperforms the direct baseline DiffAugment [47] and yields the best generation performance in terms of FID and IS compared with SOTA methods. Notably, our method achieves further improvement when using stronger augmentation (see Table 5), i.e., AugSelf-BigGAN+ (translation↑ and cutout↑). FFHQ and LSUN-Cat. Table 2 reports the FID results on FFHQ [17] and LSUN-Cat [45]. The hyper-parameter is λg = 0.2. AugSelf-GAN performs substantially better than baselines with the same network backbone. Also, stronger augmentation, i.e., AugSelf-StyleGAN2+ (translation↑ and cutout↑), further improves the performance when training data is very limited. 7Obama Grumpy cat Panda AnimalFace Cat AnimalFace Dog DA AS Figure 5: Qualitative comparison between DiffAug (DA) and AugSelf (AS) on low-shot generation. Table 3: FID comparison of AugSelf-StyleGAN2 with state-of-the-art methods with and without pre-training on five low-shot datasets. The best result is in bold and the second best is underlined. Method Pre-training? 100-shot AnimalFaces Obama Grumpy cat Panda Cat Dog Scale/shift [31] Yes 50.72 34.20 21.38 54.83 83.04 MineGAN [42] Yes 50.63 34.54 14.84 54.45 93.03 TransferGAN [41] Yes 48.73 34.06 23.20 52.61 82.38 FreezeD [30] Yes 41.87 31.22 17.95 47.70 70.46 StyleGAN2 [19] No 80.20 48.90 34.27 71.71 130.19 + AdvAug [3] No 52.86 31.02 14.75 47.40 68.28 + ADA [18] No 45.69 26.62 12.90 40.77 56.83 + APA [16] No 42.97 28.10 19.21 42.60 81.16 + DiffAugment [47] No 46.87 27.08 12.06 42.44 58.85 AugSelf-StyleGAN2 No 26.00 19.81 8.36 30.53 48.19 Low-shot image generation. Table 3 shows the FID scores on five common low-shot image generation benchmarks [36] (Obama, Grumpy cat, Panda, AnimalFace cat, and AnimalFace dog). The baselines are divided into two categories according to whether they were pre-trained. Due to its training stability, we can train AugSelf-StyleGAN2 for 5k generator update steps to ensure convergence. The hyper-parameters are λd = λg = 0.1 on Grumpy cat and AnimalFace cat, and the self-supervision is color on all datasets. Impressively, AugSelf-StyleGAN2 surpasses competing methods by a large margin on all low-shot datasets, approaching SOTA FID scores to our knowledge. Figure 5 visually compares the generated images of DiffAugment and AugSelf-GAN, revealing the latter as superior in generating images with more diversity and fewer artifacts. 6.2 Analysis of AugSelf-GAN Fixed supervision. Table 4 reports the results of AugSelf-GAN on CIFAR-10 and CIFAR-100 compared to the setup using fixed self-supervision, i.e., ω+ = −ω− = 1, which corresponds to the assumption in Theorem 1. AugSelf-GAN outperforms the fixed self-supervision in terms of both IS and FID in all training data regimes. The reason is somewhat intuitive, as fixed self-supervision does not constitute self-supervised learning for the model, therefore it cannot enable the discriminator to learn semantic information related to data augmentation to optimize the generator. Interestingly, the fixed one beats the baseline, which may be attributed to the multi-input [27] and regression loss [29]. Table 4: FID comparison with fixed self-supervision. The best results are highlighted in bold. Method 100% training data 20% training data 10% training data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) C10 Fixed (c = 1) 9.25 ±.17 8.01±.05 8.70±.12 12.58±.16 8.53±.05 17.66±.55 AugSelf-GAN 9.43±.14 7.68±.06 8.98±.09 10.97±.09 8.76±.05 15.68±.26 C100 Fixed (c = 1) 10.67 ±.06 12.02±.07 9.94±.06 17.70±.17 9.50±.13 22.84±.28 AugSelf-GAN 11.19±.09 9.88±.07 10.25±.06 16.11±.25 9.78±.08 21.30±.15 8Table 5: Study on stronger augmentation. The best is in bold and the second best is underlined. Method 100% training data 20% training data 10% training data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) C10 DiffAugment 9.29 ±.02 8.48±.13 8.84±.12 15.14±.47 8.80±.01 20.60±.13 + trans.↑ cut.↑ 9.28±.06 8.42±.18 8.78±.06 14.28±.27 8.69±.07 20.93±.21 AugSelf-GAN 9.43±.14 7.68±.06 8.98±.09 10.97±.09 8.76±.05 15.68±.26 + trans.↑ cut.↑ 9.27±.05 7.54±.04 9.08±.04 9.95±.17 8.79±.04 12.76±.14 C100 DiffAugment 11.02 ±.07 11.49±.21 9.45±.05 24.98±.48 8.50±.09 34.92±.63 + trans.↑ cut.↑ 11.10±.08 11.28±.20 9.58±.05 24.10±.66 8.59±.04 35.32±.46 AugSelf-GAN 11.19±.09 9.88±.07 10.25±.06 16.11±.25 9.78±.08 21.30±.15 + trans.↑ cut.↑ 11.12±.10 10.09±.05 10.14±.11 15.33±.20 9.93±.06 18.64±.09 0 0.1 0.2 0.5 1 2 5 10 10 15 20 λ FID CIFAR-10 0 0.1 0.2 0.5 1 2 5 10 10 20 30 λ CIFAR-100 10% data 20% data 100% data Figure 6: FID curves with varying hyper-parameters λ = λd = λg ∈ [0, 10] on CIFAR-10 and CIFAR-100. The hyper-parameter λ = 0 corresponds to the baseline BigGAN + DiffAugment. Stronger augmentation. Translation and cutout actually erase parts of image information, which help prevent the discriminator from overfitting, but could suffer from underfitting if excessive. Our self-supervised task enables the discriminator to be aware of different levels of translation and cutout, which helps alleviate underfitting and allows us to explore stronger translation and cutout. Table 5 compares AugSelf-GAN with DiffAugment in this setting. Overall, when data is limited, AugSelf-GAN can further benefit from stronger translation and cutout and achieve new SOTA FID results, while DiffAugment cannot. This implicitly indicates that our method enables the model o learn meaningful features to overcome underfitting, even under strong data augmentation. Hyper-parameters. Figure 6 plots the FID results of AugSelf-GAN with different hyper-parameters λ = λd = λg ranging from [0, 10] on CIFAR-10 and CIFAR-100. Notice that λ = 0 corresponds to the baseline BigGAN + DiffAugment. AugSelf-BigGAN performs the best when λ is near 1. It is worth noting that AugSelf-BigGAN outperforms the baseline even for λ = 10 with 10% and 20% training data, demonstrating superior robustness with respect to the hyper-parameter λ. 7 Conclusion This paper proposes a data-efficient GAN training method by utilizing augmentation parameters as self-supervision. Specifically, a novel self-supervised discriminator is proposed for predicting the augmentation parameters and data authenticity of augmented (real and generated) data simultaneously, given the original data. Meanwhile, the generator is encouraged to generate real rather than fake data of which augmentation parameters can be recognized by the self-supervised discriminator after augmentation. Theoretical analysis reveals a connection between the optimization objective of the generator and the arithmetic − harmonic mean divergence under certain assumptions. Experiments on data-limited benchmarks demonstrate superior qualitative and quantitative performance of the proposed method compared to previous methods. 9Limitations. In our experiments, we observed less significant improvement of AugSelf-GAN under sufficient training data. Furthermore, its effectiveness depends on the specific data augmentation used. In some cases, inappropriate data augmentation may limit the performance gain. Broader impacts. This work aims at improving GANs under limited training data. While this may result in negative societal impacts, such as lowering the threshold of generating fake content or exacerbating bias and discrimination due to data issues, we believe that these risks can be mitigated. By establishing ethical guidelines for users and exploring fake content detection techniques, one can prevent these undesirable outcomes. Furthermore, this work contributes to the overall development of GANs and even generative models, ultimately promoting their potential benefits for society. Acknowledgements We thank the anonymous reviewers for their valuable and constructive feedback. This work is funded by the National Natural Science Foundation of China under Grant Nos. 62272125, 62102402, U21B2046. Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI). References [1] Gulcin Baykal, Furkan Ozcelik, and Gozde Unal. Exploring deshufflegans in self-supervised generative adversarial networks. Pattern Recognition, 2022. ISSN 0031-3203. [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. [3] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective. In Advances in Neural Information Processing Systems, 2021. [4] Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [5] Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. {GAN}s can play lottery tickets too. In International Conference on Learning Representations, 2021. [6] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [7] Kaiwen Cui, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Fangneng Zhan, and Shijian Lu. Genco: Generative co-training for generative adversarial networks with limited data. Proceedings of the AAAI Conference on Artificial Intelligence, 2022. doi: 10.1609/aaai.v36i1. 19928. [8] Tiantian Fang, Ruoyu Sun, and Alex Schwing. Diggan: Discriminator gradient gap regulariza- tion for gan training with limited data. In Advances in Neural Information Processing Systems, 2022. [9] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014. [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, 2017. [12] Liang Hou, Huawei Shen, Qi Cao, and Xueqi Cheng. Self-supervised gans with label augmen- tation. In Advances in Neural Information Processing Systems, 2021. 10[13] Liang Hou, Qi Cao, Huawei Shen, Siyuan Pan, Xiaoshuang Li, and Xueqi Cheng. Condi- tional GANs with auxiliary discriminative classifier. In Proceedings of the 39th International Conference on Machine Learning, Proceedings of Machine Learning Research, 2022. [14] Jiaxing Huang, Kaiwen Cui, Dayan Guan, Aoran Xiao, Fangneng Zhan, Shijian Lu, Shengcai Liao, and Eric Xing. Masked generative adversarial networks are data-efficient generation learners. In Advances in Neural Information Processing Systems, 2022. [15] Jongheon Jeong and Jinwoo Shin. Training {gan}s with stronger augmentations via contrastive discriminator. In International Conference on Learning Representations, 2021. [16] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Deceive d: Adaptive pseudo augmentation for gan training with limited data. In Advances in Neural Information Processing Systems, 2021. [17] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [18] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Advances in Neural Information Processing Systems, 2020. [19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [20] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Advances in Neural Information Processing Systems, 2021. [21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009. [22] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [23] Lucien Le Cam. Asymptotic methods in statistical decision theory. 2012. [24] Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, and Jinwoo Shin. Improving transferability of representations via augmentation-aware self-supervision. In Advances in Neural Information Processing Systems, 2021. [25] Kwot Sin Lee, Ngoc-Trung Tran, and Ngai-Man Cheung. Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021. [26] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017. [27] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. In Advances in Neural Information Processing Systems, 2018. [28] Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized {gan} training for high-fidelity few-shot image synthesis. In International Conference on Learning Representations, 2021. [29] Xudong Mao, Qing Li, Haoran Xie, Raymond Y .K. Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. [30] Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning gans. arXiv preprint arXiv:2002.10964, 2020. 11[31] Atsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. [32] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam- plers using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016. [33] Parth Patel, Nupur Kumari, Mayank Singh, and Balaji Krishnamurthy. Lt-gan: Self-supervised gan with latent transformation detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021. [34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016. [35] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected gans converge faster. In Advances in Neural Information Processing Systems, 2021. [36] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011. [37] Inder Jeet Taneja. On mean divergence measures. Advances in Inequalities from probability theory and statistics. Nova, USA, 2008. [38] Ngoc-Trung Tran, Viet-Hung Tran, Bao-Ngoc Nguyen, Linxiao Yang, and Ngai-Man (Man) Cheung. Self-supervised gan: Analysis and improvement with multi-class minimax game. In Advances in Neural Information Processing Systems, 2019. [39] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for gan training. IEEE Transactions on Image Processing, 2021. doi: 10.1109/TIP.2021.3049346. [40] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [41] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In Proceedings of the European Conference on Computer Vision (ECCV), 2018. [42] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: Effective knowledge transfer from gans to target domains with few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [43] Ceyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. In Advances in Neural Information Processing Systems, 2021. [44] mengping yang, Zhe Wang, Ziqiu Chi, and Yanbing Zhang. Fregan: Exploiting frequency components for training gans under limited data. In Advances in Neural Information Processing Systems, 2022. [45] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. [46] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In International Conference on Learning Representations, 2020. [47] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. In Advances in Neural Information Processing Systems, 2020. [48] Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for gan training. arXiv preprint arXiv:2006.02595, 2020. 12A Proofs Proposition 1. For any generator G and given unlimited capacity in the function space, the optimal augmentation-aware self-supervised discriminator ˆD∗ has the form of: ˆD∗(ˆx, x) = R pdata(x, ω, ˆx)ω+dω + R pG(x, ω, ˆx)ω−dω pdata(x, ˆx) + pG(x, ˆx) (9) Proof. The objective function of the self-supervised discriminator can be written as follows: Lss ˆD = Ex,ω h ∥ ˆD(T(x; ω), x) − ω+∥2 2 i + Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω−∥2 2 i = ZZZ h pdata(x, ω, ˆx)∥ ˆD(ˆx, x) − ω+∥2 2 + pG(x, ω, ˆx)∥ ˆD(ˆx, x) − ω−∥2 2 i dxdωdˆx. Minimizing the integral objective is equivalent to minimizing the objective on each data point: Lss ˆD(x, ˆx) = Z h pdata(x, ω, ˆx)∥ ˆD(ˆx, x) − ω+∥2 2 + pG(x, ω, ˆx)∥ ˆD(ˆx, x) − ω−∥2 2 i dω. Getting its derivative with respect to the self-supervised discriminator and let it equals to 0, we have the optimal self-supervised discriminator as: ∂Lss ˆD ∂ ˆD(ˆx, x) = Z h pdata(x, ω, ˆx)2( ˆD(ˆx, x) − ω+) + pG(x, ω, ˆx)2( ˆD(ˆx, x) − ω−) i dω = 0 ⇒ ˆD∗(ˆx, x) = R pdata(x, ω, ˆx)ω+dω + R pG(x, ω, ˆx)ω−dω pdata(x, ˆx) + pG(x, ˆx) . Theorem 1. Assume that ω+ = −ω− = c is constant, under the optimal self-supervised discrimina- tor ˆD∗, optimizing the self-supervised task for the generator G is equivalent to: min G 4c · MAH(pdata(x, ˆx)∥pG(x, ˆx)), (10) where c = ∥c∥2 2 is constant and MAH is the arithmetic − harmonic mean divergence [37], of which the minimum is achieved if and only if pG(x, ˆx) = pdata(x, ˆx) ⇒ pG(x) = pdata(x). Proof. The objective function of the self-supervised task for the generator can be written as follows: Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω+∥2 2 i − Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω−∥2 2 i = ZZZ pG(x, ω, ˆx) h ∥ ˆD(x, ˆx) − ω+∥2 2 − ∥ˆD(x, ˆx) − ω−∥2 2 i dxdωdˆx = ZZ pG(x, ˆx) \u0014 ∥c(pdata(x, ˆx) − pG(x, ˆx)) pdata(x, ˆx) + pG(x, ˆx) − c∥2 2 − ∥c(pdata(x, ˆx) − pG(x, ˆx)) pdata(x, ˆx) + pG(x, ˆx) + c∥2 2 \u0015 dxdˆx = c · ZZ pG(x, ˆx) \u0014 ∥pdata(x, ˆx) − pG(x, ˆx) pdata(x, ˆx) + pG(x, ˆx) − 1∥2 2 − ∥pdata(x, ˆx) − pG(x, ˆx) pdata(x, ˆx) + pG(x, ˆx) + 1∥2 2 \u0015 dxdˆx = c · ZZ pG(x, ˆx) \u0014 ∥ 2pG(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)∥2 2 − ∥ 2pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)∥2 2 \u0015 dxdˆx = 4c · ZZ pG(x, ˆx) \u0014 pG(x, ˆx)2 − pdata(x, ˆx)2 (pdata(x, ˆx) + pG(x, ˆx))2 \u0015 dxdˆx = 4c · ZZ pG(x, ˆx) \u0014pG(x, ˆx) − pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx) \u0015 dxdˆx = 4c · MAH(pdata(x, ˆx)∥pG(x, ˆx)), where c = ∥c∥2 2 is a constant scalar for the constant vectorc = ω+ = −ω−. And we have the optimal generator pG(x, ˆx) = pdata(x, ˆx) ⇒ pG(x) = pdata(x) to minimize the AHM divergence. 13Table 6: IS and FID of AugSelf-BigGAN with different self-supervised tasks (SS: Equations (19) and (21); SS+: Equations (20) and (21); ASS: Equations (5) and (6)) on CIFAR-10 and CIFAR-100 with full and limited training data. The best is bold and the second best is underlined. Method 100% training data 20% training data 10% training data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) CIFAR-10 Baseline 9.29 ±0.02 8.48±0.13 8.84±0.12 15.14±0.47 8.80±0.01 20.60±0.13 SS (λg = 0) 9.28 ±0.06 8.30±0.12 8.86±0.09 13.96±0.19 8.62±0.09 20.94±0.42 SS 9.29 ±0.07 8.24±0.10 8.98±0.04 13.42±0.36 8.69±0.05 19.40±0.28 SS+ (λg = 0) 9.25 ±0.06 8.11±0.18 8.81±0.05 13.86±0.31 8.76±0.06 19.52±0.24 SS+ 9.26 ±0.04 8.26±0.28 8.85±0.05 13.81±0.27 8.63±0.05 19.85±0.18 ASS (λg = 0) 9.12 ±0.05 8.90±0.07 8.82±0.03 13.65±0.70 8.52±0.04 18.14±0.33 ASS 9.43±0.14 7.68±0.06 8.98±0.09 10.97±0.09 8.76±0.05 15.68±0.26 CIFAR-100 Baseline 11.02 ±0.07 11.49±0.21 9.45±0.05 24.98±0.48 8.50±0.09 34.92±0.63 SS (λg = 0) 11.04 ±0.09 11.48±0.43 9.81±0.07 21.17±0.19 9.11±0.09 30.48±0.57 SS 10.96 ±0.11 11.04±0.15 9.99±0.09 20.72±0.33 9.23±0.07 31.54±1.12 SS+ (λg = 0) 10.84 ±0.09 11.48±0.13 9.95±0.07 21.22±0.57 9.14±0.05 31.02±1.00 SS+ 10.94 ±0.10 10.94±0.12 9.88±0.06 22.72±0.37 9.23±0.14 31.40±0.22 ASS (λg = 0) 10.82 ±0.10 11.29±0.12 9.96±0.11 18.90±0.41 9.45±0.05 25.77±0.95 ASS 11.19±0.09 9.88±0.07 10.25±0.06 16.11±0.25 9.78±0.08 21.30±0.15 Corollary 1. The following equality and inequality hold for the AHM divergence: • MAH(pdata(x, ˆx)∥pG(x, ˆx)) + MAH(pG(x, ˆx)∥pdata(x, ˆx)) = ∆(pdata(x, ˆx)∥pG(x, ˆx)) • MAH(pdata(x, ˆx)∥pG(x, ˆx)) = 1 − W(pdata(x, ˆx)∥pG(x, ˆx)) ≤ 1 where ∆ is the Le Cam (LC) divergence [23], and W is the harmonic mean divergence [37]. Proof. We first prove the first corollary: MAH(pdata(x, ˆx)∥pG(x, ˆx)) ≤ MAH(pdata(x, ˆx)∥pG(x, ˆx)) + MAH(pG(x, ˆx)∥pdata(x, ˆx)) = ZZ pG(x, ˆx)pG(x, ˆx) − pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)dxdˆx + ZZ pdata(x, ˆx)pdata(x, ˆx) − pG(x, ˆx) pG(x, ˆx) + pdata(x, ˆx)dxdˆx = ZZ pG(x, ˆx)pG(x, ˆx) − pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)dxdˆx − ZZ pdata(x, ˆx)pG(x, ˆx) − pdata(x, ˆx) pG(x, ˆx) + pdata(x, ˆx)dxdˆx = ZZ (pG(x, ˆx) − pdata(x, ˆx))2 pdata(x, ˆx) + pG(x, ˆx) dxdˆx = ∆(pdata(x, ˆx)∥pG(x, ˆx)), where 0 ≤ ∆(pdata(x, ˆx)∥pG(x, ˆx)) ≤ 2 is the Le Cam (LC) divergence [23]. The following proves the second corollary: 0 ≤ MAH(pdata(x, ˆx)∥pG(x, ˆx)) = ZZ pG(x, ˆx)pG(x, ˆx) − pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)dxdˆx = ZZ pG(x, ˆx) \u0014 1 − 2pdata(x, ˆx) pdata(x, ˆx) + pG(x, ˆx) \u0015 dxdˆx = 1 − ZZ 2pdata(x, ˆx)pG(x, ˆx) pdata(x, ˆx) + pG(x, ˆx)dxdˆx = 1 − W(pdata(x, ˆx)∥pG(x, ˆx)) ≤ 1, where 0 ≤ W(pdata(x, ˆx)∥pG(x, ˆx)) ≤ 1 is the well known harmonic mean divergence [37]. 14B Loss Functions AugSelf-GAN adopts all kinds of augmentations of DiffAugment as the self-supervised signals by default, thus the self-supervised loss functions of the discriminator and the generator (Equations (5) and (6)) actually each comprise three sub self-supervised loss functions. Specifically, for AugSelf- GAN that uses color, translation, and cutout as self-supervision, the objective functions are: min D, ˆDcolor, ˆDtranslation, ˆDcutout Lda D + λd · \u0010 Lcolor ˆDcolor + Ltranslation ˆDtranslation + Lcutout ˆDcutout \u0011 , (11) min G Lda G + λg · \u0000 Lcolor G + Ltranslation G + Lcutout G \u0001 , (12) where Lcolor ˆDcolor , Ltranslation ˆDtranslation , Lcutout ˆDcutout , Lcolor G , Ltranslation G , and Lcutout G are defined as: Lcolor ˆDcolor = Ex,ω h ∥ ˆDcolor(T(x; ω), x) − ω+ color∥2 2 i + Ez,ω h ∥ ˆDcolor(T(G(z); ω), G(z)) − ω− color∥2 2 i , (13) Ltranslation ˆDtranslation = Ex,ω h ∥ ˆDtranslation(T(x; ω), x) − ω+ translation∥2 2 i + Ez,ω h ∥ ˆDtranslation(T(G(z); ω), G(z)) − ω− translation∥2 2 i , (14) Lcutout ˆDcutout = Ex,ω h ∥ ˆDcutout(T(x; ω), x) − ω+ cutout∥2 2 i + Ez,ω h ∥ ˆDcutout(T(G(z); ω), G(z)) − ω− cutout∥2 2 i , (15) Lcolor G = Ez,ω h ∥ ˆDcolor(T(G(z); ω), G(z)) − ω+ color∥2 2 i − Ez,ω h ∥ ˆDcolor(T(G(z); ω), G(z)) − ω− color∥2 2 i , (16) Ltranslation G = Ez,ω h ∥ ˆDtranslation(T(G(z); ω), G(z)) − ω+ translation∥2 2 i − Ez,ω h ∥ ˆDtranslation(T(G(z); ω), G(z)) − ω− translation∥2 2 i , (17) Lcutout G = Ez,ω h ∥ ˆDcutout(T(G(z); ω), G(z)) − ω+ cutout∥2 2 i − Ez,ω h ∥ ˆDcutout(T(G(z); ω), G(z)) − ω− cutout∥2 2 i , (18) where ˆDcolor = φcolor ◦ ϕ, ˆDtranslation = φtranslation ◦ ϕ, and ˆDcutout = φcutout ◦ ϕ share the backbone ϕ but differ in the heads φcolor, φtranslation, or φcutout, respectively. For the simplicity of notations, we write Equations (5) and (6) in the main text as the objective function. C Ablation Studies Self-supervised tasks. We introduce two non-adversarial self-supervised tasks for comparison. The first version is that the discriminator only learns self-supervision on real data, defined as: Lss ˆD = Ex∼pdata(x),ω∼p(ω) h ∥ ˆD(T(x; ω), x) − ω∥2 2 i . (19) The second version is that the discriminator learns self-supervised tasks on both real and generated data simultaneously, given by: Lss ˆD = Ex,ω h ∥ ˆD(T(x; ω), x) − ω∥2 2 i + Ez,ω h ∥ ˆD(T(G(z); ω), G(z)) − ω∥2 2 i . (20) For both versions, the generator is encouraged to produce augmentation-recognizable data, as follows: Lss G = Ez∼p(z),ω∼p(ω) h ∥ ˆD(T(G(z); ω), G(z)) − ω∥2 2 i . (21) According to the self-supervised task, we denote different approaches as SS (Equations (19) and (21)), SS+ (Equations (20) and (21)), and ASS (Equations (5) and (6), short for adversarial self-supervised learning, i.e., the proposed AugSelf-GAN). 15Table 7: IS and FID of AugSelf-BigGAN with different self-supervised signals on CIFAR-10 and CIFAR-100 with full and limited training data. The self-supervised signal to be predicted is marked by the symbol ✓. Notice that all methods adopt color, translation, and cutout as data augmentation. Self-Supervised Signals 100% training data 20% training data 10% training data color trans. cutout IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) CIFAR-10 ✗ ✗ ✗ 9.29±.02 8.48±.13 8.84±.12 15.14±.47 8.80±.01 20.60±.13 ✗ ✗ ✓ 9.30±.05 7.48±.07 8.94±.07 11.73±.27 8.73±.12 16.66±.39 ✗ ✓ ✗ 9.39±.07 7.51±.12 8.95±.05 11.82±.21 8.80±.03 16.27±.35 ✗ ✓ ✓ 9.38±.05 7.41±.11 8.87±.04 11.19±.08 8.63±.08 16.30±.57 ✓ ✗ ✗ 9.50±.07 7.57±.07 8.99±.06 11.38±.14 8.72±.09 16.50±.14 ✓ ✗ ✓ 9.41±.07 7.51±.05 8.92±.12 11.20±.17 8.83±.07 15.55±.21 ✓ ✓ ✗ 9.42±.06 7.43±.06 8.91±.09 11.19±.20 8.58±.02 15.17±.15 ✓ ✓ ✓ 9.43±.14 7.68±.06 8.98±.09 10.97±.09 8.76±.05 15.68±.26 CIFAR-100 ✗ ✗ ✗ 11.02±.07 11.49±.21 9.45±.05 24.98±.48 8.50±.09 34.92±.63 ✗ ✗ ✓ 11.16±.13 10.03±.14 10.18±.09 19.45±.62 9.46±.12 25.99±.62 ✗ ✓ ✗ 11.35±.10 9.88±.08 10.01±.07 18.39±.07 9.29±.10 26.50±.36 ✗ ✓ ✓ 11.26±.07 9.75±.05 10.35±.20 16.88±.46 9.92±.08 23.09±.72 ✓ ✗ ✗ 11.20±.08 10.01±.13 9.90±.11 18.16±.62 9.39±.04 21.48±.14 ✓ ✗ ✓ 11.17±.12 10.12±.20 10.14±.09 17.11±.62 9.94±.12 24.52±.76 ✓ ✓ ✗ 11.26±.16 9.65±.04 10.21±.12 17.32±.51 9.92±.06 22.94±.17 ✓ ✓ ✓ 11.19±.09 9.88±.07 10.25±.06 16.11±.25 9.78±.08 21.30±.15 Table 8: IS and FID of AugSelf-BigGAN with different architectures and fusions of φ on CIFAR-10 and CIFAR-100 with 10% training data. The best result is bold and the second best is underlined. Input can be concatenation [ϕ(x), ϕ(ˆx)] ∈ R2d, subtraction ϕ(ˆx) − ϕ(x), and augmentation ϕ(ˆx). Architecture Input CIFAR-10 10% data CIFAR-100 10% data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) two-layer MLP concatenation 8.54 ±0.07 18.47 ±0.37 8.65 ±0.11 30.15 ±0.47 two-layer MLP subtraction 8.62 ±0.07 16.94 ±0.49 8.88 ±0.06 28.63 ±0.36 two-layer MLP augmentation 8.78 ±0.05 19.06 ±0.81 8.73 ±0.10 29.40 ±0.91 linear layer concatenation 8.85±0.11 17.52 ±0.20 9.37 ±0.09 25.22 ±0.25 linear layer subtraction 8.76 ±0.05 15.68±0.26 9.78 ±0.08 21.30±0.15 linear layer augmentation 8.66 ±0.09 20.29 ±0.36 9.92±0.10 24.47 ±0.58 bilinear layer - 8.57 ±0.04 25.27 ±0.16 9.03 ±0.03 26.72 ±0.17 Table 6 reports the comparison between methods with different self-supervised tasks. We also conducted generator-free self-supervised learning experiments by setting the hyper-parameter as λg = 0 on each kind of self-supervised task. According to the FID score, ASS is significantly superior to SS and SS+. Even without the self-supervised task of the generator, ASS (λg = 0) outperforms SS and SS+ that include generator self-supervised tasks in limited (10% and 20%) data, and the introduction of generator self-supervised tasks further expands this advantage. Self-supervised signals. We empirically analyze the role of different augmentation parameters as self-supervised signals for AugSelf-GAN. All methods employ three types of data augmentations (color, translation, and cutout), with the difference lying in the predicted self-supervised signals. According to the FID scores in Table 7, predicting any augmentation parameters can significantly improve the final generative performance compared to the baseline. Although there is no significant difference among them, we still choose to predict all augmentation parameters as the default setting for AugSelf-GAN. This experiment demonstrates that the self-supervised task itself plays a decisive role in training AugSelf-GAN, rather than the specific predicted augmentations. Therefore, we believe that our method is generalizable and can be extended to other advanced data augmentations. Network architectures. We investigate the impact of different network architectures (two-layer multi-layer perceptron (MLP), linear layer, and bilinear layer) and input approaches (concatenation [ϕ(x), ϕ(ˆx)], subtraction ϕ(ˆx) − ϕ(x), and augmented samples only ϕ(ˆx)) on AugSelf-GAN. As 16Table 9: IS and FID of AugSelf-BigGAN with different loss functions on CIFAR-10 and CIFAR-100 with full and limited training data. The best result is bold and the second best is underlined. Method 100% training data 20% training data 10% training data IS (↑) FID ( ↓) IS ( ↑) FID ( ↓) IS ( ↑) FID ( ↓) CIFAR-10 λd = 0, λg = 0 9.29±.02 8.48±.13 8.84±.12 15.14±.47 8.80±.01 20.60±.13 λd = 1, λg = 0 9.12±.05 8.90±.07 8.82±.03 13.65±.70 8.52±.04 18.14±.33 saturating 9.27 ±.06 8.13±.14 8.83±.04 12.76±.29 8.42±.05 18.43±.21 non-saturating 9.37 ±.07 7.60±.08 8.91±.09 11.44±.16 8.63±.10 15.86±.26 combination 9.43±.14 7.68±.06 8.98±.09 10.97±.09 8.76±.05 15.68±.26 CIFAR-100 λd = 0, λg = 0 11.02±.07 11.49±.21 9.45±.05 24.98±.48 8.50±.09 34.92±.63 λd = 1, λg = 0 10.82±.10 11.29±.12 9.96±.11 18.90±.41 9.45±.05 25.77±.95 saturating 10.90 ±.07 10.53±.21 9.71±.08 18.78±.43 9.30±.09 25.92±.12 non-saturating 11.22±.14 9.90±.09 10.07±.02 16.12±.26 9.81±.10 21.99±.87 combination 11.19 ±.09 9.88±.07 10.25±.06 16.11±.25 9.78±.08 21.30±.15 reported in Table 8, we found that more complicated architectures of the predict head φ such as two- layer MLP with input concatenation ( ˆD(T(x, ω), x) = MLP([ϕ(T(x, ω)), ϕ(x)])) or bilinear layer ( ˆD(T(x; ω), x) = ϕ(T(x; ω))⊤W ϕ(x), W∈ Rd×d) works worse than the simple linear layer with input subtraction ( ˆD(T(x; ω), x) = φ(ϕ(T(x; ω)) − ϕ(x))), which is the design of AugSelf-GAN. The reason might be that complicated architectures of the head φ actually discourage the backbone ϕ from capturing rich and linear representations, resulting in poor generalization of discriminators. Generator loss functions. We study the effects of AugSelf-GANs with different generator loss functions. As shown in Table 9, The hyper-parameters λd = 0 , λg = 0 represent the baseline BigGAN + DiffAugment. The generation performance is improved with only augmentation-aware self-supervision for the discriminator (λd = 1, λg = 0). The saturating version of self-supervised generator loss shows no significant further improvement. The non-saturating version yields compa- rable performance with the combination one (AugSelf-GAN), and they both substantially surpass the others. The reason is that the non-saturating loss directly encourages the generator to generate augmentation-predictable real data, providing more informative guidance than the saturating one. D Additional Results 100% training data 20% training data 10% training data CIFAR-10 CIFAR-100 Figure 7: FID plots comparison of AugSelf-GAN and baselines on CIFAR-10 and CIFAR-100. 17Table 10: FID comparison on AFHQ. The number within the parentheses represents the percentage of improvement (↓) in AugSelf-StyleGAN2 compared to the baseline StyleGAN2 + DiffAugment. Method Cat Dog Wild StyleGAN2 [19] 5.13 19.4 3.48 + DiffAugment [47] 3.49 8.75 2.69 AugSelf-StyleGAN2 3.23 (↓ 7.45%) 8.17 (↓ 6.63%) 2.48 (↓ 7.81%) StyleGAN2 + DiffAugment AugSelf-StyleGAN2 Cat Dog Wild Figure 8: Visual comparison between StyleGAN2+DiffAugment and AugSelf-StyleGAN2 on AFHQ. Training stability. Figure 7 shows the FID plots during training of our method and baselines on CIFAR-10 and CIFAR-100. Overall, AugSelf-BigGAN demonstrates a more stable convergence compared to the baselines, while AugSelf-BigGAN+ goes even further. AFHQ We also conduct experiments on the AFHQ dataset [6] to compare our proposed method with DiffAugment by employing StyleGAN2 as the backbone model. The hyperparameters are set as λd = 0 .1, λd = 0 .5, and λd = 0 .2 on Cat, Dog, and Wild domains, respectively, and λg = 0.1 on all three domains. Quantitative experimental results, as shown in Table 10, suggest that AugSelf-StyleGAN2 yields approximately a 7% improvements in FID across all three domains when compared to DiffAugment. Figure 8 presents a qualitative comparison between these two methods, demonstrating a significant improvement in image quality for our method. FFHQ and LSUN-Cat Figures 9 and 10 present the randomly generated images produced by StyleGAN2 + DiffAugment and AugSelf-StyleGAN2+ on the FFHQ [ 17] and LSUN-Cat [ 45] datasets with varying amounts of training data, respectively. Our AugSelf-StyleGAN2+ demonstrates significant superiority over StyleGAN2 + DiffAugment in terms of visual quality of generated images. 18StyleGAN2 + DiffAugment AugSelf-StyleGAN2+ 1k training samples 5k training samples 10k training samples 30k training samples Figure 9: Comparison of random generated images between StyleGAN2 + DiffAugment and AugSelf- StyleGAN2+ on FFHQ with different amounts of training data. 19StyleGAN2 + DiffAugment AugSelf-StyleGAN2+ 1k training samples 5k training samples 10k training samples 30k training samples Figure 10: Comparison of randomly generated images between StyleGAN2 + DiffAugment and AugSelf-StyleGAN2+ on LSUN-Cat with different amounts of training data. 20",
      "references": [],
      "meta_data": {
        "arxiv_id": "2205.15677v5",
        "authors": [
          "Liang Hou",
          "Qi Cao",
          "Yige Yuan",
          "Songtao Zhao",
          "Chongyang Ma",
          "Siyuan Pan",
          "Pengfei Wan",
          "Zhongyuan Wang",
          "Huawei Shen",
          "Xueqi Cheng"
        ],
        "published_date": "2022-05-31T10:35:55Z",
        "github_url": "https://github.com/liang-hou/augself-gan"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes augmentation-aware self-supervised discriminator (AugSelf-GAN) to mitigate augmentation-induced invariance in discriminators when training GANs with limited data. The discriminator predicts augmentation parameters of augmented data and distinguishes real vs. generated samples through a self-supervised objective, enabling the generator to learn from augmentation-predictable real data. Establishes a theoretical link between the generator objective and the arithmetic–harmonic mean divergence under a constant augmentation pairing. Demonstrates substantial empirical gains over state-of-the-art data-efficient GAN methods across multiple benchmarks (CIFAR-10/100, FFHQ, LSUN-Cat, and five low-shot datasets) using BigGAN and StyleGAN2 backbones.",
        "methodology": "Core idea: augmentations are enhanced with augmentation-aware self-supervision. An augmentation-aware self-supervised discriminator ˆD, sharing backbone ϕ with the main discriminator D, receives (augmented sample, original sample) and predicts ω+ for real data and ω− for generated data, with ω+ = −ω− = ω. The loss is Lss ˆD = E_x,ω ||ˆD(T(x; ω), x) − ω+||^2 + E_z,ω ||ˆD(T(G(z); ω), G(z)) − ω−||^2. The self-supervised head φ maps from ϕ’s features to the augmentation space, while the backbone ϕ is shared with D = ψ∘ϕ. The generator objective includes an adversarial self-supervised term Lss G = E_z,ω ||ˆD(T(G(z); ω), G(z)) − ω+||^2 − ||ˆD(T(G(z); ω), G(z)) − ω−||^2. The overall objectives are: min_D,ˆD Lda D + λd Lss ˆD and min_G Lda G + λg Lss G. ω+ and ω− are continuous augmentation parameters; the method allows both non-saturating and saturating variants. Theoretical contribution: under ω+ = −ω− = c, optimizing the generator corresponds to minimizing 4c · MAH(pdata(x, x̂) ∥ pG(x, x̂)) (Theorem 1), linking augmentation-aware self-supervision to a robust f-divergence (AHM divergence). Corollaries discuss properties of MAH (relationship to Le Cam divergence and harmonic mean divergence). Empirically, stronger augmentations and ablations on head design, generator loss, and fixed vs. adaptive ω demonstrate robustness and effectiveness.",
        "experimental_setup": "Benchmarks: data-limited image generation on CIFAR-10/100 (class-conditional BigGAN) and FFHQ, LSUN-Cat (unconditional StyleGAN2). Low-shot datasets include Obama, Grumpy Cat, Panda, AnimalFace Cat, and AnimalFace Dog. Comparisons against DiffAugment and SOTA methods (CR-GAN, LeCam-GAN, DigGAN, MaskedGAN, GenCo, etc.). Evaluation metrics: Inception Score (IS) and Frechet Inception Distance (FID). Data regimes: 100%, 20%, 10% training data. Implemented on DiffAugment baseline with backbones unchanged for fair comparison. Hyperparameters: default λd = λg = 1; FFHQ/LSUN-Cat experiments use λg = 0.2; 32GB NVIDIA V100 hardware. Ablations include: fixed ω (c=1) vs AugSelf; stronger augmentations (translation and cutout); different self-supervised signals (color/translation/cutout); various head architectures for φ; generator loss variants (saturating vs non-saturating); AFHQ domain experiments and qualitative visualizations. Code available at project repo.",
        "limitations": "AugSelf-GAN yields smaller gains when abundant data is available; effectiveness depends on chosen augmentations and their semantics; inappropriate augmentation choices can limit performance gains; theoretical results rely on assumptions (e.g., ω+ = −ω− = c constant) and may not fully capture all practical settings; observed improvements may vary across domains and architectures.",
        "future_research_directions": "Explore broader and data-specific augmentation families, including more sophisticated continuous self-supervised signals and other pretext tasks; extend augmentation-aware self-supervision to other GANs and diffusion models; develop theory beyond the constant-ω assumption and analyze convergence with varied augmentation schedules; combine with complementary data-efficient strategies (ADA, LeCam regularization, lottery tickets) and per-sample adaptive ω; investigate per-domain adaptation, non-image data, and larger-scale datasets; assess safety, bias, and detection implications of more capable data-efficient generators.",
        "experimental_code": "# Related implementation snippets for AugSelf-GAN (augmentation-aware self-supervision)\n\n# 1) Discriminator augmentation heads and forward (augself-enabled D)\n# File: augself-biggan/BigGAN.py\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, D_ch=64, D_wide=True, resolution=128,\n                 D_kernel_size=3, D_attn='64', n_classes=1000,\n                 num_D_SVs=1, num_D_SV_itrs=1, D_activation=nn.ReLU(inplace=False),\n                 D_lr=2e-4, D_B1=0.0, D_B2=0.999, adam_eps=1e-8,\n                 SN_eps=1e-12, output_dim=1, D_mixed_precision=False, D_fp16=False,\n                 D_init='ortho', skip_init=False, D_param='SN',\n                 augself='', **kwargs):\n        super(Discriminator, self).__init__()\n        # ... other initializations ...\n        self.augself = augself\n        self.out_augself = {}\n        for aug in filter(None, self.augself.split(',')):\n            self.out_augself[aug] = self.which_linear(self.arch['out_channels'][-1], AUGMENT_DIM[aug])\n        self.out_augself = nn.ModuleDict(self.out_augself)\n\n        # Initialize weights, optimizers, etc. (omitted)\n        # ...\n\n    def forward(self, x, x_o=None, y=None):\n        # Stick x into h\n        h = x\n        if self.augself:\n            h_o = x_o\n        # Main blocks\n        for index, blocklist in enumerate(self.blocks):\n            for block in blocklist:\n                h = block(h)\n                if self.augself:\n                    h_o = block(h_o)\n        h = torch.sum(self.activation(h), [2, 3])\n        if self.augself:\n            h_o = torch.sum(self.activation(h_o), [2, 3])\n        # Real data output\n        out = self.linear(h)\n        out = out + torch.sum(self.embed(y) * h, 1, keepdim=True)\n        # Self-supervised augmentation heads\n        out_augself = {}\n        for aug in filter(None, self.augself.split(',')):\n            out_augself[aug] = self.out_augself[aug](h - h_o)\n        return out, out_augself\n\n# 2) Parallel G_D wrapper forwarding augmentation outputs\n# File: augself-biggan/BigGAN.py\n\nclass G_D(nn.Module):\n    def __init__(self, G, D):\n        super(G_D, self).__init__()\n        self.G = G\n        self.D = D\n\n    def forward(self, z, gy, x=None, dy=None, train_G=False, return_G_z=False, policy=False, CR=False, CR_augment=None, config={}):\n        if z is not None:\n            with torch.set_grad_enabled(train_G):\n                G_z = self.G(z, self.G.shared(gy))\n        else:\n            G_z = None\n\n        D_input = torch.cat([img for img in [G_z, x] if img is not None], 0)\n        D_class = torch.cat([label for label in [gy, dy] if label is not None], 0)\n        D_input_ori = D_input\n        D_input, augself_param = DiffAugment(D_input, policy=policy, config=config)\n        if CR:\n            if CR_augment:\n                x_CR_aug = torch.split(D_input, [G_z.shape[0], x.shape[0]])[1]\n                if CR_augment.startswith('flip,'):\n                    x_CR_aug = torch.where(torch.randint(0, 2, size=[x_CR_aug.size(\n                        0), 1, 1, 1], device=x_CR_aug.device) > 0, x_CR_aug.flip(3), x_CR_aug)\n                x_CR_aug = DiffAugment(x_CR_aug, policy=CR_augment.replace('flip,', ''), config=config)\n                D_input = torch.cat([D_input, x_CR_aug], 0)\n            else:\n                D_input = torch.cat([D_input, x], 0)\n            D_class = torch.cat([D_class, dy], 0)\n        D_out, D_out_augself = self.D(D_input, D_input_ori, D_class)\n        if G_z is None:\n            return D_out\n        elif x is not None:\n            if CR:\n                return torch.split(D_out, [G_z.shape[0], x.shape[0], x.shape[0]])\n            else:\n                return torch.split(D_out, [G_z.shape[0], x.shape[0]]), D_out_augself, augself_param\n        else:\n            if return_G_z:\n                return D_out, G_z\n            else:\n                return D_out, D_out_augself, augself_param\n\n# 3) DiffAugment and augmentation dimension mapping\n# File: augself-biggan/DiffAugment_pytorch.py\n\ndef DiffAugment(x, policy='', channels_first=True, config={}):\n    param = {}\n    if policy:\n        if not channels_first:\n            x = x.permute(0, 3, 1, 2)\n        for p in policy.split(','):\n            ys = []\n            for f in AUGMENT_FNS[p]:\n                x, y = f(x, config)\n                ys.append(y)\n            ys = torch.cat(ys, 1)\n            param[p] = ys\n        if not channels_first:\n            x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n    return x, param\n\nAUGMENT_FNS = {\n    'color': [rand_brightness, rand_saturation, rand_contrast],\n    'translation': [rand_translation],\n    'cutout': [rand_cutout]\n}\n\nAUGMENT_DIM = {\n    'color': 3,\n    'translation': 2,\n    'cutout': 2\n}\n\n# 4) Training loss integration for AugSelf terms\n# File: augself-biggan/train_fns.py\n\ndef GAN_training_function(G, D, GD, z_, y_, ema, state_dict, config):\n    def train(x, y):\n        G.optim.zero_grad()\n        D.optim.zero_grad()\n        x = torch.split(x, config['batch_size'])\n        y = torch.split(y, config['batch_size'])\n        counter = 0\n        # ... optionally toggle grads ...\n        for step_index in range(config['num_D_steps']):\n            D.optim.zero_grad()\n            for accumulation_index in range(config['num_D_accumulations']):\n                z_.sample_()\n                y_.sample_()\n                *D_scores, D_out_augself, augself_param = GD(z_[:config['batch_size']], y_[:config['batch_size']],\n                              x[counter], y[counter], train_G=False, policy=config['DiffAugment'],\n                              CR=config['CR'] > 0, CR_augment=config['CR_augment'], config=config)\n                D_loss_real, D_loss_fake = losses.discriminator_loss(D_scores[0], D_scores[1])\n                D_loss_augself = 0.\n                for aug in filter(None, config['augself'].split(',')):\n                    D_loss_augself += (F.mse_loss(D_out_augself[aug][config['batch_size']:], augself_param[aug][config['batch_size']:]) + \\\n                                       F.mse_loss(D_out_augself[aug][:config['batch_size']],-augself_param[aug][:config['batch_size']])) * config['D_augself']\n                D_loss = D_loss_real + D_loss_fake + D_loss_augself\n                D_loss = D_loss / float(config['num_D_accumulations'])\n                D_loss.backward()\n                counter += 1\n            D.optim.step()\n        # Generator update including AugSelf term\n        G.optim.zero_grad()\n        if not config['fix_G']:\n            for accumulation_index in range(config['num_G_accumulations']):\n                z_.sample_()\n                y_.sample_()\n                D_fake, D_out_augself, augself_param = GD(z_, y_, train_G=True, policy=config['DiffAugment'], config=config)\n                G_loss_augself = 0.\n                for aug in filter(None, config['augself'].split(',')):\n                    G_loss_augself += (F.mse_loss(D_out_augself[aug], augself_param[aug]) - \\\n                                       F.mse_loss(D_out_augself[aug],-augself_param[aug])) * config['G_augself']\n                G_loss = losses.generator_loss(D_fake) / float(config['num_G_accumulations'])\n                (G_loss + G_loss_augself).backward()\n            G.optim.step()\n            if config['ema']:\n                ema.update(state_dict['itr'])\n        return {\n            'G_loss': float(G_loss.item()) if not config['fix_G'] else 0,\n            'D_loss_real': float(D_loss_real.item()),\n            'D_loss_fake': float(D_loss_fake.item()),\n        }\n    return train\n\n# 5) Training config options for augself in train.py\n# File: augself-biggan/train.py\n\nparser.add_argument('--augself', type=str, default='')\nparser.add_argument('--D_augself', type=float, default=1.)\nparser.add_argument('--G_augself', type=float, default=1.)\n# ... in setup_training_loop_kwargs:\n#   if augself:\n#       args.D_augself = D_augself\n#       args.G_augself = G_augself\n# ... other related settings ...\n#  \n# 6) Augmentation configuration and dimensionalities\n# File: augself-biggan/DiffAugment_pytorch.py (AUGMENT_DIM already shown above)\n\n\n",
        "experimental_info": "Overview of the augmentation-aware self-supervision method (AugSelf-GAN) implemented in the repository:\n\nWhat this method adds:\n- Augmentation-aware self-supervised discriminator: A self-supervised head ˆD on top of the discriminator D shares the backbone. For each augmented input, the head predicts an augmentation parameter ω, with a corresponding ω+ and ω− pairing, and a self-supervised loss enforces D to predict ω from augmented real data and ω− from augmented fake data. The key idea is to enable the generator to learn from augmentation-predictable real data by coupling the GAN objective with a prediction task on augmentation parameters.\n\nWhere the method is implemented (highlights):\n- augself-biggan/BigGAN.py\n  - Discriminator class: initialization includes augself and a per-augmentation linear head: self.out_augself[aug] = Linear(out_channels, AUGMENT_DIM[aug]); and forward() computes out_augself[aug] = self.out_augself[aug](h - h_o) for each augmentation, returning a dict of per-augmentation outputs along with the standard real/fake score. This is the core augmentation-aware self-supervision branch in the discriminator.\n  - G_D wrapper: DiffAugment(D_input) is applied, and the forward returns D_out, D_out_augself, augself_param, enabling the generator to incorporate augmentation-predictable signal via the additional term Lss G.\n  - The D_path and G_path integration follows the loss terms that combine the adversarial loss with the augmentation-self supervision loss.\n- augself-biggan/DiffAugment_pytorch.py\n  - DiffAugment(x, policy, config): applies a policy consisting of augmentation functions; returns the augmented data and per-policy augmentation parameters (param). AUGMENT_DIM defines the dimensionality of augmentation-parameter outputs per policy (e.g., color has 3 dims; translation 2; cutout 2).\n  - AUGMENT_FNS maps augmentation types (color, translation, cutout) to a set of augmentation functions (rand_brightness, rand_translation, rand_cutout, etc.).\n- augself-biggan/train_fns.py\n  - GAN_training_function(): the D and G update steps incorporate augmentation self-supervision losses. D_loss_augself is computed by summing MSE terms between D_out_augself[aug] and augself_param[aug], with a symmetric term involving the negative of the augmentation parameter for the real data, scaled by D_augself. Similarly, G_loss_augself adds a term using G_augself. These augment-self losses are added to the standard adversarial losses.\n- augself-biggan/train.py\n  - Exposes training options augself (string listing augmentations), D_augself and G_augself (weights for the self-supervised augmentation losses), plus diffusion augmentation config (DiffAugment) and augmentation heads. The code links augself to per-augmentation heads and dims, enabling augmentation-aware self-supervision to scale with these coefficients.\n- Experimental settings in the paper (as reflected in the repository):\n  - Data regimes and backbones mirror the paper: data-limited CIFAR-10/100 with BigGAN (class-conditional) and FFHQ/LSUN-Cat with StyleGAN2-like backbones; low-shot datasets include Obama, Grumpy Cat, Panda, AnimalFace Cat, and AnimalFace Dog.\n  - Baselines include DiffAugment and various SOTA data-efficient GANs.\n  - Hyperparameters: default λd = λg = 1; FFHQ/LSUN-Cat experiments use λg = 0.2; experiments run on 32-GB NVIDIA V100 GPUs. Ablations include: fixed ω (c=1) vs AugSelf; stronger augmentations; different head architectures for φ; generator loss variants (saturating vs non-saturating); AD/Au tasks (AFHQ domain experiments).\n  - Theoretical contribution remains Theorem 1: under ω+ = −ω− = c, optimizing the generator corresponds to minimizing 4c · MAH(pdata(x, x̂) ∥ pG(x, x̂)) linking augmentation-aware self-supervision to a robust f-divergence (AHM divergence).\n\nNotes:\n- The experimental settings are implemented in code via the augself config options (augself string, D_augself, G_augself) in augself-biggan/train.py and train_fns.py; the augmentation heads output ω-like parameters and are paired with the augmentation-parameter perturbations produced by DiffAugment (which returns aug-parameters for ω per policy).\n- This extraction focuses on sections directly implementing the AugSelf mechanism and its integration into the GAN training loop rather than the broader diffusion/DiffAugment machinery.\n"
      }
    },
    {
      "title": "Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least",
      "full_text": "Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least Siddharth Joshi 1 Baharan Mirzasoleiman 1 Abstract Self-supervised learning (SSL) learns high- quality representations from large pools of un- labeled training data. As datasets grow larger, it becomes crucial to identify the examples that con- tribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this problem for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of contrastive learning on such subsets. Through ex- tensive experiments, we show that we can safely exclude 20% of examples from CIFAR100 and 40% from STL10 and TinyImageNet, without af- fecting downstream task performance. In general, subsets selected by our method outperform ran- dom subsets by over 3% across these datasets. Interestingly, we also discover the subsets that contribute the most to contrastive learning are those that contribute the least to supervised learn- ing. Code available at https://github.com/bigml- cs-ucla/sas-data-efficient-contrastive-learning. 1. Introduction Large datasets power modern machine learning models. However, a key question is: what data points are essential for learning and whether more data will always yield better performance? Answering this question is crucial as it can re- duce the substantial costs of training on large datasets, boost performance of the trained models and guide data collection. This has motivated a body of recent research on finding the 1Department of Computer Science, University of California Los Angeles, CA 90095, USA.. Correspondence to: Siddharth Joshi <sjoshi804@cs.ucla.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). most essential subsets for supervised learning (Toneva et al., 2019; Paul et al., 2021; Mirzasoleiman et al., 2020; Min- dermann et al., 2022; Sorscher et al., 2022; Swayamdipta et al., 2020). However, as datasets grow larger, obtaining high-quality labels for them becomes prohibitively expen- sive. As a result, there has been a surge in self-supervised (SSL) pretraining on large un-labeled dataset (Chen et al., 2020; Grill et al., 2020a; Chen & He, 2021; Zbontar et al., 2021). Nevertheless, finding the most important data points for SSL has remained an open question. Finding the examples that contribute the most to SSL is in- deed very challenging. When labels are available, the value of every example for learning can be quantified based on its loss (or confidence of the prediction) or gradient norm. Effectively, difficult-to-learn examples i.e. those with high loss or large gradient norm during training are the ones that contribute the most to minimizing the training loss. How- ever, in the absence of labels, SSL methods cluster examples based on their similarity to the other data points. Therefore, the SSL loss and gradient of every example is tightly cou- pled with that of the other examples in the dataset. Hence, dropping an example affects the loss and gradient of all the other examples. This makes data selection inherently more challenging for SSL as compared to supervised learning. In this work, we address the above challenge for the first time and find examples that provably contribute the most to SSL. In particular, we focus on contrastive SSL which learns representations by maximizing the alignment between augmented views of the same examples and minimizing the similarity between augmented views of different exam- ples (Chen et al., 2020; Zbontar et al., 2021; Oord et al., 2018). We prove that examples that contribute the most to contrastive learning are those that have the highest ex- pected similarity between their augmented views and the augmented views of other examples in their latent class. Effectively, such examples pull different groups of exam- ples in a class together and enable the contrastive loss to maximally push away representations of examples in differ- ent classes. We show that such examples (1) ensure a high alignment between augmented views of examples in every class, and (2) preserve the centers of class representations learned by contrastive learning on the full data. We leverage 1 arXiv:2302.09195v5  [cs.LG]  12 Mar 2024Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least the above properties to provide a generalization guarantee for a linear classifier trained on the representations obtained by applying contrastive learning to the subset. We observe that, perhaps surprisingly, examples that con- tribute the most to contrastive learning contribute the least to supervised learning. In particular, we quantify the difficulty of examples for supervised learning using confidence of the predictions as well as the forgetting score (Toneva et al., 2019), i.e. the number of times an examples is misclassified after being correctly classified during the training. We show that examples that contribute the most to contrastive learn- ing are the easy examples with a high confidence and low forgetting score for supervised learning. Such examples can be safely excluded from a supervised learning pipeline, with- out harming the accuracy (Toneva et al., 2019). In contrast, difficult-to-learn examples that contribute the most to su- pervised learning can significantly hurt contrastive learning performance. We extensively evaluate the performance of our method, SAS, which selects Subsets that maximize Augmentation Similarity to the full data, on various datasets and using different contrastive learning methods. We first apply SAS to CIFAR10, CIFAR100 (Krizhevsky et al., 2009), STL10 (Coates et al., 2011a) and TinyImageNet (Le & Yang, 2015), with ResNet50 using SimCLR (Chen et al., 2020). We show that using SAS, up to 20% of examples from CIFAR100 and 40% from STL10 and TinyImageNet (Deng et al., 2009), can be safely excluded without harming the downstream per- formance. Similarly, for BYOL, using SAS to discard 20% of examples from STL10 can even outperform downstream performance of the full data by 2%. In general, SAS subsets outperform random subsets by over 3% across these datasets and methods including SimSiam (Chen & He, 2021), MoCo (He et al., 2020) and BYOL (Grill et al., 2020a). We also demonstrate that the subsets that contribute the most to SSL can be efficiently extracted can be efficiently extracted early-in-training or using a smaller proxy model. 2. Related Work Contrastive Learning. Contrastive learning has recently emerged as a performant self-supervised framework to learn representations that capture semantically relevant informa- tion from the data. The key idea behind this family of algo- rithms is learning representations by maximizing agreement between augmented views of the same example (positive pairs) and minimizing agreement between augmented views of different examples (negative pairs) (Chen et al., 2020; Zbontar et al., 2021; Grill et al., 2020a; Chen & He, 2021; He et al., 2020). To improve the performance of contrastive learning, re-weighting the negative pairs in the contrastive loss (Chuang et al., 2020) or re-weighting the loss to em- phasize the hard negatives (Robinson et al., 2020) has been recently explored. Here, we aim to find subsets of examples that contribute the most to contrastive learning. The above reweighting strategies are orthogonal to our work and can be applied to the subsets found by our method. Contrastive Learning Theory. A recent line of theoretical works has studied contrastive learning. In particular, under conditional independence between positive pairs given the label, representations learned by contrastive learning algorithms can achieve small errors in the downstream linear classification task (Arora et al., 2019; Saunshi et al., 2019; Tosh et al., 2021). The independence assumption was relaxed by (HaoChen et al., 2021), which showed that minimizing spectral-based contrastive loss results in spec- tral clustering on the augmented distribution and provides generalization guarantee for linear evaluation. Wang & Isola (2020) proved that asymptotically, the contrastive loss optimizes alignment (similarity) of positive pairs and uniformity of the representations on the hypersphere, relating them to positive effects on downstream tasks. The recent result of (Huang et al., 2021) showed that contrastive learning using the more general InfoNCE (Oord et al., 2018) or cross-correlation loss (Zbontar et al., 2021) maximizes alignment of positive pairs as well as divergence of centers of the latent class representations. Here, we build on this work and show that subsets that contribute the most to contrastive learning introduce minimal error on the alignment and divergence of centers of class representations learned on the full data. Leveraging the above properties, we provide generalization guarantees for downstream performance of representations learned on such subsets. Essential Subsets for Supervised Learning. There has been a recent body of efforts on finding the most important subsets for supervised learning. Empirical methods com- monly rank examples from easiest to hardest—based on confidence, loss or gradient—and curate subsets preserving the hardest examples. Coleman et al. (2020) used a smaller trained proxy model to find the most uncertain examples to train a larger model. Toneva et al. (2019) selects examples with highest forgetting score, i.e., the number of times they transition from being classified correctly to incorrectly dur- ing training. Swayamdipta et al. (2020) selects examples with the highest variance of predictions during training. Paul et al. (2021) selects examples with the lowest expected gra- dient norm over multiple initializations. More theoretically motivated approaches iteratively select subsets by impor- tance sampling based on gradient norm (Katharopoulos & Fleuret, 2018) or select weighted subset of examples which closely capture the full gradient (Mirzasoleiman et al., 2020; Pooladzandi et al., 2022; Killamsetty et al., 2021). In contrast, we show, for the first time, that easy-to-learn examples with highest confidence and lowest forgetting score that contribute the least to supervised learning are the 2Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least most beneficial for unsupervised contrastive learning. 3. Problem Formulation Assume we have a dataset XXX ={xxxi}i∈V of n=|V | training examples drawn i.i.d. from an unknown distribution. Each example belongs to one of the K latent classes i.e. V = {V1 ∪ ··· ∪VK}, but the corresponding class labels are not known at training time. Contrastive Learning learns representations of examples in the training data, by learning an encoder f that max- imizes agreement between representations of differently augmented views of the same example (i.e. positive pairs) and minimizes agreement between representations of aug- mented views of different examples (i.e. negative pairs). This is achieved by minimizing the following InfoNCE loss (Oord et al., 2018): Lcl(V )= − E i,j∈V E xxx1,xxx2∈A(xxxi) xxx−∈A(xxxj) log ef(xxx1)T f(xxx2) ef(xxx1)T f(xxx2)+ef(xxx1)T f(xxx−) , (1) where A(xxx) is the set of augmented views of example xxx. The performance of contrastive learning is evaluated by training a linear classifier on the learned representation using labels: gl f (xxx) = arg max k∈[K] (WWW f(xxx) +bbb)k (2) However, to simplify the theoretical analysis, we follow (Huang et al., 2021) and consider a non-parametric nearest neighbor (NN) classifier: gf (xxx) = arg min k∈[K] ∥f(xxx) − µµµk∥, (3) where µµµk:=Ei∈Vk Exxx′∈A(xxxi)[f(xxx′)] is the center of class Vk. The linear classifier learned on the labels gl f is guaranteed to perform at least as well as the NN classifier gf (Huang et al., 2021). Therefore, we use the classification error rate of the NN classifier to bound the worst-case performance of the linear classifier: ξ(gf (V )) = KX k=1 P[gf (xxxi) ̸= k, ∀i ∈ Vk]. (4) We note that in our experiments, we evaluate our method using the downstream accuracy of the linear classifier, and our theoretical guarantees on the NN classifier also upper- bound the error of the linear classifier. Our goal is to find a subset S ⊆ V of at most r training examples, such that the encoder fS = arg minf Lcl(S) obtained by minimizing the contrastive loss on the subset, allows the NN classifier to obtain a similar error on the full data. Formally, we aim to solve the following problem: S∗ = arg min S⊆V,|S|≤r [|ξ(gfS (V )) − ξ(gf (V ))|]. (5) 4. The Most Important Subsets for SSL We start by investigating which properties the subset S∗ must satisfy, such that the learned representations on the subset provide small downstream classification error. To do so, we rely on recent theoretical results on optimiza- tion and generalization of contrastive learning. In particu- lar, the recent results of Huang et al. (2021) showed that the generalization performance of representations obtained with contrastive learning dependents on: (1) alignment of positive pairs, (2) divergence of class centers and (3) con- centration of the augmented data. Alignment captures the similarity between representations of augmented views of examples, in expectation. Good alignment requires all aug- mented views of an example to have similar representa- tions. Divergence of class centers captures how distant class centers µµµl and µµµk are. Good divergence results in large enough distance between all pairs of class centers, i.e., small µµµT l µµµk ∀l, k∈ [K]. Concentration of augmented data is de- termined by the data distribution and augmentation pipeline. Specifically, let V 0 k ⊆ Vk be the subset of examples in every class k ∈ K that share at least one very similar augmented view: supxxx1,xxx2∈Vk minxxx′ 1∈A(xxx1),xxx′ 2∈A(xxx2)∥xxx1 − xxx2∥≤ δ for small δ >0. If for every latent class k ∈ [K], V 0 k is large enough (|V 0 k | ≥σ|Vk| for large σ ∈ (0, 1]), then the classes have sharp concentration of augmented data. In this case, good alignment and divergence guarantee good generaliza- tion performance for the downstream NN classifier. While concentration of the augmentations is independent of the contrastive loss, minimizing the contrastive loss effectively aligns the augmented views of the examples and results in good divergence of the class centers. Formally, for a normalized encoder ∥f∥=1, the InfoNCE loss in Eq. (1) can be written as: Lcl(V )= 1 2[ E i∈V E xxx1,xxx2∈A(xxxi) ∥f(xxx1)−f(xxx2)∥2] | {z } Lalign(V ): Related to Alignment −1 (6) + E i,j∈V E xxx1,xxx2∈A(xxxi) xxx−∈A(xxxj) log \u0000 ef(xxx1)T f(xxx2) + ef(xxx1)T f(xxx−) | {z } Related to Divergence \u0001 . The first term in the RHS of Eq. (6) is closely related to the alignment and the second term in the RHS is related to the divergence of class centers. Alignment. Minimizing the first term in the RHS of Eq.(6) aligns augmented views of the training examples in expecta- tion, and results in a small probability Rϵ(V ) for examples to still have non-aligned augmented views, i.e, the largest distance between their augmented views is larger than ϵ: Rϵ(V ) = P h i ∈ V : sup xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ > ϵ i . (7) In particular, for a L-Lipschitz continuous encoder f, we 3Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least Figure 1.Visualizing Expected Augmentation Distance dx,x′. Pair of examples on left shows two examples that are semantically very similar as seen by their augmentations being very similar to each other, thus the expected augmentation distance between them is small. In contrast, pair of examples on the right are not as semantically similar, thus have augmentations that are very dissimilar to each other. have that (Huang et al., 2021): Rϵ(V ) ≤ η(ϵ)· q Lalign(V ), (8) where Lalign(V ) =Ei∈V Exxx1,xxx2∈A(xxxi)∥f(xxx1)−f(xxx2)∥2 is the alignment loss; η(ϵ) =O(1 ϵ )is a function of ϵ and the transformations used for data augmentations. Divergence. Minimizing the second term in RHS of Eq. (6) pushes away the class centers, i.e., expected representation of examples in a class, and yields a small µµµT k µµµl for all k, l∈ [K]. Effectively, it maximizes the distance between different class representations. Minimizing the InfoNCE loss in Eq. (6) minimizes both terms in the RHS, thus ensuring good alignment and di- vergence. With good alignment (small Rϵ) and good di- vergence (small µµµT k µµµl), the NN classifier gf can correctly classify all the examples in the main part of every class that have concentrated and aligned augmented views. If the ma- jority of examples in every class have a high concentration of augmented data is large (large σ), good generalization is guaranteed. Formally, Theorem 4.1 (Huang et al. 2021). For any l, k∈ [K], if µµµk Tµµµl < ϕ(σ, δ, ϵ), (9) then the downstream error rate of NN classifier is ξ(gf (V )) ≤ (1 − σ) + Rϵ(V ). (10) Exact form of ϕ(σ, δ, ϵ) is discussed in Appendix B. 4.1. Subsets that Preserve Alignment and Divergence We rely on the above observations to find a subset that, when used to learn representations, provides similar generalization performance to that of the full data, for the downstream NN classifier. The key idea of our approach is to find a subset, such that minimizing the contrastive loss on this subset: (1) results in good alignment for all the examples, and (2) preserves the class centers of full data. In doing so, we ensure that the divergence of the class centers is preserved. If such a subset can be found, minimizing the contrastive loss in Eq. (1) on the subset results in good alignment and divergence on the full data, hence guarantees similar generalization performance for the downstream NN classifier. Next, we introduce the notion of expected augmentation distance and discuss how it can be leveraged to find a subset that satisfies the above two conditions. We start by defining the expected augmentation distance: Definition 4.2 (Expected augmentation distance). We de- fine the expected augmentation distance between examples i, j∈ V as the expected l2 norm between all pairs (xxx,xxx′) of augmented examples, such that xxx ∈ A(xxxi) and xxx′ ∈ A(xxxj). Formally, for every pair of examplesi, j∈ V we have: di,j = E xxx∈A(xxxi),xxx′∈A(xxxj) ∥xxx − xxx′∥. (11) Intuitively, expected augmentation distance captures the se- mantic dissimilarity between every pair of examples. That is, two examples that are semantically similar have a small ex- pected augmentation distance. We visualize examples with small and large expected augmentation distance in Fig. 1. 4.2. Ensuring Good Alignment First, we address finding a subset that, when used to min- imize the contrastive loss, aligns the augmented views of all the training examples. From Eq. (8), we know that min- imizing the alignment loss Lalign, directly minimizes the probability Rϵ(V ) of examples with non-aligned augmented views. That is Rϵ(V ) ≤ η(ϵ)· p Lalign(V ). Here, we find a subset Sk ⊆ Vk of examples from ev- ery latent class k that ensures small Rϵ(Vk), i.e., proba- bility that examples in Vk are not well-aligned. For ev- ery (arbitrary) subset Sk ⊆ Vk of size rk = |Sk| se- lected from class k with nk = |Vk| examples, we can upper-bound the probability Rϵ(Vk) based on the align- ment loss of the subset i.e. Lalign(Sk). In particular, using Rϵ(Vk) ≤ η(ϵ) ·Ei∈Vk Exxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ ≤ 4Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least η(ϵ) p Lalign(V ) (Huang et al., 2021), we can write: Rϵ(Vk) ≤ η(ϵ)· E i∈Vk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ (12) = η(ϵ) nk ·  X i∈Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ + X i∈Vk\\Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ ! (13) ≤ η(ϵ) nk ·  X i∈Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ + X i∈Vk\\Sk [2 min j∈Sk E xxx1∈A(xxxi), xxx2∈A(xxxj) ∥f(xxx1)−f(xxx2)∥] ! , (14) Detailed steps of getting Eq. (14) from Eq. (13) can be found in the Appendix C. Note that the first term in Eq. (14) is exactly η(ϵ) nk p Lalign(Sk). Hence, for a L- Lipschitz continuous encoder f, where ∥f(xxx) − f(xxx′)∥ ≤ L ∥xxx − xxx′∥ ∀xxx,xxx′, we have: Rϵ(Vk) ≤ η(ϵ) nk   rk q Lalign(Sk) + 2L X i∈Vk\\Sk min j∈Sk di,j ! . The alignment loss on the subset Lalign(Sk) can be effec- tively minimized by contrastive learning on the subset using the InfoNCE loss. We also empirically show in Appendix A Fig. 8(a) that alignment loss on the subsets we find for con- trastive learning is smaller than the alignment loss on the full data, i.e., Lalign(Sk) ≤ Lalign(Vk). Therefore, training on a subset Sk ⊆ Vk introduces at most the following error on Rϵ(Vk), i.e., the probability for any example in Vk to have a distance larger than ϵ between its augmented views: νk R ≤ 2Lη(ϵ) nk X i∈Vk\\Sk min j∈Sk di,j, (15) Therefore, the subset Sk ⊆ Vk with smallest expected augmentation distance di,j (semantic similarity) to the rest of the examples in the class Vk \\ Sk can best align augmentations of all the examples in the class Vk. Remark. Eq. (15) shows that the subset Sk that aligns augmented views of all the examples in a class Vk should have an element that is sufficiently similar to any other example in the class. In other words, the subset should contain examples that are representative of every groupof examples in the class. 4.3. Preserving the Class Centers Next, we discuss finding a subset that captures the center of every latent class µµµk. Figure 2.Most representative examples: examples in top row are each representative of their group (e.g. breed) in class dog. For every (arbitrary) subset Sk ⊆ Vk of size rk = |Sk| selected from class k with nk = |Vk| examples, we can write: µµµk = E ı∈Vk, xxx′∈A(xxxi) [f(xxx′)] = E i∈Vk, xxx′∈A(xxxi) [f(xxx′)] − E j∈Sk, xxx′′∈A(xxxj) [f(xxx′′)] + E j∈Sk, xxx′′∈A(xxxj) [f(xxx′′)] = 1 nk X i∈Vk E xxx′∈A(xxxi) [f(xxx′)]− 1 rk X j∈Sk E xxx′′∈A(xxxj) [f(xxx′′)] +µµµS k = 1 nk ·rk h rk X i∈Vk E xxx′∈A(xxxi) [f(xxx′)]−nk X j∈Sk E xxx′′∈A(xxxj) [f(xxx′′)] i +µµµS k = 1 nk ·rk X i∈Vk X j∈Sk \u0002 E xxx′∈A(xxxi) [f(xxx′)] − E xxx′′∈A(xxxj) [f(xxx′′)] \u0003 +µµµS k = 1 nk ·rk X i∈Vk X j∈Sk E xxx′∈A(xxxi), xxx′′∈A(xxxj) [f(xxx′)−f(xxx′′)] +µµµS k (16) Hence, for a L-Lipschitz continuous encoder f, where ∥f(xxx) − f(xxx′)∥ ≤L ∥xxx − xxx′∥ ∀xxx,xxx′, we can upper-bound the normed difference between the center of class Vk and subset Sk as follows: νk µ = ∥µµµk − µµµS k ∥≤ L · E i∈Vk, j∈Sk [di,j]. (17) That is, the subset that preserves the center of class k, can be found by minimizing the expectation of expected augmen- tation distances (semantic similarity) between examples in the subset Sk and all the data points Vk in class k. Remark. Eq. (17) implies that a subset Sk that captures the centre of class k, should be similar to all the examples in the class, in expectation. Such a subset contains examples from dense regions with sharp concentration of augmented data. Such examples best represent the entire class. 4.4. Minimizing the Alignment and Divergence Error Based on Eq. (15) and (17), we find the subset that ensures alignment of all data points in class k and closely captures 5Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least the center of the class, by solving the following problem: S∗ k = arg min S⊆Vk,|S|≤rk E i∈Vk, j∈Sk [di,j] | {z } Captures class center s.t. min j∈Sk di,j ≤δ ∀i∈Vk | {z } Ensures alignment . (18) Problem (18) is NP-hard as it involves calculating the value of the objective over an exponential number of subsets. To efficiently find a subset that captures the class center and contains representatives from different groups of examples in a class, we rely on the following objective which minimizes the sum of expected augmentation distance between examples in the subset j ∈ Sk and the rest of examples in the class Vk \\ Sk: S∗ k = arg min S⊆Vk,|S|≤rk X i∈Vk\\Sk X j∈Sk di,j. (19) By minimizing the sum of distances (dissimilarities) between the subset and the rest of examples, Eq. (19) finds examples that are similar to many other examples in their class. In doing so, it finds a subset that ensure alignment. At the same time, the selected examples are selected from dense regions with sharp concentration of augmented data. Hence, the subset closely preserves the class center. The above minimization problem can be turned into maxi- mizing the following non-monotone submodular1 problem: S∗ k = arg max S⊆Vk,|S|≤rk X i∈Vk\\Sk X j∈Sk C − di,j, (20) where C is a big constant. C − di,j captures the similarity between i and j. Thus, we can find a nearly optimal subset using algorithms designed for maximizing non-monotone submodular functions under a cardinality constraint. First, we rely on the greedy algorithm to identify a subset and then refine it by using unconstrained submodular maximization (Mirzasoleiman et al., 2016). The greedy algorithm commences with an empty set S0 = ∅, and at each step t, it selects an element e ∈ V that maximizes the marginal utility F(e|St) = F(St ∪ {e}) − F(St). Formally, St = St−1 ∪ {arg maxe∈V F(e|St−1)}. For unconstrained maximization, we utilize the double-greedy algorithm (Buchbinder et al., 2015), which initializes Sα = ∅ and Sβ = ST , where ST is the subset found by the final iteration of the greedy algorithm. It then computes ae = F(e|Sα) and be = F(Sβ \\ {e}) for all e ∈ V . Subsequently, it adds examples for which ae ≥ be to Sα and removes examples for which ae < be from Sβ, eventually setting Sα = Sβ. The time complexity of the greedy algorithm is O(nk) to identify k out of n examples, which can be further expedited 1A set function F : 2V → R+ is submodular if F(e|S) = F(S ∪{e}) −F(S) ≥ F(T ∪{e}) −F(T), for any S ⊆ T ⊆ V and e ∈ V \\ T. Algorithm 1 SAS: finding Subsets that maximize the ex- pected Augmentation Similarity 1: Input: Subset size B, proxy model fp 2: Output: Subset S 3: {V1, ..., VK} ←approximate latent classes (Sec. 4.5) 4: for all Vk ∈ {V1, ··· , VK} do 5: for all i, j∈ Vk do 6: si,j = ⟨fp(xxxi), fp(xxxj)⟩ 7: end for 8: Sk ← {} 9: rk ← |Vk| |V | · B 10: F(Sk) = P i∈Vk\\Sk P j∈Sk si,j 11: while |Sk| ≤rk do 12: e ← argmaxe∈Vk\\Sk F(e|Sk) 13: Sk ← Sk ∪ {e} 14: end while 15: Sk ← double-greedy(Sk) 16: end for 17: return S = {S1 ∪ ··· ∪SK} through lazy evaluation (Minoux, 2005). The double-greedy approach applied to the subset has a complexity of O(k). Thus, the subset can be found very efficiently. Remark. Intuitively, as the subsets selected from different classes have a small expected augmentation distance to all the different groups in the class, they pull together all the examples in a class during contrastive learning and let the representations of a class to cluster closely. At the same time, as they preserve the class centers, they allow the representations of different classes to be effectively pushed away from each other. In doing so, the subsets effectively minimize the contrastive loss on the full data. Note that as di,j is a property of the data in the input space, the subset found by solving Problem (20) ensures good alignment and divergence throughout contrastive learning. Fig. 2 presents a visualization of examples in the dog class. The examples found by Eq. (20) resemble those in the top row, i.e., they contain the core features of the class (e.g. the head and the paws of the puppies) with minimal noise (e.g. the non-standard poses of the puppies in the bottom row). Due to the standard and clear presentation of the core features of their respective groups, the examples in top row have smaller expected augmentation distance to many examples than examples in the bottom row, where some core features may be occluded (e.g. paws not visible) and/or presented in non-standard ways (e.g. open mouth). Next, we provide a generalization guarantee for contrastive learning from the subset. The following theorem shows that if contrastive learning on the subset provides a small extra divergence on the center of examples selected from different classes compared to that of full data, the downstream NN 6Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least classifier will have a similar generalization performance to that of contrastive learning from full data. Theorem 4.3. Assume f is a normalized encoder and the subset Sk selected by Eq. (20) has νk R error (Eq. 15) in cap- turing Rϵ(f, Vk) and νk µ error (Eq. 17) in capturing the cen- ter of class k. If for any pair of classes k, l∈[K], we have: µµµS k T µµµS l <ϕ(σ, δ, ϵ)− (21) \u0000 Cνk R + 2(max{νk µ, νl µ})2 + 4 max{νk µ, νl µ}) \u0001 . where ϕ(σ, δ, ϵ) is the requirement on divergence of full data class centers in Theorem 4.1 and C is a constant, then the generalization error of the model trained on the subset can be bounded by: ξ(gfS (V )) ≤ (1 − σ) + Rϵ + νR. (22) Theorem 4.3 shows that if the subset captures the class cen- ters and alignment closely (i.e. νR and νµ are small), then minimizing the contrastive loss on the subset provides a similar divergence to that of full data, and thus a similar downstream generalization performance for the NN classi- fier is guaranteed. The proof can be found in Appendix B, where we also dis- cuss that CνR is generally small. Fig. 8(b) in Appendix A confirms that divergence offull data class centers when train- ing on sufficiently large subsets found by Eq. (20) is in fact better than that of training on the full data. This explains the similar or even superior generalization performance of mod- els trained on SAS subsets to models trained on the full data. 4.5. SAS: Finding the Subset in Practice Finally, we present our method, SAS, which finds subsets that minimize expected augmentation distance or equiva- lently maximize expected augmentation similarity, by ap- proximately finding the latent classes and estimating the expected augmentation distance, without having the labels. Approximately Finding the Latent Classes. Problem (20) requires selecting a subset from every class separately. With- out the labels, we need to approximately find the latent classes. In practice, one can find latent classes by clustering the representations of a model trained with contrastive SSL. This approach requires no extra information and thus can generalize to contrastive learning in all domains. However, if an extra small subset of labeled data and a proxy model is available, we can find latent classes much more accurately. Specifically, if a small subset of labels are available, a proxy model can be used to approximately find the latent classes. In our experiments, we show that having as small as 1% of the labels, the pretrained CLIP (Radford et al., 2021) image encoder can be used to find the latent classes more accurately. Crucially, even without having access to any downstream labels, the pretrained CLIP can be used to find the latent classes. In our experiments, we show that using CLIP’s image and text encoders, we can match image em- beddings from STL10 to the closest text embeddings from ImageNet labels to obtain approximate latent classes for STL10. In practice, any fine-grained relevant set of labels provide a superior performance. This is because linearly sep- arable representations for the fine-grained task will ensure linearly separable representations for the coarser-grained task. This is a practical way to use SAS for vision tasks as well as other domains with pretrained foundational models. Estimating the Expected Augmentation Similarity. Expected augmentation distance captures similarity of examples in the input space. However, as pixel space is extremely high-dimensional, nearly all expected augmen- tation distances will be very large and extremely sensitive to small noise. Instead, using a proxy model can better capture the semantic similarities in practice. Note that the proxy model does not necessarily have to be the same as the model being trained with SSL. Indeed, the proxy model can be much smaller than the model being trained or can be partially trained with similar augmentations, as we confirm experimentally. Having a proxy model fp, for all xxxi,xxxj ∈ Vk, we estimate expected augmentation similarity, i.e., C − di,j in Eq. (20) by si,j = ⟨fp(xxxi), fp(xxxj)⟩. The pseudocode of SAS is illustrated in Alg. 1. 5. Experiments In this section, we first evaluate the downstream general- ization performance of the models trained by contrastive learning on the subsets found bySAS vs. random subsets, on CIFAR10, CIFAR100 (Krizhevsky & Hinton, 2009), STL10 (Coates et al., 2011b) and TinyImageNet (Deng et al., 2009). Then, we do an extensive ablation study on the effect of the approximate latent classes, and the proxy model used to estimate expected augmentation distance. Finally, we investigate the relation of these subsets to subsets that are important for supervised learning. Training Setup We use SimCLR (Chen et al., 2020) as the contrastive learning method to train ResNet-50 (He et al., 2016) as the encoder architecture and a 2-layer MLP to project the representation to a 128-dimensional latent space. We use InfoNCE with temperature as our loss. Following the standard training pipeline in (Chuang et al., 2020; Robinson et al., 2020) we train for 400 epochs using the Adam opti- mizer with a learning rate of 0.001. Due to computational constraints, we use ResNet-18 as the encoder architecture for TinyImageNet and only have a single run per subset size. We also evaluateSAS on other contrastive learning methods, namely BYOL (Grill et al., 2020a), SimSiam (Chen & He, 2020) and MoCo (He et al., 2020), using ResNet-18 as the encoder architecture. Data Augmentation For data augmentations, we use ran- dom crop, random resizing, random horizontal flips and color distortion, as is done in (Chen et al., 2020). Evaluation. For evaluation, we use the widely used linear evaluation protocol (Chen et al., 2020; Chuang et al., 2020). That is, we train a linear classifier using the learned 7Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Subset Size (%) 74 76 78 80 82 84T op-1 Accuracy (%) Random SAS (a) CIFAR10 20 30 40 50 60 70 80 Subset Size (%) 56 58 60 62 64 66T op-1 Accuracy (%)  Random SAS Full Data (b) CIFAR100 20 30 40 50 60 70 80 Subset Size (%) 78 80 82 84 86 88 90T op-1 Accuracy (%)  Random SAS Full Data (c) STL10 20 30 40 50 60 70 80 Subset Size (%) 22 24 26 28 30 32T op-1 Accuracy (%)  Random SAS Full Data (d) TinyImageNet (ResNet-18) Figure 3.Downstream Classification Accuracy of SAS Subsets vs. Random Subsets (reporting mean and std over 3 runs). 20 30 40 50 60 70 80 Subset Size (%) 52.5 55.0 57.5 60.0 62.5 65.0 67.5T op-1 Accuracy (%)  Random SAS Full Data (a) BYOL (STL10) 20 30 40 50 60 70 80 Subset Size (%) 50 52 54 56 58 60 62T op-1 Accuracy (%)  Random SAS Full Data (b) SimSiam (CIFAR100) 20 30 40 50 60 70 80 Subset Size (%) 45 50 55 60T op-1 Accuracy (%)  Random SAS Full Data (c) MoCo (CIFAR100) Figure 4.Evaluating SAS on other contrastive learning methods (training a ResNet-18). representations of the training examples and their labels. Then, we evaluate the performance of the linear classifier on the test set representations and their corresponding labels. To ensure fair comparison, we compare SAS subsets with random subsets of the same size sampled from the same approximate latent classes. 5.1. Downstream Generalization Performance First, we evaluate the downstream generalization perfor- mance of the model pre-trained on subsets of different sizes found by SAS vs. random subsets of the same size. Here, we use a pre-trained ResNet-50 as the proxy to calculate sij, as discussed in Sec. 4.5. For CIFAR100 and STL10, we consider all si,j > 0 and for CIFAR10 we consider all si,j > 0.5. As examples in CIFAR10 are generally more similar to each other, a larger threshold helps identifying representative examples better. To approximately find the latent classes, we train a linear classifier on the CLIP representations of the training data with a small randomly selected subset of training labels. In particular, for CIFAR10 and CIFAR100, we use 1% of the labels of training examples selected at random, and for STL10, we use all the labels ( < 5%) available labels. We use the trained linear classifiers to predict the latent class for all the training examples. In our ablation studies, we evaluate the performance when finding latent classes in other ways. SimCLR Fig. 3 shows that training with SimCLR on sub- sets of various sizes found by SAS allows outperform ran- dom subsets by over 3% on CIFAR100 and STL10, and by up to 2% on CIFAR10. Critically, comparing the perfor- mance of the subsets with that of the full data, we can see that for CIFAR100, 20% of examples and for STL10 and TinyImageNet, 40% of examples, can be safely discarded without affecting downstream accuracy. Other Contrastive Learning Methods. We validate that SAS can effectively find examples that contribute the most to contrastive learning across variety of contrastive learning methods. For these experiments, we train a ResNet-18 using BYOL (Grill et al., 2020b), MoCo (He et al., 2020) and Sim- Siam (Chen & He, 2020). Fig. 4(a) shows that training with BYOL on subsets of various sizes found bySAS from STL10 outperforms random subsets by more than 3%. Interestingly, with BYOL, subsets of size 80% outperform training on the full data by 2%. We also show that SAS allows us to discard 20% of examples on CIFAR100 when using SimSiam (Fig. 4(b)) and to achieve nearly 2% improvement over random subsets when using MoCo. (Fig. 4(c)). 5.2. Ablation Study Next, we conduct an extensive ablation study on the effect of the approximate latent classes, and the proxy model used to estimate expected augmentation distance. Finding Approximate Latent Classes. Fig. 5(a) compares the downstream performance on CIFAR100 when latent classes are obtained by training a linear classifier using 1% labeled training data on CLIP representations, to that of using the ground-truth class labels, and k-means clustering on the representations of a pretrained model. We see that approximately finding the latent classes using 1% of the labels works nearly as well as ground-truth labels. Notably, while the accuracy of the linear classifier trained with 1% of the labels of CIFAR100 is only 70.8%, this does not negatively affect the quality of subsets found bySAS. The latent classes help us avoid confusing examples that are 8Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least 20 30 40 50 60 70 80 Subset Size (%) 56 58 60 62 64 66T op-1 Accuracy (%) Random SAS True Classes k-Means Clusters (a) Effect of approximate latent classes (CIFAR100) 20 30 40 50 60 70 80 Subset Size (%) 78 80 82 84 86 88 90T op-1 Accuracy (%) Random SAS SAS (ImageNet Labels) Full Data (b) ImageNet labels (STL10) 20 30 40 50 60 70 80 Subset Size (%) 56 58 60 62 64 66T op-1 Accuracy (%) Random SAS R50, 50% R18, 100% R10, 100% R50, 10% (c) Effect of proxy model (CIFAR100) 20 30 40 50 60 70 80 Subset Size (%) 52.5 55.0 57.5 60.0 62.5 65.0 67.5T op-1 Accuracy (%) Random SAS (best) Least Forgettable SAS (worst) Most Forgettable (d) Easy examples for SL are im- portant for SSL(CIFAR100) Figure 5.Ablation study on CIFAR100 and STL10. similar to examples across many latent classes; thus, even with relatively inaccurate latent classes, such examples can be filtered. Moreover, in the absence of any labels, using k-means clustering on the on the representations of a pre- trained model performs equally well for smaller subsets and still provides a significant improvement for larger subsets. Next, we consider using a different set of labels than the original labels of the training data to find the latent classes. In particular, we use a pretrained CLIP to label STL10 im- ages by ImageNet labels, using the zero-shot approach. That is, we match every image in STL10 to one of the ImageNet labels, by finding the CLIP text embedding of the ImageNet label that is most similar to the CLIP image embedding. Fig. 5(b) compares the downstream performance on STL10, when using ImageNet labels to find latent classes using a zero-shot approach to that of using the available ( < 5%) STL10 labels to train a linear classifier on CLIP image rep- resentations. Notably, no label information about STL is used in the first case. The results clearly shows howSAS can entirely avoid the use of labels for approximating the latent classes. Crucially, any relevant and potentially finer-grained set of labels are enough to approximately find the latent classes and achieve a superior downstream performance. Using Proxy Models to Estimate Expected Augmentation Distance. Fig. 5(c) shows estimating augmentation distance using various proxy models, such as a ResNet-50 that is partially trained for as few as 10% of epochs as well as smaller models such as a pre-trained ResNet-10, achieves a very similar downstream performance to that of using a fully pre-trained ResNet-50. 5.3. Investigating subsets found by SAS Visualization. Fig. 6(a) use t-SNE to visualize examples that are selected by SAS vs those that are not selected, from the class “bed” in CIFAR100. Examples with small expected augmentation distance to selected and not selected examples are connected. We see that the selected examples have small distance to many other examples in the class. Fig. 6(b), illustrates some examples that are selected and not selected from the “bicycle” class. We see that the selected examples are representatives of the whole class, while those not selected present uncommon poses or views of the object. (a) t-SNE of bed (pairs with small dxxxxxx′ are connected) (b) Examples from bicycle Figure 6.Visualizing selected examples from CIFAR100 Easy Examples are the Most Important. Finally, we use the forgetting score (Toneva et al., 2019), i.e. the number of times an examples is misclassified after being correctly classified during supervised learning, to quantify the diffi- culty of an example. Importantly, least forgettable exam- ples that can be safely discarded from supervised learning without harming the accuracy (Toneva et al., 2019). Fig. 5(d) shows that least forgettable examples can considerably outperform the random baseline and achieve a comparable performance to SAS for smaller subsets. On the other hand, the most forgettable examples that are most beneficial for supervised learning, perform significantly worse than the random baseline and similar to the subsets deemed worst by SAS. This illustrates how the subsets that contribute the most to contrastive learning are the least beneficial for su- pervised learning and vice-a-versa. Fig. 7 in Appendix A further shows that subsets found by SAS have low forgetting score and high confidence, in expectation. That is, they are easy for supervised learning. Effectively, the most important subsets for SSL are least important for supervised learning. 6. Conclusion We identified subsets of examples that contribute the most to contrastive SSL. Theoretically, we characterized important subsets for contrastive learning with rigorous generalization guarantees for downstream performance. Empirically, we showed that using our method 20% - 40% examples can be discarded on CIFAR100, STL10 and TinyImageNet, observ- ing no loss and even improvement in downstream accuracy. Surprisingly, we discovered that these important subsets are the least informative for supervised learning. Acknowledgment. This research was supported by the National Science Foundation CAREER Award 2146492. 9Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least References Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229, 2019. Buchbinder, N., Feldman, M., Seffi, J., and Schwartz, R. A tight linear time (1/2)-approximation for unconstrained submodular maximization. SIAM Journal on Computing, 44(5):1384–1402, 2015. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A Sim- ple Framework for Contrastive Learning of Visual Rep- resentations. February 2020. doi: 10.48550/arXiv.2002. 05709. URL https://arxiv.org/abs/2002. 05709v3. Chen, X. and He, K. Exploring simple siamese representa- tion learning, 2020. Chen, X. and He, K. Exploring simple siamese represen- tation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021. Chuang, C.-Y ., Robinson, J., Lin, Y .-C., Torralba, A., and Jegelka, S. Debiased Contrastive Learning. In Advances in Neural Information Processing Systems , volume 33, pp. 8765–8775. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/hash/ 63c3ddcc7b23daa1e42dc41f9a44a873-Abstract. html. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011a. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Gordon, G., Dunson, D., and Dud ´ık, M. (eds.), Pro- ceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Pro- ceedings of Machine Learning Research , pp. 215–223, Fort Lauderdale, FL, USA, 11–13 Apr 2011b. PMLR. URL https://proceedings.mlr.press/v15/ coates11a.html. Coleman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B., Bailis, P., Liang, P., Leskovec, J., and Zaharia, M. Selec- tion via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020a. Grill, J.-B., Strub, F., Altch ´e, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo, Z. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R., and Valko, M. Bootstrap your own latent: A new approach to self-supervised Learning, Septem- ber 2020b. URL http://arxiv.org/abs/2006. 07733. arXiv:2006.07733 [cs, stat]. HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable guarantees for self-supervised deep learning with spectral contrastive loss, 2021. URL https://arxiv.org/ abs/2106.04156. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90. He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Momen- tum Contrast for Unsupervised Visual Representation Learning, March 2020. URL http://arxiv.org/ abs/1911.05722. arXiv:1911.05722 [cs]. Huang, W., Yi, M., and Zhao, X. Towards the generalization of contrastive self-supervised learning. arXiv preprint arXiv:2111.00743, 2021. Katharopoulos, A. and Fleuret, F. Not all samples are cre- ated equal: Deep learning with importance sampling. In International conference on machine learning, pp. 2525– 2534. PMLR, 2018. Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. Grad-match: Gradient matching based data sub- set selection for efficient deep model training. In Interna- tional Conference on Machine Learning, pp. 5464–5474. PMLR, 2021. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. 10Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least Le, Y . and Yang, X. S. Tiny imagenet visual recognition challenge. 2015. Mindermann, S., Brauner, J. M., Razzak, M. T., Sharma, M., Kirsch, A., Xu, W., H ¨oltgen, B., Gomez, A. N., Morisot, A., Farquhar, S., and Gal, Y . Prioritized Training on Points that are Learn- able, Worth Learning, and not yet Learnt. In Pro- ceedings of the 39th International Conference on Machine Learning , pp. 15630–15649. PMLR, June 2022. URL https://proceedings.mlr.press/ v162/mindermann22a.html. ISSN: 2640-3498. Minoux, M. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques: Proceedings of the 8th IFIP Conference on Optimization Techniques W¨urzburg, September 5–9, 1977, pp. 234–243. Springer, 2005. Mirzasoleiman, B., Badanidiyuru, A., and Karbasi, A. Fast constrained submodular maximization: Personalized data summarization. In International Conference on Machine Learning, pp. 1358–1367. PMLR, 2016. Mirzasoleiman, B., Bilmes, J., and Leskovec, J. Core- sets for data-efficient training of machine learning models. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 6950–6960. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/mirzasoleiman20a.html. Oord, A. v. d., Li, Y ., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Paul, M., Ganguli, S., and Dziugaite, G. K. Deep Learning on a Data Diet: Finding Important Examples Early in Training. In Advances in Neural Information Processing Systems, volume 34, pp. 20596–20607. Curran Asso- ciates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/hash/ ac56f8fe9eea3e4a365f29f0f1957c55-Abstract. html. Pooladzandi, O., Davini, D., and Mirzasoleiman, B. Adap- tive second order coresets for data-efficient machine learn- ing. In International Conference on Machine Learning, pp. 17848–17869. PMLR, 2022. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021. Robinson, J., Chuang, C.-Y ., Sra, S., and Jegelka, S. Con- trastive Learning with Hard Negative Samples. Oc- tober 2020. doi: 10.48550/arXiv.2010.04592. URL https://arxiv.org/abs/2010.04592v2. Saunshi, N., Plevrakis, O., Arora, S., Khodak, M., and Khandeparkar, H. A theoretical analysis of con- trastive unsupervised representation learning. In Chaud- huri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pp. 5628–5637. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ saunshi19a.html. Sorscher, B., Geirhos, R., Shekhar, S., Ganguli, S., and Mor- cos, A. S. Beyond neural scaling laws: beating power law scaling via data pruning, August 2022. URL http:// arxiv.org/abs/2206.14486. arXiv:2206.14486 [cs, stat]. Swayamdipta, S., Schwartz, R., Lourie, N., Wang, Y ., Ha- jishirzi, H., Smith, N. A., and Choi, Y . Dataset Cartog- raphy: Mapping and Diagnosing Datasets with Train- ing Dynamics. In Proceedings of the 2020 Confer- ence on Empirical Methods in Natural Language Pro- cessing (EMNLP), pp. 9275–9293, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.746. URL https:// aclanthology.org/2020.emnlp-main.746. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y ., and Gordon, G. J. An Empirical Study of Ex- ample Forgetting During Deep Neural Network Learning, November 2019. URL http://arxiv.org/abs/ 1812.05159. arXiv:1812.05159 [cs, stat]. Tosh, C., Krishnamurthy, A., and Hsu, D. Contrastive esti- mation reveals topic posterior information to linear mod- els. J. Mach. Learn. Res., 22:281–1, 2021. Wang, T. and Isola, P. Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020. Zbontar, J., Jing, L., Misra, I., LeCun, Y ., and Deny, S. Bar- low twins: Self-supervised learning via redundancy reduc- tion. In International Conference on Machine Learning, pp. 12310–12320. PMLR, 2021. 11Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least A. Extension to Experiments A.1. Details for STL10 Investigatory Experiments BYOL We consider a ResNet-18 trained for 40 epochs on STL10 with batch size 64 using SGD with learning rate of 0.001. A.2. Easy Examples are Important Here, we present results showing that the subsets SAS selects are easier for supervised learning by various metrics. We consider the number of forgetting events (Toneva et al., 2019) and the confidence of the prediction to quantify difficulty of a given example. Fig. 7 shows that SAS consistently picks examples with lower average forgetting events and higher confidence than the random subsets. 20 40 60 80 Subset Size (%) 0 2 4 6 8 10Average # Forgetting Events Random SAS (a) Forgetting Score 20 40 60 80 Subset Size (%) 0 2 4 6 8 10Average # Forgetting Events Random SAS (b) Confidence Figure 7.Examples found by SAS are easy (smaller number of forgetting events or higher confidence) for supervised learning. A.3. Empirical Proof of Good Alignment and Divergence 20 40 60 80 Subset Size (%) 0.000 0.025 0.050 0.075 0.100 0.125 0.150Alignment Loss on Subset Random SAS Full Data (a) Lalign(S) 20 40 60 80 Subset Size (%) 0.000 0.002 0.004 0.006Similarity of Class Centers Random SAS Full Data (b) Divergence Figure 8.Empirically verifying we find subsets that achieve good alignment and divergence In Fig. 8, we empirically measure Lalign(S) and the mean similarity of class centers to show that the subsets chosen by SAS do indeed have better alignment and divergence than random subsets. Moreover, Fig. 8(a) also empirically verifies our claim that Lalign(Sk) ≤ Lalign(Vk) 12Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least B. Proof for Theorem 4.3 Proof. First, we bound the discrepancy in divergence of subset class centers and divergence of full data class centers, relying on the discrepancy between class centers on the subset and full data. Let νk µ = ∥µµµS k − µµµk∥ and νl µ = ∥µµµS l − µµµl∥. Then, µµµS k T µµµS l − µµµT k µµµl = ((µµµS k − µµµk) +µµµk) T ((µµµS l − µµµl) +µµµl) − µµµT k µµµl (23) = (µµµS k − µµµk) T (µµµS l − µµµl) + (µµµS k − µµµk) T µµµl + µµµT k (µµµS l − µµµl) +µµµT k µµµl − µµµT k µµµl (24) ≤ νk µνl µ + νk µ ∥µµµl∥ + νl µ ∥µµµk∥ (25) Thus, for a normalized encoder ∥f∥ = r we get µµµS k T µµµS l − µµµT k µµµl ≤ r(νk µ + νl µ) + νk µνl µ. (26) Next, we use Theorem B.1 to provide a generalization guarantee for the downstream NN classifier. Let V ϵ ⊆ V be the subset of examples of the full data that are well-aligned i.e. ∀xxxi ∈ V ϵ, s.t. supxxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ ≤ϵ Recall V 0 k ⊆ Vk is the subset of examples with sharp concentration of augmented data in latent class k, i.e., supi,j∈V 0 k minxxx∈A(xxxj),xxx′∈A(xxxj) ∥xxx − xxx′∥ ≤δ and |V 0 k | ≥σ|Vk| for σ ∈ (0, 1] Theorem B.1 (Complete version of Theorem 4.1 (Huang et al., 2021)). For any l, k∈ [K], if µµµk Tµµµl < ϕ(σ, δ, ϵ) = r2(1 − ρk(σ, δ, ϵ) − p 2ρk(σ, δ, ϵ) − 1 2∆µ), (27) then every example in V 0 k ∩ V ϵ can be classified correctly by the NN classifier, whereρk(σ, ϵ, δ) = 2(1 − σ) + Rϵ pk + (σ − Rϵ pk )(Lδ r + 2ϵ r ), pk = probability of an example being from latent class k and ∆µ = 1 − mink ∥µµµk∥2/r2. If for any latent class k ∈ [K], all examples in V 0 k ∩ V ϵcan be classified correctly by the NN classifier, then the downstream error rate of NN classifier ξ(gf (V )) ≤ (1 − σ) + Rϵ(V ) (28) The above Theorem cannot be directly used as the training on the subset introduces an additional error in capturing the alignment for latent class k, i.e., νk R. Incorporating this, we get: µµµk Tµµµl < r2(1 − ρ′ k(σ, δ, ϵ) − q 2ρ′ k(σ, δ, ϵ) − 1 2∆µ), (29) where ρ′ k(σ, ϵ, δ) = 2(1 − σ) + Rϵ+νk R pk + (σ − Rϵ+νk R pk )(Lδ r + 2ϵ r ), and Rϵ is the probability of examples not having aligned augmented views and νk R is the alignment error on latent class k due to training on the subset. From (26), we have: µµµS k T µµµS l + r(νk µ + νl µ) + νk µνl µ <r2 \u0010 1 − ρ′ k(σ, ϵ, δ) − q 2ρ′ k(σ, ϵ, δ) − 1 2∆µ \u0011 . (30) Then, as long as the following bound on the divergence of the class centers of the subset holds: µµµS k T µµµS l <r2 \u0010 1 − ρ′ k(σ, ϵ, δ) − q 2ρ′ k(σ, ϵ, δ) − 1 2∆µ \u0011 − r(νk µ + νl µ) − νk µνl µ, (31) by Theorem B.1, we have that the NN classifier can correctly classify all the examples in V 0 k ∩ V ϵ for any latent class k ∈ [K] 13Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least Thus, then incorporating our additional error in alignment νR into the generalization error bound in Theorem B.1, we get ξ(gfS (V )) ≤ (1 − σ) + Rϵ(V ) + νR (32) Now, we can bound how much smaller the inner product of the class centers on the subset must be than that on the full data to achieve equivalent generalization guarantees (Eq. (32)), i.e. how much better the divergence on the subset should be than divergence on the full data. Let εk,l = r(νk µ + νl µ) + νk µνl µ. Then, comparing the bounds on divergence from Eq. (27) from Theorem B.1 and Eq. (31), we have r2(1 − ρk(σ, δ, ϵ) − p 2ρk(σ, δ, ϵ) − 1 2∆µ) − r2(1 − ρ′ k(σ, δ, ϵ) − q 2ρ′ k(σ, δ, ϵ) − 1 2∆µ) + εk,l (33) = r2 \u0010 ρ′ k(σ, δ, ϵ) − ρk(σ, δ, ϵ) + q ρ′ k(σ, δ, ϵ) − p ρk(σ, δ, ϵ)) \u0011 + εk,l. (34) Let ζ = νk R pk (1 − Lδ+2ϵ r ) where pk is probability of an example being from latent class k. Since √x + a − √ x + b ≈ a−b 2√x for large x, we get: ≈ r2 \u0010 ζ + ζ 2 p ρ(σ, δ, ϵ) \u0011 + νk µ 2 + 2rνk µ + r2 + r(νk µ + νl µ) + νk µνl µ (35) = Cνk R + 2(max{νk µ, νl µ})2 + 4 max{νk µ, νl µ}. (36) where C = r2 pk (1 − Lδ+2ϵ r )(1 + 1 2 √ ρk(σ,δ,ϵ) ). Hence, we can rewrite Eq. (31) as µµµS k T µµµS l <ϕ(σ, δ, ϵ) − \u0000 Cνk R + 2(max{νk µ, νl µ})2 + 4 max{νk µ, νl µ} \u0001 (37) When examples in every class have a high concentration of augmented data, i.e., when δ is small, ρ(σ, δ, ϵ) is small and C is large. However, in this settings, picking a subset according the objective in Eq. (15) guarantees a very small νk R. Therefore, Cνk R is small. On the other hand, when examples in every class do not have a high concentration of augmented data, δ is relatively large and hence C is small. As a result, Cνk R in Eq. (37) is small in both cases. Thus, for small νµ, the required divergence of subset class centers for the model trained on the subset is similar to the required divergence of full data class centers for the model trained on full data. C. Detailed Steps to derive Eq. (14) from Eq. (13) Let j ∈ Sk and xxxj′ = arg minxxxjk ∈A(xxxj) Exxx1∈A(xxxi) ∥f(xxx1) − f(xxxjk )∥. Then ∀i ∈ Vk \\ Sk: E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ (38) ≤ E xxx1,xxx2∈A(xxxi) [∥f(xxx1) − f(xxxj′)∥ + ∥f(xxxj′) − f(xxx2)∥] (39) ≤ E xxx1∈A(xxxi) ∥f(xxx1) − f(xxxj′)∥ + E xxx2∈A(xxxi) ∥f(xxxj′) − f(xxx2)∥. (40) But by definition of xxxj′, we have: ≤ E xxx1∈A(xxxi) xxxjk ∈A(xxxj) ∥f(xxx1) − f(xxxjk )∥ + E xxx2∈A(xxxi) xxxjk ∈A(xxxj) ∥f(xxxjk ) − f(xxx2)∥ (41) 14Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least = 2 E xxx1∈A(xxxi) xxx2∈A(xxxj) ∥f(xxx1) − f(xxx2)∥. (42) Since this inequality holds for any j ∈ S, we get: E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ ≤2 min j∈S E xxx1∈A(xxxi) xxx2∈A(xxxj) ∥f(xxx1) − f(xxx2)∥. (43) Thus, substituting the aforementioned bound to upper bound the second term (summation over i ∈ Vk \\ Sk) Eq. (13), we get Eq. (14) i.e.: η(ϵ) nk ·  X i∈Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ + X i∈Vk\\Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ ! (44) ≤ η(ϵ) nk ·  X i∈Sk E xxx1,xxx2∈A(xxxi) ∥f(xxx1) − f(xxx2)∥ + X i∈Vk\\Sk [2 min j∈Sk E xxx1∈A(xxxi), xxx2∈A(xxxj) ∥f(xxx1)−f(xxx2)∥] ! . (45) 15",
      "references": [],
      "meta_data": {
        "arxiv_id": "2302.09195v5",
        "authors": [
          "Siddharth Joshi",
          "Baharan Mirzasoleiman"
        ],
        "published_date": "2023-02-18T00:15:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This work studies data-efficient contrastive self-supervised learning (SSL) by identifying subsets of unlabeled data that contribute most to learning a good representation and guaranteeing that downstream linear/classification performance remains close to using full data. Key contributions: (i) theoretical result showing that examples with the highest expected augmentation similarity to others within their latent class drive alignment and center divergence in contrastive learning; (ii) a practical data-subsampling method SAS that selects small, representative subsets per latent class, provably preserving alignment and centers; (iii) generalization guarantees bounding downstream error when training on the subset; (iv) extensive empirical validation across CIFAR-100, STL-10, TinyImageNet and multiple SSL methods showing safe removal of 20-40% of data with no loss or even gains, and consistently beating random subsets by >3%.",
        "methodology": "The paper formalizes contrastive SSL via InfoNCE with an encoder f and augmented views; defines the expected augmentation distance di,j as the semantic dissimilarity between two examples; decomposes the contrastive loss into an alignment term Lalign and a divergence term that promotes separation of class centers. Subsets are chosen to ensure good alignment across all examples (low Rε) and to preserve the centers of latent classes (small center deviation νµ). The SAS objective selects, for each latent class Vk, a subset Sk of size rk that minimizes the sum of cross-distances di,j between Sk and Vk\\Sk (Eq. 19) and is solved by converting to a non-monotone submodular maximization problem (Eq. 20) using a greedy algorithm followed by unconstrained refinement (double greedy). Latent classes are obtained approximately via clustering or using a small labeled proxy (e.g., 1% labels or CLIP-based labeling). The similarity between examples within a class is estimated with a proxy model fp (e.g., ResNet-50 or smaller) via si,j = ⟨fp(xi), fp(xj)⟩. The SAS pseudocode is summarized in Algorithm 1.",
        "experimental_setup": "Experiments train a ResNet-50 encoder with SimCLR (and projection to 128-d) on CIFAR-10/100, STL-10, and TinyImageNet for 400 epochs with Adam, using standard SSL augmentations. Downstream evaluation uses a linear classifier on the learned representations. Baselines compare SAS-selected subsets to random subsets of equal size and to full data. SAS achieves safe data reduction: up to 20% of CIFAR-100 and 40% of STL-10 and TinyImageNet can be discarded without harming downstream accuracy (SAS even matches or slightly surpasses full-data performance for some settings, e.g., BYOL on STL-10). The method is validated across multiple SSL methods (SimCLR, BYOL, SimSiam, MoCo) with ResNet-18 encoders. Additional ablations examine latent-class estimation via CLIP-based labeling, 1% labels, or k-means, and the use of proxy models to estimate augmentation similarity. Visualizations (t-SNE) and forgetting/confidence analyses support the findings.",
        "limitations": "The approach relies on meaningful latent-class structure in the data and on accurate estimation of those classes (via labels, clustering, or CLIP-based labeling). The theoretical guarantees assume good alignment and diverging class centers, which depend on the data and augmentations, and the subset selection is NP-hard, requiring approximation via submodular optimization. There is additional computational overhead to estimate pairwise augmentation distances and latent classes, especially for large datasets. The performance on datasets with weak cluster structure or highly imbalanced classes, or in domains with different augmentation semantics, remains to be validated. The guarantees bound downstream error of the NN classifier; practical impact on more complex downstream tasks may vary.",
        "future_research_directions": "Extend SAS to other SSL frameworks and modalities (text, audio, video) and larger-scale datasets (ImageNet-scale). Investigate more accurate, label-free latent-class discovery methods and stronger proxies for estimating augmentation similarity. Explore dynamic or curriculum-based subset selection during training and integration with active data selection and core-set techniques. Study the interaction of SAS with hard-negative mining and reweighting strategies in SSL losses. Evaluate robustness to inaccurate latent-class estimation and augmentations, and assess performance when class distributions are highly imbalanced. Develop faster, scalable algorithms for subset selection on massive datasets and theoretical guarantees under weaker assumptions.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification",
      "full_text": "SIMPLICIAL EMBEDDINGS IN SELF -SUPERVISED LEARNING AND DOWNSTREAM CLASSIFICATION Samuel Lavoie⋄, Christos Tsirigotis⋄, Max Schwarzer⋄, Ankit Vani⋄, Michael Noukhovitch⋄, Kenji Kawaguchi‡, Aaron Courville⋄♣ ⋄Mila, Université de Montréal, ‡National University of Singapore, ♣CIFAR Fellow {samuel.lavoie.m,mnoukhov,aaron.courville}@gmail.com {christos.tsirigotis,max.schwarzer,ankit.vani}@umontreal.ca kenji@comp.nus.edu.sg ABSTRACT Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into Lsimplices of V dimen- sions each using a softmax operation. This procedure conditions the representation onto a constrained space during pre-training and imparts an inductive bias for group sparsity. For downstream classiﬁcation, we formally prove that the SEM representation leads to better generalization than an unnormalized representation. Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classiﬁcation task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes. 1 I NTRODUCTION 0.4 1.1 3.7 FLOPs (109) 72 73 74 75 76 77T op-1 accuracy BYOL BYOL (x2) BYOL (x4) BYOL+SEM BYOL+SEM (x2) BYOL+SEM (x4) Figure 1: Linear probe accuracy of BYOL and BYOL + SEM on ImageNet trained for 200 epochswith a ResNet-50 architecture. Self-supervised learning (SSL) is an emerging family of methods that aim to learn representations of data without manual supervision, such as class labels. Re- cent works (Hjelm et al., 2019; Grill et al., 2020; Saeed et al., 2020; You et al., 2020) learn dense representa- tions that can solve complex tasks by simply ﬁtting a linear model on top of the learned representation. While SSL is already highly effective, we show that changing the type of representation learned can improve both the performance and interpretability of these methods. For this we draw inspiration from overcomplete repre- sentations: representations of an input that are non- unique combinations of a number of basis vectors greater than the input’s dimensionality (Lewicki & Se- jnowski, 2000). Mostly studied in the context of the sparse coding literature (Gregor & LeCun, 2010; Goodfellow et al., 2012; Olshausen, 2013), sparse overcomplete representations have been shown to increase stability in the presence of noise (Donoho et al., 2006), have applications in neuroscience (Olshausen & Field, 1996; Lee et al., 2007), and lead to more interpretable representations (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). However, the choice of basis vectors is generally assumed to be learned using traditional methods such as ICA (Teh et al., 2003) or ﬁtting linear models (Lewicki & Sejnowski, 2000), limiting the expressive power of the encoding function. In this work, we show that SSL may be used to learn sparse and overcomplete representations. Prior work has considered sparse representation but not sparse and overcomplete representation learning with SSL; for example, Dessì et al. (2021) propose to discretize the output of the encoder in a SSL model using Gumbel-Softmax (Jang et al., 2017). However, we show that discretization during pre-training is not necessary to achieve a sparse representation. Instead, we propose to project the 1 arXiv:2204.00616v2  [cs.LG]  30 Sep 2022encoder’s output intoLvectors of V dimensions onto which we apply a softmax function to impart an inductive bias toward sparse vectors (Correia et al., 2019; Goyal et al., 2022), also alleviating the need to use biased or high-variance gradient estimators to train the encoder. We refer to this embedding as Simplicial Embeddings (SEM), as the softmax functions map the unnormalized representations onto Lsimplices. The procedure to induce SEM is simple, efﬁcient, and generally applicable. The SSL pre-training phase, used with SEM, learns a set of Lapproximately-sparse vectors. Key to controlling the inductive bias of SEM during pre-training is the softmax temperature parameter: the lower the temperature, the stronger the bias toward sparsity. Consistent with earlier attempts at sparse representation learning (Coates & Ng, 2011), we ﬁnd that the optimal sparsity for pre-training need not match the optimal level for downstream learning. For downstream classiﬁcation, we may discretize the learned representation by, for example, taking the argmax for each simplex. But, we can also use SEM to control the representation’s expressivity via the softmax’s temperature. We provide a theoretical bound showing that the expected error follows a trade-off between the training error and the representations’ expressivity, controlled by the softmax’s temperature used to normalize the representation for downstream classiﬁcation. Our bound also shows improved downstream generalization as we increase Land V for SEM. SEM is generally applicable to recent SSL methods. Applying it to seven different SSL methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; Caron et al., 2020; 2021; Zbontar et al., 2021; Bardes et al., 2022), we ﬁnd accuracy increases of 2% to 4% on CIFAR-100. We observe monotonic improve- ment as we increase the number of vectorsL, showing the beneﬁt of the overcomplete representations learned by SEM, while this improvement is absent when we do not use softmax normalization. When training a SSL method with SEM on ImageNet we also observe improvements on in-distribution compared to the baseline (Figure 1). We also observe improvement on out-of-distribution test sets, semi-supervised learning benchmark and transfer learning datasets, demonstrating the potential of SEM for large scale applications. Finally, we ﬁnd that SEM learns features that are closely aligned to the semantic categories in the data. This demonstrates that SEM learns disentangled and interpretable representations, as previously observed in overcomplete representations (Faruqui et al., 2015). 2 R ELATED WORK The softmax operation has been used in other contexts, notably as an architectural component for models to attend to context-dependent queries via, for example, an attention mechanism (Bahdanau et al., 2016; Vaswani et al., 2017; Correia et al., 2019; Goyal et al., 2022), a mixture of experts (Jordan & Jacobs, 1993) or memory augmented networks (Graves et al., 2014). This operation is also used for the computation of several SSL objectives such as InfoNCE (van den Oord et al., 2018; Hjelm et al., 2019), and as a normalization of the output to compute the objective in DINO and SWaV (Caron et al., 2020; 2021). Different from these, our method places the softmax at the output of an encoder to constrain the representation into a set of Lsparse vectors. Similar to our approach, other architectural constraints such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015) and LayerNorm (Ba et al., 2016) also improve the training of large neural networks. However, contrary to SEMs, they are not used to induce sparsity on the representation or control its expressivity for downstream tasks. Closer to our work, Liu et al. (2021) propose to constrain the expressivity of the representation of a neural network with a set of discrete-valued symbols obtained using a set of Vector Quantized (Oord et al., 2018) bottlenecks. Similarly, Dessi et al. (2021) propose a communication game with a discrete bottleneck. The idea of discretizing the encoder’s output is similar to using SEM vectors that are one-hot (e.g. temperature = 0) and only one symbol (e.g. L= 1,V = 2048). In our work, we ﬁnd success in removing the hard-discretization and having L> 1, which can be interepreted as combining several symbols. 3 S IMPLICIAL EMBEDDINGS Simplicial Embeddings (SEM) are representations that can be integrated easily into a contrastive learning model (Hjelm et al., 2019; Chen et al., 2020b), the BYOL method (Grill et al., 2020), and other SSL methods (Caron et al., 2020; 2021; Zbontar et al., 2021). For example, in BYOL, we insert the SEM after the encoder and before the projector and the rest is unchanged as shown in Figure 2c. 2SEM  normalization  ...  Concat  (a) 0.0 0.5 1.0 1.5 2.0 2.5 Entropy H(zi) 0 1 2 3 4 : 10.0 : 1.0 : 0.5 : 0.1 : 0.01 (b) stop-grad SSL  loss (c) Figure 2: (a) Procedure to obtain Simplicial Embeddings (SEM). A matrix z ∈RL×V contains L vectors zi ∈RV. The vectors zi are normalized with στ, the softmax operation with temperature τ. The normalized vectors are concatenated into the vector ˆz. (b) Normalized histogram of the entropies H(¯zi) of each simplex ¯zi for the sample in CIFAR’s training dataset at the end of pre-training with various τ. The peak at ln(2) for τ = 0.01 and τ = 0.1 are a large number of simplices with two elements close to 0.5. (c) Integration of SEM with BYOL (Grill et al., 2020). The encoder outputs a latent vector which is embedded into the matrix z∈RL×V and then transformed into SEM. In this ﬁgure, tand t′are augmentations deﬁned by the practitioner, ξare parameters of the target network that are updated as moving average of the parameters θof the online networks trained with SGD. So, ξare updated as follow: ξ←αξ+ (1 −α)θ, with α∈[0,1]. To produce SEM representation, the encoder’s output eis embedded into Lvectors zi ∈RV. A temperature parameter τ scales zi, and then a softmax re-normalizes each vector zi to produce ¯zi. Finally, the normalized vectors ¯zi are concatenated to produce the vector ˆz of length L·V. We illustrate SEM in Figure 2a. Formally, the re-normalization is as follows: ¯zi := στ(zi), σ τ(zi)j = ezij/τ ∑V k=1 ezik/τ , ˆz:= Concat(¯z1,..., ¯zL), ∀i∈[L],∀j ∈[V]. (1) 3.1 I NDUCTIVE BIAS TOWARDS SPARSITY DURING PRE -TRAINING In SEM, Lcontrols the numbers of simplices and V controls the dimensionality of each simplex. As such, the higher V is, the sparser the representation can be. During pre-training, the constraint induced by embedding the representation into a simplex biases each vector towards sparse vectors by creating a zero-sum competition between the components of the vector. In order for a component to increase by α, then the other elements must decrease by α, and all elements are bounded by 0. For networks to learn useful features and minimize their objective, they must prioritize some components at the expense of others. The strength of this bias is controlled via the pretraining temperature τp of the softmax, and the size of the vectors V as it was noted in the context of attention (Vaswani et al., 2017; Wang et al., 2021b). For SSL methods with a target network, the temperature for the target network can be different to the online network’s as no gradient is back-propagated through it. To visualize the effect of the temperature on SEM after pre-training, we interpret each simplex as a probability mass function p(¯zij) where, for all i∈[L], ∑V j=1 p(¯zij) = 1 and p(¯zij) ≥0 ∀j. The entropy of a simplex ¯zi, deﬁned as H(¯zi) := −∑V j=1 p(¯zij) logp(¯zij), informs whether the simplex is a sparse or a dense vector. That is, if H(¯z(x) i ) = 0 then the vector is one-hot. On the other hand, if 3H(¯z(x) i ) = ln(V) then the vector is dense and uniform. While the temperature τp is merely a scaling of the logits, it has an important control over the learned representation’s entropy and resulting SEM sparsity. We demonstrate this by learning a representation on CIFAR-100 using BYOL, and analyze the entropies of the resulting simplices. In Figure 2b, we plot the histogram of the entropies H(¯zi), for a given τp, of each simplex for each sample in the training set of CIFAR-100. We observe that even after pre-training, small temperatures (τp = 0.01) yields representations that are close to one-hot vectors while high temperatures yields vectors that are close to uniform vectors. By pre-training using a softmax, SEMs create representations that are conditioned to ﬁt onto simplices. In pre-training, we select τp for optimal inductive bias: τp too small yields vanishing gradients (Wang et al., 2021b) and τp too large yields a bias that is too weak. We may select a different optimal τd for downstream performance as discussed formally in the next subsection. 3.2 SEM IMPROVEMENT ON THE GENERALIZATION OF THE DOWNSTREAM CLASSIFIER In this subsection, we theoretically demonstrate the beneﬁt of training a downstream classiﬁer with SEM normalized input compared to a baseline classiﬁer with unnormalized input. We show that: (1) there is a trade-off between the training loss and the generalization gap, which is controlled by the value of τd (denoted τ := τd in this subsection), (2) SEM can improve the base model performance when we attain good balance in this trade-off, and (3) the improvement due to SEM is expected to increase or stay constant as Land V increase. In the remainder of this subsection, we introduce the notation and assumptions needed to understand and derive the result, then present our theoretical claim and discuss its implications. Notation. We use a training dataset S = (z(i),y(i))n i=1 of nsamples for supervised training of a classiﬁer, using the representation zextracted from the pre-trained model1 and the corresponding label y ∈Y where Yis the space of possible labels. Assume that z ∈Z = [−1,+1]L×V, which means that zis a matrix with Lrows and V columns. We denote the element of zat row iand column jas zij. Let grepresent the downstream classiﬁer. We refer to the baseline downstream model with unnormalized input as fbase, and fbase(z) = g(z). The corresponding downstream model trained with the SEM normalization is fSEM(τ)(z) = (g◦στ)(z), where στ is applied element-wise along each row of z such that στ(zij) = ezij/τ ∑V t=1 ezit/τ for j = 1,...,V . Moreover, we deﬁne fS base and fS SEM(τ) the base and the SEM normalized models obtained by ﬁtting the dataset S. Finally, let Hbe the union of the hypothesis spaces of fSEM(τ) and fbase. To compare the quality of the base model and the model with SEM normalization, we analyze the generalization gap Ez,y[l(fS(z),y)] −1 n ∑n i=1 l(fS(z(i)),y(i)) for each fS ∈{fS SEM(τ),fS base}, where l: R ×Y→ R≥0 is the per-sample loss. The key insight that we exploit for the theorem is that the softmax operation στ controls the ex- pressivity of the input’s representation to g via the temperature τ. We denote ϕfbase as an upper bound on the expressivity of zi for the baseline model fbase, and ϕfSEM(τ) as the upper bound on the expressivity of στ(zi) for the model with SEM normalization fSEM(τ). The formal deﬁnition of ϕfbase and ϕfSEM(τ) requires proof devices that will hinder the readability of this section, so we refer the reader to Appendix A for a detailed deﬁnition. Let ϕf ∈{ϕfbase ,ϕfSEM(τ) }. Intuitively, ϕfS measures the largest possible distance that two embeddings can have such that the largest component remains the same for both embeddings. We note that this measure depends only on V for fbase, and on both V and τ for fSEM(τ). We use ϕfS(V,τ ) to denote the measure given by either model and note that τ has no effect for fbase. Assumptions. We assume that the per-sample loss is bounded such thatl(f(z),y) ≤Bfor all f ∈H and for all (z,y) ∈Z×Y . For example, B = 1 for the 0-1 loss. Next, let ly be the per-sample loss given y. We assume thatly◦gare uniformly Lipschitz functions for ally∈Y and g∈GS, where GS is the set of classiﬁers greturned by the training algorithm using the dataset S. Let Rbe such a uniform Lipschitz constant. This means that |(ly ◦g)(σf(z)) −(ly ◦g)(σf(z′))|≤ R∥σf(z) −σf(z′)∥F, where ly(g◦σf(z)) = l(g◦σf(z),y), and σf = στ when f = fSEM(τ) and σf is identity when f = fbase. Finally, we assume that there exists ∆ > 0 such that for all representations z of the underlying distribution we have that for any i∈[L], if k= arg maxj∈[V] zij, then zik ≥zij + ∆ for 1In this subsection, we refer to the extracted representation as z, the embedder’s output 4any j ̸= k. Since ∆ can be arbitrarily small (e.g. as small as machine precision), this assumption typically holds in practice. We are now ready to state our theoretical claim. Theorem 1 illuminates the advantage of SEM and the effect of the hyper-parameter τ on the perfor- mance of the downstream classiﬁer. We present the proof in Appendix A and we present empirical evidence of the theorem’s prediction in Figure 5. Theorem 1. Let V ≥2. For any1 ≥δ >0, with probability at least1 −δ, the following holds for any fS ∈{fS SEM(τ),fS base}: Ez,y[l(fS(z),y)] ≤1 n n∑ i=1 l(fS(z(i)),y(i)) + R √ LϕfS(V,τ ) + c √ ln(2/δ) n , where c> 0 is a constant in(n,f, H,δ,τ,S ). Moreover, ϕfS SEM(τ) →0 as τ →0 and ϕfS SEM(τ) −ϕfS base ≤3 4(1 −V) <0 ∀τ >0. The ﬁrst statement of Theorem 1 shows that the expected loss is bounded by the three terms: the training loss 1 n ∑n i=1 l(fS(z(i)),y(i)), the second term R √ LϕfS, and the third term c √ ln(2/δ) n . Since cis a constant in (n,f, H,δ,τ,S ), the third term goes to zero as n →∞ and is the same with and without SEM. Thus, for the purpose of assessing the impact of SEM, we can focus on the second term, where a difference arises. Theorem 1 shows that R √ LϕfS goes to zero with SEM; i.e., ϕ(fS SEM(τ)) →0 as τ →0. Also, for any τ >0, the second term with SEM is strictly smaller than that without SEM as ϕfS SEM(τ) −ϕfS base ≤3 4 (1 −V) < 0 and demonstrates that the improvement due to SEM is expected to asymptotically increase as V increases. Moreover, Lis a multiplicative constant of ϕwhich shows that, as Lincreases, the improvement due to SEM is also expected to be higher. Overall, Theorem 1 shows the beneﬁt of SEM as well as the trade-off with τ. When τ →0, the second term goes to zero, but the training loss (the ﬁrst term) can increase due to underﬁtting resulting from the reduction in representation expressivity. Thus, τ should be chosen to optimally balance this trade-off. 4 E MPIRICAL ANALYSIS We empirically study the effect of SEM on the representation of SSL methods and demonstrate that SEM improves the test set accuracy on CIFAR-100 (Krizhevsky, 2009). We compare SEM with other methods for inducing sparse representations during pretraining and demonstrate that SEM lead to better downstream accuracy. On IMAGE NET (Deng et al., 2009), we study the effect of SEM on robustness, semi-supervised learning and transfer learning datasets, demonstrating consistent improvement attributed to SEM. Finally, we present evidences that features produced by SEMs are more naturally aligned with the semantic categories of the data. The code for reproducing the results is available at: https://github.com/lavoiems/simplicial-embeddings/. Training setup. For all experiments, we build off the implementation of the baseline models from the Solo-Learn library (da Costa et al., 2021). We probe the encoder’s output for the baseline methods, as typically done in the literature. For models with SEM, we probe the SEM normalized representation (i.e. ˆz). In our experiments, the embedder is a linear layer followed by BatchNorm (Ioffe & Szegedy, 2015). Unless mentioned otherwise, we use L = 5000 and V = 13 for the SEM representation. We do not perform any search for the non-SEM hyper-parameters. The SEM hyper-parameters are selected by using a validation set of 10% of the training set of CIFAR-100 and 10 samples per class Table 1: Linear probe top-1 accuracy on CIFAR-100 trained for 1000 epochswith a ResNet-18 encoder. We compare the test accuracyof several SSL models with and without SEM. Boldface indicates highest accuracy. Green rows indicate a SSL method + SEM. The mean and the standard deviation are calculated over 5 seeds SIMCLR M OCO BYOL B ARLOW -TWINS SWAV DINO V ICREG Baseline 65.8 ± 0.3 69 .3 ± 0.3 70 .7 ± 0.2 70 .7 ± 0.3 64 .6 ± 0.3 66 .8 ± 0.3 68 .5 ± 0.2 With SEM 69.5 ±0.2 71.0 ±0.3 73.9 ±0.2 73.0 ±0.2 67.7 ±0.2 69.2 ±0.3 71.4 ±0.4 5on the in distribution dataset for IMAGE NET. The test accuracy is obtained by retraining the model with all of the training data using the parameters found with the validation set. We pre-train the SSL models for 200 epochs on IMAGE NET and 1000 epochs on CIFAR-100. 4.1 SEM IMPROVES ON DOWNSTREAM CLASSIFICATION Baseline comparison. We evaluate the effect of adding SEMs in seven modern SSL approaches. We take standard SimCLR (Chen et al., 2020b), MoCo-v2 (He et al., 2020), BYOL (Grill et al., 2020) Barlow-Twins (Zbontar et al., 2021), SwA V (Caron et al., 2020), DINO (Caron et al., 2021) and VicReg (Bardes et al., 2022) models and implement SEM after the encoder. We compare our approach on CIFAR-100 with a ResNet-18 in Table 1. For every SSL methods, using SEMs improves the baseline methods by 2% to 4% demonstrating that SEM is a general approach that improves in-distribution generalization for SSL methods. 130 6501.3K 6.5K13K 65K130K Embedding size 68 70 72 74 76 78T op-1 accuracy BYOL + SEM BYOL + Embed Figure 3: Effect of the Softmax when scaling up L on the linear probe accuracy. Using a RN-50. Table 2: Comparing SEM with hard discretization using Gubel Straight- Through and Vector Quantization (V .Q.). RN-18 base on CIFAR-100. Accuracy BYOL 70.7 BYOL+Gumbel S.-T. 48.6 BYOL+V .Q. 65.6 BYOL+SEM (τd = 0) 73.2 BYOL+SEM (τd = 0.1) 73.9 Increasing the representation’s size of SEM increases the performance. We ﬁnd that increasing L(the number of sim- plices of SEM) beyond the over-complete regime increases the downstream accuracy. This increased performance is not observed when we abstain from using the softmax normal- ization of SEM. In Figure 3, using a ResNet-50 encoder, we compare BYOL + SEM, with an identical model without the Softmax normalization which we call BYOL + Embed. As this is a control experiment, the extracted representation of BYOL + Embed is the embedder’s outputzθ. We ﬁx V = 13 and scale L ∈[10,10000] to get a range of representation sizes. The mean and standard deviation over 5 seeds is plotted. This experiment demonstrates that SEM offers a simple way to scale up the capacity of the model and that the softmax normalization is necessary to attein increase performance. Comparison of SEM with hard discretization approaches. Several other methods can be used to induce a sparse and over- complete representation during pre-training and downstream classiﬁcation. For example, we may sample Ldiscrete one-hot codes of V dimensions using Gumbel Softmax (Jang et al., 2017) as done in Dessì et al. (2021). We can also use Vector Quantization (VQ) (Oord et al., 2018) and consider Llatent embedding spaces with V embedding vectors each, wherein the vectors are in Rd. In contrast to SEM, it is not possible to propagate the gradient through the bottleneck trivially and VQ uses straight-through estimation in the embedding space to back- propagate the gradient to the encoder. Here, we observe that these alternative approaches exhibit a considerable decrease in performance in comparison to the baseline as demonstrated in Table 2. In this table, we reproduce the same setup as SEM but we replace the Softmax with hard discretization baselines methods. For discretization with Gumbel Straight-Through estimation, we use the same setup as SEM with L= 5000 and V = 13, that is 5000 one-hot vectors of 13 dimensions and τ = 22. For VQ, we found that L = 512 and V = 128 led to the best performance. That is, we have 512 latent embedding spaces, each with 128 possible embedding vectors that are in R32. We note that while we have not found hard-discretization to be successful during pre-training, we may hard-discretize a SEM representation for downstream task. In Table 2, we also present SEM with τDS = 0, which correspond to using the discretized representation for downstream classiﬁcation. We obtain the discrete representation by taking the argmax for each simplex. This result demonstrating that SEM with pre-training can be used to learn meaningful discrete codes for downstream applications and yields better performance than the baselines, implying that pre-training with SEM could be be used in applications that require discretization. Memory and computational efﬁciency of SEM. SEM’s performance improvements come at a cost of increased memory allocation (VRAM) due to additional parameters needed to perform the matrix multiplication, and slightly more computation (FLOPs/sample). For very large over-complete 2A hyper-parameter search was performed to select the best performing hyper-parameter. 6representation the increased memory requirement can impede practical application. We propose a more efﬁcient version of SEM by sparsifying the matrix multiplication of the embedder and of the projector and detail this procedure in Appendix D.1. As shown in Table 17, SEM with sparse matrix multiplication use only slightly more memory and compute but outperforms the BYOL baseline on CIFAR-100 though underperforming the regular SEM. We also note that SEM’s memory cost becomes relatively minor as we scale up the encoder. As well, the computational cost of SEM is small compared to the total cost of pre-training and achieves higher accuracy using fewer FLOPs compared to scaling the encoder as shown in Figure 1. 4.2 A NALYZING THE PARAMETERS OF SEM We present two ﬁgures in this section to better understand the effect of the parameters of SEM on the downstream accuracy. In Figure 4, we evaluate the effect of changingτp and τd on the downstream accuracy. In Figure 5, we evaluate the effect of L and V on the downstream accuracy and also contrast fbase and fSEM(τ = 1), allowing us to conﬁrm two predictions made in Section 3.2: the expected generalization improvement from SEM increases as we increase Land as we increase V. Now, we discuss the effect of each of SEM’s parameter on the resulting downstream classiﬁcation. Increasing V yields a steep performance increase for smallV but quickly plateau. In Figure 5b, we observe a steep increase of the accuracy for V < 13 followed by a plateau for V > 13. In Figure 4a, we observe that the optimal accuracy obtained for V = 1024 and L= 64 is similar to the one obtained for L= 50 (Embedding size=650) in Figure 3. Increasing Lyields monotonical improvement for downstream classiﬁcation. In the regime that we can test it, increasingLlead to consistent improvement on the downstream accuracy as observed in Figure 3 and Figure 5a. Using SEM in pre-training only is not enough and using it in the downstream classiﬁer is necessary for the improved performance as demonstrated in Figure 5a. The optimal τp depends on V. As previously noted in the context of Attention (Vaswani et al., 2017; Wang et al., 2021a), the optimal attention’s temperature is proportional to attention’s vector size. We also observe this in SEM. As presented in Figure 4a, the optimal τp for larger V is higher. Models with larger Lare more robust to smaller τd. In Figure 4, we observe that SSL models are more robust to smaller τd as Lincrease. We speculate that the information can be scattered across the simplices for large L, allowing to reduce the expressivity of each vector with minimal impact on the downstream accuracy. 0.01 0.1 0.5 1 2 5 10 Pretrain tau 65 68 71 74 77Accuracy L=5000, V=13 L=64,V=1024 (a) 0.01 0.1 0.5 1 5 10 Downstream tau 65 68 71 74 77Accuracy L=100 L=1000 L=5000 (b) Figure 4: Effect of τp and τd on a RN-50. 50 100 5001000 5000 L 62 64 66 68 70 72 74Accuracy fbase fSEM( = 1) (a) 2 5 8 13 34 V 62 64 66 68 70 72 74Accuracy fbase fSEM( = 1)  (b) Figure 5: Comparing fSEM and fbase on a RN-18. 4.3 SEM IMPROVEMENT ON LARGE -SCALE DATASETS WITH IMAGE NET Figure 1 in the introduction demonstrates that using SEM leads to better in distribution generalization for IMAGE NET and is a more efﬁcient method of scaling up the model as compared to scaling up the width of the ResNet-50 encoder. Here, we demonstrate that SEM generally improves the accuracy on several robustness test sets, a semi-supervised learning benchmark and transfer learning datasets. We use BYOL +SEM with an embedding size of 105 000 features ( L= 5000 and V = 21) for these experiments. The embedding is pre-trained for 200 epochs using the BYOL SSL procedure. Robustness to out-of-distribution test sets. We perform a comparative study using several test sets: (IN) the in-distribution test set provided inIMAGE NET; (IN-C) IMAGE NET-C, which exhibits a set of common image corruptions (Hendrycks & Dietterich, 2019); (IN-R) IMAGE NET-R (Hendrycks et al., 2021) which consists of different renderings for severalIMAGE NET classes; and (IN-V2) IMAGE NET- 7Table 3: Robustness via linear probe top- 1 test accuracies on IMAGE NET variant datasets, using representations pre-trained for 200 epochs. * Taken from (Chen & He, 2020) IN IN-V2 IN-R IN-C IN-A BYOL* 70.6 - - - - BYOL 71.9 59 .2 18 .8 39.5 1.65 BYOL+SEM 74.1 61.2 22.1 43.4 2.53 Table 4: Top-1 transfer learning accuracy from IMA- GENET pre-trained representation. FOOD 101 C10 C100 S UN DTD F LOWER Linear probe: BYOL 74.2 91 .8 74.9 60.9 72.2 88.9 BYOL+SEM 74.7 93.5 78.6 62.1 71.9 91.5 Fine-tuned: BYOL 83.1 97 .2 83.6 59.1 69.2 85 .4 BYOL+SEM 84.7 97.2 85.6 63.3 71.3 91.7 V2 (Recht et al., 2019), a distinct test set for IMAGE NET collected using the same process; (IN-A) IMAGENET -A (Chen et al., 2020a) contains a set of samples that are miclassiﬁer by a IMAGE NET ResNet-50 classiﬁer. We use the methodology and software proposed in Djolonga et al. (2020; 2021) to perform our experiments. We observe that BYOL + SEM outperforms BYOL on every robustness datasets probed, demonstrating that SEM also improves generalization to out-of-distribution test sets. Transfer learning. We probe the effectiveness of SEM in BYOL and MoCo when transferring representations trained on IMAGE NET to other classiﬁcation tasks. We follow the linear evaluation and ﬁne-tuning methodologies described in previous works (Grill et al., 2020; Lee et al., 2021), which entails training a linear classiﬁer with logistic regression using sklearn (Pedregosa et al., 2011) on the embeddings of the samples and ﬁne-tuning the encoder respectively. To avoid out-of-memory issues that may occur in the linear probe experiment with the sklearn solver when the number of features, we discretize our features and use sparse matrix to ﬁt the logistic regression. This is equivalent to forcing τd = 0 for all the experiments. For the ﬁne-tuning experiments, we ﬁx τd = 1 since the evaluation method allows for mini-batch gradient descent. We perform our transfer learning experiments on the following datasets: FOOD (Bossard et al., 2014), CIFAR-10 (C-10 ) (Krizhevsky, 2009), CIFAR-100 (C-100 ) (Krizhevsky, 2009), SUN (Xiao et al., 2010), DTD (Cimpoi et al., 2014) and FLOWER (Nilsback & Zisserman, 2008). This task evaluates the generality of the encoder as it has to encode samples from various out-of- distribution domains with categories that it may not have seen during training. We present our results in Table 4 and observe that SEM improves the transfer accuracy over the baseline for every datasets but DTD for the linear probe experiment. For DTD, we hypothesize that the drop in performance is due to the fact that we use a temperature that is too small. Since this is a texture dataset with higher frequency, it might be the case that we need more expressivity to correctly ﬁt the data. We support the conjecture with the ﬁne-tuning experiment where BYOL + SEM out-performs the baseline. Table 5: Semi-supervised learning accu- racy by ﬁne-tuning on IMAGE NET. Top-1 Top-5 1% 10% 1% 10% BYOL 51.6 67 .5 78.0 88 .9 BYOL+SEM 56.7 69.9 81.0 90.0 Semi-supervised learning. We evaluate the effect of us- ing SEM when ﬁne-tuning on a classiﬁcation task with a small subset of IMAGE NET’s training set. We follow the semi-supervised learning procedure of Chen et al. (2020b); Grill et al. (2020) and use the same ﬁxed splits of 1% and 10% of ImageNet labelled training set. In Table 5, we demonstrate that using SEM lead to an important increased performance, especially in the low supervised data regime. 4.4 S EMANTIC COHERENCE OF SEM FEATURES Here we demonstrate that SEM features are coherently aligned with the semantics present in the training data. Qualitatively, we visualize the most predictive features of a downstream linear classiﬁer trained on CIFAR-100 and see that the classes with similar predictive features are semantically related. Quantitatively we propose a metric that returns the ratio of features mostly predictive for a classes that are in the same super class to total number of class predictive for this feature. For both our analysis, we use a linear classiﬁer trained on the features extracted from BYOL with and without SEM. Consider the trained linear classiﬁer with a weight matrix W ∈RN×C, with N features, and Cclasses. By preserving the top Kparameters of the weight matrix W for each class and pruning the features predictive for only one class, we create a bipartite graph between two set of nodes: the CIFAR-100 classes and the features of the representation. We denote this graph WK. 8tulip rose orchid poppy plate cup bowlbottlecan manbabywomanboyflatfishgirl whaleturtledolphinray shark (a) BYOL + SEM tablefox chimpanzeemountaincattlewardrobeplatebabyworm caterpillartelevision tigerleopardbridgecastleplainraccooncloudmanbedcouchtelephonecamelseaskunk mushroom wolforangechair skyscraperbutterfly (b) BYOL 0 10 20 30 40 50 T op K features 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Semantic coherence BYOL BYOL + embed large BYOL + SEM BYOL + ICA (c) Figure 6: Semantic coherence of the features. (a) and (b) Subset of W5, the bipartite graph of the most 5 highest magnitude features on BYOL + SEM features (a) and BYOL on the encoded features (b). (c) Coherence of the top K features to the semantics of the super-class of the categories of CIFAR-100 . It is taken as the number of pairwise categories in the same super-class for which a feature is among its top Kmost predictive features over the total number of pairwise categories. The qualitative analysis is given by plotting the subset W5, obtained by taking the top 5 features for each class. We present a subset of the graph for BYOL+SEM in Figure 6a and for BYOL in Figure 6b. The full graphs are presented in the Appendix. In the SEM plot, a set of connected components emerge, and the connected components of the graph are semantically related. For example, the ﬁrst set of connected components are ﬂowers, and the last set of connected components are aquatic mammals.The same class coherence is not observed with either the BYOL baseline or with BYOL augmented with a large representation. In particular, we do not see a small number of semantically related connected components. Instead, we see a large fully connected graphs. Next, we describe how we quantitatively measure the semantic coherence of the features. Notice that two classes share a common predictive feature on WK if they are 2-neighbour. Let N(ci) returns all pairs (ci,cj) for all j 2-neighbour of ci. Moreover, deﬁne the operation is_super(ci,cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass and 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 21 in the Appendix. We measure semantic coherence as follows: Coherence(WK) := 1 C C∑ i=1 ∑ (ci,cj)∈N(ci) is_super(ci,cj) |N(ci)| , (2) where C = 100 for CIFAR-100 and |·| is the cardinality of a set. We compare the semantic coherence ofBYOL +SEM with the control experiments onBYOL : regular BYOL , BYOL with an embedding of the same size as BYOL +SEM but without the normalization and BYOL to which we applied linear ICA (Hyvärinen & Oja, 2000) in an attempt to disentangle the features. In Figure 9, we plot the full graph W5 for BYOL+SEM and the baselines. We observe that using the SEM yields semantically coherent features for all the classes of CIFAR-100 . This observation is consistent with the qualitative and quantitative experiments presented earlier and demonstrates that SEM’s inductive bias during pre-training leads to features that are semantically coherent with the semantic categories extant in the data. This arguably have important implications for improving the interpretability of SSL representations. 5 C ONCLUSION SEM is a simple, drop-in module that creates sparse overcomplete representations for standard SSL methods using a softmax operation. This simple modiﬁcation leads to improved generalization on downstream classiﬁcation across several state-of-the-art SSL methods. Furthermore, SEM improves 9performance on out-of-distribution, semi-supervised, and transfer learning tasks across the board and also scales with encoder size. By analyzing semantic coherence, we ﬁnd that SEMs naturally disentangle data into semantic categories without any explicit training objectives. We hope this work motivates the investigation of representational inductive biases for SSL, in addition to models or different training procedures. ACKNOWLEDGEMENTS The authors are grateful for the insightful discussions with Xavier Bouthillier, Hattie Zhou, Sébastien Lachapelle, Tristan Deleu, Yuchen Lu, Eeshan Dhekane, Maude Lizaire, Julien Roy and David Dobre. We acknowledge funding support from Samsung and Hitachi, as well as support from Aaron Courville’s CIFAR CCAI chair. We also wish to acknowledge Mila and Compute Canada for providing the computing infrastructure that enabled this project. Finally, this project would not have been possible without the contribution of the following open source projects: Pytorch (Paszke et al., 2019), Orion (Bouthillier et al., 2022), Solo-Learn (da Costa et al., 2021), Scikit-Learn (Pedregosa et al., 2011), and Numpy (Harris et al., 2020). REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https://arxiv.org/abs/1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv. org/abs/1409.0473. arXiv: 1409.0473. Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=xm6YD62D1Ub. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative compo- nents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision – ECCV 2014, pp. 446–461, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10599-4. Xavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong, Pierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael Noukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk, Steven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal Lamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 9912–9924. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 70feb62b69f16e0238f741fab228fec2-Paper.pdf. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. arXiv:2104.14294 [cs], May 2021. URL http://arxiv.org/abs/2104.14294. arXiv: 2104.14294. Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial robustness: From self-supervised pre-training to ﬁne-tuning. In CVPR 2020, June 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020b. 10Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De- scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606–3613, 2014. Adam Coates and Andrew Y . Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, pp. 921–928, 2011. URL https://icml.cc/2011/papers/ 485_icmlpaper.pdf. Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174–2184, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1223. URL https://aclanthology.org/D19-1223. Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library of self-supervised methods for visual representation learning, 2021. URL https://github. com/vturrisi/solo-learn. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Roberto Dessì, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from scratch(with a generic visual processor emerging on the side). CoRR, abs/2106.04258, 2021. URL https://arxiv.org/abs/2106.04258. Roberto Dessi, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from scratch (with a generic visual processor emerging on the side). In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=1AvtkM4H-y7. Josip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijnders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/ google-research/robustness_metrics. Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Syl- vain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil- ity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http: //arxiv.org/abs/2007.08558. arXiv: 2007.08558. D.L. Donoho, M. Elad, and V .N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006. doi: 10.1109/TIT.2005.860430. Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. Sparse over- complete word vector representations. In Proceedings of the 53rd Annual Meeting of the As- sociation for Computational Linguistics and the 7th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pp. 1491–1500, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1144. URL https: //aclanthology.org/P15-1144. Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. A compositional and interpretable semantic space. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 32–41, Denver, Colorado, May–June 2015. Association for Computational Linguistics. doi: 10.3115/v1/N15-1004. URL https://aclanthology.org/N15-1004. 11Ian J. Goodfellow, Aaron Courville, and Yoshua Bengio. Large-scale feature learning with spike-and- slab sparse coding. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML’12, pp. 1387–1394, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851. Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Ra- haman, Jonathan Binas, Charles Blundell, Michael Curtis Mozer, and Yoshua Bengio. Coordination among neural modules through a shared global workspace. InInternational Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=XzTtHjgPDsT. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs], December 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401. Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, pp. 399–406, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf. Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2 . Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http: //arxiv.org/abs/1911.05722. arXiv: 1911.05722. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generaliza- tion. arXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv: 2006.16241. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bklr3j0cKX. Aapo Hyvärinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural Networks, 13:411–430, 2000. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings. mlr.press/v37/ioffe15.html. 12Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=rkE3y85ee. Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=YevsQ05DEN7. M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pp. 1339–1344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791. Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa- tion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005. Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https: //www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf . Honglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area v2. In J. Platt, D. Koller, Y . Singer, and S. Roweis (eds.),Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URLhttps://proceedings.neurips. cc/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf. Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive Visual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv. org/abs/2109.12909. arXiv: 2109.12909. Michael S. Lewicki and Terrence J. Sejnowski. Learning Overcomplete Representations. Neural Computation, 12(2):337–365, 02 2000. ISSN 0899-7667. doi: 10.1162/089976600300015826. URL https://doi.org/10.1162/089976600300015826. Dianbo Liu, Alex M Lamb, Kenji Kawaguchi, Anirudh Goyal ALIAS PARTH GOYAL, Chen Sun, Michael C Mozer, and Yoshua Bengio. Discrete-valued neural communication. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad- vances in Neural Information Processing Systems, volume 34, pp. 2109–2121. Curran Asso- ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 10907813b97e249163587e6246612e21-Paper.pdf. Brian Murphy, Partha Pratim Talukdar, and Tom Michael Mitchell. Learning effective and inter- pretable semantic models using non-negative sparse embedding. In COLING, 2012. Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pp. 722–729, 2008. doi: 10.1109/ICVGIP.2008.47. B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, June 1996. Bruno A. Olshausen. Highly overcomplete sparse coding. In Bernice E. Rogowitz, Thrasyvoulos N. Pappas, and Huib de Ridder (eds.), Human Vision and Electronic Imaging XVIII, volume 8651 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, pp. 86510S, March 2013. doi: 10.1117/12.2013504. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. arXiv:1711.00937 [cs], May 2018. URL http://arxiv.org/abs/1711.00937. arXiv: 1711.00937. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 138024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers Generalize to ImageNet? In Proceedings of the 36th International Conference on Machine Learning, pp. 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/ v97/recht19a.html. ISSN: 2640-3498. Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio Representations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/ abs/2010.10915. arXiv: 2010.10915. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html. Yee Whye Teh, Max Welling, Simon Osindero, and Geoffrey E. Hinton. Energy-based models for sparse overcomplete representations. J. Mach. Learn. Res., 4(null):1235–1260, dec 2003. ISSN 1532-4435. Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748. Aad W. van der Vaart and Jon A. Wellner.Weak Convergence and Empirical Processes. Springer New York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007% 2F978-1-4757-2545-2 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017. URL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5. Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to Unseen Domains: A Survey on Domain Generalization. arXiv:2103.03097 [cs], December 2021a. URL http://arxiv.org/abs/2103.03097. arXiv: 2103.03097. Shulun Wang, Bin Liu, and Feng Liu. Escaping the gradient vanishing: Periodic alternatives of softmax in attention mechanism, 2021b. URL https://arxiv.org/abs/2108.07153. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485–3492. IEEE, 2010. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv. org/abs/2010.13902. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. 14A P ROOF OF THEOREM 1 Let us introduce additional notations used in the proofs. Deﬁne r= (z,y) ∈R, ℓ(f,r) = l(f(z),y), ˜Cy,k1,...,kL = {(z,ˆy) ∈Z×Y : ˆy= y,kj = arg max t∈[V] zj,t ∀j ∈[L]}, and ˜Zk1,...,kL = {z∈Z : kj = arg max t∈[V] zj,t ∀j ∈[L]}. We then deﬁne Ck to be the ﬂatten version of ˜Cy,k1,...,kL; i.e., {Ck}K k=1 = {˜Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V] with C1 = ˜C1,1,...,1, C2 = ˜C2,1,...,1, C|Y| = ˜C|Y|,1,...,1, C|Y|+1 = ˜C1,2,1,...,1, C2|Y|= ˜C|Y|,2,1,...,1, and so on. Similarly, deﬁne Zk to be the ﬂatten version of ˜Zk1,...,kL. We also use Qi = {q ∈[−1,+1]V : i = arg maxj∈[V] qj},Ik := IS k := {i ∈[n] : ri ∈Ck}, and αk(h) := Er[ℓ(h,r)|r ∈Ck].Moreover, we deﬁne ϕ(fS base) = supi∈[V] supq,q′∈Qi ∥q−q′∥2 2, and ϕ(fS SEM(τ)) = sup i∈[V] supq,q′∈Qi ∥στ(q) −στ(q′)∥2 2 where στ(q)j = eqj/τ ∑V t=1 eqt/τ for j = 1,...,V . We ﬁrst decompose the generalization gap into two terms using the following lemma: Lemma 1. For anyδ >0, with probability at least1 −δ,the following holds for allh∈H: Er[ℓ(h,r)] −1 n n∑ i=1 ℓ(h,ri) ≤1 n K∑ k=1 |Ik|  αk(h) − 1 |Ik| ∑ i∈Ik ℓ(h,ri)  + c √ ln(2/δ) n . Proof. We ﬁrst write the expected error as the sum of the conditional expected error: Er[ℓ(h,r)] = K∑ k=1 Er[ℓ(h,r)|r∈Ck] Pr(r ∈Ck) = K∑ k=1 Erk[ℓ(h,rk)] Pr(r ∈Ck), where rk is the random variable for the conditional with r ∈Ck. Using this, we decompose the generalization error into two terms: Er[ℓ(h,r)] −1 n n∑ i=1 ℓ(h,ri) (3) = K∑ k=1 Erk[ℓ(h,rk)] ( Pr(r ∈Ck) −|Ik| n ) +   K∑ k=1 Erk[ℓ(h,rk)]|Ik| n −1 n n∑ i=1 ℓ(h,ri)  . The second term in the right-hand side of (3) is further simpliﬁed by using 1 n n∑ i=1 ℓ(h,ri) = 1 n K∑ k=1 ∑ i∈Ik ℓ(h,ri), as K∑ k=1 Erk[ℓ(h,rk)]|Ik| n −1 n n∑ i=1 ℓ(h,ri) = 1 n K∑ k=1 |Ik|  Erk[ℓ(h,rk)] − 1 |Ik| ∑ i∈Ik ℓ(h,ri)   Substituting these into equation (3) yields Er[ℓ(h,r)] −1 n n∑ i=1 ℓ(h,ri) (4) = K∑ k=1 Erk[ℓ(h,rk)] ( Pr(r ∈Ck) −|Ik| n ) + 1 n K∑ k=1 |Ik|  Erk[ℓ(h,rk)] − 1 |Ik| ∑ i∈Ik ℓ(h,ri)   ≤B K∑ k=1 ⏐⏐⏐⏐Pr(r ∈Ck) −|Ik| n ⏐⏐⏐⏐+ 1 n K∑ k=1 |Ik|  Erk[ℓ(h,rk)] − 1 |Ik| ∑ i∈Ik ℓ(h,ri)   15By using the Bretagnolle-Huber-Carol inequality (van der Vaart & Wellner, 1996, A6.6 Proposition), we have that for any δ >0, with probability at least 1 −δ, K∑ k=1 ⏐⏐⏐⏐Pr(r ∈Ck) −|Ik| n ⏐⏐⏐⏐≤ √ 2Kln(2/δ) n . (5) Here, notice that the term of ∑K k=1 ⏐⏐⏐Pr(r ∈Ck) −|Ik| n ⏐⏐⏐does not depend on h ∈H. Moreover, note that for any (f,h,M ) such that M >0 and B ≥0 for all X, we have that P(f(X) ≥M) ≥ P(f(X) >M ) ≥P(Bf(X) + h(X) >BM + h(X)),where the probability is with respect to the randomness of X. Thus, by combining (4) and (5), we have that for any h∈H, for any δ >0, with probability at least 1 −δ, the following holds for all h∈H, Er[ℓ(h,r)] −1 n n∑ i=1 ℓ(h,ri) ≤1 n K∑ k=1 |Ik|  αk(h) − 1 |Ik| ∑ i∈Ik ℓ(h,ri)  + c √ ln(2/δ) n . In particular, the ﬁrst term from the previous lemma will be bounded with the following lemma: Lemma 2. For anyf ∈{fS SEM(τ),fS base}, 1 n K∑ k=1 |Ik|  αk(f) − 1 |Ik| ∑ i∈Ik ℓ(f,ri)  ≤R √ Lϕ(f). Proof. By using the triangle inequality, 1 n K∑ k=1 |Ik|  Er[ℓ(f,r)|r∈Ck] − 1 |Ik| ∑ i∈Ik ℓ(f,ri)   ≤1 n K∑ k=1 |Ik| ⏐⏐⏐⏐⏐⏐ Er[ℓ(f,r)|r∈Ck] − 1 |Ik| ∑ i∈Ik ℓ(f,ri) ⏐⏐⏐⏐⏐⏐ . Furthermore, by using the triangle inequality,⏐⏐⏐⏐⏐⏐ Er[ℓ(f,r)|r∈Ck] − 1 |Ik| ∑ i∈Ik ℓ(f,ri) ⏐⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐⏐ 1 |Ik| ∑ i∈Ik Er[ℓ(f,r)|r∈Ck] − 1 |Ik| ∑ i∈Ik ℓ(f,ri) ⏐⏐⏐⏐⏐⏐ ≤ 1 |Ik| ∑ i∈Ik ⏐⏐Er[ℓ(f,r)|r∈Ck] −ℓ(f,ri) ⏐⏐ ≤ sup r,r′∈Ck ⏐⏐ℓ(f,r) −ℓ(f,r′) ⏐⏐. If f = fS SEM(τ) = gS SEM(τ) ◦στ, since gS SEM(τ) ∈GS, by using the Lipschitz continuity, boundedness, and non-negativity, sup r,r′∈Ck ⏐⏐ℓ(f,r) −ℓ(f,r′) ⏐⏐= sup y∈Y sup z,z′∈Zk |(ly ◦gS SEM(τ))(στ(z)) −(ly ◦gS SEM(τ))(στ(z′))| ≤R sup z,z′∈Zk ∥στ(z) −στ(z′)∥F = R sup z,z′∈Zk √ L∑ t=1 V∑ j=1 (στ(zt,j) −στ(z′ t,j))2 2 ≤R √ L∑ t=1 sup i∈[V] sup q,q′∈Qi ∥στ(q) −στ(q′)∥2 2 = R √ Lϕ(fS SEM(τ)) 16Similarly, if f = fS base = gS base, since gS base ∈GS, by using the Lipschitz continuity, boundedness, and non-negativity, sup r,r′∈Ck ⏐⏐ℓ(f,r) −ℓ(f,r′) ⏐⏐= sup y∈Y sup z,z′∈Zk |(ly ◦gS base)(z) −(ly ◦gS base)(z′)| ≤R sup z,z′∈Zk ∥z−z′∥F ≤R √ Lϕ(fS base). Therefore, for any f ∈{fS SEM(τ),fS base}, 1 n K∑ k=1 |Ik|  αk(f) − 1 |Ik| ∑ i∈Ik ℓ(f,ri)  ≤1 n K∑ k=1 |Ik|R √ Lϕ(f) = R √ Lϕ(f). Combining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap: Lemma 3. For any δ > 0, with probability at least1 −δ, the following holds for anyf ∈ {fS SEM(τ),fS base}: Er[ℓ(f,r)] −1 n n∑ i=1 ℓ(f,ri) ≤R √ Lϕ(f) + c √ ln(2/δ) n . Proof. This follows directly from combining Lemma 1 and Lemma 2. We now provide an upper bound on ϕ(fS SEM(τ)) in the following lemma: Lemma 4. For anyτ >0, ϕ(fS SEM(τ)) ≤ ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐ 2 + (V −1) ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐ 2 . Proof. Recall the deﬁnition: ϕ(fS SEM(τ)) = sup i∈[V] sup q,q′∈Qi ∥στ(q) −στ(q′)∥2 2. where στ(q)j = eqj/τ ∑V t=1 eqt/τ , for j = 1,...,V . By the symmetry and independence over i∈[V] inside of the ﬁrst supremum, we have ϕ(fS SEM(τ)) = sup q,q′∈Q1 ∥στ(q) −στ(q′)∥2 2. For any q,q′∈Q1 and i∈{2,...,V }(with q= (q1,...,q V) and q′= (q′ 1,...,q ′ V)), there exists δi,δ′ i >0 such that qi = q1 −δi and q′ i = q′ 1 −δ′ i. Here, since zik −∆ ≥zij from the assumption, we have that for all i∈{2,...,V }, δi,δ′ i ≥∆ >0. 17Thus, we can rewrite V∑ t=1 eqt/τ = eq1/τ + V∑ i=2 e(q1−δi)/τ = eq1/τ + eq1/τ V∑ i=2 e−δi/τ = eq1/τ  1 + V∑ i=2 e−δi/τ   Similarly, V∑ t=1 eq′ t/τ = eq′ 1/τ  1 + V∑ i=2 e−δ′ i/τ  . Using these, στ(q)1 = eq1/τ ∑V t=1 eqt/τ = eq1/τ eq1/τ ( 1 + ∑V i=2 e−δi/τ ) = 1 1 + ∑V i=2 e−δi/τ and for all j ∈{2,...,V }, στ(q)j = eqj/τ ∑V t=1 eqt/τ = e(q1−δj)/τ eq1/τ ( 1 + ∑V i=2 e−δi/τ ) = e−δj/τ 1 + ∑V i=2 e−δi/τ = 1 1 + eδj/τ + ∑V i∈Ij e(δj−δi)/τ where Ij := {2,...,V }\\{j}. Similarly, στ(q′)1 = 1 1 + ∑V i=2 e−δ′ i/τ, and for all j ∈{2,...,V }, στ(q′)j = 1 1 + eδ′ j/τ + ∑V i∈Ij e(δ′ j−δ′ i)/τ. Using these, for any q,q′∈Q1, |στ(q)1 −στ(q′)1|= ⏐⏐⏐⏐⏐ 1 1 + ∑V i=2 e−δi/τ − 1 1 + ∑V i=2 e−δ′ i/τ ⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐ 1 1 + ∑V i=2 e−2/τ − 1 1 + ∑V i=2 e−∆/τ ⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐, 18and for all j ∈{2,...,V }, |στ(q)j −στ(q′)j|= ⏐⏐⏐⏐⏐⏐ 1 1 + eδj/τ + ∑V i∈Ij e(δj−δi)/τ − 1 1 + eδ′ j/τ + ∑V i∈Ij e(δ′ j−δ′ i)/τ ⏐⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐⏐ 1 1 + e∆/τ + ∑V i∈Ij e(∆−2)/τ − 1 1 + e2/τ + ∑V i∈Ij e(2−∆)/τ ⏐⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐ 1 1 + e∆/τ + (V −2)e(∆−2)/τ − 1 1 + e2/τ + (V −2)e(2−∆)/τ ⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐. By combining these, sup q,q′∈Q1 ∥στ(q) −στ(q′)∥2 2 = sup q,q′∈Q1 V∑ j=1 |στ(q)j −στ(q′)j|2 ≤ ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐ 2 + (V −1) ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐ 2 . Using the previous lemma, we will conclude the asymptotic behavior of ϕ(fS SEM(τ)) in the following lemma: Lemma 5. It holds that ϕ(fS SEM(τ)) →0 as τ →0. Proof. Using Lemma 4, lim τ→0 ϕ(fS SEM(τ)) ≤lim τ→0 ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐ 2 + n(V −1) lim τ→0 ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐ 2 . Moreover, lim τ→0 ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐ 2 = ⏐⏐⏐⏐ 1 1 −1 1 ⏐⏐⏐⏐ 2 = 0, and lim τ→0 ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐ 2 = |0 −0|2 = 0. Therefore, lim τ→0 ϕ(fS SEM(τ)) ≤0. Since ϕ(fS SEM(τ)) ≥0, this implies the statement of this lemma. 19As we have analyzed ϕ(fS SEM(τ)) in the previous two lemmas, we are now ready to compare ϕ(fS SEM(τ)) and ϕ(fS base), which is done in the following lemma: Lemma 6. For anyτ >0, ϕ(fS SEM(τ)) −ϕ(fS base) ≤3 4(1 −V) <0. Proof. From Lemma 4, for any τ >0, ϕ(fS SEM(τ)) ≤ ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1)e−∆/τ ⏐⏐⏐⏐⏐ 2 + n(V −1) ⏐⏐⏐⏐⏐ 1 1 + e∆/τ(1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)e−∆/τ) ⏐⏐⏐⏐⏐ 2 ≤ ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ − 1 1 + (V −1) ⏐⏐⏐⏐⏐ 2 + (V −1) ⏐⏐⏐⏐⏐ 1 1 + (1 + (V −2)e−2/τ) − 1 1 + e2/τ(1 + (V −2)) ⏐⏐⏐⏐⏐ 2 = ⏐⏐⏐⏐⏐ 1 1 + (V −1)e−2/τ −1 V ⏐⏐⏐⏐⏐ 2 + (V −1) ⏐⏐⏐⏐⏐ 1 2 + (V −2)e−2/τ − 1 1 + e2/τ(V −1) ⏐⏐⏐⏐⏐ 2 ≤ ⏐⏐⏐⏐ 1 1 −1 V ⏐⏐⏐⏐ 2 + (V −1) ⏐⏐⏐⏐ 1 2 −0 ⏐⏐⏐⏐ 2 = (1 1 −1 V )2 + (V −1)1 4. Recall the deﬁnition of ϕ(fS base) = sup i∈[V] sup q,q′∈Qi ∥q−q′∥2 2. By choosing an element in the set over which the supremum is taken, for any δ≥∆ >0, ϕ(fS base) ≥ sup q,q′∈Q1 ∥q−q′∥2 2 ≥∥ˆq−ˆq′∥2 2 = V∑ j=1 (ˆqj −ˆq′ j)2 2 = (2 −δ)2V, where ˆq1 = 1, ˆqj = 1 −δfor j ∈{2,...,V }, ˆq′ 1 = δ−1, and ˆq′ j = −1 for j ∈{2,...,V }. By combining those, for for any τ >0 and δ≥∆ >0, ϕ(fS SEM(τ)) −ϕ(fS base) ≤ (1 1 −1 V )2 + (V −1)1 4 −(2 −δ)2V ≤1 + 1 4V −1 4 −(2 −δ)2V = 3 4 + 1 4V −(2 −δ)2V = 3 4 −V ( (2 −δ)2 −1 4 ) ≤3 4 −V ( 1 −1 4 ) = 3 4(1 −V) 20We combine the lemmas above to prove Theorem 1, which is restated below with its proof: Theorem 1. Let V ≥2. For any1 ≥δ >0, with probability at least1 −δ, the following holds for any fS ∈{fS SEM(τ),fS base}: Ez,y[l(fS(z),y)] ≤1 n n∑ i=1 l(fS(z(i)),y(i)) + R √ LϕfS(V,τ ) + c √ ln(2/δ) n , where c> 0 is a constant in(n,f, H,δ,τ,S ). Moreover, ϕfS SEM(τ) →0 as τ →0 and ϕfS SEM(τ) −ϕfS base ≤3 4(1 −V) <0 ∀τ >0. Proof. The ﬁrst statement directly follows from Lemma 3. The second statement is proven by Lemma 5 and Lemma 6. B E XPERIMENT DETAILS FOR IMAGE NET B.1 I MAGE AUGMENTATION The augmentation applied in order during training are: • Random Resize crop to a 224 ×224 image. A random patch of the image is selected and resized to a 224 ×224 image. • Random color jitter. Modifying the brightness, the contrast, the saturation and the hue. • Random gray scale. Randomly applying a gray scale ﬁlter to the image • Random Gaussian blur. Randomly applying a Gaussian bluer ﬁlter. • Random solarization. Randomly applying a solarization ﬁlter. The parameters of the augmentations are presented in Table 16. At validation and test time, we resize the images to 256 ×256 and then center crop a patch of 224 ×224. For both training and evaluation, we re-normalize the image using the statistic of the training set.g B.2 L INEAR EVALUATION We follow the evaluation protocol from (Chen et al., 2020b). The linear evaluation is done by training a linear classiﬁer on the frozen representation of the ImageNet training samples. We train a linear classiﬁer with a cross-entropy objective for 100 epochs using SGD with nesterov, a momentum of 0.9 and a batch size of 256. We perform learning rate scheduling at epoch 60 and epoch 80 where we divide the learning rate by a factor of10. During training, we apply random resized crop to 224 ×224 pixels and random horizontal ﬂip. We sweep over a set of 4 learning rates: {0.5,0.1,0.05,0.01}, 3 l1 weight decays: {0,1e−6,1e−5}and 3 τd for SEM: {0.01,0.1,1}, using a validation set of 10 images per class and re-traing using the full training set. We report the results on the test set. B.3 R OBUSTNESS EXPERIMENTS We follow the evaluation procedure from (Lee et al., 2021). We treated the robustness datasets as additional \"test sets\" in that we simply evaluated them using the evaluation procedure described above. The images were resized to a 256 ×256 before being center cropped to a 224 ×224 image. The evaluation procedure was performed using the public robustness benchmark evaluation code of (Djolonga et al., 2020)3. B.4 T RANSFER LEARNING LINEAR PROBE We follow the linear evaluation protocol of (Kolesnikov et al., 2019; Chen et al., 2020b) We train a linear classiﬁer using a regularized multinomial logistic regression from the scikit-learn package (Pe- dregosa et al., 2011). The representation is frozen, so that we do not train the encoder backbone nor 3https://github.com/google-research/robustness_metrics 21the batch-normalization statistics. We do not perform any augmentations and the images are resized to 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training set. We tune the regularizer term from a range of 45 logarithmically-spaced values between 10−6 and 105 using a small validation set and re-train using the full training set. For SEM, we set τd = 0 for all experiments. B.5 T RANSFER LEARNING FINE -TUNING We follow the same ﬁne-tuning protocol of (Chen et al., 2020b; Grill et al., 2020). We initialize the encoder with the pre-trained model and a classiﬁer head with random initialization. We train for 20,000 steps with a batch size of 256 using SGD with a Nesterov momentum of 0.9. We set the momentum parameter for the batch normalization to be max(1 −10/s,0.9) where sis the number of steps per epoch. During pre-training, we use random resize to 224 ×224 pixels and random horizontal ﬂipping. At test time, we resize the images along the shortest size to 256 pixels using cubic resampling following by a center resize to 224 ×224 pixels. Due to computational constraint, we only tune the learning rate using a search of 7 values spaces on logarithmic scales between 0.0001 and 0.1. For SEM, we set τd = 1. for all experiments After choosing the best learning rate of a validation set, we re-run the models using the full training set and evaluate it on the test set, which we use to report the numbers. B.6 S EMI -SUPERVISED LEARNING We follow the semi-supervised learning protocol of (Chen et al., 2020b; Grill et al., 2020). We initialize the network using the pre-trained representation and initialize a classiﬁcation head using random initialization. We ﬁne-tune the encoder while training the classiﬁcation head using a small subset of ImageNet. We choose the same subset used in prior works which is deﬁned in the TensorFlow-Dataset software. During training, we random resize the images to 224 ×224 pixels along the shorter size using bicubic resampling followed by a center crop and random horizontal ﬂipping. At test time, we resize the image to 224 ×224. We optimize the cross entropy loss with nestorov and a momentum of 0.9 using batch sizes of 224. We train models for {30,50}and take the best performing on the validation set. The learning rate used is chosen among a set of 5 learning rates: {0.01,0.02,0.05,0.1,0.005}. For SEM, we also search τd ∈{0.01,0.1,1}. We perform the search on the best performing one on the validation set and the number are returned are obtained using the test set after re-training using the full training set. C H YPERPARAMETERS The implementation of the SSL methods used in this work are taken from Solo-Learn (da Costa et al., 2021) to which we added the SEM module. The pre-training hyper-parameters of every SSL methods trained on CIFAR-100 with ResNet-18 used in this work are the default provided in the companion repository of Solo-Learn. The hyper-parameters are also provided in the launch scripts accompanying this work. Due to the large number of SSL methods probed in this work and the amount of space it would require to exhaustively detail all of the hyper-parameters, we refer the reader to the code. For the CIFAR-100 results obtained with BYOL and a ResNet-50, we have slightly modiﬁed the default parameters. Otherwise, the baseline BYOL model would not obtain competitive results. The hyper-parameters were tuned using the BYOL baseline and the SEM module was not considered in the selection of the SSL hyper-parameters. The BYOL hyper-parameters are presented in the launch script accompanying this work and presented below for completeness. For the ImageNet experiments, we took the hyper-parameters proposed in the launch scripts of Solo-Learn to which we only modiﬁed the amount of epochs (100 epochs to 200 epochs.) Here, we present all of the SEM hyper-parameters used in every experiments. These hyper-parameters can also be found in the launch scripts accompanying this work. We present the hype-parameters used to train for BYOL+SEM and MoCo+SEM on CIFAR100. Unless mentioned otherwise, these are the parameters used. 22Table 6: BYOL with ResNet-50 for CIFAR-100. precision 16 Learning rate 0.5 Weight-decay 1e-4 Optimizer sgd + lars LR scheduler warmup + cosine eta lars 0.001 exclude bias n norm (lars) True batch size 256 base ema momentum 0.99 ﬁnal ema momentum 1.0 proj output dim 256 proj hidden dim 4096 pred hidden dim 4096 augmentations: solarization_prob view 1: 0 view 2: 0.2 crop size 32 hue 0.1 saturation 0.2 contrast 0.4 brightness 0.4 Table 7: SEM SimCLR RN-18 for CIFAR-100 L V τp τ′ p 5000 13 0 .17 0 .78 Table 8: SEM MoCo RN-18 for CIFAR- 100 L V τp τ′ p 5000 13 0 .04 0 .01 Table 9: SEM BYOL RN-18 for CIFAR- 100 L V τp 5000 13 1 .0 1 .0 Table 10: SEM SwA V RN-18 for CIFAR-100 L V τp τ′ p 5000 13 0 .85 1 .5 Table 11: SEM DINO RN-18 for CIFAR- 100 L V τp τ′ p 5000 13 1 .0 1 .0 Table 12: SEM Barlow RN-18 for CIFAR-100 L V τp τ′ p 5000 13 1 .0 0 .99 Table 13: SEM VicREG RN-18 for CIFAR-100 L V τp τ′ p 5000 13 1 .0 1 .0 Table 14: SEM BYOL RN-50 for CIFAR-100 L V τp τ′ p 5000 13 1 1 Table 15: SEM BYOL all ResNets for ImageNet L V τp τ′ p 5000 21 0 .16 0 .04 C.1 C OMPUTATIONAL RESOURCES For all our CIFAR-100 training, we used1 RTX-8000 per experiment. For our ImageNet experiments, we used parallel training with 2 40GB A100 for the training with ResNet50 and ResNet50-x2 and 4 40GB A100 for the training with ResNet50-x4. With this setup, the training takes about a week for the ResNet50 experiments and about 10 days for the ResNet50-x2 and ResNet50-x4 experiments. D A DDITIONAL STUDIES OF SEM In Section 4.2, we discussed the effect of scalingLand V as well as changing the Softmax temperature during pre-training of the online network and changing the Softmax temperature for the downstream task. Here, we propose additional studies of SEM to provide a better mastery of the method. We provide a method for reducing the memory overhead of SEM and experiments demonstrating that despite this version still largely outperform the baseline. We additionally present the effect of modifying the embedder contributing to the insight on how to get the most out of SEM. Next, we have discussion with a study of the spectrum of the covariance matrix of the SEM representation and the BYOL representation, showing insight how SEM can particularly improve the training signal 23Table 16: BYOL with all ResNet-50 architectures for ImageNet. precision 16 Learning rate 0.4 Weight-decay 1e-6 Optimizer sgd + lars LR scheduler warmup + cosine eta lars 0.001 exclude bias n norm (lars) True batch size 256 base ema momentum 0.99 ﬁnal ema momentum 1.0 proj output dim 256 proj hidden dim 4096 pred hidden dim 4096 augmentations: solarization_prob view 1: 0 view 2: 0.2 gaussian_prob view 1: 1.0 view 2: 0.1 crop size 224 hue 0.1 saturation 0.2 contrast 0.4 brightness 0.4 Table 17: # of parameters, # of activations, allocated memory, computation efﬁciency (FLOPs/sample) and CIFAR-100 accuracy of BYOL, BYOL with SEM and its memory-efﬁcient variant with 8 blocks (denoted BYOL + SEM/8). # params # activations vRAM (GiB) FLOPs Accuracy Resnet-18: BYOL 16.5M 0.731M 4.0 7 .20e8 70 .7 BYOL+SEM 313.7M 0.797M 13.1 1 .01e9 73 .9 BYOL+SEM/8 51.9M 0.796M 5.3 7 .46e8 73 .3 Resnet-50: BYOL 35M 4.05M 11.1 1 .65e9 74 .3 BYOL+SEM 425.6M 4.12M 21.9 2 .04e9 77 .4 BYOL+SEM/8 76.7M 4.12M 11.8 1 .69e9 76 .6 during pre-training. We provide a scaling analysis of BYOL and BYOL + SEM on CIFAR-100. We end with an experiment showing that pre-training with SEM is necessary to get the best performance. D.1 A N EFFICIENT VARIANT OF SEM A large over-complete representation may induce a signiﬁcant memory footprint due to the additional parameters of the fully connected linear layer used to map to and from the representation. For SEM we require two such mappings as depicted in Figure 2c for BYOL. To reduce the amount of parameters, we propose to sparsify the weight matrix of the fully connected linear layer. We propose to do so by taking the block diagonal of the parameters of the matrix multiplication and setting the parameters outside the block diagonal to 0. Formally, let v ∈Rb×m, w ∈Rm×o and y = v·wbe the fully connected matrix multiplication. Instead, we partition vinto nblocks with vi ∈Rb×m n and deﬁne n smaller wi ∈R m n×o n, where i∈[L] is the ith block. Then, we perform a batch matrix multiplication of vi and wi that we concatenate as follows: yi = vi ·wi and ¯yi = Concat([y1,...,y n]). Thus, the amount of parameters of this matrix multiplication scales in O(m·o n ), allowing us to reduce the memory consumption by increasing n, the number of blocks. We perform an experiment where we partition the embedder and the ﬁrst linear layer of the projector into 8 blocks. We present the results in Table 17 in which we compare the $ of parameters, the 24# of activations, the allocated vRAM by pytorch, the FLOPs/sample and the accuracy of BYOL, BYOL+SEM and BYOL+SEM/8 representing the model with8 blocks obtained following the method described above. We observe that partitioning the matrix multiplications of SEM allows to vastly reduce the computation parameters while still yielding an important improvement over the baseline. This result demosntrate that SEM can be beneﬁcial while inducing minimal computational overhead. Attentive readers may notice that this performance is better compared to the ablation presented in Figure 3. The difference in performance is due to probing the embedder’s output (i.e. zθ) in Figure 3 and probing the encoder’s output (i.e.eθ) in Table 17. Using the each ablation’s representation for probing to the other recovers the performance observed by each. D.2 A DDITIONAL ABLATION OF THE SEM PARAMETERS Ablating the embedder In the main text, we mentioned that we use batch normalization at the output of the embedder. The reason we use batch normalization is mostly due to the fact that we wanted to avoid tuning any hyper-parameters that were not related to SEM to emphasize its contribution. Using BatchNorm gave the best performance without tuning the hyper-parameters of the baseline models. Here, we want to emphasize that SEM can be used without batch norm, but more hyper-parameters might need to be tuned for it to perform as well as the model with batch norm in the encoder. For example, we found that using no weight decay was important to get better performance when we did not have batch normalization as illustrated in Table table 18. We leave the full study of the interaction of SEM with the SSL related parameters for future work. Table 18: Understanding the relationship between the use of BatchNorm in the embedder and the weight decay hyper-parameter. BatchNorm weight decay Accuracy 0 67.2 1e-5 57.9 ✓ 0 68.3 ✓ 1e-5 73.9 Another decision is to use a linear layer as the embedder. Other alternative may include using the Identidy function (i.e. the output of the encoder is used for SEM). However, if we want to systematically use the same encoder as the SSL model, then we are constrained to a representation size that is the one of the ResNet encoder (i.e. 512 for a ResNet-18). Finally, we showcase that using a more expressive embedder leads to exacerbated performance and recommend practitioner to limit the expressivity of their embedder. Table 19: Comparing alternative embedders. Accuracy Identity 63.0 Linear 73.9 1 hidden layer MLP 65.0 D.3 A NALYZE OF THE SPECTRUM OF THE COVARIANCE MATRIX OF THE REPRESENTATION To obtain a better insight on why the SEM representation leads to better downstream performance, we analyze the spectrum of the covariance matrix of the representation using the methodology presented in Jing et al. (2022). That is, we collect the embedding vectors of the test set of CIFAR-100 using a pre-trained model using ResNet-50. For BYOL, we have an additional embedder without softmax normalization (as done in Figure 3). For BYOL and BYOL+SEM we use the embedder’s output (zθ) to perform the evaluation. To compute the covariance matrix C ∈RL·V×L·V of the embedding layer z, we deﬁne ¯z:= ∑N i=1 zi/Nthe average representation over the N samples and compute the 25covariance as follows: C := 1 N N∑ i=1 (zi −¯z)(zi −¯z)⊤. (6) To plot the spectrum of the covariance matrix, we take the singular value decomposition of the matrix (C = USV ⊤) with S the diagonal of the singular values, which we plot in sorted order and logarithm scale in Figure 7. This experiment demonstrates that the softmax normalization counters the dimensionality collapse that was discussed in Jing et al. (2022). Interestingly, the drop observed with SEM with L≥500 occurs at the index 2048 which is the dimensionality output of the ResNet-50 encoder. 0 2000 4000 6000 8000 10000 12000 eigenvalue index 20 15 10 5 0 5 log eigenvalue SEM L=1000 SEM L=500 SEM L=100 BYOL L=500 Figure 7: Spectrum of the covariance matrix of the represention for BYOL and BYOL + SEM obtained with a ResNet-50 encoder. D.4 S CALING THE RESNET ENCODER FOR CIFAR-100 7.2e+08 1.65e+09 4.65e+09 1.52e+10 FLOPs 64 66 68 70 72 74 76 78T op-1 Accuracy RN18 RN50 RN50-X2 RN50-X4 RN18 RN50 RN50-X2 RN50-X4 BYOL + SEM BYOL Figure 8: Scaling the ResNet encoder for CIFAR-100. We perform a scaling experiment on CIFAR-100 where we compare the scaling behaviour of BYOL and BYOL + SEM. We evaluate the computational cost of the methods and the resulting downstream accuracy for a range of four resnets: ResNet-18, ResNet-50, ResNet-50 x2 and ResNet-50 x4. In Figure 8, we observe that SEM has a better scaling behaviour than the baseline, especially as we increase the width of the ResNet-50. For BYOL, we observe that the performance decays for ResNet-50 with width x2 and x4. This is not unprecendented, as prior works as demonstrated other methods where scaling up the capacity of a model led to decrease in performance. When comparing the discrepancy with Figure 1, we attribute that to the fact that CIFAR-100 is a small dataset. In fact, we observe that the training accuracy stays constant to about 79% for all the ResNet-50 scales demonstrating overﬁtting for the baseline BYOL. Nevertheless, SEM prevents the decrease in performance and even lead to further improved performance as we increase the scale of the ResNet-50. 26Table 20: Downstream accuracy of training a classiﬁer with SEM normalization of the representation while using unormalized representation during pretraining. Experiments performed with a ResNet-50 encoder. Pre-train model Probe location SEM( τ = 0.1) Accuracy BYOL + Embed Encoder No 74.2 BYOL + Embed Embedder No 69.8 BYOL + Embed Embedder Yes 72.3 BYOL + SEM Embedder Yes 77.3 D.5 T HE ROLE OF PRE -TRAINING WITH SEM Here, we present the downstream accuracy obtained if one take a model pre-trained without SEM and add SEM normalization only for downstream classiﬁcation. For this experiment, we take a pre-trained model with embedder (i.e. BYOL + embed) with L = 5000 and V = 13 and add the softmax normalization for downstream classiﬁcation. We observe that such approach leads to an imprtant reduction in downstream accuracy in comparison to the model with SEM pre-training. 27E CIFAR100 SUPERCLASS The 100 classes of CIFAR-100 (Krizhevsky, 2009) are grouped into 20 superclasses. The list of superclass for each class in Table 21 Table 21: Set of classes for each superclass on CIFAR-100. Superclass Classes aquatic mammals beaver, dolphin, otter, seal, whale ﬁsh aquarium ﬁsh, ﬂatﬁsh, ray, shark, trout ﬂowers orchids, poppies, roses, sunﬂowers, tulips food containers bottles, bowls, cans, cups, plates fruit and vegetables apples, mushrooms, oranges, pears, sweet peppers household electrical devices clock, computer keyboard, lamp, telephone, television household furniture bed, chair, couch, table, wardrobe insects bee, beetle, butterﬂy, caterpillar, cockroach large carnivores bear, leopard, lion, tiger, wolf large man-made outdoor things bridge, castle, house, road, skyscraper large natural outdoor scenes cloud, forest, mountain, plain, sea large omnivores and herbivores camel, cattle, chimpanzee, elephant, kangaroo medium-sized mammals fox, porcupine, possum, raccoon, skunk non-insect invertebrates crab, lobster, snail, spider, worm people baby, boy, girl, man, woman reptiles crocodile, dinosaur, lizard, snake, turtle small mammals hamster, mouse, rabbit, shrew, squirrel trees maple, oak, palm, pine, willow vehicles 1 bicycle, bus, motorcycle, pickup truck, train vehicles 2 lawn-mower, rocket, streetcar, tank, tractor 28F A DDITIONAL CIFAR-100 COHERENCE GRAPHS 29S363 otter kangaroo bottleS382 lamp S225 bear tank S144 streetcar road S249 S167 train S94 lobster S273 boy girl house woman S29 S106 cockroach S6 apple dolphin seal whale aquarium_fish flatfish ray shark orchid poppy rose sunflower tulip bowl can cup plate mushroom orange pear sweet_pepper clock computer_keyboard telephone television bed chair couch table wardrobe bee beetle butterfly caterpillar leopard tiger wolf bridge castle skyscraper cloud forest mountain plain sea camel cattle chimpanzee fox possum raccoon skunk crab snail worm baby man lizard snake turtle hamster mouse rabbit shrew squirrel maple_tree oak_tree palm_tree willow_tree bicycle bus motorcycle pickup_truck lawn_mower rocket tractor S55 S63 S47 S279 S162 S69 S134 S242 S34 S241 S0 S98 S158 S361 S91 S66 S12 S95 S388 S53 S374 S190 S22 S59 S183 S261 S1 S375 S27 S35 S130 S110 S127 S43 S111 S258 S380 S214 S30 S85 S292 S193 S238 S336 S128 S132 S284 S14 S99 S93 S332 S236 S298 S260 S281 S68 S204 S161 S182 S177 S381 S64 S227 S320 S318 S266 S230 S196 S126 S181 S184 S114 S187 (a) BYOL baseline flatfish table S16 trout caterpillar S194 cattleS276 orchid computer_keyboardS229 palm_tree S184 cloud rocket skunk chimpanzee S256 S187 pickup_truck snail camel tiger spider S307 dolphin sweet_pepper S85 lion S434 S415 lobster sunflower mushroomS66 orange squirrel S189 S319 man S190 snakeS195 baby wardrobe wolf seal willow_tree can S323 maple_tree S252 S404 turtle leopard hamster rabbit cockroach S353 clock forest S87 S61 pine_tree S369 couch S397 tank otter woman bicycle S365 S148 bed beaver tulip bee lawn_mower crocodile lamp telephone S435 tractor S422 oak_tree S233 motorcycle shrew S284 S33 S146 S13 plain S78 whale bus (b) BYOL baseline with a large rep- resentation beeS248 spider beetle cockroachS178 S240 bear kangaroo tigerS393 leopard house castle S40 roadS107 plain S403 motorcycle lawn_mower tankS350 tractor sealS61 beaverS284 porcupine bedS168 couchS360 chair lion fox S216 wolf S272 skyscraperS227 mountain S78 sea snakeS98 S49S219S2 lizard worm orange S185 S30 sweet_pepper pear S198 S193 apple bus S18 S133 streetcar S370 S106 pickup_truck train rose S306 poppy S289S329 tulip S334S135 orchid S210 can cupS31 S259 S321 S291 bowl plate bottle willow_treepine_treemaple_treeoak_tree forest S71 palm_tree S138 S327 S405 dolphin ray S151S347S17S379 turtle sharkS176S189S355 whale S381 S221 S200 S22 boy S157 girl woman baby S204 flatfish man mouseraccoonS336 S271 S300 possumsquirrel S239 S128 shrewhamsterrabbit (c) BYOL + SEM Figure 9: Comparison of the full semantic coherence graph W5 between BYOL and BYOL + SEM. 30",
      "references": [],
      "meta_data": {
        "arxiv_id": "2204.00616v2",
        "authors": [
          "Samuel Lavoie",
          "Christos Tsirigotis",
          "Max Schwarzer",
          "Ankit Vani",
          "Michael Noukhovitch",
          "Kenji Kawaguchi",
          "Aaron Courville"
        ],
        "published_date": "2022-04-01T17:59:40Z",
        "github_url": "https://github.com/lavoiems/simplicial-embeddings"
      },
      "llm_extracted_info": {
        "main_contributions": "Simplicial Embeddings (SEM) introduce a simple, drop-in self-supervised learning (SSL) representation by projecting an encoder’s outputs onto L vectors of dimension V using a per-row softmax. This enforces an inductive bias toward sparsity and overcompleteness, enabling better downstream generalization and interpretable representations. The paper provides a theoretical bound showing downstream error decreases as the SEM expressivity is controlled (via the softmax temperature) and scales favorably with L and V. Empirically, SEM improves generalization across seven SSL methods on CIFAR-100 and ImageNet, enhances robustness to distribution shifts, aids transfer and semi-supervised learning, and yields semantically coherent downstream features tightly aligned with data categories. The approach is compatible with existing SSL frameworks and can be implemented with modest memory/compute overhead (mitigated by a sparse SEM variant).",
        "methodology": " SEM inserts a softmax-based embedding step after the encoder: for z ∈ R^{L×V}, apply a softmax with temperature τ to each of L vectors to obtain ¯z_i ∈ R^{V}, then concatenate to form ˆz ∈ R^{L×V}. The temperature controls sparsity: smaller τ yields sparser simplices; larger τ yields denser representations. Pre-training uses this SEM representation, with different temperatures for online (τ_p) and target networks. The downstream classifier receives στ(z) and uses a second temperature τ_d to control expressivity. A key theoretical result (Theorem 1) provides a generalization bound for downstream performance: the expected loss is bounded by the training loss plus a term that scales with √(L) and a measure ϕ(f_S(V,τ)) capturing expressivity; ϕ decreases to zero as τ → 0, and SEM yields a strictly smaller ϕ than the base model for any τ > 0, with larger gains as V increases. Empirically, SEM is tested across multiple SSL methods (SimCLR, MoCo-v2, BYOL, Barlow Twins, SwAV, DINO, VICReg), demonstrating performance gains that grow with L and V, and showing that discretization baselines (Gumbel-Softmax, VQ) perform worse than SEM’s softmax-based approach. The paper also analyzes semantic coherence by inspecting downstream linear classifiers and the top predictive features for CIFAR-100 classes, showing that SEM yields semantically organized features.",
        "experimental_setup": "Datasets: CIFAR-100 (ResNet-18 SSL pretraining, 1000 epochs) with linear probe evaluation; ImageNet (ResNet-50 family, BYOL+SEM pretraining up to 200 epochs) for in-distribution and transfer tasks; robustness benchmarks (IN-C, IN-R, IN-V2, IN-A) and semi-supervised learning (1% and 10% labels) on ImageNet; transfer datasets for linear probing and fine-tuning: FOOD-101, CIFAR-10, CIFAR-100, SUN, DTD, FLOWER. Evaluation protocols include linear evaluation on frozen representations and finetuning; transfer learning uses sparse logistic regression for linear probes and standard SGD-based finetuning. Hyperparameters include embedding size L = 5000, simplex dimension V = 13 (default), pretraining temperature τ_p, downstream temperature τ_d, and multiple seeds. The study compares SEM against baseline SSL methods (e.g., SimCLR, MoCo-v2, BYOL, Barlow Twins, SwAV, DINO, VICReg) and against discretized bottlenecks (Gumbel-Softmax, VQ) as controls. Memory and compute considerations are discussed, including a sparse SEM variant (8-blocks) to reduce parameters, with reported results on CIFAR-100. Code and experiments are available in the authors’ repository.",
        "limitations": "SEM introduces additional memory and computation due to the extra projection and concatenation into the SEM representation; memory footprint grows with L and V, though a sparse SEM variant mitigates this; discretization baselines can underperform compared to SEM’s softmax approach; effectiveness depends on hyperparameters, especially pretraining temperature τ_p and downstream temperature τ_d, and may vary with dataset (e.g., texture-rich datasets like DTD may require higher expressivity). The theoretical bound (Theorem 1) relies on assumptions such as Lipschitz loss, bounded per-sample loss, and a separation condition on embeddings; empirical gains may depend on the choice of SSL backbone and dataset scale. Some ablations (e.g., SEM without pretraining or with pure downstream normalization) show reduced performance, indicating SEM’s benefits rely on pretraining with SEM. The method may require normalization choices (BatchNorm in embedder) and careful architectural considerations for optimal gains across tasks.",
        "future_research_directions": "Explore adaptive or per-layer SEM configurations (varying L and V by layer), dynamic temperature schedules during pretraining and fine-tuning, and integration with vision transformers or other modalities. Investigate more memory-efficient SEM variants beyond the proposed 8-block approach, and explore combining SEM with discrete bottlenecks or other inductive biases. Extend SEM to multi-modal SSL settings and continual or few-shot learning, and deepen theoretical analysis to tighten generalization bounds and guide hyperparameter choices. Analyze semantic coherence at scale across diverse datasets, and study SEM’s impact on interpretability and disentanglement in broader domains.",
        "experimental_code": "# SEM-inspired softmax embedding blocks extracted from repository\n\n# 1) Softmax bottleneck helper used by SEM-style methods\nclass SoftmaxBridge(nn.Module):\n    def __init__(self, message_size, voc_size, tau, **kwargs):\n        super().__init__()\n        self.message_size = message_size\n        self.voc_size = voc_size\n        self.tau = tau\n\n    def forward(self, x, tau=None):\n        # x is expected to be (B, message_size*voc_size)\n        logits = x.view(-1, self.message_size, self.voc_size)\n        taus = tau or self.tau\n        return F.softmax(logits / taus, -1).view(x.shape[0], -1)\n\n# 2) SEM-like embedding after encoder in SDSimCLR\nclass SDSimCLR(BaseMethod):\n    def __init__(self, proj_output_dim: int, proj_hidden_dim: int, temperature: float,\n                 message_size: int, voc_size: int, taus: List[float], **kwargs):\n        super().__init__(**kwargs)\n        self.message_size = message_size\n        self.voc_size = voc_size\n        self.taus = taus\n        self.embedder = nn.Sequential(\n            nn.Linear(self.features_dim, message_size*voc_size, bias=False),\n            nn.BatchNorm1d(message_size*voc_size)\n        )\n        self.softmax = SoftmaxBridge(message_size, voc_size, temperature, **kwargs)\n        self.projector = nn.Sequential(\n            nn.Linear(message_size*voc_size, proj_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(proj_hidden_dim, proj_output_dim),\n        )\n\n    def forward(self, X: torch.Tensor) -> Dict[str, Any]:\n        out = super().forward(X)\n        emb = self.embedder(out[\"feats\"])  # (B, message_size*voc_size)\n        y = self.softmax(emb)                 # SEM embedding: (B, message_size*VOC_SIZE)\n        z = self.projector(y)                   # downstream projection\n        return {**out, \"z\": z, \"y\": y, \"emb\": emb}\n\n# 3) SEM-like usage in training loop (contrastive/aux tasks)\n# (excerpt showing per-row softmax on embeddings)\n# emb shape: (B, message_size, voc_size); converted via view in SoftmaxBridge\nclass SDSimCLR_TrainingSnippet(SDSimCLR):\n    def training_step(self, batch, batch_idx):\n        out = super().training_step(batch, batch_idx)\n        feats = out[\"feats\"]\n        emb = self.embedder(feats.flatten(1))  # (B, message_size*VOC_SIZE)\n        y = self.softmax(emb)\n        z = self.projector(y)\n        # ... further SEM-related losses/metrics\n        return out\n\n# 4) Example of downstream term (optional) using softmax bottleneck outputs for labels\n# (the repository uses similar patterns in SDSimCLR where outs_y is built from\n# the SEM-embedded representations and temperatures per tau)\n# for tau in self.taus:\n#     outs_y[tau] = F.softmax(emb / tau, dim=-1).view(emb.shape[0], -1)\n",
        "experimental_info": "SEM implementation in this repository is realized via a softmax bottleneck applied to the per-encoder embeddings. The key pieces include:\n- A SoftmaxBridge helper that reshapes a flat embedding to a (B, M) x (V) arrangement and applies a per-row softmax with a temperature parameter, modelling a simplicial/ probability-simplex embedding:\n  - forward maps raw features of size (B, M*V) to (B, M*V) after softmax, effectively yielding a sparse embedding.\n- An embedding module placed after the encoder (embedder) that outputs a tensor of shape (B, M*V). This is then passed through SoftmaxBridge to obtain the SEM representation y of shape (B, M*V) via a per-row softmax over V, controlled by temperature tau_p (pretraining) or tau (online usage).\n- A downstream projector that consumes the SEM representation y and outputs a latent vector z (authentication of expressivity controlled via V and L).\n- Usage pattern in SDSimCLR where the embedding is produced, passed through the softmax bottleneck, and then fed to the projector. For example, emb = self.embedder(feats); y = self.softmax(emb); z = self.projector(y).\n- Training/auxiliary computations also utilize per-tau softmaxed embeddings (outs_y) to compute downstream losses or supervised/classification tasks, where the temperature tau controls sparsity or discreteness of the simplex.\n- The paper setting that motivates this design uses SEM with an expressivity bound controlled by the softmax temperature; smaller tau yields sparser simplices and larger tau yields denser representations, enabling a tunable trade-off between sparsity and expressivity. \n- The experimental code in the repository demonstrates that this SEM bottleneck is compatible with multiple SSL backbones/frameworks, such as SimCLR, MoCo, BYOL, Barlow Twins, SwAV, DINO, VICReg, and others, as shown in SDSimCLR and related variants."
      }
    },
    {
      "title": "Systematic Comparison of Semi-supervised and Self-supervised Learning for Medical Image Classification",
      "full_text": "Systematic comparison of semi-supervised and self-supervised learning for medical image classification Zhe Huang* Ruijie Jiang* Shuchin Aeron Michael C. Hughes Tufts University, School of Engineering {Zhe.Huang, Ruijie.Jiang, Shuchin.Aeron, Michael.Hughes}@tufts.edu Abstract In typical medical image classification problems, labeled data is scarce while unlabeled data is more available. Semi- supervised learning and self-supervised learning are two different research directions that can improve accuracy by learning from extra unlabeled data. Recent methods from both directions have reported significant gains on tradi- tional benchmarks. Yet past benchmarks do not focus on medical tasks and rarely compare self- and semi- methods together on an equal footing. Furthermore, past bench- marks often handle hyperparameter tuning suboptimally. First, they may not tune hyperparameters at all, leading to underfitting. Second, when tuning does occur, it often unrealistically uses a labeled validation set that is much larger than the training set. Therefore currently published rankings might not always corroborate with their practi- cal utility This study contributes a systematic evaluation of self- and semi- methods with a unified experimental pro- tocol intended to guide a practitioner with scarce overall labeled data and a limited compute budget. We answer two key questions: Can hyperparameter tuning be effective with realistic-sized validation sets?If so, when all meth- ods are tuned well, which self- or semi-supervised meth- ods achieve the best accuracy? Our study compares 13 representative semi- and self-supervised methods to strong labeled-set-only baselines on 4 medical datasets. From 20000+ GPU hours of computation, we provide valuable best practices to resource-constrained practitioners: hy- perparameter tuning is effective, and the semi-supervised method known as MixMatch delivers the most reliable gains across 4 datasets. 1. INTRODUCTION Deep neural networks can deliver exceptional performance on classification tasks when trained with vast labeled *Authors Zhe Huang and Ruijie Jiang contributed equally to this work. Code: github.com/tufts-ml/SSL-vs-SSL-benchmark [MIT license]. datasets. However, in medical imaging applications as- sembling a large dataset with appropriate label can be pro- hibitively costly due to manual effort required by a human expert. In contrast, images alone, without labels, are of- ten readily available in health records databases. In recent years, significant research has focused on developing meth- ods that leverage both the large unlabeled set and a small labeled set to enhance image classifier training, aiming to surpass models trained solely on labeled data. Two ways of leveraging unlabeled data are particu- larly popular: semi-supervised and self-supervised learn- ing, both often abbreviated as SSL. Recent efforts in semi- supervised learning [74, 88] usually train deep classifiers jointly [5, 69] using an objective with two loss terms, one favoring labeled-set accuracy and the other favoring label-consistency or label-smoothness. Alternatively, self- supervised methods [65] take a two-stage approach, first training deep representations on the unlabeled set, then fine- tuning a classifier on the labeled set. Exemplars are numer- ous [10, 17, 19, 20, 37]. Despite the remarkable progress reported in each direction, these two paradigms have been largely developed independently [21]. A direct compar- ison of the two paradigms has been notably absent. A practitioner building a medical image classifier from lim- ited labeled data may ask, “ Which recent semi- or self- supervised methods are likely to be most effective?” Performance can be quite sensitive to hyperparameters for both semi-SL [69, 71] and self-SL [75]. We argue that any careful comparison must consider tuning hyperparame- ters of all methods in a fair fashion. However, recent work has not handled this well. First, as reviewed in Table 1, re- cent benchmarks for both semi- and self- paradigms often omit any tuning of hyperparameters (see the Fungi semi- SL experiments of Su et al. [71] or Ericsson et al. [27]’s self-SL benchmark). This practice of using off-the-shelf de- faults likely leads to under-performing given a new task, and may impact some methods more than others. An even big- ger issue with prevailing semi-SL practice was originally raised by Oliver et al. [62]: many published papers per- form hyperparameter tuning on a labeled validation set that arXiv:2307.08919v3  [cs.CV]  29 Mar 2024is larger (sometimes much larger) than the limited labeled train set. Such experimental settings are unrealistic. SSL is intended for practitioners without abundant available la- beled data. Practitioners that need to make the most of 1000 available labeled images will not elect to put more images in validation than training, and thus are not helped by bench- marks that do. Unfortunately, five years later after [62] we find this issue is still widespread (see Tab. 1). For example, Wang et al. [76]’s semi-SL results on TissueMNIST tune on a validation set over 50x larger than the labeled train set. Oliver et al. [62] further cast some doubt on whether ef- fective tuning is possible with small labeled sets, saying “Extensive hyperparameter tuning may be somewhat fu- tile due to an excessively small collection of held-out data to measure performance on”. There thus remains a press- ing question for resource-constrained practitioners: “Given limited available labeled data and limited compute, is hyperparameter tuning worthwhile?” This study makes progress toward answering these ques- tions by delivering a comprehensive comparison of semi- and self-supervised learning algorithms under a resource- constrained scenario that matches how SSL might be used on real-world medical tasks. Our goal is to enable practi- tioners to tackle new image classification challenges where only modest-sized labeled datasets exist. We target settings with roughly 2-10 class labels of interest, where each class has 30-1000 available labeled images for all model devel- opment (including training and validation). We select rep- resentative tasks across 4 datasets that span low-resolution (28x28) to moderate-resolution (112x112 and 384x384). On each task, we run careful experiments with represen- tative recent methods from both semi- and self-supervised learning to clarify what gains are possible with unlabeled data and how to achieve them. We emphasize realism throughout, in 4 distinct ways: (1) using validation set sizes that arenever bigger than the train set, avoiding the unrealistically-large validation sets com- mon in past work (Tab. 1); (2) performing hyperparameter search using thesame protocol, same compute budget(fixed number of hours)* and same hardware (one NVIDIA A100 GPU) for all algorithms for fair comparison; (3) respect- ing natural class imbalance; (4) profiling performance over time, to inform labs with smaller runtime budgets. In summary, the contributions of this study are: 1. We provide a systematic comparison of semi- and self- supervised methods on equal footing to connect two re- search directions that have been heretofore separate. 2. We adopt a realistic experimental protocol designed to consider the same constraints on available labels and runtime that SSL practitioners face, avoiding the unre- *Oliver et al. [62] give each method 1000 trials of a cloud hyperpa- rameter tuning service. However, training speed can differ substantially across SSL methods. We argue a fixed wallclock time budget is more fair. alistic aspects of past benchmarks, especially the use of far too large validation sets. 3. We show that hyperparameter tuning is viable with a realistic-sized validation set, and in many cases neces- sary to do well at a new classification task with new data. Ultimately, we hope this study guides practitioners with limited data toward successful deployment of semi- and self-supervised methods on real problems. 2. BACKGROUND AND METHODS Unified Problem Formulation. Following the recent sur- vey by Chen et al. [21], we adopt a unified perspective for supervised, semi-supervised and self-supervised image classification with limited available labeled data. For model development (including training and hyperparameter selec- tion), we assume there are two available datasets. First, a small labeled dataset L of feature-label pairs (x, y), where each image is represented by a D-dimensional feature vec- tor x ∈ RD and its corresponding class label takes one of C possible values: y ∈ {1, 2, . . . C}. Second, an unlabeled set U containing only feature vectors. Typically, we assume the unlabeled set is much larger: |U| ≫ |L|. Given labeled setL and unlabeled set U, we wish to train a neural network that can map each input x to a probabil- ity vector in the C-dimensional simplex ∆C representing a distribution over C class labels. Let fv(·) : RD → RF de- note a backbone neural network with parameters v produc- ing an F-dimensional embedding given any input image. Let gw(·) : RF → ∆C denote a final linear-softmax clas- sification layer with parameters w. The following unified objective can capture all three learning paradigms: v∗, w∗ ← arg min v,w P x,y∈L λLℓL(y, gw(fv(x))) (1) + P x∈U λU ℓU (x, fv, gw). Here, ℓL represents a labeled-set loss (e.g. multi-class cross-entropy), and ℓU represents a unlabeled-set loss. λL, λU ≥ 0 are weights for the corresponding loss terms. The design of ℓU is usually what differs substantially across methods. For instance, setting ℓU to be cross-entropy com- puted with pseudo-labels generated from classifier g re- covers PseudoLabel [58]; a temperature-scaled instance- similarity contrastive loss recovers SimCLR [17]. All three learning paradigms that we study optimize Eq. (1), yet differ in the number of phases and in how to set the scalar weights λL, λU on each loss term. Super- vised learning ignores the unlabeled term throughout train- ing (λU = 0), thus learning parameters using only the la- beled set. Semi-supervised methods include both terms in one end-to-end training, keeping both λU > 0 and λL > 0. Self-supervised learning has two phases. In phase 1 (“pretraining”), the labeled term is omitted ( λL = 0, λU = 1) and the focus of learning is an effective representationBenchmark Methods Unrealistic Experiments Labeled train size Labeled val. size Acc vs. Time? Realistic eval. SSL [a] Semi CIFAR-10 (Tab. 1-2, Fig. 2) 4000 5000 no SVHN (Tab. 1-2, Fig. 3-4) 1000 7325 Fine-grained SSL [b] Semi & 1 Self∗ Semi-Aves 5959 8000 no Semi-Fungi 4141 None USB [c] Semi TissueMNIST 80/400 23640 † no Semi-Aves 5959 None Self benchmark [d] Self ImageNet 1.28 mil. None no SSL-vs-SSL (ours) Semi & Self no 400-1660 no bigger than train yes Table 1. Comparison of related benchmarks of semi-supervised and self-supervised learning.Past works either do no hyperparameter tuning at all (val. size = None), or use an unrealistically large validation set (defined as larger than the labeled train set ) in some or all experiments. Further, each work almost exclusively looks at methods from one paradigm, either semi- or self- (*: [b] includes one self-supervised method, MOCO). In contrast, in this paper we benchmark 6 semi- and 7 self-supervised algorithms with hyperparameter tuning on realistic validation sets. Acc vs. Time? indicates whether the work analyzes performance over training time. Number marked † confirmed via GitHub comment by USB authors. Citations: a: Oliver et al. [62], b: Su et al. [71] c: Wang et al. [76] d: Ericsson et al. [27]. layer fv (classifier gw is not included in this phase). In phase 2 (“fine-tuning”), we focus on the labeled term and omit the unlabeled term ( λL = 1 , λU = 0 ). We fix the representation parameter v and fine-tune the classifier w. We now identify the 16 methods we will evaluate: Supervised methods. The goal of leveraging unlabeled data U is to obtain better performance than what we could obtain using only the labeled set L. Therefore, we naturally compare to 3 high-quality supervised baselines that use only the labeled set. First, “Sup” denotes a classifier trained with supervised loss ℓL set to multi-class cross-entropy. Sec- ond, “MixUp” trains with cross entropy with the addition of mixup data augmentation [85]. Finally, “SupCon” pur- sues a supervised contrastive learning loss for ℓL [49]. Semi-supervised methods. We compare 6 semi- supervised methods that train deep classifiers on both la- beled and unlabeled data simultaneously as in Eq. (1). To represent the state-of-the-art of semi-supervised image clas- sification, we select Pseudo Label (“PseudoL”) [58], Mean Teacher (“MeanTch”) [73], MixMatch [5], FixMatch [69], FlexMatch [84] and CoMatch [59]. These choices cover a reasonably wide spectrum of unlabeled loss design strat- egy, year of publication, and computation cost. CoMatch represents a recent trend of combining semi- and self- supervision. See App D.2 for a wider literature review. Self-supervised methods. We compare 7 self- supervised algorithms: SimCLR [17], MOCO (v2) [20, 37], SwA V [10], BYOL [34], SimSiam [19], DINO [11] and Barlow Twins (“BarlowTw”) [82]. These algorithms epito- mize the field of self-supervised learning as of this writing. SimCLR, MOCO (v2), and SwA V are based on contrastive learning, which learns effective representations when pro- vided with similar and dissimilar samples. BYOL was de- signed to circumvent the need for dissimilar samples. Sim- Siam is a simple yet effective Siamese representation learn- ing method. DINO uses a teacher-student network architec- ture for representation distillation. Barlow Twins preserves information and reduces redundancy by favoring a close-to- identity cross-correlation matrix between augmented pairs of data. See App D.3 for further review. 3. RELATED WORK Table 1 summarizes key attributes of existing major bench- marks for semi-supervised and self-supervised methods. We now discuss how our study situates in this context. Comparison to Oliver et al. Oliver et al. [62] provide an influential benchmark of deep semi-supervised methods. Like our work, they highlight the pressing issue of using un- realistically large validation sets for hyperparameter tuning. However, most of their experiments (e.g. all their tables and their figures 2-4) still use this unrealistic setting to be comparable to other works; only one subsection (Sec. 4.6) examines validation sets no larger than the train set. Instead, we exclusively use validation set sizes no larger than train- ing set , mimicking what practitioners would face in real- applications. Our definition of a “realistic” size for a vali- dation set – no larger than the labeled train set – is broader than Oliver et al. ’s definition of 10% of the train set. We fa- vor our definition because Oliver et al. find specifically that “for validation sets of the same size (100%) as the training set, some differentiation between the approaches is possi- ble.” In contrast, they found reliable differentiation was not always possible with much smaller validation sets. Furthermore, the benchmarking results presented by Oliver et al. require each algorithm to complete “1000 tri- als of Gaussian Process-based black-box optimization using Google Cloud ML Engine”. This resource-intensive pro- cess seems impractical for researchers outside well-funded industrial labs. We use more modest compute budgets and specifically allow each method the same total wallclock run- time for tuning, again obeying practical constraints. Other semi-supervised benchmarks. TorchSSL [84]benchmarked eight popular semi-supervised learning algo- rithm using a unified codebase. The same group later ex- tended that effort to natural language and audio process- ing in their USB benchmark [76]. Another recent effort for fine-grained classification by Su et al. [71] evaluates semi-supervised learning on datasets that exhibit class im- balance and contains images from novel classes in the unla- beled set, studying the effect of different initializations and the contents of unlabeled data on the performance of semi- supervised methods. While making significant contribu- tions, these works focused on semi-supervision almost ex- clusively and did not include multiple self-supervised learn- ing methods, thus leaving open the questions “ How do the two paradigms compare? Which methods are best overall?” Prior self-supervised benchmarks. Several works have proposed different benchmarks for self-supervised meth- ods. Goyal et al. [32] introduced a benchmark that covers 9 different tasks, such as object detection and visual nav- igation. Ericsson et al. [27] compared thirteen top self- supervised models on 40 downstream tasks. Da Costa et al. [23] presented a library of self-supervised methods that can be easily plugged into different downstream tasks and datasets. However, these works mainly consider other self-SL methods without direct comparison to the semi-SL paradigm. Moreover, they mostly do not incorporate the use of a validation set for hyperparameter tuning at all. This can lead to suboptimal accuracy at test time and complicate comparisons to methods that do tune. Self-supervision for medical images. Our work is complementary to recent efforts that assess self-supervised pipelines for medical image classification [2, 3]. They fo- cus primarily on how to design multi-stage transfer learning pipelines and do not comprehensively compare many differ- ent self-supervised methods or any semi-supervised meth- ods. Further, many datasets studied by [3] come from pro- prietary projects conducted at Google. In contrast, all our data and experiments are open and reproducible by others. Combining semi- and self-supervision. Recent re- search has explored the fusion of semi-supervised learning and self-supervised learning ideas [50, 59, 83, 87]. Some recent surveys [21, 65] compare semi- and self-supervised learning, but they focus on literature review while we offer realistic and comprehensive benchmarking experiments. 4. DATASETS AND TASKS We study four open-access medical image classification datasets, all with 2D images that are fully deidentified. Ta- ble 2 reports statistics for all train/test splits. The exact splits used in our study can be found in our open-source codebase, documented in App. A. Two datasets – PathMNIST and TissueMNIST – are selected from the MedMNIST collection [80] (criteria in App. B.2). Prior experiments by Yang et al. [80] suggest TissueMNIST PathMNIST Labeled Unlab. Train Val Test Train total 400 400 47280 165066 GE 15 15 1677 5851 DCT 19 19 2233 7795 POD 19 19 2202 7686 LEU 28 28 3369 11761 IE 37 37 4402 15369 TAL 59 59 7031 24549 PT 95 95 11201 39108 CD/CT 128 128 15165 52947 Labeled Unlab. Train Val Test Train total 450 450 7180 89546 NORM 39 39 741 7847 MUC 40 40 1035 7966 ADI 47 47 1338 9319 STR 47 47 421 9354 BACK 48 48 847 9461 DEB 52 52 339 10308 LYM 52 52 643 10349 MUS 61 61 592 12121 TUM 64 64 1233 12821 TMED-2 AIROGS Labeled Unlab. Train Val Test Train total 1660 235 2019 353500 PSAX 223 50 342 - A2C 325 28 319 - A4C 462 39 423 - PLAX 650 118 935 - Labeled Unlab. Train Val Test Train total 600 600 6000 94242 Glaucoma 60 60 600 - No Glauc. 540 540 5400 - Table 2. Summary statistics of train/validation/test splitsfor all datasets in our study. Each table’s rows are arranged in ascending order based on the number of per-class labeled train images. Full description of class names can be found in App B.3. their 28x28 resolution is a reasonable choice for rapid proto- typing; using larger 224x224 resolution does not yield much more accurate classifiers for these two datasets. Two other datasets represent more moderate resolutions closely tied to contemporary clinical research: the 112x112 Tufts Medical Echocardiogram Dataset (TMED-2) and the 384x384 AIROGS dataset. Further details for each dataset are provided in a dedicated paragraph below. For each dataset, our data splitting strategy closely mir- rors the conditions of real SSL applications. First, we let la- beled training and validation sets contain a natural distribu- tion of classes even if that may be imbalanced. This reflects how data would likely be affordably collected (by random sampling) and avoids artificially balanced training sets that will not match the population an algorithm would encounter in a deployment. Second, to be realistic we ensure vali- dation sets are never larger than the available labeled train sets. This is in contrast to previous benchmarks: Wang et al. [76]’s TissueMNIST hyperparameter search used a valida- tion set of 23,640 images even though the labeled training set contained only 80 or 400 images. In practice, such a large validation set would almost certainly be re-allocated to improve training, as noted in Oliver et al. [62]. TissueMNIST is a lightweight dataset of 28x28 images of human kidney cortex cells, organized into 8 categories. The original dataset is fully-labeled and distributed with predefined train/val/test split of 165,466/23,640/47,280 im-ages with some class imbalance. We assume a total label- ing budget of 800 images, evenly split between training and validation (to ensure hyperparameter tuning can differenti- ate methods). We form a labeled training set of 400 images from the predefined training set, sampling each class by its frequency in the original training set. We form a labeled validation set of 400 images sampled from the predefined validation set. For unlabeled set, we keep all remaining im- ages in the original training split, discarding known labels. PathMNIST is another lightweight dataset of 28x28 im- ages of patches from colorectal cancer histology slides that comprise of 9 tissue types. The original dataset is fully- labeled and distributed with predefined train/val/test split of 89,996/10,004/7,180 images. Class imbalance is less severe than TissueMNIST. We assume a total labeled data budget of 900 images, again evenly split between training and val- idation. Labeled train, labeled valid, and unlabeled sets are sampled via the same procedures as in TissueMNIST. For both MedMNIST datasets, we use the predefined test set. TMED-2 [39, 40] is an open-access dataset of 112x112 2D grayscale images captured from routine echocardiogram scans. Each scan produces dozens of ultrasound images of the heart, captured from multiple acquisition angles (i.e., different anatomic views). In this study, we adopt Huang et al.’s view-classification task: the goal is to classify each image into one of 4 canonical view types (see App. B.3). View classification is clinically important: measurements or diagnoses of heart disease can only be made when looking at the right anatomical view [60, 77]. We used the provided train/validation/test split (id #1), which is naturally imbal- anced and matches our criteria for realistic sizing. We fur- ther use the provided large unlabeled set of 353,500 images. TMED-2’s unlabeled set is both authentic (no true labels are available at all, unlike other benchmarks that “forget” known labels) and uncurated [41] (contains images of view types beyond the 4 classes in the labeled task). AIROGS [24] is a public dataset released for a recent competition where the binary classification task is to decide whether the patient should be referred for glaucoma or not, given a color fundus image of the retina (eye). We selected this dataset because it represents an active research chal- lenge even with all available labels. Furthermore, because labels for this dataset were acquired at great expense (mul- tiple human annotators graded each of over 100k images), we hope our work helps assess how self-/semi-supervision could reduce the annotation costs of future challenges. We selected the 384x384 resolution favored by several high- performing competition entries. We chose a labeling budget of 1200 total images, evenly split between train and valida- tion; the rare positive class (9 to 1 imbalance) is representa- tive of many screening tasks. We took the rest of available images as the unlabeled train set. 5. EXPERIMENTAL DESIGN Performance metric. We use balanced accuracy [33, 35] as our primary performance metric. For a task with C ≥ 2 classes, let y1:N denote true labels for N examples in a test set, and ˆy1:N denote a classifier’s predicted labels. Let TPc count true positives for class c (number of correctly classified examples whose true label is c), and let Nc count the total number of examples with true label c. Then we compute balanced accuracy (BA) as a percentage: BA(y1:N , ˆy1:N ) = 1 C CX c=1 TPc(y1:N , ˆy1:N ) Nc(y1:N ) · 100% (2) Balanced accuracy is more suitable than standard accuracy for imbalanced problems when each class matters equally. The expected BA of a uniform random guess is 100 C %. On AIROGS, we also track metrics recommended by its creators [24] for clinical utility: AUROC, partial AUROC (>90% specificity) and sensitivity-at-95%-specificity. Architectures. CNN backbones are popular in medical imaging [6, 28–31, 42, 45, 54–56, 77, 78]. We use ResNet- 18 [36] on Tissue and Path, and WideResNet-28-2 [81] on TMED-2. We experiment with both ResNet-18 and 50 [36] on AIROGS to assess architectural differences. Training with early stopping. For each training phase, we perform minibatch gradient descent on Eq. (1) with Adam [51] optimizer and a cosine learning rate sched- ule [69]. Each training phase proceeds for up to 200 epochs, where one epoch represents enough minibatch updates to process the equivalent of the entire combined training set L ∪ U. After every epoch, we record balanced accuracy on the validation set. If this value plateaus for 20 consecutive epochs, we stop the current training phase early. Hyperparameters. Semi- and self-supervised learning can both be sensitive to hyperparameters [71, 75]. Our evaluations tune both shared variables (e.g. learning rates, weight decay, unlabeled loss weightλU ) as well as hyperpa- rameters unique to each algorithm. See App. F.1 for details on all hyperparameters for all methods. Unified procedure for training and hyperparameter tuning. We formulate a unified procedure mindful of real- istic hardware and runtime constraints for a non-industrial- scale lab working on a new medical dataset. We assume each algorithm has access to one NVIDIA A100 GPU for a fixed number of hours. Within the alotted compute budget, for each algorithm we execute a serial random search over hyperparameters, sequentially sampling each new configu- ration and then training until either an early stopping criteria is satisfied or the maximum epoch is reached. We track the best-so-far classifier in terms of validation-set performance every epoch. Algorithm D.1 provides pseudocode for the procedure. Each time a new hyperparameter configuration is needed, we sample each hyperparameter value indepen-dently from a distribution designed to cover its common set- tings in prior literature (App. F.1). Our choice of random search on a budget is thought to yield better performance than grid search [4], fairly expends the same effort to train all methods regardless of their cost-per-epoch or hyperpa- rameter complexity, and does not assume industrial-scale access to 1000 cloud-computing trials as in Oliver et al. [62]. We find that performance saturates after about 25 hours per 80000 unlabeled examples. We thus allocate for the total time budget 25 hours for PathMNIST, 50 hours for TissueMNIST, and 100 hours for TMED-2. We allow 100 hours for AIROGS due to its larger resolution. Due to lim- ited resources, we select only a few competitive methods to run on AIROGS based on the results on other datasets. Self-supervised classification phase. Self-supervised methods by definition do not utilize label information when training representation layer weights v. To enable a proper comparison, for all self-SL methods our Alg. D.1 introduces an additional classification layer, training it anew after each epoch, each time using a 10 trial random search for an L2- penalty regularization hyperparameter. We retain the best performing weights w on the validation set. We emphasize that w does not impact overall self-supervised training ofv. Data augmentation. Random flip and crop are used for all semi-supervised and supervised methods. MixMatch also utilizes MixUp [85], while RandAugment [22] is used by FixMatch and FlexMatch. For each self-supervised method as well as SupCon, we apply the same SimCLR augmentation: random flip, crop, color jitter, and grayscale. Multiple trials. We repeat Alg. D.1 for each method across 5 separate trials (distinct random seeds). We record the mean balanced accuracy on valid and test sets of these trials every 60 minutes (every 30 min. for Tissue and Path). 6. RESULTS & ANALYSIS In each panel of Fig. 1, we focus on one dataset and archi- tecture, plotting for each algorithm one line showing test set balanced accuracy (averaged across 5 trials) as a function of time spent running Alg. D.1. Variance across trials is visu- alized in Fig C.4. Extra results on AIROGS showing other performance metrics over time (AUROC, partial AUROC, and sensitivity at 95% specificity) are found in Fig. C.3. We emphasize that following best practice, each checkpoint represents the best hyperparameters as selected on the val- idation set; we then report that checkpoint’s test set perfor- mance. These results help us answer two key questions: Is hyperparameter tuning worthwhile for SSL meth- ods when validation set sizes cannot be larger than the train set? Across Fig. 1 and C.3, all 16 algorithms show roughly monotonic improvements in test performance over time, despite using a realistic-sized validation set. The final performance of every algorithm and dataset shows improve- ment due to our unified training and tuning procedure (fur- Gain in Bal. Acc. over best labeled-set-only name Path Tissue TMED AIROGS median worst MixMatch 12.4 3.7 -0.2 5.7 4.7 -0.2 MOCO 7.9 4.4 -18.8 n/a 4.4 -18.8 SwAV 8.4 4.3 -10.1 n/a 4.3 -10.1 SimCLR 11.0 5.9 -7.8 1.6 3.8 -7.8 BYOL 12.1 6.2 -6.0 0.9 3.5 -6.0 BarlowTw 11.2 2.2 -1.9 n/a 2.2 -1.9 SimSiam 1.9 2.1 -11.6 n/a 1.9 -11.6 FlexMtch 11.1 4.5 -1.1 -4.2 1.7 -4.2 DINO 12.6 6.5 -3.4 -3.8 1.6 -3.8 FixMatch 9.3 2.9 0.2 -4.5 1.6 -4.5 CoMatch 14.1 0.2 -3.0 n/a 0.2 -3.0 MeanTch -1.6 -1.9 -1.2 n/a -1.6 -1.9 PseudoL 0.6 -1.4 -4.4 -3.2 -2.3 -4.4 ref. val. 69.8 33.9 94.9 71.5 Table 3. Gains from SSL methods over only using labeled set , across 4 datasets. Self (italicized), Semi (normal). We report gains in percentage balanced accuracy (higher is better) in the mean final test set performance (averaged over 5 runs of Alg. D.1) over a reference value that represents the best labeled-set-only run among Sup (minimize cross entropy), SupCon, and MixUp. MixMatch has the highest median gain, and is the only method to never score notably worse (> 1 point) than the best labeled set only run. ther discussion in App. E.1). Therefore, we answer: Yes, realistically-sized validation sets for hyperparameter tun- ing and checkpoint selection can be effective. What are the best SSL methods? . Among the many methods we study in Fig. 1, no method clearly outperforms others across all datasets. On Path, CoMatch, MixMatch, DINO, and BYOL perform best. On Tissue, self-supervised methods like DINO, BYOL, and SimCLR score well. On TMED-2, ultimately only FixMatch and MixMatch are competitive with the MixUp baseline. On AIROGS, only MixMatch, SimCLR and BYOL beat the Sup baseline. For each semi- and self- SSL method, Tab. 3 reports its relative gain in balanced accuracy over the best labeled-set- only supervised baseline on each dataset. We conclude that MixMatch represents the best overall choice as it consis- tently performs at or near the top across all tasks. It is the only method to never deliver results worse by more than 1 percentage point than the best labeled-set-only baseline (see “worst” column of Tab. 3). This is in stark contrast to USB [76], where FixMatch and FlexMatch are ranked notably higher that MixMatch. Such differences stress the importance of choosing proper evaluation protocol tailored to specific needs. We do caution that hyperparameter tun- ing is strongly recommended for MixMatch to succeed on a new dataset (see AIROGS result in Tab. 4). Pretraining vs. from scratch. Plots in App. C show accuracy-over-time profiles using initial weights pretrained0 1 2 4 8 16 hours 60 65 70 75 80 85bal. acc. on test PathMNIST ResNet18 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanTch PseudoL SimCLR BYOLDINO BarlowTw SwAV SimSiam MOCO semi self 0 1 2 4 8 16 32 hours 25 30 35 40bal. acc. on test TissueMNIST ResNet18 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanTchPseudoL SimCLRBYOL DINO BarlowTw SwAV SimSiam MOCO semi self 0 1 2 4 8 16 32 64 hours 75 80 85 90 95bal. acc. on test TMED-2 WideRes28 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanT ch PseudoL SimCLR BYOL DINO BarlowT w SwAV SimSiam MOCO semi self 0 1 2 4 8 16 32 64 hours 60 65 70 75bal. acc. on test AIROGS ResNet18 Sup FixMatch FlexMtch MixMatch PseudoL SimCLRBYOL DINO semi self 0 1 2 4 8 16 32 64 hours 60 65 70 75bal. acc. on test AIROGS ResNet50 Sup FixMatch FlexMtch MixMatch PseudoL SimCLRBYOL DINO semi self SupCon labeled-set-only MixUp Sup FixMatch semi FlexMtch MixMatch CoMatch MeanT ch PseudoL SimCLR self BYOL DINO BarlowTw SwAV SimSiam MOCO Figure 1. Balanced accuracy over time profiles of semi- and self-supervised methods across all 4 datasets. At each time, we report the test set bal. acc. of each method (mean over 5 trials of Alg. D.1). Best viewed electronically.Top Row: On these 3 datasets, we compare all 6 semi- and 7 self- methods (see legend in lower right, citations in Sec. 2), to 3 labeled-set-only baselines.Bottom Row: On larger AIROGS dataset, we compare selected methods representative of the best in the top row. Thin lines show final performance to ease comparison. From these charts, we suggest that our unified training and tuning (Alg. D.1) is effective, as all methods show gains over time. on ImageNet. Compared to training from scratch, we see only slight gains (less than 3 points of BA) in ultimate test-set accuracy when top methods use pretraining, which aligns with observations in [66] but counters the “large mar- gin” gains (sometimes over 10 points) reported by Su et al. [71]. Pretraining’s benefits still include improved conver- gence speed and reduced variance across trials. ResNet-18 vs. ResNet-50. The bottom row of Fig 1 compares ResNet-18 and ResNet-50 architectures on AIROGS. We broadly find that the method rankings are similar. More notably, ResNet-50 is not substantially bet- ter than ResNet-18 in this task when compared using the same runtime budget. Profiles of other performance metrics in App C.3 suggest the same conclusions. Tuning vs. Transferring from another dataset. To make the most of limited labeled data, one strategy is to use the entire labeled set for training, reserving no vali- dation set at all. This approach relies on pre-established “best” hyperparameters sourced from other datasets and transferred to the target task, obviating the need for data- specific tuning. Su et al. [71] employed this strategy in their benchmark, avoiding tuning due to concerns that small validation sets would yield unreliable selection. However, this claim in their work was not supported by experiments. Here, we directly compare our hyperparameter tuning strat- egy (Alg. D.1) with Su et al.’s transfer strategy. We train the latter on the combined train + validation sets, using “best” hyperparameters from CIFAR-10 and Tissue (details in App. F.2). Tab. 4 reports the final test-set balanced accu- racy of each strategy on Path, TMED-2 and AIROGS. Our hyperparameter tuning strategy is competitive across all datasets: the “Tune” columns of Tab. 4 have the highest fraction of green cells. The alternative trans- fer strategy is faster and occasionally yield reasonable re- sults. However, there is no guarantee of good performance from hyperparameter transfer, as evidenced by the inferior cases in Tab. 4 highlighted in yellow and red. There is also no consistent distinction between transferring from Tissue (an arguably more related medical dataset) and transferring from CIFAR-10, underscoring the challenge of identifying a “closely related” dataset for hyperparameter transfer. Overall, we recommend researchers pursuing SSL for a new medical classification task should adopt our hyperpa- rameter tuning strategy. Transferring off-the-shelf hyper- parameters has highly variable performance and can even result in divergence (see MixMatch on AIROGS in Tab. 4). Even with reasonable hyperparameters, having no valida- tion set makes checkpoint selection challenging. Adjacent epochs that exhibit similar low training losses may differ by over 5% in test accuracy in our experiments. Improving transfer from existing hyperparameters to a new task is an interesting further direction of research.PathMNIST TMED-2 AIROGS Hyperparam. strategy: Tune Best from Best from Tune Best from Best from Tune Best from Best from Tissue CIFAR-10 Tissue CIFAR-10 Tissue CIFAR-10 semi MixMatch 82.24 83.87 73.51 94.71 92.67 74.73 77.14 61.75 Diverged CoMatch 83.95 80.51 86.42 91.87 92.51 92.67 FixMatch 79.11 82.64 79.73 95.06 95.12 95.73 66.92 66.31 65.50 FlexMatch 80.91 78.70 80.23 93.77 93.97 93.70 67.23 72.82 74.52 Pseudo-Label 70.42 66.12 64.71 90.52 88.78 82.13 68.21 64.43 66.93 Mean-Teacher 68.21 55.90 56.13 93.70 91.21 58.26 self SimCLR 81.05 79.86 80.05 87.04 84.41 87.24 73.01 67.15 67.95 BYOL 81.79 80.35 80.16 88.90 88.41 85.15 72.40 70.13 70.63 SwA V 77.90 78.24 81.30 84.76 82.17 82.20 MoCo 77.73 79.95 79.12 76.14 78.56 79.11 SimSiam 71.78 68.62 70.44 83.24 80.93 80.20 BT 80.85 72.79 80.25 92.96 91.63 90.81 DINO 82.52 77.50 80.67 91.45 84.80 90.03 67.61 66.46 67.13 Table 4. Hyperparameter strategy evaluation. Table reports ultimate test-set balanced accuracy as percentage (mean over 5 trials, higher is better) on three target datasets: PathMNIST, TMED-2, and AIROGS. 3 different hyperparameter strategies are compared for each SSL method. Due to computation constraints, we only select a few algorithms to run on AIROGS, explained in Sec. 5. Tune strategy: tuning for best hyperparameters via our proposed Alg. D.1 for 100 hours (TMED-2 and AIROGS) and 25 hours (PathMNIST), using provided train/validation split. Best from Tissue/CIFAR-10 strategy: just one training phase on the target dataset using the “best” hyperparameters selected from the named source data. These runs combine train and validation data for fitting following Su et al. [71], and typically take 1-4/5-30/10-40 hours on Path/TMED-2/AIROGS depending on the algorithm and whether early stopping is triggered. To highlight which strategies are most effective, we bold the best strategy within each dataset-specific row, and color cells relative to this bold value as green (within 2.5 of best), yellow (within 5 of best), and red (worse by 5 or more points). 7. DISCUSSION & CONCLUSION We have contributed a benchmark that helps practioners quantify what gains in a classification task are possible from the addition of unlabeled data and which methods help achieve them. We offer a unified approach to training and hyperparameter selection of semi-supervised methods, self- supervised methods, and supervised baselines that is both affordable and realistic for research labs tackling new chal- lenges without industrial-scale resources. Limitations. We deliberately focused on a modest num- ber of labeled examples (30 - 1000) per class, motivated by projects where substantive effort has already gone into labeled data collection. Applications in more scarce-label regimes (e.g. zero-shot or few-shot) may need to also con- sult other benchmarks, as should those looking at hundreds of fine-grained classes. We also did not specifically study situations where the distribution of labeled and unlabeled data significantly differs [41, 44, 62, 67]. To enable thorough experiments, we focused on moder- ate ResNet architectures that have proven effective in clin- ical applications [42, 78]. As ViT-based methods become popular in medical imaging [1, 43], it would be valuable to understand if similar findings hold with vision trans- formers [26]. We leave this to future work. Ideally, our results would be verified across multiple train-validation splits; however the resources needed remain prohibitive. Our analysis here is limited the specific metrics of bal- anced accuracy and sensitivity/specificity (for AIROGS). Other clinically useful metrics, such as AUPRC, calibra- tion, or net benefit, may be needed to decide if a classifier is appropriate for deployment [70]. For medical applications, it is also key to understand fairness across subpopulations [13] to avoid propagating structural disadvantages. Broader impact. All data analyzed here represent fully- deidentified open-access images approved for widespread use by their creators. We think the benefit of promoting these medical tasks to advance ML research outweighs the slight risk of patient reidentification by a bad actor. Outlook. Our experiments show that real benefits from the addition of unlabeled data are sometimes possible: our recommended methods see gains of +5 points of balanced accuracy on TissueMNIST, +10 points on PathMNIST, and +5 points on AIROGS against strong labeled-set-only base- lines. In contrast, most tested methods do not add signif- icant gains on TMED-2, perhaps due to that benchmark’s uncurated nature [41]. We hope our work enables the re- search community to convert decades of effort on SSL into improved patient outcomes and better scientific understand- ing of disease and possible treatments. We further hope that our benchmark inspires those that pursue improved method- ological contributions to favor realistic evaluation protocols and clinically-relevant datasets.References [1] N Ahmadi, MY Tsang, AN Gu, TSM Tsang, and P Abolmae- sumi. Transformer-based spatio-temporal analysis for clas- sification of aortic stenosis severity from echocardiography cine series. IEEE Transactions on Medical Imaging, 2023. 8 [2] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, and Mohammad Norouzi. Big Self-Supervised Models Advance Medical Image Classification. In Interna- tional Conference on Computer Vision (ICCV). arXiv, 2021. 4 [3] Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting Chen, Nenad Toma- sev, Jovana Mitrovi ´c, Patricia Strachan, S. Sara Mahdavi, Ellery Wulczyn, Boris Babenko, Megan Walker, Aaron Loh, Po-Hsuan Cameron Chen, Yuan Liu, Pinal Bav- ishi, Scott Mayer McKinney, Jim Winkens, Abhijit Guha Roy, Zach Beaver, Fiona Ryan, Justin Krogue, Mozziyar Etemadi, Umesh Telang, Yun Liu, Lily Peng, Greg S. Cor- rado, Dale R. Webster, David Fleet, Geoffrey Hinton, Neil Houlsby, Alan Karthikesalingam, Mohammad Norouzi, and Vivek Natarajan. Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging. Nature Biomedical Engineering, 7(6):756–779, 2023. 4, 23 [4] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012. 6 [5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems , 32, 2019. 1, 3, 21, 23 [6] Benjamin Billot, Colin Magdamo, Steven E Arnold, Sudeshna Das, and Juan Eugenio Iglesias. Robust segmen- tation of brain mri in the wild with hierarchical cnns and no retraining. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 538– 548. Springer, 2022. 5 [7] Avrim Blum and Tom Mitchell. Combining labeled and un- labeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100, 1998. 21 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 21 [9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European confer- ence on computer vision (ECCV), pages 132–149, 2018. 21 [10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Ad- vances in neural information processing systems , 33:9912– 9924, 2020. 1, 3, 21 [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 9650–9660, 2021. 3 [12] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. 21 [13] Leo Anthony Celi, Jacqueline Cellini, Marie-Laure Charpignon, Edward Christopher Dee, Franck Dernoncourt, Rene Eber, William Greig Mitchell, Lama Moukheiber, Ju- lian Schirmer, et al. Sources of bias in artificial intelligence that perpetuate healthcare disparities—A global review. PLOS Digital Health, 1(3), 2022. 8 [14] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Net- works, 20(3):542–542, 2009. 21 [15] Chen Chen, Chen Qin, Huaqi Qiu, Cheng Ouyang, Shuo Wang, Liang Chen, Giacomo Tarroni, Wenjia Bai, and Daniel Rueckert. Realistic adversarial data augmentation for mr image segmentation. In Medical Image Computing and Computer Assisted Intervention MICCAI, 2020. 23 [16] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee- woo Jun, David Luan, and Ilya Sutskever. Generative pre- training from pixels. In International conference on machine learning, pages 1691–1703. PMLR, 2020. 21 [17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597–1607. PMLR, 2020. 1, 2, 3, 21 [18] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised mod- els are strong semi-supervised learners. Advances in neural information processing systems, 33:22243–22255, 2020. 21 [19] Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 15750–15758, 2021. 1, 3 [20] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 1, 3 [21] Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata. Semi-supervised and unsupervised deep vi- sual learning: A survey. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, 2022. 1, 2, 4 [22] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmen- tation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702–703, 2020. 6, 21 [23] Victor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self- supervised methods for visual representation learning. J. Mach. Learn. Res., 23(56):1–6, 2022. 4[24] Coen de Vente, Koenraad A. Vermeer, Nicolas Jaccard, He Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temir- gali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le, Adrian Galdran, Miguel Ángel González Ballester, Gustavo Carneiro, Devika R. G, Hrishikesh P. S, Densen Puthussery, Hong Liu, Zekang Yang, Satoshi Kondo, Satoshi Ka- sai, Edward Wang, Ashritha Durvasula, Jónathan Heras, Miguel Ángel Zapata, Teresa Araújo, Guilherme Aresta, Hrvoje Bogunovi ´c, Mustafa Arikan, Yeong Chan Lee, Hyun Bin Cho, Yoon Ho Choi, Abdul Qayyum, Imran Raz- zak, Bram van Ginneken, Hans G. Lemij, and Clara I. Sánchez. AIROGS: Artificial Intelligence for RObust Glau- coma Screening Challenge, 2023. 5, 15 [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 21 [26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8 [27] Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5414–5423, 2021. 1, 3, 4 [28] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. nature, 542(7639):115–118, 2017. 5 [29] Loveleen Gaur, Ujwal Bhatia, NZ Jhanjhi, Ghulam Muham- mad, and Mehedi Masud. Medical image-based detection of covid-19 using deep convolution neural networks. Multime- dia systems, 29(3):1729–1738, 2023. [30] Hemant Ghayvat, Muhammad Awais, AK Bashir, Sharnil Pandya, Mohd Zuhair, Mamoon Rashid, and Jamel Nebhen. Ai-enabled radiologist in the loop: novel ai-based frame- work to augment radiologist performance for covid-19 chest ct medical image annotation and classification from pneu- monia. Neural Computing and Applications, 35(20):14591– 14609, 2023. [31] Amirata Ghorbani, David Ouyang, Abubakar Abid, Bryan He, Jonathan H Chen, Robert A Harrington, David H Liang, Euan A Ashley, and James Y Zou. Deep learning interpre- tation of echocardiograms. NPJ digital medicine , 3(1):10, 2020. 5 [32] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual rep- resentation learning. In Proceedings of the ieee/cvf Inter- national Conference on computer vision , pages 6391–6400, 2019. 4 [33] Margherita Grandini, Enrico Bagli, and Giorgio Visani. Metrics for multi-class classification: an overview. arXiv preprint arXiv:2008.05756, 2020. 5 [34] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. 3 [35] Isabelle Guyon, Kristin Bennett, Gavin Cawley, Hugo Jair Escalante, Sergio Escalera, Tin Kam Ho, Núria Macià, Bisakha Ray, Mehreen Saeed, Alexander Statnikov, et al. Design of the 2015 ChaLearn AutoML challenge. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2015. 5 [36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5 [37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 9729–9738, 2020. 1, 3, 21 [38] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409–426, 1994. 23 [39] Zhe Huang, Gary Long, Benjamin Wessler, and Michael C Hughes. A new semi-supervised learning benchmark for classifying view and diagnosing aortic stenosis from echocardiograms. In Proceedings of the Machine Learning for Healthcare Conference. PMLR, 2021. 5, 15 [40] Zhe Huang, Gary Long, Benjamin S Wessler, and Michael C Hughes. TMED 2: A dataset for semi-supervised classifica- tion of echocardiograms. In DataPerf: Benchmarking Data for Data-Centric AI Workshop, 2022. 5, 15 [41] Zhe Huang, Mary-Joy Sidhom, Benjamin S Wessler, and Michael C Hughes. Fix-a-step: Semi-supervised learn- ing from uncurated unlabeled data. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics (AISTATS), 2023. 5, 8 [42] Zhe Huang, Benjamin S Wessler, and Michael C Hughes. Detecting heart disease from multi-view ultrasound images via supervised attention multiple instance learning. In Ma- chine Learning for Healthcare Conference, pages 285–307. PMLR, 2023. 5, 8 [43] Zhe Huang, Xiaowei Yu, Benjamin S Wessler, and Michael C Hughes. Semi-supervised multimodal multi- instance learning for aortic stenosis diagnosis.arXiv preprint arXiv:2403.06024, 2024. 8 [44] Zhe Huang, Xiaowei Yu, Dajiang Zhu, and Michael C Hughes. Interlude: Interactions between labeled and un- labeled data to enhance semi-supervised learning. arXiv preprint arXiv:2403.10658, 2024. 8 [45] Tongtong Huo, Yi Xie, Ying Fang, Ziyi Wang, Pengran Liu, Yuyu Duan, Jiayao Zhang, Honglin Wang, Mingdi Xue, Songxiang Liu, et al. Deep learning-based algorithm im- proves radiologists’ performance in lung cancer bone metas- tases detection on computed tomography. Frontiers in On- cology, 13:1125637, 2023. 5 [46] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning.In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 5070–5079, 2019. 21 [47] Longlong Jing and Yingli Tian. Self-supervised visual fea- ture learning with deep neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence , 43(11):4037–4058, 2020. 21 [48] Jakob Nikolas Kather, Johannes Krisam, Pornpimol Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron Weis, Timo Gaiser, Alexander Marx, Nektarios A Valous, Dyke Ferber, et al. Predicting survival from colorectal cancer his- tology slides using deep learning: A retrospective multicen- ter study. PLoS medicine, 16(1):e1002730, 2019. 15 [49] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661–18673, 2020. 3 [50] Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho Joe, Seungjai Min, and Youngjune Gwon. Selfmatch: Com- bining contrastive self-supervision and consistency for semi- supervised learning. arXiv preprint arXiv:2101.06480, 2021. 4 [51] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 5 [52] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep gen- erative models. Advances in neural information processing systems, 27, 2014. 21 [53] Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Manifold invariance with improved inference. Advances in neural information processing systems, 30, 2017. 21 [54] Devidas T Kushnure, Shweta Tyagi, and Sanjay N Talbar. Lim-net: Lightweight multi-level multiscale network with deep residual learning for automatic liver segmentation in ct images. Biomedical Signal Processing and Control , 80: 104305, 2023. 5 [55] Zhengfeng Lai, Chao Wang, Luca Cerny Oliveira, Brittany N Dugger, Sen-Ching Cheung, and Chen-Nee Chuah. Joint semi-supervised and active learning for segmentation of gi- gapixel pathology images with cost-effective labeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 591–600, 2021. [56] Zhengfeng Lai, Chao Wang, Henrry Gunawan, Sen-Ching S Cheung, and Chen-Nee Chuah. Smoothed adaptive weight- ing for imbalanced semi-supervised learning: Improve re- liability against unknown distribution data. In Interna- tional Conference on Machine Learning , pages 11828– 11843. PMLR, 2022. 5 [57] Samuli Laine and Timo Aila. Temporal ensembling for semi- supervised learning. arXiv preprint arXiv:1610.02242, 2016. 21 [58] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning at ICML, 2013. 2, 3, 21 [59] Junnan Li, Caiming Xiong, and Steven CH Hoi. Comatch: Semi-supervised learning with contrastive graph regulariza- tion. In Proceedings of the IEEE/CVF international confer- ence on computer vision, pages 9475–9484, 2021. 3, 4 [60] Ali Madani, Ramy Arnaout, Mohammad Mofrad, and Rima Arnaout. Fast and accurate view classification of echocar- diograms using deep learning. NPJ digital medicine, 1(1):6, 2018. 5 [61] Shaobo Min, Xuejin Chen, Hongtao Xie, Zheng-Jun Zha, and Yongdong Zhang. A mutually attentive co-training framework for semi-supervised recognition. IEEE Transac- tions on Multimedia, 23:899–910, 2020. 21 [62] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. Advances in neural in- formation processing systems, 31, 2018. 1, 2, 3, 4, 6, 8, 13, 22, 23 [63] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 21 [64] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. 20 [65] Guo-Jun Qi and Jiebo Luo. Small data challenges in big data era: A survey of recent progress on unsupervised and semi- supervised methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):2168–2187, 2020. 1, 4 [66] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. Advances in neural information processing systems, 32, 2019. 7 [67] Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set consistency regularization for semi-supervised learning with outliers. arXiv preprint arXiv:2105.14148, 2021. 8 [68] Azizi Shekoofeh, Mustafa Basil, Ryan Fiona, Beaver Zachary, Freyberg Jan, Deaton Jonathan, Loh Aaron, Karthikesalingam Alan, Kornblith Simon, Chen Ting, et al. Big self-supervised models advance medical image classifi- cation. arXiv preprint arXiv:2101.05224, 2021. 23 [69] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 1, 3, 5, 21 [70] Ewout W Steyerberg and Yvonne Vergouwe. Towards bet- ter clinical prediction models: seven steps for development and an abcd for validation. European Heart Journal, 35(29), 2014. 8 [71] Jong-Chyi Su, Zezhou Cheng, and Subhransu Maji. A real- istic evaluation of semi-supervised learning for fine-grained classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12966– 12975, 2021. 1, 3, 4, 5, 7, 8, 13, 16, 23, 25[72] Teppei Suzuki. Consistency regularization for semi- supervised learning with pytorch. https://github. com / perrying / pytorch - consistency - regularization, 2020. 13 [73] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017. 3, 21 [74] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine learning , 109(2):373– 440, 2020. 1, 21 [75] Diane Wagner, Fabio Ferreira, Danny Stoll, Robin Tibor Schirrmeister, Samuel Müller, and Frank Hutter. On the im- portance of hyperparameters and data augmentation for self- supervised learning. arXiv preprint arXiv:2207.07875, 2022. 1, 5 [76] Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, et al. Usb: A unified semi-supervised learning bench- mark for classification. Advances in Neural Information Pro- cessing Systems, 35:3938–3961, 2022. 2, 3, 4, 6 [77] Benjamin S Wessler, Zhe Huang, Gary M Long Jr, Stefano Pacifici, Nishant Prashar, Samuel Karmiy, Roman A Sandler, Joseph Z Sokol, Daniel B Sokol, Monica M Dehn, et al. Au- tomated detection of aortic stenosis using machine learning. Journal of the American Society of Echocardiography, 36(4): 411–420, 2023. 5 [78] Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanisław Jastrz˛ ebski, Thibault Févry, Joe Katsnelson, Eric Kim, et al. Deep neural networks im- prove radiologists’ performance in breast cancer screening. IEEE transactions on medical imaging , 39(4):1184–1194, 2019. 5, 8 [79] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist clas- sification decathlon: A lightweight automl benchmark for medical image analysis. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) , pages 191–195. IEEE, 2021. 14 [80] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medm- nist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023. 4, 14, 15 [81] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In Proceedings of the British Machine Vision Confer- ence (BMVC), 2016. 5 [82] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Ma- chine Learning, pages 12310–12320. PMLR, 2021. 3 [83] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lu- cas Beyer. S4l: Self-supervised semi-supervised learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1476–1485, 2019. 4 [84] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin- dong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curricu- lum pseudo labeling. Advances in Neural Information Pro- cessing Systems, 34:18408–18419, 2021. 3 [85] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 3, 6, 21 [86] Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang, Andrew Makmur, Qingpeng Cai, and Beng Chin Ooi. Boost- MIS: Boosting medical image semi-supervised learning with adaptive pseudo labeling and informative active annotation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 23 [87] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14471–14481, 2022. 4 [88] Xiaojin Zhu. Semi-Supervised Learning Literature Survey. Technical Report 1530, Department of Computer Science, University of Wisconsin Madison., 2005. 1, 21 [89] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International confer- ence on Machine learning (ICML-03), pages 912–919, 2003. 21 [90] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43–76, 2020. 21Acknowledgments RJ and SA acknowledge support from the U.S. National Science Foundation under award # 1931978. All authors are grateful for computing infrastructure support from the Tufts High-Performance Computing cluster, partially funded by NSF under grant OAC CC* 2018149. Appendix Contents A . Code and Data Resources for Reproducibility 13 B . Dataset Details 14 B.1. Example Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.2. Dataset Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.3. Classification Task Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 C . Additional Results 16 C.1. Impact of pretraining on accuracy-over-time profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 C.2. Validation-set profiles of accuracy-over-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 C.3. Additional performance metrics: Profiles over time on AIROGS . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.4. Variability in Performance Across Trials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 D . Method Details 20 D.1 . Algorithm : Unified training and hyperparameter tuning via random search on a budget . . . . . . . . . . . . . 20 D.2 . Semi-supervised method details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 D.3 . Self-supervised method details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E . Additional Analysis and Discussion 22 E.1. Effectiveness of Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.2. Differentiating Between Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3. Answers to Common Questions from Reviewers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F. Hyperparameter Details 24 F.1. Hyperparameter Tuning Strategy: Random Search Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F.2. Hyperparameter transfer strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A. Code and Data Resources for Reproducibility All code and data resources needed to reproduce our analysis, including information on exact splits we used for each of the 4 datasets ( TissueMNIST, PathMNIST, TMED-2, AIROGS) can be found in our github repo: https://github.com/tufts-ml/SSL-vs-SSL-benchmark Primer on our codebase. Our codebase builds upon the open-source PyTorch repo by Suzuki [72]. Suzuki’s code was originally intended as a reimplementation in PyTorch of Oliver et al. [62]’s benchmark of semi-supervised learning (while Oliver et al’s original repo was in Tensorflow, we prefer PyTorch). We added many additional algorithms (we added MixMatch, FixMatch, FlexMatch, and CoMatch, as well as all 7 self- supervised methods) and customized the experiments, especially providing a runtime-budgeted hyperparameter tuning strat- egy as outlined in App. D. In a way, this makes our repo a “cousin” of the codebase of Su et al. [71]’s fine-grained classification benchmark, because their github repo also credits Suzuki’s repo as an ancestor.B. Dataset Details B.1. Example Images Below we show a few examples for each dataset. For full details, please refer to the original papers. PathMNIST TissueMNIST TMED2 AIROGS Figure B.1. Showing 5 random examples for each dataset. B.2. Dataset Selection We selected PathMNIST and TissueMNIST from 12 candidate datasets in the MedMNIST collections [79, 80] by matching two criteria: (i) contains at least 5 imbalanced classes; (ii) can build a large unlabeled set (at least 50000 images). Prior experiments from dataset creator Yang et al. [80] suggest 28x28 resolution is a reasonable choice. They report that a larger resolution (224x224) does not yield much more accurate classifiers for these two datasets. B.3. Classification Task Description TissueMNIST contains 28x28 images of human kidney cortex cells. The dataset contains 8 classes. See [80] for details.Class ID Abbreviation Description 0 CD/CT Collecting Duct, Connecting Tubule 1 DCT Distal Convoluted Tubule 2 GE Glomerular endothelial cells 3 IE Interstitial endothelial cells 4 LEU Leukocytes 5 POD Podocytes 6 PT Proximal Tubule Segments 7 TAL Thick Ascending Limb PathMNIST contains 28x28 patches from colorectal cancer histology slides that comprise 9 tissue types. See [48, 80] for details. Class ID Abbreviation Description 0 ADI adipose 1 BACK background 2 DEB debris 3 LYM lymphocytes 4 MUC mucus 5 MUS smooth muscle 6 NORM normal colon mucosa 7 STR cancer-associated stroma 8 TUM colorectal adenocarcinoma epithelium TMED-2 contains 112x112 2D grayscale images captured from routine echocardiogram scans (ultrasound images of the heart). In this study, we adopt the view classification task from [39]. For more detail please see [39, 40] Class ID Abbreviation Description 0 PLAX parasternal long axis 1 PSAX parasternal short axis 2 A2C apical 2-chamber 3 A4C apical 4-chamber AIROGS is a dataset of color fundus photographs of the retina. The binary classification task is to detect evidence of referable glaucoma [24]. We use 384x384 resolution, as suggested by several challenge participants. Class ID Abbreviation Description 0 No Glauc. no referable glaucoma 1 Glaucoma referable glaucoma (signs associated with visual field defects on standard automated perimetry)C. Additional Results C.1. Impact of pretraining on accuracy-over-time profiles To study the impact of pretraining, we compare the accuracy-over-time profiles of TissueMNIST and PathMNIST based on the two different initialization strategy. Fig. C.1 shows balanced-accuracy-over-time profiles for initialization of neural net parameters to values pretrained on ImageNet (left column) and random initialization (right column). Pretraining time on a source dataset is NOT counted to the runtime reported in x-axis. On TissueMNIST (top row), SimCLR (green) and BYOL (blue) are the top two methods for both initialization types. Performance gains from pretraining are slight, BA for BYOL is around 42 with pretraining and 40 with random initialization. On PathMNIST (bottom row), FixMatch and CoMatch are best in the pretraining case, with MixMatch and Flexmatch only a few points of balanced accuracy lower. MixMatch and CoMatch are best in the random initialization case. Across both datasets, pretraining does not seem to impact the top-performing methods’ ultimate accuracy by much, usually just a slight increase in BA of 0.5-3 points. One exception is FixMatch on PathMNIST, which improves by about 5 percentage points. We do not see the 10+ point gains reported by Su et al. [71] in their Table 3. Considering more limited time budgets (e.g. after only a few hours), we do see initialization from pretraining understand- ably tends to improve some methods. 0 1 2 4 8 16 32 hours 25 30 35 40bal. acc. on test TissueMNIST ResNet18 SupCon MixUpSup FixMatch FlexMtchMixMatch CoMatch MeanT ch PseudoL SimCLR BYOL DINOBarlowT wSwAVSimSiam semi self (a) TissueMNIST : pretrained initial weights 0 1 2 4 8 16 32 hours 25 30 35 40bal. acc. on test TissueMNIST ResNet18 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanTchPseudoL SimCLRBYOL DINO BarlowTw SwAV SimSiam MOCO semi self (b) TissueMNIST : random initial weights 0 1 2 4 8 16 hours 60 65 70 75 80 85bal. acc. on test PathMNIST ResNet18 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanT chPseudoL SimCLRBYOL DINO BarlowT wSwAVSimSiam semi self (c) PathMNIST : pretrained initial weights 0 1 2 4 8 16 hours 60 65 70 75 80 85bal. acc. on test PathMNIST ResNet18 SupCon MixUp Sup FixMatch FlexMtchMixMatch CoMatch MeanTch PseudoL SimCLR BYOLDINO BarlowTw SwAV SimSiam MOCO semi self (d) PathMNIST: random initial weights Figure C.1. Balanced accuracy on test set over time for semi- and self-supervised methods, with (left) and without (right) initial weight pretraining on ImageNet. Curves represent mean of each method at each time over 5 trials of Alg. D.1.C.2. Validation-set profiles of accuracy-over-time Fig. C.2 shows profiles of accuracy over time on the validation set, in contrast to the test set performance shown in the main paper’s Fig. 1. All curves here by definition must be monotonically increasing, because our unified algorithm selects new checkpoints only when they improve the validation-set balanced accuracy metric. The important insight our work reveals is that the same model checkpoints selected here, based on validation-set accuracy, also tend to produce improved test-set accuracy over time (in Fig. 1). This helps provide empirical confidence in using realistically-sized validation sets. 0 1 2 4 8 16 32 hours 30 35 40 45 50bal. acc. on val TissueMNIST ResNet18 SupCon MixUp Sup FixMatchFlexMtch MixMatch CoMatchMeanT chPseudoL SimCLRBYOL DINO BarlowT w SwAV SimSiam semi self (a) TissueMNIST 0 1 2 4 8 16 hours 40 50 60 70 80 90 100bal. acc. on val PathMNIST ResNet18 SupCon MixUpSup FixMatchFlexMtchMixMatch CoMatch MeanT chPseudoL SimCLR BYOLDINOBarlowT w SwAV SimSiam semi self (b) PathMNIST 0 1 2 4 8 16 32 64 hours 85 90 95 100bal. acc. on val TMED-2 WideRes28 SupCon MixUpSupFixMatch FlexMtchMixMatchCoMatch MeanT ch PseudoL SimCLR BYOL DINO BarlowT w SwAVSimSiam MOCO semi self (c) TMED2 0 1 2 4 8 16 32 64 hours 60 65 70 75 80 85bal. acc. on val AIROGS ResNet18 Sup FixMatch FlexMtch MixMatch PseudoL SimCLRBYOL DINO semi self (d) AIROGS Figure C.2. Validation-set accuracy over time profiles of semi- and self-supervised methods on 4 datasets (panels a-d). All curves here by definition must be monotonically increasing. The increasing profiles here on the validation set translate to similar trends in test set performance in Fig. 1, indicating successful generalization.C.3. Additional performance metrics: Profiles over time on AIROGS In Fig. C.3, we report the test performance over time on the AIROGS dataset across all 4 metrics of interest, including the partial AUROC and sensitivity at 95% specificity metrics recommended by the AIROGS data creators as being particularly relevant for the glaucoma detection task. Broadly, our takeaway is that our proposed hyperparameter tuning method is viable for all these metrics, not just the BA metric covered in the main paper. Furthermore, this viability appears consistent across both ResNet-18 and ResNet-50 architectures. 0 1 2 4 8 16 32 64 128 Hours 50 55 60 65 70 75 80Test Balanced Acclabeled-only FixMatch(2020) FlexMatch(2022) MixMatch(2019) Pseudo-Label(2013) SimCLR(2020) BYOL(2020) DINO(2021) (a) Balanced Acc 0 1 2 4 8 16 32 64 128 Hours 0.4 0.5 0.6 0.7 0.8 0.9Test full_auc (b) full auc 0 1 2 4 8 16 32 64 128 Hours 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75Test partial_auc_90spec (c) Partial AUC 0 1 2 4 8 16 32 64 128 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6Test sensitivity_95spec (d) Sensitivity 0 1 2 4 8 16 32 64 128 Hours 50 55 60 65 70 75 80Test Balanced Acc (e) Balanced Acc 0 1 2 4 8 16 32 64 128 Hours 0.4 0.5 0.6 0.7 0.8 0.9Test full_auc (f) full auc 0 1 2 4 8 16 32 64 128 Hours 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75Test partial_auc_90spec (g) Partial AUC 0 1 2 4 8 16 32 64 128 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6Test sensitivity_95spec (h) Sensitivity Figure C.3. Profiles of several clinically-relevant performance metrics over time on the AIROGS test set. Top row: ResNet-18. Bottom row: ResNet-50. Columns, left-to-right: Balanced Accuracy, AUROC, Partial AUROC focused on the 90% - 100% specificity regime, and sensitivity at 95% specificity. At each time, we report mean of each method over 5 trials of Alg. D.1. C.4. Variability in Performance Across Trials In Fig. C.4 on the next page, we explicitly visualize the variability in performance of each method across the 5 separate trials of Alg. D.1 (most other figures show the mean of these 5 trials for visual clarity).CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 20 25 30 35 40 45 50 55 60Accuracy Val T est (a) TissueMNIST after 16h CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 20 25 30 35 40 45 50 55 60Accuracy Val T est (b) TissueMNIST after 50h CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 60 65 70 75 80 85 90 95 100Accuracy Val T est (c) PathMNIST after 8h CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 60 65 70 75 80 85 90 95 100Accuracy Val T est (d) PathMNIST after 25h CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 60 65 70 75 80 85 90 95 100Accuracy Val T est (e) TMED-2 after 32h CM Fl Fi MM MT PL DI SC BL BT SS SV MC Algorithms 60 65 70 75 80 85 90 95 100Accuracy Val T est (f) TMED-2 after 100h Fl Fi MM PL DI SC BL Algorithms 50 55 60 65 70 75 80 85 90Accuracy Val T est (g) AIROGS(res18) after 32h Fl Fi MM PL DI SC BL Algorithms 50 55 60 65 70 75 80 85 90Accuracy Val T est (h) AIROGS(res18) after 100h Figure C.4. Balanced accuracy of different methods across 2 time budgets (columns) and four datasets (rows). For each method, the interval indicates the low and high performance of 5 separate trials of Alg. D.1, while dot indicates the mean performance. Horizontal lines indicate the best labeled-set-only baseline at that time. Abbreviation: CM, Fl, Fi, MM, MT, PL, DI, SC, BL, BT, SS, SV , MC denote CoMatch, FlexMatch, FixMatch, MixMatch, Mean Teacher, Pseudo Label, DINO, SimCLR, BYOL, Barlow Twins, SimSiam, SwA V , MOCO (v2).D. Method Details D.1. Algorithm : Unified training and hyperparameter tuning via random search on a budget Algorithm D.1 outlines the hyperparameter tuning procedures used across all algorithms under comparison. The algorithm requires three sources of data: a labeled training set L = {X, Y}, an unlabeled set for training U = XU , and a separate realistically-sized labeled validation set{Xval, Yval}. We further require some budget restrictions: a common computational budget T (maximum number of hours), and a maximum training epoch per hyperparameter configuration E. We proceed as follows: We begin by randomly sampling a hyperparameter configuration from a defined range (see Ap- pendix F.1 for details). A model is then initialized and trained using the ADAM optimizer with the sampled hyperparameters. Each configuration is trained for a maximum of E (200) epochs or stopped early if the validation performance does not improve for 20 consecutive epochs. The model’s performance on the validation set is measured using balanced accuracy. Upon completion of training for a given hyperparameter configuration (either after reaching maximum epochE or after early stopping), a new configuration is sampled and the process repeats until the total compute budget T is expended. We track the best-so-far model performance every 30 minutes, and save the best-so-far model along with its validation and test performance. Semi-supervised algorithms simultaneously train the representation layers v and classifier layer w, while self-supervised algorithms train the representation layers v for each epoch and then fine-tune a linear classifier with weights w anew at the end of each epoch using an sklearn logistic regression model [64] with representation parameters v frozen. Algorithm D.1 Unified Procedure for Training + Hyperparameter selection via random search Input: • Train set of features X paired with labels Y, with extra unlabeled features U • Validation set of features Xval and labels Yval • Runtime budget T, Max Epoch E Output: Trained weights {v, w}, where v is the representation module, w is the classifier layer 1: while time elapsed < T do 2: λ ∼ DRAWHYPERS ▷ Sample hyperparameters from pre-defined range (App. F.1) 3: ξ ← CREATE OPTIM (λ) ▷ Initialize stateful optimizer e.g., ADAM 4: {v, w} ∼INIT WEIGHTS ▷ Initialize model weights 5: for epoch e in 1, 2, . . . , Edo 6: if self-supervised then 7: v ← TRAIN ONEEPOCH (U, v, λ, ξ) ▷ Optimize Eq. (1) with λL = 0 8: w ← TRAIN CLASSIFIER (Y, fv(X)) 9: else if semi-supervised then 10: v, w← TRAIN ONEEPOCH (X, Y, U, v, w, λ, ξ) ▷ Optimize Eq. (1) 11: else 12: v, w← TRAIN ONEEPOCH (X, Y, v, w, λ, ξ) ▷ Optimize Eq. (1) with λU = 0 13: end if 14: me ← CALC PERF (Xval, Yval, v, w) ▷ Record performance metric on val. 15: if first try or me > m∗ then 16: v∗, w∗ ← v, w 17: λ∗ ← λ 18: m∗ ← me ▷ Update best config found so far 19: end if 20: if EARLY STOP (m1, m2, . . . , me) or time elapsed > Tthen 21: break 22: end if 23: end for 24: end while 25: return v∗, w∗, λ∗, m∗D.2. Semi-supervised method details Semi-supervised learning trains on the labeled and unlabeled data simultaneously, usually with the total loss being a weighted sum of a labeled loss term and an unlabeled loss term. Different methods mainly differs in how unlabeled data is used to form training signals. Many approaches have been proposed and refined over the past decades. These include co-training, which involves training multiple classifiers on various views of the input data [7, 61]; graph-structure-based models [46, 89]; generative models [52, 53]; consistency regularization-based models that enforce consistent model outputs [5, 57, 73]; pseudo label-based models that impute labels for unlabeled data [12, 58]; and hybrid models that combines several methods [69]. Comprehensive reviews can be found in Chapelle et al. [14], Van Engelen and Hoos [74], Zhu [88]. Among the deep classifier methods following Eq. (1), below we describe each method we selected and how its specific unlabeled loss is constructed. Pseudo-Labeling uses the current model to assign class probabilities to each sample in the unlabeled batch. If, for an unlabeled sample, the maximum class probability P(yi) exceeds a certain threshold τ, this sample contributes to the calculation of the unlabeled loss for the current batch. The cross-entropy loss is computed as if the true label of this sample is class i. Mean-Teacher constructs the unlabeled loss by enforcing consistency between the model’s output for a given sample and the output of the same sample from the Exponential Moving Average (EMA) model. MixMatch uses the MixUp [85] technique on both labeled data (features and labels) and unlabeled data (features and guessed labels) within each batch to produce transformed labeled and unlabeled data. The labeled and unlabeled losses are then calculated using these transformed samples. Specifically, the unlabeled loss is derived from the mean squared error between the model’s output for the transformed unlabeled samples and their corresponding transformed guessed labels. FixMatch generates two augmentation of an unlabeled sample, one with weak augmentation and the other using strong augmentations (e.g., RandAug [22]). The unlabeled loss is then formulated by enforcing the model’s output for the strongly augmented sample to closely resemble that of the weakly augmented sample using cross-entropy loss. FlexMatch builds directly upon FixMatch by incorporating a class-specific threshold on the unlabeled samples during training. CoMatch marks the first introduction of contrastive learning into semi-supervised learning. The model is equipped with two distinct heads: a classification head, which outputs class probabilities for a given sample, and a projection head, which maps the sample into a low-dimensional embedding. These two components interact in a unique manner. The projection head-derived embeddings inform the similarities between different samples, which are then used to refine the pseudo-labels against which the classification head is trained. Subsequently, these pseudo-labels constitute a pseudo-label graph that trains the embedding graph produced by the projection head. D.3. Self-supervised method details In recent years, self-supervised learning algorithms have emerged rapidly and are known as one of the most popular field of machine learning. These include contrastive learning, which involves learning representations by maximizing agreement between differently augmented views of the same data [17, 37]; predictive models that forecast future instances in the data sequence [63]; generative models that learn to generate new data similar to the input [16]; clustering-based approaches that learn representations by grouping similar instances [9, 10]; context-based models that predict a specific part of the data from other parts [8, 25]; and hybrid models that combine various methods for more robust learning [18]. A more comprehensive review can be found in [47, 90]. Below, we provide for each selected self-supervised method a summary of its internal workings. SimCLR generates two augmented versions of each image. Then feed these pairs of images into a base encoder network to generate image embeddings. This encoder is followed by a projection head, which is a multilayer neural network, to map these embeddings to a space where contrastive loss can be applied. Next, calculate the contrastive loss. The idea is to make the embeddings of augmented versions of the same image (positive pairs) as similar as possible and to push apart embeddings from different images (negative pairs). The loss function used is NCE loss. MOCO V2 creates two augmented versions of each image. These pairs are processed by two encoder networks: a query encoder, and a key encoder updated by a moving average of the query encoder. The contrastive loss is computed by comparing a positive pair (the query and corresponding key) against numerous negative pairs drawn from a large queue of keys. Note on runtime: We notice that the performance on MoCo can be increased when Shuffling BN across multiple GPUs. However, to ensure a fair comparison given our single-GPU setup, we refrained from employing any techniques to simulate multiple GPUs on one, as this would change the encoder’s structure.SwA Vbegins by creating multiple augmented versions of each image. Then, these versions are input into a deep neural network to generate embeddings. Uses a clustering approach, called online stratified sampling, to predict assignments of each view’s prototypes (or cluster centers) to others, encouraging the model to match the representations of different augmentations of the same image. Note on runtime: We’ve observed that applying multiple augmentations can enhance the effectiveness of various methods. To prevent the results from being influenced by these augmentations, we’ve standardized the number of augmentations to two in SwA V , in line with the approach taken by other methods. BYOL starts by creating two differently augmented versions of each image. These versions are processed through two identical neural networks, known as the target and online networks, which include a backbone and a projection head. The online network is updated through backpropagation, while the target network’s weights are updated as a moving average of the online network’s weights. The unique aspect of BYOL is that it learns representations without the need for negative samples. SimSiam creates two differently augmented versions of each image. These versions are passed through two identical networks: one predictor network and one encoder network. The encoder network contains a backbone and a projection head. DINO utilizes two differently augmented images, processed by a student and a teacher network. The teacher’s weights evolve as a moving average of the student’s. The key idea is self-distillation, where the student’s outputs match the teacher’s for one view but differ for the other, without traditional negative samples. Barlow Twins processes two augmented views of an image through identical networks. The aim is to have similar repre- sentations between these networks while minimizing redundancy in their components, sidestepping the need for contrasting positive and negative pairs. E. Additional Analysis and Discussion E.1. Effectiveness of Hyperparameter Tuning While Oliver et al. [62] caution that extensive hyperparameter search may be futile with realistic validation set. Our experi- ments on the 4 dataset show that the validation set performance for each examined algorithm rise substantially over the course of hyperparameter tuning. This increase in validation set performance further translates to increased test set performance. Given the trends we observed across 4 datasets, we think that for a chosen algorithm on a new dataset, following our hyperparameter tuning protocol (even with limited labeling budget and computation budget), we can likely expect better generalization (measured by test set performance) compared to not tuning hyperparameters at all. E.2. Differentiating Between Methods Oliver et al. [62] offer both empirical and theoretical analysis of how well one can distinguish if one method is truly better than another on a limited labeled dataset. Below, we revisit each analysis for our specific experiments. E.2.1 Empirical Analysis of Differentiation Oliver et al. [62] in their Fig 5 and 6 show that on SVHN, between 10 random samples of the validation set across several level of validation set size (1000, 500, 250, 100), the validation accuracy of the trained Pi-model, V AT, Pseudo-labeling and Mean Teacher model has substantial variability and overlap with each other. Thus, they caution that differentiating between models might be infeasible with realistic validation set size. In our present study, we employ a relaxed notion of “realistic validation set”, by letting the validation set to be at most as large as the training set. Our experiments cover validation set size 235 (TMED), 400 (Tissue), 450 (Path), 600 (AIROGS); test set size 2019 (TMED2), 47280 (Tissue), 7180 (Path), 6000 (AIROGS). Our experiment shows that within the wide range of methods considered, differentiating between some models are possible. For example, in Fig. C.4 we can see that MixMatch is clearly better than Mean Teacher in TissueMNIST and PathMNIST, in both the validation set and test set, without overlap in the intervals. The field of semi-supervised learning has made significant advancements in recent years. It is crucial to reevaluate previous conclusions in light of the new developments.E.2.2 Theoretical Analysis of Differentiation Here, we show that the performance gain we observe on the test set are real. We perform the same theoretical analysis using the Hoeffding’s inequality [38] as in Oliver et al. [62]. P(|¯V − E[V ]| < p) > 1 − 2 exp(−2np2) (3) where ¯V is the empirical estimate of some model performance metric, E[V ] is its hypothetical true value, p is the desired maximum deviation between our estimate and the true value, and n is the number of examples used. On TissueMNIST, we have 47280 test samples, we will be more than 99.98% confident that the test accuracy is within 1% of its true value. On Path, we have 7180 test samples, we will be more than 99% confident that the test accuracy is within 2% its true value. In Fig 1, we see that after hyperparameter tuning, the final test accuracy of each algorithms improves much more than 1% on TissueMNIST and 2% on PathMNIST showing the efficacy of hyperparameter tuning. Similarly, we can see that the difference between top-performing algorithms (e.g., MixMatch) and worst-performaning alogrithm (e.g., Mean Teacher) is clearly larger then 1% on TissueMNIST, 2% on PathMNIST. Thus we can argue that differentiation between certain methods are viable. The same analysis can also be applied to TMED-2 and AIROGS. E.3. Answers to Common Questions from Reviewers Here we answer a few questions that were common to several reviewers of our paper. E.3.1 For a medical image application, would a larger labeled dataset be more important than than developing semi-supervised or self-supervised methods? Yes, in general, is is preferable to collect as large of a labeled dataset as possible, at least up to the point of performance saturation. Investing in data collection likely has a larger payoff than investing in SSL. However, extensive collection of labeled examples is not practical for many real-world clinical tasks due to reasons like financial cost, logistics, privacy and legal issues (see Oliver et al. [62],Berthelot et al. [5], Shekoofeh et al. [68]). For this reason, methods for overcoming limited labeled data, such as semi-SL and self-SL, are important topics in medical imaging applications. The clinical use case of SSL motivates several recent methodological works, such Zhang et al. [86], Azizi et al. [3], and Shekoofeh et al. [68]. E.3.2 Isn’t it already well-known that hyperparameter tuning with a realistic-sized validation set is viable? When labeled data is abundant, as in common supervised learning settings, hyperparameter tuning is widely known as effective. However, our work focuses on the semi/self-SL setting, where labels are limited. We carefully reviewed semi/self- SL literature and argue that the viability of tuning on realistic-sized validation sets is not well-known in this setting. As our paper’s Table 1 shows, existing SSL benchmarks often use validation sets larger than the training set! Seminal work by Oliver et al. [62] cautions that “Extensive hyperparameter tuning may be somewhat futile due to an excessively small collection of held-out data ...”. Su et al. [71] use a similar claim to justify not doing any tuning on their Semi-Fungi dataset experiments. E.3.3 Does MixMatch outperform Flex/Fix/CoMatch because RandAug not suitable for medical imaging? In general, RandAug-type augmentation can be successful for medical imaging tasks [15, 86], though we agree that it might not be “optimal”. Instead, we hypothesize that MixMatch’s primary advantage is lower runtime cost per iteration com- pared to FixMatch and successors. In our AIROGS ResNet-18 experiments (Fig. 1), MixMatch explores at least 80% more hyperparameter combinations than its counterparts (111 vs. 59 for FixMatch).F. Hyperparameter Details F.1. Hyperparameter Tuning Strategy: Random Search Details Below, in a specific table for each of the 16 methods (supervised, semi-, or self-), we provide a method-specific table showing the random sampling distribution used for each hyperparameter for the random search of Alg. D.1. . Settings Common to All Methods Optimizer Adam Learning rate schedule Cosine F.1.1 Supervised Baselines Labeled only Batch size 64 Learning rate 3 × 10 x , X ∼ Unif (− 5, − 2) Weight decay 4 × 10 x , X ∼ Unif (− 6, − 3) MixUp Batch size 64 Learning rate 3 × 10 x , X ∼ Unif (− 5, − 2) Weight decay 4 × 10 x , X ∼ Unif (− 6, − 3) Beta shape α x, X ∼ Unif (0 .1, 10) Sup Contrast Batch size 256 Learning rate 3 × 10 x , X ∼ Unif (−5.5, −1.5) Weight decay 4 × 10 x , X ∼ Unif (−7.5, −3.5) Temperature x, X ∼ Unif (0 .05 , 0.15) F.1.2 Semi-Supervised Methods FlexMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Unlabeled loss coefficient 10x, X∼Unif(−1,1) Unlabeled loss warmup schedule No warmup Pseudo-label threshold 0.95 Sharpening temperature 1.0 FixMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Unlabeled loss coefficient 10x, X∼Unif(−1,1) Unlabeled loss warmup schedule No warmup Pseudo-label threshold 0.95 Sharpening temperature 1.0 CoMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Unlabeled loss coefficient 10x, X∼Unif(−1,1) Unlabeled loss warmup schedule No warmup Contrastive loss coefficient 5 ×10x, X∼Unif(−1,1) Pseudo-label threshold 0.95 Sharpening temperature 0.2 MixMatch Labeled batch size 64 Unlabeled batch size 64 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Beta shapeα x, X ∼Unif(0.1,1) Unlabeled loss coefficient 7.5 ×10x, X∼Unif(0,2) Unlabeled loss warmup schedule linear Sharpening temperature 0.5 Mean Teacher Labeled batch size 64 Unlabeled batch size 64 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Unlabeled loss coefficient 8 ×10x, X∼Unif(−1,1) Unlabeled loss warmup schedule linear Pseudo-label Labeled batch size 64 Unlabeled batch size 64 Learning rate 3 ×10x, X∼Unif(−5,−2) Weight decay 4 ×10x, X∼Unif(−6,−3) Unlabeled loss coefficient 10x, X∼Unif(−1,1) Unlabeled loss warmup schedule Linear Pseudo-label threshold 0.95F.1.3 Self-supervised Methods SwA V Batch size 256 Learning rate 1 × 10x, X∼ Unif (−4.5, −1.5) Weight decay 1 × 10x, X∼ Unif (−6.5, −3.5) Temperature x, X∼ Unif (0.07, 0.12) Num. prototypes 1 × 10x, X∼ Unif (1, 3) MoCo Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) Temperature x, X ∼ Unif (0 .07 , 0.12) Momentum x, X ∼ Unif (0 .99 , 0.9999) SimCLR Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) Temperature x, X ∼ Unif (0 .07 , 0.12) SimSiam Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) BYOL Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) Temperature x, X ∼ Unif (0 .07 , 0.12) Momentum x, X ∼ Unif (0 .99 , 0.9999) DINO Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) Temperature x, X ∼ Unif (0 .07 , 0.12) Momentum x, X ∼ Unif (0 .99 , 0.9999) Barlow Twins Batch size 256 Learning rate 1 × 10 x , X ∼ Unif (−4.5, −1.5) Weight decay 1 × 10 x , X ∼ Unif (−6.5, −3.5) Temperature x, X ∼ Unif (0 .07 , 0.12) Momentum x, X ∼ Unif (0 .99 , 0.9999) F.2. Hyperparameter transfer strategy To make the most of limited labeled data, one potential strategy recommended by Su et al. [71] is to use the entire labeled set for training, reserving no validation set at all. This relies on pre-established hyperparameters from other dataset/experiments. In this study, we experiment with two scenarios: using pre-determined hyperparameters tuned for CIFAR-10, or using hyper- parameters tuned for TissueMNIST. The CIFAR-10 hyperparameters are sourced from repositories published by each method’s original authors, as this is a common benchmark in the SSL literature. We ensure that each hyperparameter choice, when applied using the re-implented code for each method in our codebase, matches previously reported results on CIFAR-10. The TissueMNIST hyperparameters originate from our experiments as depicted in Figure C.2 ( a). For exact values, see App. F.2.1. For each method using the transfer strategy, we perform training on the combined train+validation set, setting the max- imum number of epochs to 100 for PathMNIST and AIROGS (80 epochs for TMED2). Training is terminated early if the train loss does not improve over 20 consecutive epochs. Empirically, we observe that all models which did not trigger early stopping reached a plateau in training loss. F.2.1 Best Hyperparameters on TissueMNIST for Semi-Supervised Methods Below we report the chosen hyperparameters on TissueMNIST for each semi-supervised method, as used in the hyperparam- eter transfer experiments. FlexMatch seed0 seed1 seed2 seed3 seed4 Learning rate 0.00036 0.00016 0.00016 0.00068 0.00006 Weight decay 0.00259 0.00001 0.00371 0.00023 0.002103 Unlabeled loss coefficient 2.22 0.82 5.00 1.94 6.09 FixMatch seed0 seed1 seed2 seed3 seed4 Learning rate 0.00074 0.00034 0.00392 0.00102 0.00037 Weight decay 0.00045 0.00315 0.00001 0.00005 0.00058 Unlabeled loss coefficient 3.08 6.70 1.85 1.46 0.47CoMatch seed0 seed1 seed2 seed3 seed4 Learning rate 0.00124 0.00145 0.00061 0.00026 0.00113 Weight decay 0.00042 0.00009 0.00005 0.00009 0.00017 Unlabeled loss coefficient 0.30 1.71 1.26 2.74 0.46 Contrastive loss coefficient 1.26 2.21 3.71 0.56 1.37 MixMatch seed0 seed1 seed2 seed3 seed4 Learning rate 0.00028 0.00003 0.00018 0.00009 0.00005 Weight decay 0.000005 0.00195 0.00005 0.00085 0.00082 Beta shapeα 0.2 0.9 0.9 0.8 0.7 Unlabeled loss coefficient 9.13 37.96 8.06 25.16 11.17 Mean Teacher seed0 seed1 seed2 seed3 seed4 Learning rate 0.00062 0.00022 0.00005 0.00128 0.00125 Weight decay 0.00189 0.00001 0.00008 0.00001 0.00001 Unlabeled loss coefficient 67.67 0.87 1.25 7.60 13.56 Pseudo-label seed0 seed1 seed2 seed3 seed4 Learning rate 0.00007 0.00021 0.00005 0.00063 0.00060 Weight decay 0.00033 0.00093 0.00383 0.00005 0.00087 Unlabeled loss coefficient 0.19 0.16 8.73 0.82 0.25 F.2.2 Best Hyperparameters on TissueMNIST for Self-Supervised Methods Below we report the chosen hyperparameters on TissueMNIST for each self-supervised method, as used in the hyperparam- eter transfer experiments. SwA V seed0 seed1 seed2 seed3 seed4 Learning rate 0.00065 0.00325 0.00012 0.00086 0.00196 Weight decay 0.0001497 0.0000056 0.0000006 0.0000021 0.0000003 Num. prototypes 845 131 36 201 59 MoCo seed0 seed1 seed2 seed3 seed4 Learning rate 0.00288 0.00023 0.00043 0.00005 0.02629 Weight decay 0.000002 0.0000008 0.0000003 0.0000005 0.0000004 temperature 0.09331 0.07097 0.10987 0.07414 0.07080 Momentum 0.99242 0.99672 0.99267 0.99950 0.99538 SimCLR seed0 seed1 seed2 seed3 seed4 Learning rate 0.00217 0.00131 0.000640 0.00380 0.00136 Weight decay 0.00002 0.00001 0.00001 0.00001 0.00001 temperature 0.11719 0.10426 0.08652 0.07784 0.11478 SimSiam seed0 seed1 seed2 seed3 seed4 Learning rate 0.0002 0.00056 0.00013 0.00338 0.00098 Weight decay 0.000066 0.000046 0.000023 0.000001 0.000001 BYOL seed0 seed1 seed2 seed3 seed4 Learning rate 0.000245 0.001308 0.000371 0.001653 0.001959 Weight decay 0.0000007 0.0000057 0.0000004 0.000003 0.000001 Momentum 0.9928618 0.996167 0.9988484 0.9940063 0.9934791 DINO seed0 seed1 seed2 seed3 seed4 Learning rate 0.000245 0.001308 0.000371 0.001653 0.001959 Weight decay 0.0000007 0.0000057 0.0000004 0.000003 0.000001 Momentum 0.9928618 0.996167 0.9988484 0.9940063 0.9934791 Barlow Twins seed0 seed1 seed2 seed3 seed4 Learning rate 0.000245 0.001308 0.000371 0.001653 0.001959 Weight decay 0.0000007 0.0000057 0.0000004 0.000003 0.000001 Momentum 0.9928618 0.996167 0.9988484 0.9940063 0.9934791",
      "references": [],
      "meta_data": {
        "arxiv_id": "2307.08919v3",
        "authors": [
          "Zhe Huang",
          "Ruijie Jiang",
          "Shuchin Aeron",
          "Michael C. Hughes"
        ],
        "published_date": "2023-07-18T01:31:47Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper provides a rigorous, fair, and realistic benchmarking of semi-supervised and self-supervised learning (SSL) for medical image classification under practical label and compute constraints. It systematically compares 13 SSL methods (6 semi-supervised, 7 self-supervised) against strong labeled-only baselines across four open medical datasets, investigates whether hyperparameter tuning is effective with realistically sized validation sets, and identifies MixMatch as a consistently strong performer. The study emphasizes practical guidance for resource-constrained practitioners and releases open-source code for reproducibility (approx. 20k+ GPU-hours of computation).",
        "methodology": "A unified objective framework that encompasses supervised, semi-supervised, and self-supervised learning (minimizing labeled loss plus an unlabeled loss term). The methodology includes: (1) a two-phase self-supervised pretraining/fine-tuning paradigm; (2) a broad set of representative methods across both paradigms (Semi: PseudoLabel, Mean Teacher, MixMatch, FixMatch, FlexMatch, CoMatch; Self: SimCLR, MoCo v2, SwAV, BYOL, SimSiam, DINO, Barlow Twins; plus three supervised baselines); (3) a fixed-time budget random-search hyperparameter tuning (Alg. D.1) with early stopping, across 1 NVIDIA A100 GPU; (4) time-sliced evaluation of validation/test performance to quantify gains; (5) consistent data augmentation and architecture choices (ResNet-18 for Tissue/PathMNIST, WideResNet-28-2 for TMED-2, ResNet-18/50 for AIROGS); (6) a structured experimental protocol to ensure fair comparisons and realistic resource usage; (7) hyperparameter transfer experiments to compare tuning-on-target vs transfer-from CIFAR-10/Tissue.",
        "experimental_setup": "Datasets: PathMNIST and TissueMNIST (MedMNIST, 28x28), TMED-2 (112x112 grayscale heart ultrasound images), and AIROGS (384x384 color fundus images). Each dataset is partitioned to mimic realistic SSL deployment with a small labeled set (roughly 2–10 classes, 30–1000 labeled images per class) and a much larger unlabeled pool; validation sets are kept no larger than the labeled training set. Splits and exact sizes are defined in the codebase at github.com/tufts-ml/SSL-vs-SSL-benchmark. Training and evaluation: models trained with Adam, cosine learning rate schedules, up to 200 epochs per phase with early stopping after 20 stagnant epochs; evaluation metric is balanced accuracy (BA). Hardware/time budgets: per-dataset time budgets of 25h (PathMNIST), 50h (TissueMNIST), 100h (TMED-2 and AIROGS) on a single NVIDIA A100 GPU; 5 trials per method to assess variance; accuracy curves and tables (e.g., BA over time, tabular gains relative to labeled-only baselines).",
        "limitations": "Limitations include focus on 2D medical images and moderate labeling budgets (30–1000 per class), potentially limiting generalization to zero-shot/few-shot or multiclass-extensive tasks. The study uses moderate-capacity CNN backbones (ResNet variants) and does not evaluate transformer-based architectures (ViT) in depth. Only balanced accuracy (and select clinical metrics for AIROGS) are reported, leaving calibration, net benefit, AUPRC, and fairness analyses for future work. Dataset distribution shifts, domain differences, and unlabeled data quality/curation are not exhaustively explored. Reproducibility depends on open-source code and specified splits, but some results may vary with different random initializations or additional unlabeled data. ",
        "future_research_directions": "Extend benchmarking to vision transformers (ViT) and other backbone families; study SSL under distribution shift or domain adaptation between labeled and unlabeled pools; explore advanced hyperparameter transfer methods and meta-learning for SSL on new medical tasks; broaden evaluation to additional clinically-relevant metrics (calibration, AUPRC, net benefit) and fairness across subpopulations; investigate open-set or out-of-distribution unlabeled data handling; apply the unified tuning protocol to more diverse medical imaging tasks and 3D data; and further release tooling to facilitate adoption by resource-constrained labs.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Graph inference learning for semi-supervised classification",
      "full_text": "Published as a conference paper at ICLR 2020 GRAPH INFERENCE LEARNING FOR SEMI -SUPERVISED CLASSIFICATION Chunyan Xu, Zhen Cui∗, Xiaobin Hong, Tong Zhang, and Jian Yang School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China {cyx,zhen.cui,xbhong,tong.zhang,csjyang}@njust.edu.cn Wei Liu Tencent AI Lab, China wl2223@columbia.edu ABSTRACT In this work, we address semi-supervised classiﬁcation of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner, but the performance could degrade signiﬁcantly when labeled data is scarce. To this end, we propose a Graph Inference Learning (GIL) framework to boost the performance of semi- supervised node classiﬁcation by learning the inference of node labels on graph topology. To bridge the connection between two nodes, we formally deﬁne a structure relation by encapsulating node attributes, between-node paths, and local topological structures together, which can make the inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted to testing nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classiﬁcation task. 1 I NTRODUCTION Graph, which comprises a set of vertices/nodes together with connected edges, is a formal structural representation of non-regular data. Due to the strong representation ability, it accommodates many potential applications, e.g., social network (Orsini et al., 2017), world wide data (Page et al., 1999), knowledge graph (Xu et al., 2017), and protein-interaction network (Borgwardt et al., 2007). Among these, semi-supervised node classiﬁcation on graphs is one of the most interesting also popular topics. Given a graph in which some nodes are labeled, the aim of semi-supervised classiﬁcation is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works (Brandes et al., 2008; Zhou et al., 2004; Zhu et al., 2003; Yang et al., 2016; Zhao et al., 2019) devoted to semi-supervised node classiﬁcation based on explicit graph Laplacian regularizations, it is hard to efﬁciently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information. With the progress of deep learning on grid-shaped images/videos (He et al., 2016), a few of graph convolutional neural networks (CNN) based methods, including spectral (Kipf & Welling, 2017) and spatial methods (Niepert et al., 2016; Pan et al., 2018; Yu et al., 2018), have been proposed to learn local convolution ﬁlters on graphs in order to extract more discriminative node representations. Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing ﬁlters, they are limited into a conventionally semi-supervised framework and lack of an efﬁcient inference mechanism on graphs. Especially, in the case of few-shot learning, where a small number of training nodes are labeled, this kind of methods would drastically compromise the performance. For example, the Pubmed graph dataset (Sen et al., 2008) consists ∗Corresponding author: Zhen Cui. 1 arXiv:2001.06137v1  [cs.LG]  17 Jan 2020Published as a conference paper at ICLR 2020       (b) The process of Graph inference learning.   We extract the local representation from the local subgraph (the circle with dashed line       The red wave line denote the node reachability from     to     .   d t  t h hb i l i t f   d t t h  d     Figure 1: The illustration of our proposed GIL framework. For the problem of graph node labeling, the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g., vj) and these labeled reference nodes (e.g., vi). We consider the similarity from three points: node attributes, the consistency of local topological structures (i.e., the circle with dashed line), and the between-node path reachability (i.e., the red wave line from vi to vj). Speciﬁcally, the local structures as well as node attributes are encoded as high-level features with graph convolution, while the between-node path reachability is abstracted as reachable probabilities of random walks. To better make the inference generalize to test nodes, we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes. of 19,717 nodes and 44,338 edges, but only 0.3% nodes are labeled for the semi-supervised node classiﬁcation task. These aforementioned works usually boil down to a general classiﬁcation task, where the model is learnt on a training set and selected by checking a validation set. However, they do not put great efforts on how to learn to infer from one node to another node on a topological graph, especially in the few-shot regime. In this paper, we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes, and ﬁnally boost the performance of semi-supervised node classiﬁcation in the case of a few number of labeled samples. Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations. The between-node relations are structured as the integration of node attributes, connection paths, and graph topological structures. It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes, the consistency of local topological structures, and the between-node path reachability, as shown in Fig. 1. The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al., 2016) for the sake of high-level feature extraction. For the between-node path reachability, we adopt the random walk algorithm to obtain the characteristics from a labeled reference node vi to a query unlabeled node vj in a given graph. Based on the computed node representation and between-node reachability, the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph. Inspired by the recent meta-learning strategy (Finn et al., 2017), we learn to infer the structure relations from a training set to a validation set, which can beneﬁt the generalization capability of the learned model. In other words, our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples, such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds: •We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way. The structure relations are well deﬁned by jointly considering node attributes, between-node paths, and graph topological structures. •To make the inference model better generalize to test nodes, we introduce a meta-learning procedure to optimize structure relations, which could be the ﬁrst time for graph node classiﬁcation to the best of our knowledge. •Comprehensive evaluations on three citation network datasets (including Cora, Citeseer, and Pubmed) and one knowledge graph data (i.e., NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classiﬁcation task. 2Published as a conference paper at ICLR 2020 2 R ELATED WORK Graph CNNs: With the rapid development of deep learning methods, various graph convolution neural networks (Kashima et al., 2003; Morris et al., 2017; Shervashidze et al., 2009; Yanardag & Vishwanathan, 2015; Jiang et al., 2019; Zhang et al., 2020) have been exploited to analyze the irregular graph-structured data. For better extending general convolutional neural networks to graph domains, two broad strategies have been proposed, including spectral and spatial convolution methods. Speciﬁcally, spectral ﬁltering methods (Henaff et al., 2015; Kipf & Welling, 2017) develop convolution-like operators in the spectral domain, and then perform a series of spectral ﬁlters by decomposing the graph Laplacian. Unfortunately, the spectral-based approaches often lead to a high computational complex due to the operation of eigenvalue decomposition, especially for a large number of graph nodes. To alleviate this computation burden, local spectral ﬁltering methods (Defferrard et al., 2016) are then proposed by parameterizing the frequency responses as a Chebyshev polynomial approximation. Another type of graph CNNs, namely spatial methods (Li et al., 2016; Niepert et al., 2016), can perform the ﬁltering operation by deﬁning the spatial structures of adjacent vertices. Various approaches can be employed to aggregate or sort neighboring vertices, such as diffusion CNNs (Atwood & Towsley, 2016), GraphSAGE (Hamilton et al., 2017), PSCN (Niepert et al., 2016), and NgramCNN (Luo et al., 2017). From the perspective of data distribution, recently, the Gaussian induced convolution model (Jiang et al., 2019) is proposed to disentangle the aggregation process through encoding adjacent regions with Gaussian mixture model. Semi-supervised node classiﬁcation: Among various graph-related applications, semi-supervised node classiﬁcation has gained increasing attention recently, and various approaches have been proposed to deal with this problem, including explicit graph Laplacian regularization and graph- embedding approaches. Several classic algorithms with graph Laplacian regularization contain the label propagation method using Gaussian random ﬁelds (Zhu et al., 2003), the regularization framework by relying on the local/global consistency (Zhou et al., 2004), and the random walk- based sampling algorithm for acquiring the context information (Yang et al., 2016). To further address scalable semi-supervised learning issues (Liu et al., 2012), the Anchor Graph regularization approach (Liu et al., 2010) is proposed to scale linearly with the number of graph nodes and then applied to massive-scale graph datasets. Several graph convolution network methods (Abu-El-Haija et al., 2018; Du et al., 2017; Thekumparampil et al., 2018; Velickovic et al., 2018; Zhuang & Ma, 2018) are then developed to obtain discriminative representations of input graphs. For example, Kipf et al. (Kipf & Welling, 2017) proposed a scalable graph CNN model, which can scale linearly in the number of graph edges and learn graph representations by encoding both local graph structures and node attributes. Graph attention networks (GAT) (Velickovic et al., 2018) are proposed to compute hidden representations of each node for attending to its neighbors with a self-attention strategy. By jointly considering the local- and global-consistency information, dual graph convolutional networks (Zhuang & Ma, 2018) are presented to deal with semi-supervised node classiﬁcation. The critical difference between our proposed GIL and those previous semi-supervised node classiﬁcation methods is to adopt a graph inference strategy by deﬁning structure relations on graphs and then leverage a meta optimization mechanism to learn an inference model, which could be the ﬁrst time to the best of our knowledge, while the existing graph CNNs take semi-supervised node classiﬁcation as a general classiﬁcation task. 3 T HE PROPOSED MODEL 3.1 P ROBLEM DEFINITION Formally, we denote an undirected/directed graph as G= {V,E,X,Y}, where V= {vi}n i=1 is the ﬁnite set of n(or |V|) vertices, E∈ Rn×n deﬁnes the adjacency relationships (i.e., edges) between vertices representing the topology of G, X∈ Rn×d records the explicit/implicit attributes/signals of vertices, and Y∈ Rn is the vertex labels of Cclasses. The edge Eij = E(vi,vj) = 0 if and only if vertices vi,vj are not connected, otherwise Eij ̸= 0. The attribute matrix Xis attached to the vertex set V, whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi. It means that vi ∈V carries a vector of d-dimensional signals. Associated with each node vi ∈V, there is a discrete label yi ∈{1,2,··· ,C}. We consider the task of semi-supervised node classiﬁcation over graph data, where only a small number of vertices are labeled for the model learning, i.e., |VLabel|≪|V| . Generally, we have three node sets: a training set Vtr, a validation set Vval, and a testing set Vte. In the standard protocol 3Published as a conference paper at ICLR 2020 of prior literatures (Yang et al., 2016), the three node sets share the same label space. We follow but do not restrict this protocol for our proposed method. Given the training and validation node sets, the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections. A sophisticated machine learning technique used in most existing methods (Kipf & Welling, 2017; Zhou et al., 2004) is to choose the optimal classiﬁer (trained on a training set) after checking the performance on the validation set. However, these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes, as the graph structure itself implies node connectivity/reachability. Moreover, due to the scarcity of labeled samples, the performance of such a classiﬁer is usually not satisfying. To address these issues, we introduce a meta-learning mechanism (Finn et al., 2017; Ravi & Larochelle, 2017; Sung et al., 2017) to learn to infer node labels on graphs. Speciﬁcally, the graph structure, between-node path reachability, and node attributes are jointly modeled into the learning process. Our aim is to learn to infer from labeled nodes to unlabeled nodes, so that the learner can perform better on a validation set and thus classify a testing set more accurately. 3.2 S TRUCTURE RELATION For convenient inference, we speciﬁcally build a structure relation between two nodes on the topology graph. The labeled vertices (in a training set) are viewed as the reference nodes, and their information can be propagated into those unlabeled vertices for improving the label prediction accuracy. Formally, given a reference node vi ∈VLabel, we deﬁne the score of a query node vj similar to vi as si→j = fr(fe(Gvi),fe(Gvj ),fP(vi,vj,E)), (1) where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj, respectively. fe,fr,fPare three abstract functions that we explain as follows: •Node representation fe(Gvi) −→Rdv , encodes the local representation of the centralized subgraph Gvi around node vi, and may thus be understood as a local ﬁlter function on graphs. This function should not only take the signals of nodes therein as input, but also consider the local topological structure of the subgraph for more accurate similarity computation. To this end, we perform the spectral graph convolution on subgraphs to learn discriminative node features, analogous to the pixel-level feature extraction from convolution maps of gridded images. The details of feature extraction fe are described in Section 4. •Path reachability fP(vi,vj,E) −→Rdp, represents the characteristics of path reachability from vi to vj. As there usually exist multiple traversal paths between two nodes, we choose the function as reachable probabilities of different lengths of walks from vi to vj. More details will be introduced in Section 4. •Structure relation fr(Rdv ,Rdv ,Rdp) −→R, is a relational function computing the score of vj similar to vi. This function is not exchangeable for different orders of two nodes, due to the asymmetric reachable relationship fP. If necessary, we may easily revise it as a symmetry function, e.g., summarizing two traversal directions. The score function depends on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and fe(Gvj ), respectively, and the path reachability from vi to vj. In semi-supervised node classiﬁcation, we take the training node set Vtr as the reference samples, and the validation set Vval as the query samples during the training stage. Given a query node vj ∈Vval, we can derive the class similarity score of vj w.r.t. the c-th (c= 1,··· ,C) category by weighting the reference samples Cc = {vk|yvk = c}. Formally, we can further revise Eqn. (1) and deﬁne the class-to-node relationship function as sCc→j = φr(FCc→vj ∑ vi∈Cc wi→j ·fe(Gvi),fe(Gvj )), (2) s.t. wi→j = φw(fP(vi,vj,E)), (3) where the function φw maps a reachable vector fP(vi,vj,E) into a weight value, and the function φr computes the similar score between vj and the c-th class nodes. The normalization factor FCc→vj of the c-th category w.r.t. vj is deﬁned as FCc→vj = 1∑ vi∈Cc wi→j . (4) For the relation function φr and the weight function φw, we may choose some subnetworks to instantiate them in practice. The detailed implementation of our model can be found in Section 4. 4Published as a conference paper at ICLR 2020 3.3 I NFERENCE LEARNING According to the class-to-node relationship function in Eqn. (2), given a query nodevj, we can obtain a score vector sC→j = [sC1→j,··· ,sCC→j]⊺ ∈RC after computing the relations to all classes . The indexed category with the maximum score is assumed to be the estimated label. Thus, we can deﬁne the loss function based on cross entropy as follows: L= − ∑ vj C∑ c=1 yj,clog ˆyCc→j, (5) where yj,c is a binary indicator (i.e., 0 or 1) of class label cfor node vj, and the softmax operation is imposed on sCc→j, i.e., ˆyCc→j = exp(sCc→j)/∑C k=1 exp(sCk→j). Other error functions may be chosen as the loss function, e.g., mean square error. In the regime of general classiﬁcation, the cross entropy loss is a standard one that performs well. Given a training set Vtr, we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr. Given a trained/pretrained model Θ = {fe,φw,φr}, we perform iteratively gradient updates on the training set Vtr to obtain the new model, formally, Θ′= Θ −α∇ΘLtr(Θ), (6) where αis the updating rate. Note that, in the computation of class scores, since the reference node and query node can be both from the training set Vtr, we set the computation weight wi→j = 0 if i= jin Eqn. (3). After several iterates of gradient descent on Vtr, we expect a better performance on the validation set Vval, i.e., min Θ Lval(Θ′). Thus, we can perform the gradient update as follows Θ = Θ −β∇ΘLval(Θ′), (7) where βis the learning rate of meta optimization (Finn et al., 2017). During the training process, we may perform batch sampling from training nodes and validation nodes, instead of taking all one time. In the testing stage, we may take all training nodes and perform the model update according to Eqn. (6) like the training process. The updated model is used as the ﬁnal model and is then fed into Eqn. (2) to infer the class labels for those query nodes. 4 M ODULES In this section, we instantiate all modules (i.e., functions) of the aforementioned structure relation. The implementation details can be found in the following. Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing the graph convolution operation on subgraph Gvi. Similar to gridded images/videos, on which local convolution kernels are deﬁned as multiple lattices with various receptive ﬁelds, the spectral graph convolution is used to encode the local representations of an input graph in our work. Given a graph sample G = {V,E,X}, the normalized graph Laplacian matrix is L = In − D−1/2ED−1/2 = UΛUT, with a diagonal matrix of its eigenvalues Λ. The spectral graph convo- lution can be deﬁned as the multiplication of signal Xwith a ﬁlter gθ(Λ) = diag(θ) parameterized by θ in the Fourier domain: conv(X) = gθ(L) ∗X = Ugθ(Λ)UTX, where parameter θ ∈Rn is a vector of Fourier coefﬁcients. To reduce the computational complexity and obtain the local information, we use an approximate local ﬁlter of the Chebyshev polynomial (Defferrard et al., 2016), gθ(Λ) = ∑K−1 k=0 θkTk(ˆΛ),where parameter θ ∈RK is a vector of Chebyshev coefﬁcients and Tk(ˆΛ) ∈Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ /λmax −In, a diagonal matrix of scaled eigenvalues. The graph ﬁltering operation can then be expressed as gθ(Λ) ∗X = ∑K−1 k=0 θkTk(ˆL)X, where Tk(ˆL) ∈Rn×n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian ˆL = 2L/λmax−In. Further, we can construct multi-scale receptive ﬁelds for each vertex based on the Laplacian matrix L, where each receptive ﬁeld records hopping neighborhood relationships around the reference vertex vi, and forms a local centralized subgraph. Path Reachability fP(vi,vj,E): Here we compute the probabilities of paths from vertex ito vertex jby employing random walks on graphs, which refers to traversing the graph fromvi to vj according to the probability matrix P. For the input graph Gwith nvertices, the random-walk transition matrix 5Published as a conference paper at ICLR 2020 Datasets Nodes Edges Classes Features Label Rates Cora 2,708 5,429 7 1,433 0.052 Citeseer 3,327 4,732 6 3,703 0.036 Pubmed 19,717 44,338 3 500 0.003 NELL 65,755 266,144 210 5,414 0.001 Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised classiﬁcation task. can be deﬁned as P = D−1E, where D∈ Rn×n is the diagonal degree matrix with Dii = ∑ iEij. That is to say, each element Pij is the probability of going from vertex ito vertex jin one step. The sequence of nodes from vertexito vertex jis a random walk on the graph, which can be modeled as a classical Markov chain by considering the set of graph vertices. To represent this formulation, we show that Pt ij is the probability of getting from vertex vi to vertex vj in tsteps. This fact is easily exhibited by considering a t-step path from vertex vi to vertex vj as ﬁrst taking a single step to some vertex h, and then taking t−1 steps to vj. The transition probability Pt in tsteps can be formulated as Pt ij =    Pij if t= 1 ∑ h PihPt−1 h,j if t> 1 , (8) where each matrix entry Pt ij denotes the probability of starting at vertex iand ending at vertex jin t steps. Finally, the node reachability from vi to vj can be written as a dp-dimensional vector: fP(vi,vj,E) = [Pij,P2 ij,...,P dp ij ], (9) where dp refers to the step length of the longest path from vi to vj. Class-to-Node Relationship sCc→j: To deﬁne the node relationship si→j from vi to vj, we simulta- neously consider the property of path reachability fP(vi,vj,E), local representations fe(Gvi), and fe(Gvj ) of nodes vi,vj. The function φw(fP(vi,vj,E)) in Eqn. (3), which is to map the reachable vector fP(vi,vj,E) ∈Rdp into a weight value, can be implemented with two 16-dimensional fully connected layers in our experiments. The computed value wi→j can be further used to weight the local features at node vi, fe(Gvi) ∈Rdv . For obtaining the similar score between vj and the c-th class nodes Cc in Eqn. (2), we perform a concatenation of two input features, where one refers to the weighted features of vertex vi, and another is the local features of vertex vj. One fully connected layer (w.r.t. φr) with C-dimensions is ﬁnally adopted to obtain the relation regression score. 5 E XPERIMENTS 5.1 E XPERIMENTAL SETTINGS We evaluate our proposed GIL method on three citation network datasets: Cora, Citeseer, Pubmed (Sen et al., 2008), and one knowledge graph NELL dataset (Carlson et al., 2010). The statistical properties of graph data are summarized in Table 1. Following the previous protocol in (Kipf & Welling, 2017; Zhuang & Ma, 2018), we split the graph data into a training set, a validation set, and a testing set. Taking into account the graph convolution and pooling modules, we may alternately stack them into a multi-layer Graph convolutional network. The GIL model consists of two graph convolution layers, each of which is followed by a mean-pooling layer, a class-to-node relationship regression module, and a ﬁnal softmax layer. We have given the detailed conﬁguration of the relationship regression module in the class-to-node relationship of Section 4. The parameter dp in Eqn. (9) is set to the mean length of between-node reachability paths in the input graph. The channels of the 1-st and 2-nd convolutional layers are set to 128 and 256, respectively. The scale of the respective ﬁled is 2 in both convolutional layers. The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-ﬁtting, and the ReLU unit is leveraged as a nonlinear activation function. We pre-train our proposed GIL model for 200 iterations with the training set, where its initial learning rate, decay factor, and momentum are set to 0.05, 0.95, and 0.9, respectively. Here we train the GIL model using the stochastic gradient descent method with the batch size of 100. We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set, where the meta-learning rates αand βare both set to 0.001. 6Published as a conference paper at ICLR 2020 5.2 C OMPARISON WITH STATE -OF-THE -ARTS We compare the GIL approach with several state-of-the-art methods (Monti et al., 2017; Kipf & Welling, 2017; Zhou et al., 2004; Zhuang & Ma, 2018) over four graph datasets, including Cora, Citeseer, Pubmed, and NELL. The classiﬁcation accuracies for all methods are reported in Table 2. Our proposed GIL can signiﬁcantly outperform these graph Laplacian regularized methods on four graph datasets, including Deep walk (Zhou et al., 2004), modularity clustering (Brandes et al., 2008), Gaussian ﬁelds (Zhu et al., 2003), and graph embedding (Yang et al., 2016) methods. For example, we can achieve much higher performance than the deepwalk method (Zhou et al., 2004), e.g., 43.2% vs 74.1% on the Citeseer dataset, 65.3% vs 83.1% on the Pubmed dataset, and 58.1% vs 78.9% on the NELL dataset. We ﬁnd that the graph embedding method (Yang et al., 2016), which has considered both label information and graph structure during sampling, can obtain lower accuracies than our proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset, respectively. This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization. We further compare our proposed GIL with several existing deep graph embedding methods, including graph attention network (Velickovic et al., 2018), dual graph convolutional networks (Zhuang & Ma, 2018), topology adaptive graph convolutional networks (Du et al., 2017), Multi-scale graph convolution (Abu-El-Haija et al., 2018), etc. For example, our proposed GIL achieves a very large gain, e.g., 86.2% vs 83.3% (Du et al., 2017) on the Cora dataset, and 78.9% vs 66.0% (Kipf & Welling, 2017) on the NELL dataset. We evaluate our proposed GIL method on a large graph dataset with a lower label rate, which can signiﬁcantly outperform existing baselines on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma, 2018), 4.1% over classic GCN (Kipf & Welling, 2017) and TAGCN (Du et al., 2017), 3.2% over AGNN (Thekumparampil et al., 2018), and 3.6% over N-GCN (Abu-El-Haija et al., 2018). It demonstrates that our proposed GIL performs very well on various graph datasets by building the graph inference learning process, where the limited label information and graph structures can be well employed in the predicted framework. Table 2: Performance comparisons of semi-supervised classiﬁcation methods. Methods Cora Citeseer Pubmed NELL Clustering (Brandes et al., 2008) 59.5 60.1 70.7 21.8 DeepWalk (Zhou et al., 2004) 67.2 43.2 65.3 58.1 Gaussian (Zhu et al., 2003) 68.0 45.3 63.0 26.5 G-embedding (Yang et al., 2016) 75.7 64.7 77.2 61.9 DCNN (Atwood & Towsley, 2016) 76.8 - 73.0 - GCN (Kipf & Welling, 2017) 81.5 70.3 79.0 66.0 MoNet (Monti et al., 2017) 81.7 - 78.8 - N-GCN (Abu-El-Haija et al., 2018) 83.0 72.2 79.5 - GAT (Velickovic et al., 2018) 83.0 72.5 79.0 - AGNN (Thekumparampil et al., 2018) 83.1 71.7 79.9 - TAGCN (Du et al., 2017) 83.3 72.5 79.0 - DGCN (Zhuang & Ma, 2018) 83.5 72.6 80.0 74.2 Our GIL 86.2 74.1 83.1 78.9 5.3 A NALYSIS Meta-optimization: As can be seen in Table 3, we report the classiﬁcation accuracies of semi-supervised classiﬁcation with several variants of our proposed GIL and the classical GCN method (Kipf & Welling, 2017) when evaluating them on the Cora dataset. For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process, we report the classiﬁcation accuracies of GCN (Kipf & Welling, 2017) and our proposed GIL on the Cora dataset under two different situations, including “only learning with the training setVtr\" and “with jointly learning on a training set Vtr and a validation set Vval\". “GCN /w jointly learning on Vtr & Vval\" achieves a better result than “GCN /w learning onVtr\" by 3.6%, which demonstrates that the network performance can be improved by employing validation samples. When using structure relations, “GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”), which can be attributed to the building connection between nodes. The meta-optimization strategy (“GIL /w meta-training from Vtr →Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%, which indicates that a good inference capability can be learnt through meta-optimization. It is worth noting that, GIL adopts a meta-optimization strategy to learn the inference model, which is a process of migrating 7Published as a conference paper at ICLR 2020 from a training set to a validation set. In other words, the validation set is only used to teach the model itself how to transfer to unseen data. In contrast, the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset. Methods Acc. (%) GCN (Kipf & Welling, 2017) /w learning on Vtr 81.4 /w jointly learning on Vtr & Vval 84.0 GIL /w learning on Vtr 83.3 /w meta-train Vtr → Vval 86.2 GIL+mean pooling /w 1 conv. layer 84.5 /w 2 conv. layers 86.2 /w 3 conv. layers 85.4 GIL+2 conv. layers /w max-pooling 85.2 /w mean pooling 86.2 Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling mechanism, but with different numbers of convolutional layers, i.e., “GIL + mean pooling\" with one, two, and three convolutional layers, respectively. As can be seen in Table 3, the proposed GIL with two convolutional layers can obtain a better performance on the Cora data than the other two network settings (i.e., GIL with one or three convolutional layers). For example, the performance of ‘GIL /w 1 conv. layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2 conv. layers + mean pooling\" on the Cora dataset. Furthermore, we report the classiﬁcation results of our proposed GIL by using mean and max-pooling mechanisms, respectively. GIL with mean pooling (i.e., “GIL /w 2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e., “GIL /w 2 conv layers + max-pooling\"), e.g., 86.2% vs 85.2% on the Cora graph dataset. The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings, but when increasing the network layers, more parameters of a certain graph model need to be optimized, which may lead to the over-ﬁtting issue. Inﬂuence of different between-node steps: We compare the classiﬁcation performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling, 2017), as illustrated in Fig. 2(a). The length of between-node steps can be computed with the shortest path between reference nodes and query nodes. When the step between nodes is smaller, both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set. The reason may be that the node category information may be disturbed by its nearest neighboring nodes with different labels and fewer nodes are within 1 or 2 steps in the testing set. The GIL and GCN methods can infer the category information for a part of unlabeled nodes by adopting node attributes, when two nodes are not connected in the graph (i.e., step= ∞). By increasing the length of reachability path, the inference process of the GIL method would become difﬁcult and more graph structure information may be employed in the predicted process. GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps, which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. 1 3 5 7 9 11 step 0.0 0.2 0.4 0.6 0.8accuracy our GIL GCN (a) label rate 0.30% 0.60% 0.90% 1.20% 1.50% 1.80% GCN 0.792 0.797 0.805 0.824 0.829 0.834 GIL(ours) 0.817 0.824 0.831 0.836 0.838 0.842 1x 2x 3x 4x 5x 6x 77.0% 79.0% 81.0% 83.0% 85.0% 1x 2x 3x 4x 5x 6x GCN GIL(ours) Label rates  Accuracy (b) Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset. The accuracy equals to the number of correctly classiﬁed nodes divided by all testing samples, and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset. 8Published as a conference paper at ICLR 2020 Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method with different label rates, and the detailed results on the Pubmed dataset can be shown in Fig. 2(b). When label rates increase by multiplication, the performances of GIL and GCN are improved, but the relative gain becomes narrow. The reason is that, the reachable path lengths between unlabeled nodes and labeled nodes will be reduced with the increase of labeled nodes, which will weaken the effect of inference learning. In the extreme case, labels of unlabeled nodes could be determined by those neighbors with the 1 ∼2 step reachability. In summary, our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classiﬁcation task. Inference learning process: Classiﬁcation errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig. 3. Classiﬁcation errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations, while they are with a slow descent from 400 iterations to 1200 iterations. It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes. The performance of semi-supervised classiﬁcation can be further increased by improving the generalized capability of the Graph CNN model. the number of iterations  error  Figure 3: Classiﬁcation errors of different itera- tions on the validation set of the Cora dataset. Table 4: Performance comparisons with different mod- ules on the Cora dataset, where fe, fP, and fr denote node representation, path reachability, and structure re- lation, respectively. fe fr fP Acc.(%) - - - 56.0 ✓ - - 81.5 ✓ ✓ - 85.0 ✓ ✓ ✓ 86.2 Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework, including node representation fe, path reachability fP, and structure relation fr. Note that the last one fr deﬁnes on the former two ones, so we consider the cases in Table 4 by adding modules. When not using all modules, only original attributes of nodes are used to predict labels. The case of only using fe belongs to the GCN method, which can achieve 81.5% on the Cora dataset. The large gain of using the relation module fr (i.e., from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe. The path information fPcan further boost the performance by 1.2%, e.g., 86.2% vs 85.0%. It demonstrates that three different modules of our method can improve the graph inference learning capability. Computational complexity: For the computational complexity of our proposed GIL, the cost is mainly spent on the computations of node representation, between-node reachability, and class-to- node relationship, which are about O((ntr + nte) ∗e∗din ∗dout), O((ntr + nte) ∗e∗P), and O(ntr ∗nted2 out), respectively. ntr and nte refer to the numbers of training and testing nodes, din and dout denote the input and output dimensions of node representation, eis about the average degree of graph node, and P is the step length of node reachability. Compared with those classic Graph CNNs (Kipf & Welling, 2017), our proposed GIL has a slightly higher cost due to an extra inference learning process, but can complete the testing stage with several seconds on these benchmark datasets. 6 C ONCLUSION In this work, we tackled the semi-supervised node classiﬁcation task with a graph inference learning method, which can better predict the categories of these unlabeled nodes in an end-to-end framework. We can build a structure relation for obtaining the connection between any two graph nodes, where node attributes, between-node paths, and graph structure information can be encapsulated together. For better capturing the transferable knowledge, our method further learns to transfer the mined knowledge from the training samples to the validation set, ﬁnally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set. The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem, even in the few-shot paradigm. In the future, we would extend the graph inference method to handle more graph-related tasks, such as graph generation and social network analysis. 9Published as a conference paper at ICLR 2020 ACKNOWLEDGMENT This work was supported by the National Natural Science Foundation of China (Nos. 61972204, 61906094, U1713208), the Natural Science Foundation of Jiangsu Province (Grant Nos. BK20191283 and BK20190019), and Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201922). REFERENCES Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-gcn: Multi-scale graph convolution for semi-supervised node classiﬁcation. arXiv preprint arXiv:1802.08888, 2018. James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NeurIPS, pp. 1993– 2001, 2016. Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, and Nicol N Schraudolph. Graph ker- nels for disease outcome prediction from protein-protein interaction networks. Paciﬁc Symposium on Biocomputing Paciﬁc Symposium on Biocomputing, pp. 4–15, 2007. Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski, and Dorothea Wagner. On modularity clustering. IEEE transactions on knowledge and data engineering, 20(2):172–188, 2008. Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010. Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NeurIPS, pp. 3844–3852, 2016. Jian Du, Shanghang Zhang, Guanhang Wu, José MF Moura, and Soummya Kar. Topology adaptive graph convolutional networks. arXiv preprint arXiv:1710.10370, 2017. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pp. 1126–1135, 2017. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, pp. 1025–1035, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770–778, 2016. Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015. Jiatao Jiang, Zhen Cui, Chunyan Xu, and Jian Yang. Gaussian-induced convolution for graphs. In AAAI, volume 33, pp. 4007–4014, 2019. Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs. In ICML, pp. 321–328, 2003. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. ICLR, 2016. Wei Liu, Junfeng He, and Shih-Fu Chang. Large graph construction for scalable semi-supervised learning. In ICML, 2010. Wei Liu, Jun Wang, and Shih-Fu Chang. Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE, 100(9):2624–2638, 2012. Zhiling Luo, Ling Liu, Jianwei Yin, Ying Li, and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks. IEEE Transactions on Knowledge and Data Engineering, 29(10): 2125–2139, 2017. 10Published as a conference paper at ICLR 2020 Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR, pp. 5115–5124, 2017. Christopher Morris, Kristian Kersting, and Petra Mutzel. Glocalized weisfeiler-lehman graph kernels: Global-local feature maps of graphs. In ICDM, pp. 327–336. IEEE, 2017. Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In ICML, pp. 2014–2023, 2016. Francesco Orsini, Daniele Baracchi, and Paolo Frasconi. Shift aggregate extract networks. arXiv preprint arXiv:1703.05537, 2017. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, 1999. Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. In IJCAI, pp. 2609–2615, 2018. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008. Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efﬁcient graphlet kernels for large graph comparison. In Artiﬁcial Intelligence and Statistics, pp. 488–495, 2009. Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic networks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017. Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. ICLR, 2018. Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In CVPR, pp. 5410–5419, 2017. Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In SIGKDD, pp. 1365–1374, 2015. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. ICML, 2016. Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep learning framework for trafﬁc forecasting. In IJCAI, pp. 3634–3640, 2018. Tong Zhang, Zhen Cui, Chunyan Xu, Wenming Zheng, and Jian Yang. Variational pathway reasoning for eeg emotion recognition. In AAAI, 2020. Wenting Zhao, Zhen Cui, Chunyan Xu, Chengzheng Li, Tong Zhang, and Jian Yang. Hashing graph convolution for node classiﬁcation. In CIKM, 2019. Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. In NeurIPS, pp. 321–328, 2004. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML, pp. 912–919, 2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised classiﬁcation. In WWW, pp. 499–508, 2018. 11",
      "references": [],
      "meta_data": {
        "arxiv_id": "2001.06137v1",
        "authors": [
          "Chunyan Xu",
          "Zhen Cui",
          "Xiaobin Hong",
          "Tong Zhang",
          "Jian Yang",
          "Wei Liu"
        ],
        "published_date": "2020-01-17T02:52:30Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Graph Inference Learning (GIL) for semi-supervised node classification on graphs. Introduces a structured between-node relation that combines (i) node attribute consistency, (ii) local topological structure similarity via graph convolutions on subgraphs, and (iii) between-node path reachability via random walks. Builds a meta-learning objective to adapt the learned structure-relations from training nodes to validation nodes, enabling better generalization to unseen test nodes. Demonstrates state-of-the-art performance on four datasets (Cora, Citeseer, Pubmed, NELL) under low-label regimes.",
        "methodology": "Defines si→j = fr(fe(Gvi), fe(Gvj), fP(vi,vj,E)); fe is spectral graph convolution on subgraphs around vi and vj; fP captures path reachability as a dp-length random-walk probabilities; φw maps fP to a weight wi→j; φr computes class similarity scores using weighted features; class-to-node scores sCc→j = φr( Fc c→vj ∑ vi∈Cc wi→j * fe(Gvi), fe(Gvj) ); training uses cross-entropy L; meta-optimization: update Θ via Ltr then meta-update on Lval to better transfer; testing uses updated Θ to infer labels.",
        "experimental_setup": "Datasets: Cora (2708 nodes, 5429 edges, 7 classes, 1433 features, 0.052 label rate), Citeseer (3327, 4732, 6, 3703, 0.036), Pubmed (19717, 44338, 3, 500, 0.003), NELL (65755, 266144, 210, 5414, 0.001). Model: two graph convolution layers with mean-pooling; meta-learning rates alpha, beta; pretrain 200 iterations, then meta-learning 1200 iterations; batch sizes 100; dropout 0.5; learning rate 0.05 with decay. Compared against baseline methods including GCN, GAT, etc. Evaluation by accuracy on test set.",
        "limitations": "The GIL framework introduces extra computations over vanilla GCN, leading to higher computational complexity (inference-learning component adds overhead). Performance is sensitive to label rate and graph characteristics; relies on subgraph extraction and path reachability computations via random walks; hyperparameters such as the path length dp and the number of GCN layers affect performance and may risk overfitting at deeper configurations; requires a validation split for meta-learning which may not always be available or representative in all scenarios.",
        "future_research_directions": "Extend GIL to other graph-related tasks such as graph generation and social network analysis; explore alternative structure-relations or attention mechanisms to further improve transferability; scale to very large graphs and dynamic graphs; develop unsupervised or self-supervised variants of graph inference learning; investigate robustness to noisy labels and missing edges.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification",
      "full_text": "Hypergraph-enhanced Dual Semi-supervised Graph Classification Wei Ju1 Zhengyang Mao 1 Siyu Yi∗ 2 Yifang Qin 1 Yiyang Gu 1 Zhiping Xiao 3 Yifan Wang4 Xiao Luo 3 Ming Zhang∗ 1 Abstract In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs. Despite the promising capability of graph neu- ral networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized. Moreover, GNNs are inherently limited to encoding local neighborhood information us- ing message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes. To tackle these challenges, we pro- pose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classifi- cation, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to better explore the higher-order relationships among nodes, we de- sign a hypergraph structure learning to adaptively learn complex node dependencies beyond pair- wise relations. Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby bet- ter mining the underlying semantic structures. Fi- nally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance. Ex- tensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods. 1School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China 2School of Statis- tics and Data Science, Nankai University, China3Department of Computer Science, University of California, Los Angeles, USA 4School of Information Technology & Management, University of International Business and Economics, China. Correspon- dence to: Siyu Yi <siyuyi@mail.nankai.edu.cn>, Ming Zhang <mzhang cs@pku.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Graph classification, which involves identifying the class labels of graphs, is a significant problem with diverse prac- tical applications in various fields. Data originating from domains such as bioinformatics, chemoinformatics, and so- cial network analysis, can naturally be represented as graphs. For example, molecules in chemoinformatics can be repre- sented as graphs by viewing atoms as nodes and chemical bonds between pairs of atoms as edges. The objective of this task is to effectively recognize the class label of each graph, such as predicting the quantum mechanical properties (Hao et al., 2020; Ju et al., 2023a) and assessing the functionality of chemical compounds (Kojima et al., 2020). To solve this problem, early methods leverage the idea of graph kernels that compute a similarity measure between graphs by comparing their substructures (Kashima et al., 2003; Shervashidze et al., 2009; 2011), and have been proven to effectively capture the structural properties of graphs. Despite their efficacy, graph kernels may face chal- lenges in scalability and computational efficiency when dealing with large datasets or complex graphs. Recently, graph neural networks (GNNs) (Kipf & Welling, 2016; Ju et al., 2024a) have emerged as a prominent and powerful paradigm for graph classification (Mao et al., 2023; Yi et al., 2023b; Luo et al., 2023c). The key idea of GNNs is to learn effective graph representations by iteratively aggregating information from neighboring nodes (Gilmer et al., 2017), which have achieved remarkable success. However, most prevailing methods typically follow the framework of supervised learning, which demands a sub- stantial amount of labeled graphs to train GNN models. However, in the field of graph analytics, obtaining labeled graphs can be a costly endeavor. Annotating graph data re- quires expert domain knowledge, manual efforts, and often extensive human involvement, making the process time- consuming and expensive. For instance, molecular labels are often acquired through costly Density Functional Theory (DFT) calculations or generated from complex experiments in the field of chemistry (Hao et al., 2020; Ju et al., 2023a). This scarcity of labeled graphs and the high cost of annota- tion pose significant challenges for developing accurate and robust GNNs for graph classification. 1 arXiv:2405.04773v2  [cs.LG]  28 May 2024Hypergraph-enhanced Dual Semi-supervised Graph Classification This inspires us to study semi-supervised graph classifica- tion, where we leverage both labeled and unlabeled graphs. Despite the unavailability of properties (i.e., labels) in the unlabeled graphs, their structures contain valuable informa- tion that could potentially enrich the capabilities of GNNs if utilized effectively. Actually, there are several approaches along this line (Li et al., 2019; Hao et al., 2020; Luo et al., 2022; Ju et al., 2022; 2023b). ASGN (Hao et al., 2020) adopts a teacher-student framework to fully utilize labeled and unlabeled graphs. DualGraph (Luo et al., 2022) incor- porates contrastive learning to encourage the consistency of unlabeled graphs in a dual manner. KGNN (Ju et al., 2022) and TGNN (Ju et al., 2023b) unify the GNNs and graph kernels in a semi-supervised framework. Despite the encouraging performance achieved by existing methods, they still suffer from two key limitations. First, GNNs are restricted to capturing only low-order local neigh- borhood information and struggle to model high-order de- pendencies between nodes. For instance, in chemoinformat- ics, GNNs may be unable to effectively capture the complex interactions and long-range dependencies between atoms in a molecule, thereby potentially limiting their ability to accurately predict properties like molecular activity or tox- icity. Second, the utilization of unlabeled graphs remains underexploited, despite their containing valuable structural information. The unlabeled graphs can act as a regular- izer, facilitating the exploration of intrinsic graph semantics, even in the presence of a scarce amount of labeled graphs. For example, in social network analysis, there might be a large amount of unlabeled user interaction graphs, yet these graphs hold insightful community structures and social re- lationships. As such, we are highly desired to look for an approach that is able to better capture high-order dependen- cies among nodes and meanwhile sufficiently leverage the unlabeled graphs to overcome the scarcity of labeled graphs. To address these challenges, in this paper we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification. The key idea of HEAL is to capture graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to explore the intricate interdependencies among nodes, we de- velop a learnable hypergraph structure learning, which pos- sesses the remarkable ability to adaptively acquire higher- order node relationships beyond pairwise connections, and is more flexible to model complex data structures than pre- defined hypergraph construction. Moreover, due to the pres- ence of higher-order semantic interactions in complex data structures, we hence leverage the learned hypergraph to in- troduce a line graph, effectively capturing the interactions between hyperedges, thus unlocking deeper insights into the underlying semantic structures of graphs. Finally, since the hypergraph and the line graph explore graph semantics at dif- ferent levels of higher-order structures, it is crucial to jointly train these two branches to enable mutual knowledge trans- fer between them. We thus present relational consistency learning, in which two branches are required to produce consistent similarity scores for each unlabeled graph. By encouraging the consistency between two similarity distri- butions, our method effectively enhances the potential of the model by fully using unlabeled graphs, thereby better serv- ing the semi-supervised graph classification. Experiments validate the effectiveness of our proposed model HEAL. 2. Problem Definition & Preliminaries Definition 1. A graph can be defined as G = (V, E,X, y), where V is a set of nodes, and E is a set of edges. X ∈ R|V |×df denotes the feature matrix of nodes, where df is the dimension of features. y is the class label of graph G and A ∈ {0, 1}|V |×|V | represents the adjacency matrix. Definition 2. A hypergraph is a generalization of a graph where edges are allowed to connect more than two nodes. Formally, a hypergraph is represented as H = (V, EH), where V is the node set same as in graph G, and EH is the set of hyperedges, which can contain any number of nodes. Each hyperedge eh ∈ EH is a subset of the node set V . Definition 3. A line graph of the hypergraph is de- fined as a graph L(H) = (VL, EL), where each node vl ∈ VL corresponds to an edge in H, and two nodes in VL are adjacent in L(H) if and only if the corresponding edges in H share a common node (Whitney, 1992). For- mally, VL = {vl : vl ∈ EH}, and EL = \b\u0000 vli, vlj \u0001 : vli , vlj ∈ EH, \f\fvli ∩ vlj \f\f ≥ 1 \t . The weight of each edge Wi,j is assigned to Wi,j = \f\fvli ∩ vlj \f\f/ \f\fvli ∪ vlj \f\f. Semi-supervised Graph Classification. Given a set of graphs G = {GL, GU }, in which GL = \b G1, ··· , G|GL| \t are labeled graphs and GU = \b G|GL|+1, ··· , G|GL|+|GU| \t are unlabeled graphs. the problem of semi-supervised graph classification can be defined as learning a mapping function from graphs to class labels f : G → Yto predict the labels of GU , where Y represents the labels corresponding to G. GNN-based Encoder. The general mechanism of GNNs is to iteratively update node embeddings by aggregating the information of its neighbor nodes via message-passing (Gilmer et al., 2017). Formally, the node embeddings H = [h1, h2, . . . ,h|V |]⊤ ∈ R|V |×d can be updated as: H = σ( ˆAXW), ˆA = ˜D−1 2 ˜A ˜D−1 2 , (1) where ˜A = A + I, ˜D is the degree matrix of ˜A, W is the trainable weight matrix, and σ(·) is the activation function. Then the whole graph representation hG can be computed based on all node embeddings as: hG = X|V | i=1 hi. (2) 2Hypergraph-enhanced Dual Semi-supervised Graph Classification Graphs GNNEncoder Embedding HypergraphStructure Learning z Node aggregation Hyperedge aggregationHypergraph Convolution zLineGraph 1/6 1/61/7 Graph Convolution 𝑒! 𝑒\" 𝑒# 𝑒!𝑒#𝑒\" 𝒔! 𝒕! Relational Consistency Learning Anchor Anchor Consistency Loss ℒ!\"# Supervised Loss ℒ$%& Figure 1: Illustration of the proposed framework HEAL. 3. Methodology In this section, we introduce our HEAL framework for semi-supervised graph classification, which captures graph semantics from both the hypergraph and line graph per- spectives. HEAL consists of three modules: hypergraph high-order dependency learning, graph convolution on the line graph, and relational consistency learning. Figure 1 provides an overview of the whole framework. 3.1. Hypergraph High-order Dependency Learning GNNs have achieved significant success in learning expres- sive representations. However, they are inherently limited to capturing only local neighborhood information via message- passing mechanisms (Gilmer et al., 2017) and cannot effec- tively capture higher-order substructures. This limitation is crucial since many real-world graph data exhibit complex hierarchical relationships that extend beyond immediate neighbors. To address this issue, we propose using hyper- graphs to overcome the aforementioned limitation of GNNs. Hypergraphs (Feng et al., 2019) provide a more powerful framework for modeling higher-order dependencies and in- teractions among nodes, enabling us to better capture the rich structural information present in the graphs. Hypergraph Structure Learning. Existing approaches typically construct hypergraphs using predefined criteria based on distances (Yu et al., 2012), representations (Wang et al., 2015), or attributes (Huang et al., 2015). However, these methods may suffer from sub-optimal performance and high computational costs due to their lack of flexibil- ity. To overcome these limitations, we develop a flexible way to parameterize a learnable hypergraph structure, being optimized jointly with the network parameters. Neverthe- less, directly learning a dense adjacency matrix could incur excessive computational overhead, so we instead adopt a low-rank strategy to efficiently model the hypergraph struc- tural matrix Λ ∈ R|V |×k, where k represents the number of hyperedges, calculated as: Λ = H · W, (3) where H ∈ R|V |×d is the hidden embedding matrix derived from the GNN-based encoder, and d is the dimension of hidden embeddings. We introduce a learnable weight matrix W ∈ Rd×k to model the hyperedges. As a result, learning the hypergraph structural matrix requires only O(k × d) time complexity (d <<|V |), which significantly improves model efficiency without compromising performance. Hypergraph Convolution. After obtaining a flexible hyper- graph, we can effectively capture higher-order dependencies among nodes. To achieve this, we design a hypergraph con- volution to learn high-level node representations. First, we learn hyperedge embeddings by aggregating neighbor nodes connected in the hypergraph. Then, the acquired hyperedge embeddings are leveraged to perform higher-order updates on the node representations. Technically, the hyperedge embedding matrix R ∈ Rk×d is computed as follows: R = σ \u0000 UΛ⊤H \u0001 + Λ⊤H, (4) where U ∈ Rk×k denotes the additional trainable matrix, and σ(·) is the activation function. Afterward, the updated node embeddings S = [s1, s2, . . . ,s|V |]⊤ ∈ R|V |×d can be calculated as follows: S = Λ · R = Λ \u0000 σ \u0000 UΛ⊤H \u0001 + Λ⊤H \u0001 . (5) In this way, the updated node representations can effec- tively capture high-order semantic features. Finally, for each graph, the graph-level representation sG can be ob- tained by summing all the refined node representations as: sG = X|V | i=1 si. (6) 3Hypergraph-enhanced Dual Semi-supervised Graph Classification 3.2. Graph Convolution on the Line Graph Hypergraph modeling empowers our model to capture high- order semantic features and long-range dependencies. How- ever, we argue that real-world graphs may involve interac- tions among higher-order substructures, whose interactions often reveal underlying meaningful semantic patterns. For instance, in biology, certain graph motifs may interact and collectively determine the properties of the graph (Borg- wardt et al., 2005; Chen et al., 2006). To this end, we leverage the learned hypergraph to introduce the line graph, effectively capturing interactions among hy- peredges and providing a more profound exploration of the underlying semantic structure of the graph. Specifically, based on the hypergraph adjacency matrix Λ ∈ R|V |×k, we can construct the line graph L(G) = (VL, EL) according to the Definition 3, where each node feature of the line graph is represented by the previously mentioned hyperedge em- beddings R ∈ Rk×d, and the adjacency matrix is denoted by AL ∈ Rk×k. Then, we can treat the line graph as a regular graph and adopt a GNN-based encoder to obtain its graph-level representation tG, which is calculated as: T = σ( ˆALRW), ˆAL = ˜D −1 2 L ˜AL ˜D −1 2 L , tG = Xk i=1 ti, (7) where T = [t1, t2, . . . ,tk]⊤ ∈ Rk×d, ˜AL = AL + I, ˜DL is the degree matrix of ˜AL, W is the trainable weight matrix. In this way, the representation tG can be viewed as another perspective of the original graph, considering interactions among high-order substructures and better capturing the underlying semantic structure. 3.3. Relational Consistency Learning Having acquired the graph representations from the two perspectives, i.e., sG from the hypergraph view andtG from the line graph view, they capture semantic knowledge of the graph from different levels. More specifically, when we re- gard nodes within the same hyperedge as a substructure, the hypergraph convolution and line graph convolution channels within our network can be viewed as distinct perspectives describing intra-substructure and inter-substructure informa- tion. Thus, it is natural to consider how to integrate these two representations, enabling them to mutually supervise and reinforce each other. Furthermore, in semi-supervised scenarios, the availabil- ity of limited label annotations often leads to unreliable and biased pseudo labels. As a result, striving to align the graph representations or pseudo labels of the same instance directly might not prove to be the most effective strategy, especially for unlabeled graphs. To address this challenge, we suggest enhancing the representation of each instance by transfering knowledge among instances. This is achieved by comparing the similarities of the instance to other labeled graphs in the embedding spaces of the two branches. Technically, we begin by randomly selecting a subset of labeled graphs {G1, . . . , GM } ∈ GL as anchor graphs, which are stored in a memory bank. Then, we employ two branches to embed these anchor graphs, yielding the respec- tive representations {sm}M m=1 and {tm}M m=1. To ensure that the anchor graphs sufficiently cover the neighborhoods of any unlabeled graph in the embedding space and facil- itate the transfer of knowledge from labeled to unlabeled graphs, a large number of anchor graphs is required. How- ever, processing excessive anchor graphs in a single iteration can become computationally expensive due to limitations in computation and memory resources. To address this challenge, we maintain a memory bank as a queue, which dynamically stores a set of anchor graphs selected from the most recent iterations of the two branches. Specifically, take the hypergraph branch as an example, for an unlabeled graph Gu, we can calculate the relational similarity distribution between its graph representation su with the representations {sm}M m=1 of anchor graphs as: Pm u = exp (cos (su, sm) /τ)PM m′=1 exp (cos (su, sm′ ) /τ) . (8) In accordance with You et al. (2020), τ is the temperature parameter set to 0.5, cos(a, b) denotes the cosine similarity defined as a·b ∥a∥2∥b∥2 . Analogously, the relational similarity distribution in the line graph branch can be obtained as: Qm u = exp (cos (tu, tm) /τ)PM m′=1 exp (cos (tu, tm′ ) /τ) . (9) In this way, we propose the relational consistency learning to encourage the consistency between distributions Pu = [Pu 1 , . . . ,Pu M ] and Qu = [Qu 1 , . . . ,Qu M ] by minimizing the Kullback-Leibler (KL) Divergence as: Lcon = 1 |GU | X u∈GU 1 2 (DKL (Pu∥Qu) +DKL (Qu∥Pu)) . (10) Optimization Framework. To introduce the supervision signals to guide the model, for each labeled graph Gl, we concatenate the graph representations sl and tl from the two branches and feed the fused representation to a classifier (multi-layer perception) for label prediction ˆy. We then adopt cross-entropy loss to compute the supervised loss: Lsup = − 1 |GL| X i∈GL yi log (ˆyi) , (11) where yi denote the ground-truth label for labeled graph Gi. Finally, we integrate the supervised loss Lsup with 4Hypergraph-enhanced Dual Semi-supervised Graph Classification Algorithm 1 Optimization Framework of the HEAL Input: Labeled graphs GL, unlabeled graphs GU , the dimen- sion of hidden embeddings d, the number of hyperedges k, the number of anchor graphs M, the hyper-parameter β Output: Trained classifier 1: Initialize the parameters of the GNN-based encoder, hypergraph structure Learning, and classifier. 2: Select M anchor graphs from labeled set GL to con- struct the memory bank. 3: while not convergence do 4: Sample a minibatch BL and BU . 5: Forward propagation BL and BU via twin branches. 6: Compute consistency loss Lcon by Eq. (10). 7: Compute supervised loss Lsup by Eq. (11). 8: Update the parameters by gradient descent to mini- mize L by Eq. (12). 9: Update the memory bank of the two branches follow- ing the first-in-first-out principle. 10: end while relational consistency loss Lcon in our combined loss: L = Lsup + β · Lcon, (12) where β is a balance hyper-parameter. Our training algo- rithm is detailed in Algorithm 1. 3.4. Computational Complexity Analysis With |V | as the average number of nodes in input graphs, d is the hidden dimensions, andk is the hyperedge number, the total time complexity of obtaining hyperedge structure and performing hyperedge convolution is O(kd(|V | + k)). And the time complexity of line graph convolution is O(kd(k + d)). Moreover, the time complexity of computing relational consistency loss for a graph is O(Md), where M is the number of anchor graphs. Therefore, the total computational complexity of HEAL is O(Md + kd(|V | + k + d)). 4. Experiment 4.1. Experimental Setups Datasets. We assess our HEAL on six publicly avail- able datasets, comprising two bioinformatics datasets PRO- TEINS (Neumann et al., 2016) and DD (Dobson & Doig, 2003); three datasets derived from social networks, specif- ically IMDB-B, IMDB-M, and REDDIT-M-5k (Yanardag & Vishwanathan, 2015); and one dataset from scientific col- laborations, COLLAB (Yanardag & Vishwanathan, 2015). We employ the same data split with DualGraph (Luo et al., 2022), where the labeled training set, unlabeled training set, validation set, and test set are proportioned in a 2:5:1:2 ratio. Unless explicitly stated, we use 50% of the labeled data (corresponding to 10% of all samples) for training. Baseline Methods. We conduct thorough comparisons with diverse methods categorized into three groups: traditional graph algorithms, conventional semi-supervised learning methods, and graph-specific semi-supervised learning ap- proaches. Traditional graph methods include WL (Sher- vashidze et al., 2011), Graph2Vec (Narayanan et al., 2017), and Sub2Vec (Adhikari et al., 2018). Conventional semi- supervised learning approaches include EntMin (Grandvalet & Bengio, 2004), Mean-Teacher (Tarvainen & Valpola, 2017) and V AT (Miyato et al., 2018)). The category of graph-specific semi-supervised learning methods encom- passes InfoGraph (Sun et al., 2020), GraphCL (You et al., 2020), ASGN (Hao et al., 2020), JOAO (You et al., 2021)), DualGraph (Luo et al., 2022), KGNN (Ju et al., 2022), and TGNN (Ju et al., 2023b). Implementation Details. For the implementation of HEAL, we employ the GIN (Xu et al., 2019) to configure the GNN- based encoder. We empirically set the embedding dimension to 32, the batch size to 64, and the training epochs to 300. For our hypergraph structure learning module, we empiri- cally set the number of hyperedge k to 32. Moreover, we set the weight balance hyper-parameter β for Lcon to 0.01. The model HEAL is optimized using the Adam optimizer with an initial learning rate of 0.01, and the weight decay is set to 0.0005. Results are reported as the average classification accuracy (in %) and the standard deviation over five runs. 4.2. Results and Analysis The quantitative outcomes of semi-supervised graph classi- fication are presented in Table 1, and the following obser- vations can be made from the results. (i) Traditional graph methods generally underperform compared to other meth- ods, highlighting the superior capability of graph neural net- works in harnessing valuable semantic information through advanced representation learning from graph-structured data. (ii) Graph-specific semi-supervised learning methods show enhanced performance over conventional semi-supervised learning techniques, demonstrating the suitability of recent graph semi-supervised learning for challenging graph clas- sification tasks. Notably, KGNN and TGNN achieve nearly the best performance on most datasets, outperforming pre- vious state-of-the-art approaches. The success of these ap- proaches can be attributed to their proficient use of unla- beled samples, which boosts consistency across different modules in processing unlabeled graphs. (iii) Our proposed HEAL outperforms other baseline methods across the major- ity of benchmarks, demonstrating the robustness of our ap- proach. The enhancement in performance can be attributed to the utilization of hypergraph and line graph convolution branches, which enable the capture of higher-order relation- ships among nodes. Additionally, the relational consistency learning module facilitates knowledge transfer between the two branches, leading to enhanced mutual guidance and 5Hypergraph-enhanced Dual Semi-supervised Graph Classification Table 1: Overview of performance (in %) across six benchmark graph classification datasets, with standard deviations calculated over five runs. The highest scores are marked in bold, and the second-highest scores are underlined. Methods PROTEINS DD IMDB-B IMDB-M REDDIT-M-5k COLLAB WL 63.5 ± 1.6 57 .3 ± 1.2 58 .1 ± 2.3 33 .3 ± 1.4 37 .0 ± 0.9 62 .9 ± 0.7 Sub2Vec 52.7 ± 4.5 46 .4 ± 3.2 44 .9 ± 3.5 31 .8 ± 2.7 35 .1 ± 1.5 60 .8 ± 1.4 Graph2Vec 63.1 ± 1.8 53 .7 ± 1.6 61 .2 ± 2.6 38 .1 ± 2.2 38 .1 ± 1.4 63 .6 ± 0.9 EntMin 62.7 ± 2.7 59 .8 ± 1.3 67 .1 ± 3.7 37 .4 ± 1.2 38 .7 ± 2.8 63 .8 ± 1.6 Mean-Teacher 64.3 ± 2.1 60 .6 ± 1.8 66 .4 ± 2.7 38 .8 ± 3.6 39 .2 ± 2.1 63 .6 ± 1.4 V AT 64.1 ± 1.2 59 .9 ± 2.6 67 .2 ± 2.9 39 .6 ± 1.4 38 .9 ± 3.2 64 .1 ± 1.1 InfoGraph 68.2 ± 0.7 67 .5 ± 1.4 71 .8 ± 2.3 42 .3 ± 1.8 41 .5 ± 1.7 65 .7 ± 0.4 ASGN 67.7 ± 1.2 68 .5 ± 0.6 70 .6 ± 1.4 41 .2 ± 1.4 42 .2 ± 0.8 65 .3 ± 0.8 GraphCL 69.4 ± 0.8 68 .7 ± 1.2 71 .2 ± 2.5 43 .7 ± 1.3 42 .3 ± 0.9 66 .4 ± 0.6 JOAO 68.7 ± 0.9 67 .9 ± 1.3 71 .0 ± 1.9 42 .6 ± 1.5 42 .1 ± 1.2 65 .8 ± 0.4 DualGraph 70.1 ± 1.2 69 .8 ± 0.8 72 .1 ± 0.7 44.8 ± 0.4 42.9 ± 1.4 67 .2 ± 0.6 KGNN 70.9 ± 0.5 70 .5 ± 0.6 72 .5 ± 1.6 43.3 ± 0.7 44.8 ± 0.6 67.4 ± 0.5 TGNN 71.0 ± 0.7 70.8 ± 0.9 72.8 ± 1.7 42.9 ± 0.8 43.8 ± 1.0 67 .7 ± 0.4 HEAL 73.4 ± 0.8 72.1 ± 0.9 73.5 ± 1.5 44.3 ± 0.6 45.9 ± 1.0 68.3 ± 0.5 1% 5% 10% Label Rate 60.0 65.0 70.0Accuracy (%) InfoGraph ASGN GraphCL JOAO DualGraph KGNN HEAL (a) PROTEINS 1% 5% 10% Label Rate 32.5 35.0 37.5 40.0 42.5 45.0Accuracy (%) InfoGraph ASGN GraphCL JOAO DualGraph KGNN HEAL  (b) REDDIT-M-5k Figure 2: Results of HEAL and baselines with different labeling ratios on PROTEINS and REDDIT-M-5k datasets. improved performance. As for IMDB-M dataset, this dis- crepancy in performance can be attributed to the relatively smaller size of nodes (13.00) and edges (65.94) in the IMDB- M dataset compared to the others. In such cases, the use of hypergraph convolution may not be as effective, since the construction of a hypergraph might be unnecessary when dealing with a small-scale graph. Influence of Labeling Ratio. We evaluate our model HEAL and baselines on the PROTEINS and REDDIT-M-5k datasets by varying the labeling ratio of the training data, as shown in Figure 2. The findings illustrate that as the number of labeled instances rises, the performance of both HEAL and the baselines enhances, suggesting that adding more labeled data effectively boosts performance. Notably, our proposed HEAL demonstrates the best performance among all methods in most scenarios, underscoring the advantages of effectively incorporating graph semantics across different levels of higher-order structure. Table 2: Ablation study of HEAL with several variants. Methods PROTEINS REDDIT-M-5K COLLAB Hyper-Sup 69.1 ±1.0 41 .2 ±1.2 64 .4 ±0.7 Line-Sup 66.7 ±1.2 40 .7 ±1.4 64 .2 ±0.9 Dual-Sup 70.1 ±0.9 42 .4 ±1.2 65 .3 ±0.8 Hyper-Ensemble 71.8 ±1.1 43 .6 ±1.3 66 .7 ±0.7 Line-Ensemble 71.5 ±1.1 43 .9 ±1.4 66 .0 ±0.8 HEAL 73.4±0.8 45.9 ±1.0 68.3 ±0.5 4.3. Ablation Study We carry out ablation studies to evaluate the impact of each component within our model. We test several model variants as outlined below: (i) Hyper-Sup trains a single hypergraph convolution network using only supervised signals. (ii) Line-Sup trains a single line graph convolution network in a supervised manner. (iii) Dual-Sup trains a dual branch of the hypergraph and line graph convolution network in a supervised manner. (iv) Hyper-Ensemble replaces the line graph convolution branch with another hypergraph learning module, using a different initialization. (v) Line-Ensemble replaces hypergraph convolution branch with another line graph convolution module, also with different initialization. The outcomes for various variants are displayed in Ta- ble 2. Firstly, it is evident that Hyper-Sup generally out- performs Line-Sup. Moreover, the combination of both hypergraph and line graph convolution (Dual-Sup) leads to improved performance, validating the joint effectiveness of both branches. Secondly, Hyper-Ensemble (Line-Ensemble) outperforms Hyper-Sup (Line-Sup), indicating that our re- lational consistency learning module effectively leverages 6Hypergraph-enhanced Dual Semi-supervised Graph Classification 16 32 64 128 256 k 66.0 68.0 70.0 72.0 74.0 76.0Accuracy (%) PROTEINS COLLAB (a) Hyperedge Number 16 32 64 128 256 d 64.0 66.0 68.0 70.0 72.0 74.0Accuracy (%) PROTEINS COLLAB  (b) Embedding Dimensions Figure 3: Hyper-parameter sensitivity study of HEAL on PROTEINS and COLLAB datasets. unlabeled samples and improve the model via ensemble techniques. Lastly, our full model outperforms both en- semble versions, underscoring its enhanced effectiveness in extracting similarities by simultaneously considering both hypergraph and line graph perspectives. 4.4. Hyper-parameter Study We analyze how the performance of HEAL changes with different hyper-parameter configurations. In particular, we assess the influence of the number of embedding dimen- sions d and the number of hyperedges k used within the hypergraph structure learning module. Influence of Hyperedge Numbers. We first conducted the influence of the number of hyperedges, varyingk in {16, 32, 64, 128, 256} while keeping all other hyperparameters fixed. The results are shown in Figure 3(a), which reveal that the accuracy initially increases as the hyperedge number rises from 16 to 32. However, after reaching a peak, the accuracy starts to decrease with further increases in the number of hyperedges. This trend can be attributed to the potential capture of noise or aggregation of redundant information from hyperedges as the number of hyperedges grows. Influence of Embedding Dimensions. We further evalu- ated the impact of embedding dimensions by varying d in {16, 32, 64, 128, 256 } while maintaining all other hyper- parameters constant. The results depicted in Figure 3(b) indicate that performance reaches the peak when the embed- ding dimensions approach 32. This trend suggests that while increasing d initially enhances the model’s representation ability, it may result in overfitting if d continues to rise. 4.5. Analysis of Consistency Loss The proposed relational consistency loss Lcon aims to en- hance each instance representation by exchanging instance knowledge from two correlated views. To highlight the advantages of the consistency learning module, we carry out experiments that compare Lcon against other commonly employed contrastive losses (i.e. InfoNCE loss and mean squared error (MSE) loss) and consistency learning ap- 1% 10% 60.0 62.5 65.0 67.5 70.0 72.5Accuracy (%) Consistency InfoNCE MSE FixMatch w/o con (a) PROTEINS 1% 10% 64.0 66.0 68.0Accuracy (%) Consistency InfoNCE MSE FixMatch w/o con (b) COLLAB Figure 4: Performance comparison with different labeling ratios w.r.t. different types of Lcon. proaches (i.e., FixMatch loss). The results are presented in Figure 4, from which we can draw the following conclu- sions. Firstly, we observe a significant performance decline in HEAL when Lcon is omitted (w/o Lcon), compared to its inclusion with various forms of contrastive loss. This obser- vation highlights the importance of inter-branch knowledge communication in enhancing semi-supervised classification. Secondly, contrastive losses (Consistency, InfoNCE, MSE) generally surpass the pseudo-labeling consistency loss (Fix- Match), which may be due to the biased pseudo-labels de- termined by unreliable prediction probabilities. Finally, our proposed consistency loss achieves better results than both InfoNCE and MSE. These methods typically concentrate on strictly enforcing similarities between two graph representa- tions. Our findings suggest that a more flexible alignment of similarity distributions between hypergraph and line graph views enhances the effectiveness of consistency learning. 4.6. Visualization Analysis We conducted a case study on the PROTEINS dataset to vi- sualize the learned hypergraph structure and corresponding line graph, thus demonstrating the effectiveness of the hy- pergraph structure learning module and line graph module, respectively. Here the thresholds for visualizing the learned hypergraph structure and corresponding line graph are both set to 0. In the PROTEINS dataset, each node represents secondary structure elements (helices, sheets, and turns), and the edges represent sequential or structural connections between nodes. Figure 5(a) reveals that elements in the protein are only connected to their nearest spatial neighbors, making it difficult to model higher-order interactions. How- ever, in Figure 5(b), we showcase part of the hyperedges learned by our hypergraph structure learning module. The hypergraph structure allows elements in the protein to inter- act in a high-order manner, facilitating the capturing of more complex and intricate relationships within protein structures. The results demonstrate that our hypergraph structure learn- ing module exhibits remarkable adaptability in acquiring higher-order node relationships beyond pairwise connec- tions, enabling enhanced flexibility in modeling complex 7Hypergraph-enhanced Dual Semi-supervised Graph Classification (a) Original Graph  (b) Learned Hypergraph /0 /0 /1 /2/3 /2/3 /4/3 /5/2 /5 /2 /6 /2 /7 /2 /8 /2 /9/4 /1/4 /3 /3/10 /3 /4/3 /8/2 /10/4 /1 /2/4 /5 /6 /3 /5 /3 /6 /3 /10/3 /7/2 /10/4 /1 /0 /3 /1/3 /3/3 /2/2 /2 /2 /4/2 /6 /2 /8/2 /9 /0 /0 /3/2 /5 /8 /3 /1/3 /4 /3 /5/3 /7/3 /9/2 /1/2 /2 /0 /5/6 /10 /7 /8 /9 /3 /1 /3 /3/3 /10/3 /7/3 /8 /2 /1/2 /3/2 /2/2 /4 /3 /9 /0 /0 /1 /3/3 /2/3 /4 /2 /5/2 /6/2 /8/2 /9 /3 /5/8 /3 /1 /3 /7/3 /9 /2 /1 /2 /2 /11 /12 /13 /14 /15 /14 /16 /17 /14 /3 /18 /19 /16 /14 /i255 /21 /22 /16 /14 /23 (c) Learned Line Graph Figure 5: Visualization of the original graph, hypergraph structure and line graph learned by HEAL (only demonstrat- ing the most significant hyperedges for simplicity). data structures. Analogously, figure 5(c) also depicts the effectiveness of the learned line graph, potentially involving interactions among hypergraphs, whose interactions often reveal underlying meaningful semantic patterns. 5. Related Work Graph Neural Networks (GNNs) have risen as a powerful tool for handling graph-structured data, facilitating effec- tive node and graph-level representation learning (Ju et al., 2024b). The essence of GNNs lies in their iterative process of enhancing node representations by aggregating informa- tion from neighboring nodes, allowing nodes to propagate and exchange information throughout the graph (Gilmer et al., 2017). This message passing enables GNNs to cap- ture local neighborhood information and learn expressive node representations, which are beneficial for a wide range of tasks such as node classification (Yuan et al., 2023; Luo et al., 2023a), node clustering (Yi et al., 2023a), link pre- diction (Zhang & Chen, 2018; Qin et al., 2024), and graph classification (Ju et al., 2022; Luo et al., 2023b). Compared with existing methods for supervised graph classification, our work goes further and studies a promising yet challeng- ing semi-supervised graph classification. Hypergraph Learning have gained increasing attention for their ability to model complex relationships beyond pair- wise interactions in traditional graphs. The underlying idea behind hypergraphs is to extend the concept of edges in graphs to hyperedges, which can connect multiple nodes si- multaneously. This flexibility allows hypergraphs to capture higher-order dependencies and interactions among nodes. Various techniques have been proposed to leverage hyper- graphs for diverse applications, including clustering (Takai et al., 2020), classification (Sun et al., 2021), link predic- tion (Yadati et al., 2020), traffic flow prediction (Zhao et al., 2023), knowledge graphs (Fatemi et al., 2019), and recom- mender systems (Xia et al., 2021). Recently, hypergraph convolutional networks have been proposed as a generaliza- tion of GCNs to handle hypergraph-structured data, enabling effective feature aggregation and representation learning in hypergraphs (Feng et al., 2019; Jiang et al., 2019; Zhang et al., 2022; Cai et al., 2022). Our HEAL also inherits the advantages of hypergraphs in modeling higher-order node relationships and additionally introduces a line graph to capture the semantic interactions between hyperedges. Semi-supervised Learning has been proven to be a promi- nent approach to address the limitations of traditional su- pervised learning, especially when labeled data is scarce or expensive to obtain. Early works in semi-supervised learn- ing focus on spreading label knowledge from labeled data to neighboring unlabeled data points, effectively expanding the labeled set and providing more informative data points for training (Subramanya & Talukdar, 2022; Wan et al., 2021). Another class involves consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Lucas et al., 2022), which encourages the model to maintain stability in its pre- dictions for perturbed versions of the same input, whether labeled or unlabeled. Compared with existing methods, our approach leverages the idea of hypergraphs for both labeled or unlabeled graphs to explore the inherent structure and relationships within the data. 6. Conclusion In this work, we present a hypergraph-enhanced dual frame- work HEAL for semi-supervised graph classification, and our HEAL effectively captures graph semantics from the perspectives of hypergraph and line graph. It incorporates hypergraph structure learning to explore higher-order node dependencies and introduces a line graph to capture hy- peredge interactions. Then, relational consistency learning is developed to facilitate knowledge transfer between the two branches. Experiments reveal superior performance compared to baseline methods in real-world graph datasets. Acknowledge This paper is partially supported by the National Natural Sci- ence Foundation of China (NSFC Grant Numbers 62306014 and 62276002) as well as the China Postdoctoral Science Foundation with Grant No. 2023M730057. 8Hypergraph-enhanced Dual Semi-supervised Graph Classification Impact Statement The proposed HEAL framework advances semi-supervised graph classification by incorporating hypergraph and line graph perspectives, addressing the limitations of traditional pairwise node relationships. By learning higher-order node dependencies through hypergraph structure learning and capturing hyperedge interactions via a line graph, HEAL enhances the extraction of underlying semantic structures. This dual approach facilitates improved knowledge transfer and mutual guidance between the two graph representations, contributing to more accurate and insightful graph classi- fication. This work holds potential for broad applications in domains requiring effective and efficient graph analy- sis, such as social network analysis, biological networks, knowledge graphs, and recommender systems. References Adhikari, B., Zhang, Y ., Ramakrishnan, N., and Prakash, B. A. Sub2vec: Feature learning for subgraphs. In Ad- vances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II 22 , pp. 170–182. Springer, 2018. Borgwardt, K. M., Ong, C. S., Sch¨onauer, S., Vishwanathan, S., Smola, A. J., and Kriegel, H.-P. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1): i47–i56, 2005. Cai, D., Song, M., Sun, C., Zhang, B., Hong, S., and Li, H. Hypergraph structure learning for hypergraph neu- ral networks. In Proceedings of the Thirty-First Inter- national Joint Conference on Artificial Intelligence, pp. 1923–1929, 2022. Chen, J., Hsu, W., Lee, M. L., and Ng, S.-K. Nemofinder: Dissecting genome-wide protein-protein interactions with meso-scale network motifs. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 106–115, 2006. Dobson, P. D. and Doig, A. J. Distinguishing enzyme struc- tures from non-enzymes without alignments. Journal of Molecular Biology, 330(4):771–783, 2003. Fatemi, B., Taslakian, P., Vazquez, D., and Poole, D. Knowl- edge hypergraphs: Prediction beyond binary relations. arXiv preprint arXiv:1906.00137, 2019. Feng, Y ., You, H., Zhang, Z., Ji, R., and Gao, Y . Hypergraph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pp. 3558–3565, 2019. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International Conference on Machine Learning, pp. 1263–1272. PMLR, 2017. Grandvalet, Y . and Bengio, Y . Semi-supervised learning by entropy minimization. Advances in Neural Information Processing Systems, 17:529–536, 2004. Hao, Z., Lu, C., Huang, Z., Wang, H., Hu, Z., Liu, Q., Chen, E., and Lee, C. Asgn: An active semi-supervised graph neural network for molecular property prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 731–752, 2020. Huang, S., Elhoseiny, M., Elgammal, A., and Yang, D. Learning hypergraph-regularized attribute predictors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 409–417, 2015. Jiang, J., Wei, Y ., Feng, Y ., Cao, J., and Gao, Y . Dynamic hy- pergraph neural networks. In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intel- ligence, pp. 2635–2641, 2019. Ju, W., Yang, J., Qu, M., Song, W., Shen, J., and Zhang, M. Kgnn: Harnessing kernel-based networks for semi- supervised graph classification. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pp. 421–429, 2022. Ju, W., Liu, Z., Qin, Y ., Feng, B., Wang, C., Guo, Z., Luo, X., and Zhang, M. Few-shot molecular property prediction via hierarchically structured learning on relation graphs. Neural Networks, 163:122–131, 2023a. Ju, W., Luo, X., Qu, M., Wang, Y ., Chen, C., Deng, M., Hua, X.-S., and Zhang, M. Tgnn: A joint semi-supervised framework for graph-level classification. arXiv preprint arXiv:2304.11688, 2023b. Ju, W., Fang, Z., Gu, Y ., Liu, Z., Long, Q., Qiao, Z., Qin, Y ., Shen, J., Sun, F., Xiao, Z., et al. A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207, 2024a. Ju, W., Yi, S., Wang, Y ., Long, Q., Luo, J., Xiao, Z., and Zhang, M. A survey of data-efficient graph learning. arXiv preprint arXiv:2402.00447, 2024b. Kashima, H., Tsuda, K., and Inokuchi, A. Marginalized kernels between labeled graphs. In Proceedings of Inter- national Conference on Machine Learning, pp. 321–328, 2003. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. 9Hypergraph-enhanced Dual Semi-supervised Graph Classification Kojima, R., Ishida, S., Ohta, M., Iwata, H., Honma, T., and Okuno, Y . kgcn: a graph-based deep learning framework for chemical structures. Journal of Cheminformatics, 12: 1–10, 2020. Laine, S. and Aila, T. Temporal ensembling for semi- supervised learning. In Proceedings of International Conference on Learning Representations, 2017. Li, J., Rong, Y ., Cheng, H., Meng, H., Huang, W., and Huang, J. Semi-supervised graph classification: A hi- erarchical graph perspective. In The World Wide Web Conference, pp. 972–982, 2019. Lucas, T., Weinzaepfel, P., and Rogez, G. Barely-supervised learning: Semi-supervised learning with very few labeled images. In Proceedings of the AAAI Conference on Artifi- cial Intelligence, volume 36, pp. 1881–1889, 2022. Luo, X., Ju, W., Qu, M., Chen, C., Deng, M., Hua, X.-S., and Zhang, M. Dualgraph: Improving semi-supervised graph classification via dual contrastive learning. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 699–712. IEEE, 2022. Luo, X., Ju, W., Gu, Y ., Qin, Y ., Yi, S., Wu, D., Liu, L., and Zhang, M. Towards effective semi-supervised node classification with hybrid curriculum pseudo-labeling. ACM Transactions on Multimedia Computing, Communi- cations and Applications, 20, 2023a. Luo, X., Zhao, Y ., Mao, Z., Qin, Y ., Ju, W., Zhang, M., and Sun, Y . Rignn: A rationale perspective for semi- supervised open-world graph classification. Transactions on Machine Learning Research, 2023b. Luo, X., Zhao, Y ., Qin, Y ., Ju, W., and Zhang, M. Towards semi-supervised universal graph classification. IEEE Transactions on Knowledge and Data Engineering, 36: 416–428, 2023c. Mao, Z., Ju, W., Qin, Y ., Luo, X., and Zhang, M. Rah- net: Retrieval augmented hybrid network for long-tailed graph classification. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 3817–3826, 2023. Miyato, T., Maeda, S.-i., Koyama, M., and Ishii, S. Vir- tual adversarial training: a regularization method for su- pervised and semi-supervised learning. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 41 (8):1979–1993, 2018. Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y ., and Jaiswal, S. graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005, 2017. Neumann, M., Garnett, R., Bauckhage, C., and Kersting, K. Propagation kernels: efficient graph kernels from propagated information. Machine Learning, 102(2):209– 245, 2016. Qin, Y ., Ju, W., Wu, H., Luo, X., and Zhang, M. Learning graph ode for continuous-time sequential recommenda- tion. IEEE Transactions on Knowledge and Data Engi- neering, 2024. Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., and Borgwardt, K. Efficient graphlet kernels for large graph comparison. In Proceedings of International Conference on Artificial Intelligence and Statistics , pp. 488–495, 2009. Shervashidze, N., Schweitzer, P., Van Leeuwen, E. J., Mehlhorn, K., and Borgwardt, K. M. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9):2539–2561, 2011. Subramanya, A. and Talukdar, P. P. Graph-based semi- supervised learning. Springer Nature, 2022. Sun, F.-Y ., Hoffmann, J., Verma, V ., and Tang, J. Infograph: Unsupervised and semi-supervised graph-level represen- tation learning via mutual information maximization. In Proceedings of International Conference on Learning Representations, 2020. Sun, X., Yin, H., Liu, B., Chen, H., Cao, J., Shao, Y ., and Viet Hung, N. Q. Heterogeneous hypergraph embedding for graph classification. In Proceedings of the 14th ACM International Conference on Web Search and Data Min- ing, pp. 725–733, 2021. Takai, Y ., Miyauchi, A., Ikeda, M., and Yoshida, Y . Hy- pergraph clustering based on pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1970–1978, 2020. Tarvainen, A. and Valpola, H. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pp. 1195–1204, 2017. Wan, S., Pan, S., Yang, J., and Gong, C. Contrastive and generative graph convolutional networks for graph- based semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10049–10057, 2021. Wang, M., Liu, X., and Wu, X. Visual classification by ℓ1- hypergraph modeling. IEEE Transactions on Knowledge and Data Engineering, 27(9):2564–2574, 2015. 10Hypergraph-enhanced Dual Semi-supervised Graph Classification Whitney, H. Congruent graphs and the connectivity of graphs. Hassler Whitney Collected Papers, pp. 61–79, 1992. Xia, X., Yin, H., Yu, J., Wang, Q., Cui, L., and Zhang, X. Self-supervised hypergraph convolutional networks for session-based recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 4503–4511, 2021. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In Proceedings of Interna- tional Conference on Learning Representations, 2019. Yadati, N., Nitin, V ., Nimishakavi, M., Yadav, P., Louis, A., and Talukdar, P. Nhp: Neural hypergraph link prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management , pp. 1705– 1714, 2020. Yanardag, P. and Vishwanathan, S. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365–1374, 2015. Yi, S., Ju, W., Qin, Y ., Luo, X., Liu, L., Zhou, Y ., and Zhang, M. Redundancy-free self-supervised relational learning for graph clustering. IEEE Transactions on Neural Net- works and Learning Systems, 2023a. Yi, S.-Y ., Mao, Z., Ju, W., Zhou, Y .-D., Liu, L., Luo, X., and Zhang, M. Towards long-tailed recognition for graph clas- sification via collaborative experts. IEEE Transactions on Big Data, pp. 1683–1696, 2023b. You, Y ., Chen, T., Sui, Y ., Chen, T., Wang, Z., and Shen, Y . Graph contrastive learning with augmentations. Ad- vances in Neural Information Processing Systems , 33: 5812–5823, 2020. You, Y ., Chen, T., Shen, Y ., and Wang, Z. Graph contrastive learning automated. In International Conference on Ma- chine Learning, pp. 12121–12132. PMLR, 2021. Yu, J., Tao, D., and Wang, M. Adaptive hypergraph learning and its application in image classification. IEEE Transac- tions on Image Processing, 21(7):3262–3272, 2012. Yuan, J., Luo, X., Qin, Y ., Mao, Z., Ju, W., and Zhang, M. Alex: Towards effective graph transfer learning with noisy labels. In Proceedings of the 31st ACM Interna- tional Conference on Multimedia, pp. 3647–3656, 2023. Zhang, M. and Chen, Y . Link prediction based on graph neu- ral networks. Advances in Neural Information Processing Systems, 31:5171–5181, 2018. Zhang, Z., Feng, Y ., Ying, S., and Gao, Y . Deep hypergraph structure learning. arXiv preprint arXiv:2208.12547 , 2022. Zhao, Y ., Luo, X., Ju, W., Chen, C., Hua, X.-S., and Zhang, M. Dynamic hypergraph structure learning for traffic flow forecasting. In 2023 IEEE 39th International Con- ference on Data Engineering (ICDE) , pp. 2303–2316. IEEE, 2023. 11",
      "references": [],
      "meta_data": {
        "arxiv_id": "2405.04773v2",
        "authors": [
          "Wei Ju",
          "Zhengyang Mao",
          "Siyu Yi",
          "Yifang Qin",
          "Yiyang Gu",
          "Zhiping Xiao",
          "Yifan Wang",
          "Xiao Luo",
          "Ming Zhang"
        ],
        "published_date": "2024-05-08T02:44:13Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "We introduce HEAL, a hypergraph-enhanced dual framework for semi-supervised graph classification that addresses two limitations of GNNs: (i) limited higher-order dependency modeling and (ii) underutilization of unlabeled graphs. HEAL builds a learnable hypergraph to capture high-order node relations, constructs a line graph to model interactions between hyperedges, and uses relational consistency learning to transfer knowledge between the two branches, achieving state-of-the-art performance on multiple benchmarks.",
        "methodology": "HEAL comprises three components: (1) Hypergraph high-order dependency learning with a learnable hypergraph structure Λ = H W that reduces to efficient low-rank modeling; hyperedge embeddings R are computed to perform hypergraph convolution and produce refined node representations S, aggregated to sG; (2) Graph convolution on the line graph built from Λ, using a GNN encoder on the line graph to obtain tG from hyperedge embeddings; (3) Relational consistency learning that aligns two similarity distributions between unlabeled graphs and a bank of labeled anchors across the hypergraph and line-graph branches, using KL divergence; a supervised loss on labeled graphs is combined with the consistency loss (L = Lsup + β Lcon).",
        "experimental_setup": "Datasets: PROTEINS, DD (bioinformatics); IMDB-B, IMDB-M, REDDIT-M-5k, COLLAB (social/scientific collaboration). Data splits follow the protocol of DualGraph: labeled/unlabeled/validation/test in 2:5:1:2, with 50% of labeled data when not otherwise stated. Implementation uses a GIN encoder, embedding dimension 32, batch size 64, 300 training epochs, hyperedge number k = 32, optimizer Adam with learning rate 0.01 and weight decay 0.0005. Baselines include traditional graph kernels, standard SSL methods, and graph-specific SSL methods (InfoGraph, GraphCL, ASGN, JOAO, DualGraph, KGNN, TGNN). Evaluation metric is accuracy (mean ± std over 5 runs).",
        "limitations": "Limitations include potential inefficacy on very small graphs where hypergraph construction adds little value (e.g., IMDB-M with small nodes/edges). Hyperparameter sensitivity (k and embedding dimension d) can affect performance and may introduce noise with too many hyperedges. Computational overhead comes from dual branches and memory bank operations, though mitigated by low-rank hypergraph learning. The relational consistency objective relies on anchor sampling, which may introduce bias if anchors are not representative.",
        "future_research_directions": "Explore adaptive or dynamic hyperedge counts and richer hypergraph priors to further reduce noise; extend HEAL to other semi-supervised tasks (node-level, link prediction) and heterogeneous graphs; investigate theoretical guarantees for relational consistency across branches; optimize memory-bank strategy and scalability for large-scale graphs; combine HEAL with additional self-supervised signals (contrastive, generative) and domain-specific priors; study robustness to label noise and domain shifts.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Semantic Prompt for Few-Shot Image Recognition",
      "full_text": "Semantic Prompt for Few-Shot Image Recognition Wentao Chen1,2*, Chenyang Si3*, Zhang Zhang2,4, Liang Wang2,4, Zilei Wang1, Tieniu Tan1,2,4 1 University of Science and Technology of China 2 Center for Research on Intelligent Perception and Computing, NLPR, CASIA 3 Nanyang Technological University, Singapore4 University of Chinese Academy of Sciences wentao.chen@cripac.ia.ac.cn, chenyang.si.mail@gmail.com, zzhang@nlpr.ia.ac.cn Abstract Few-shot learning is a challenging problem since only a few examples are provided to recognize a new class. Several recent studies exploit additional semantic information, e.g. text embeddings of class names, to address the issue of rare samples through combining semantic prototypes with visual prototypes. However, these methods still suffer from the spurious visual features learned from the rare support sam- ples, resulting in limited benefits. In this paper, we propose a novel Semantic Prompt (SP) approach for few-shot learn- ing. Instead of the naive exploitation of semantic informa- tion for remedying classifiers, we explore leveraging seman- tic information as prompts to tune the visual feature extrac- tion network adaptively. Specifically, we design two com- plementary mechanisms to insert semantic prompts into the feature extractor: one is to enable the interaction between semantic prompts and patch embeddings along the spatial dimension via self-attention, another is to supplement vi- sual features with the transformed semantic prompts along the channel dimension. By combining these two mecha- nisms, the feature extractor presents a better ability to at- tend to the class-specific features and obtains more gen- eralized image representations with merely a few support samples. Through extensive experiments on four datasets, the proposed approach achieves promising results, improv- ing the 1-shot learning accuracy by 3.67% on average. 1. Introduction Few-shot learning (FSL) [21] is a fundamental and chal- lenging task and remains largely unsolved as it aims to pre- dict a new class with rare samples. To address this problem, most effective FSL approaches leverage the prior knowl- edge learned from a large labeled base dataset, and encode the prior knowledge as a set of initial network parame- ters [12, 37, 42], or a fixed embedding function shared by *Equal contribution 𝑓 A unicycle is a vehicle with  only one wheel...𝑔 {‘unicycle’} Input image Attention map Semantic prompt-guided  feature extraction Figure 1. Given only one image about a new class ‘unicycle’, the feature extractor is easily confused by the spurious features, such as the rider on the unicycle, and fails to obtain generalized image representations about the new class. In this paper, we propose Semantic Prompt, a new method to condition the feature extraction on rich semantic prior knowledge, such that the feature extractor captures the intrinsic class-specific features about the novel class. all classes [16, 45, 46, 49]. As the labeled images of novel classes are scarce, a straightforward alternative is to use auxiliary information from other modalities, e.g. natural language, to assist in learning new concepts, which has been extensively studied in zero-shot learning [13,26,40,43]. These methods usually directly use textual embeddings as the image classifiers for novel classes. Following this idea, a recent FSL study [52] proposes to infer textual prototypes from class names and combine them with the visual prototypes ( i.e., classifiers) extracted from the rare support images. Others [32, 53] im- prove this work by introducing more sophisticated textual prototype predictors (e.g. Graph Convolutional Network) or producing more accurate textual prototypes through lever- aging the benefits of large-scale pre-trained language mod- els. In spite of their success, most of the above methods for directly inferring class prototypes from textual features ig- nore the information gap between textual and visual fea- tures. Specifically, the textual features may contain the semantic relationship between a novel class and known arXiv:2303.14123v1  [cs.CV]  24 Mar 2023classes. However, they fail to provide the exact discrimi- native visual features of the new class because of lacking interaction with the underlying visual representations. As a result, the rich semantic information has derived limited benefit for recognizing novel classes when directly inject- ing it into classifiers. Moreover, with only limited support images, the learned visual features still suffer from spuri- ous features, such as background clutters, and struggles to produce an accurate class prototype. For example, as illus- trated in Figure 1, given one support image of a novel class ‘unicycle’, the feature extractor may capture image features containing both unicycles and other distractors, like riders and tile roofs, and fail to recognize the unicycle in other environments. Actually, human perception system has a unique visual perceptual mechanism, called cognitive pene- trability [30], which uses linguistic prior knowledge to tune ongoing visual perceptual processing to category-relevant stimulus features, promoting the learning of novel objects. Hence, it is necessary to develop a new architecture for ef- fectively leveraging textual information to remedy the de- fective representation caused by rare samples. In this paper, we propose Semantic Prompt, a novel ap- proach that leverages textual information of class names to significantly improve the representation ability of visual features for few-shot learning. Instead of directly infer- ring prototypes from textual features, we explore leverag- ing the textual features as semantic prompts to adaptively tune the feature extraction network for the rare support sam- ples. As shown in Figure 1, with the guidance of semantic prompts, the feature extractor is expected to capture the in- trinsic class-specific features for the novel class rather than other background clutters. Moreover, the advent of large- scale training has produced a cornucopia of powerful Natu- ral Language Processing (NLP) models, such as BERT [9] and GPT [36], which bootstrap extracting rich textual in- formation from class names. Through the interaction be- tween semantic prompts and visual features, such seman- tically rich representations have powerful potential to pro- vide the feature extractor with additional discriminative vi- sual features about the new class, and subsequently produce more generalized class prototypes. To condition the visual feature extraction on semantic prompts, we propose two complementary mechanisms to inject semantic information into the feature extractor, which allow the interaction between semantic prompts and visual features on the spatial and the channel dimensions, respec- tively. Specifically, to facilitate the interaction on the spatial dimension, we extend the image patch sequence with se- mantic prompts and feed them into a Transformer encoder. Through self-attention layers, the semantic prompts can in- form the feature extractor to attend to the class-specific fea- tures while suppressing other distractors. For the interaction on the channel dimension, we first concatenate the semantic prompts with the visual context extracted from all patches, and then feed them into an MLP module. The extracted feature vector is added to each patch token to modulate and augment the visual features channel-by-channel. By com- bining the two interaction mechanisms, the proposed Se- mantic Prompt approach (SP) can effectively leverage the textual information in class names to boost FSL. Through comprehensive experiments on four benchmarks, the pro- posed SP presents consistent performance improvements with different types of text encoders and architecture de- signs, demonstrating its strong generality for the FSL prob- lem. In summary, our contribution are three-folds: • We propose a novel Semantic Prompt approach to leveraging textual information in class names for few- shot image recognition, which is inspired by the top- down cognitive penetrability effect in human percep- tion and aims to adaptively tune the feature extrac- tion to class-specific features according to the semantic prompts. • To condition visual feature extraction on semantic prompts, we propose two complementary mechanisms to inject semantic prompts into the visual feature ex- tractor, which allow the interaction on the spatial and the channel dimensions, respectively. • The proposed method achieves remarkable perfor- mance on four FSL benchmarks, improving the FSL accuracy by 3.67% on average under the challenging 1-shot setting. 2. Related work Few-shot learning. FSL aims to recognize novel classes given only a few examples for each class. Previous work usually adopts a meta-learning paradigm, in which a learner is trained on a sequence of few-shot training tasks (named episodes) sampled from a large base dataset in or- der to rapidly adapt to unseen testing tasks. In particular, optimization-based methods [12,37,42] aim to learn a set of optimal initial parameters shared by all tasks with fast adap- tation ability. Metric learning-based methods [16,45,46,49] learn a fixed embedding function, which maps input images into a low-dimension embedding space and classifies unla- beled queries according to certain distances to the support samples, e.g., Euclidean distance [45], cosine-similarity distance [31], and Earth Mover’s Distance [56]. Few-shot learning with language. To leverage addi- tional information from other modalities (especially lan- guage) to help recognize novel classes, a line of recent stud- ies [3, 24, 32, 52] propose to integrate both visual features and auxiliary text features to represent a novel class. Forexample, Xing et al. [52] propose an adaptive fusion mech- anism to combine a visual prototype with a semantic pro- totype obtained by the word embedding of the class label. Peng et al. [32] adopt a Graph Convolutional Network [58] as the predictor to incorporate additional knowledge from a knowledge graph. Yan et al. [54] propose a word vector- guided attention mechanism to obtain label prototypes for the multi-label few-shot learning problem. Different from previous work that leverages semantic information at the level of classifiers or class prototypes, we explore the aux- iliary information as a kind of semantic prompt to enhance the feature extraction for the limited support samples. Transformer and prompt-based learning. Trans- former is general network architecture for NLP tasks [5, 9, 36, 55], and has also demonstrated great potential to deal with computer vision tasks [11,28,44,59]. Specially, Doso- vitskiy et al. [11] propose a simple Vision Transformer (ViT) architecture that regards image patches as a sequence and inputs them into a Transformer encoder to extract visual features. Due to the limited inductive bias in its architecture design, Transformer usually requires a lot of data to learn a new task. To address this problem, prompt-based meth- ods [5, 34] have been proposed to adapt a pre-trained lan- guage model to down-stream tasks in a data-efficient way. For example, Brown et al. [5] wrap the input sentence with several hand-crafted prompt words, which inform the model of the task prior knowledge and modulate the model’s be- havior to the desired mode. Other studies [23, 25, 57] pro- pose to replace the discrete prompt words with continu- ous prompt vectors that are easier to optimize than the dis- crete prompts. Recently, Tsimpoukelli et al. [48] propose a cross-modal prompt approach, which regards image fea- tures as the prompts for language inputs to perform multi- modal few-shot learning. In this paper, we propose to re- gard textual features as the semantic prompts for image in- puts, which can tune the ongoing visual feature extraction to class-specific features and facilitate learning novel classes with few examples. As far as we know, this is the first time to adopt semantic features as prompts to tune visual feature extractors for few-shot learning. 3. Problem formulation The FSL problem is usually defined as a N-way K-shot classification task, where a model should classify a query sample xq from the query set Q into one of N classes Cnovel, based on a few labeled examples(xs i , ys i )N×K i=1 from the support set S. Since it is very difficult to train a model from scratch with the small support set S, a large labeled dataset Dbase is provided to pre-train the model before per- forming few-shot learning. Previous work usually adopts a meta-training strategy [49] to split the base dataset into mul- tiple N-way K-shot episodes. Each episode also contains a support set and a query set, mimicking the few-shot learning problem during testing. Note that the base classes Cbase do not overlap with the novel classes,i.e., Cbase ∩Cnovel = ϕ. Therefore, the model is expected to acquire the ability to generalize to unseen classes after meta-training. Variant: In most previous work, the image labely is usu- ally represented as a one-hot vector, e.g. y = [0, 1, 0, 0, ...]. However, this representation erases the semantic relation- ships among object concepts and ignores the valuable lin- guistic information contained in the textual labels. In this paper, we retain text labels ( e.g. ‘cat’, ‘dog’) besides the one-hot labels in order to extract semantics from text labels. We denote ytext as the text label to distinguish it with the one-hot label y. 4. Method Following [6], our approach consists of two training stages. In the first stage, we pre-train a feature extractor f by classifying all images in the base setDbase. In the sec- ond stage, we fine-tune f with Semantic Prompt (SP) under the meta-learning paradigm, such that f acquires the ability to extract generalized and class-relevant visual features for data-scarce scenarios. 4.1. Pre-training Learning a general feature extractor is the key to trans- fer knowledge to down-stream learning tasks [15, 19, 35], including few-shot learning [47]. Given the labeled base dataset Dbase, we adopt a simple supervised learning paradigm to learn the feature extractor. A linear classifi- cation head [W, b] is added on the top of the feature extrac- tor, which maps the input feature vector f(x) into one of the base classes. We jointly train the feature extractor and the classification head by minimizing the standard cross en- tropy loss: Lpre = 1 |Dbase| X (x,y)∈Dbase −log exp(WT y f(x) +by)P i exp(WT i f(x) +bi), (1) where Wi, bi denote the classifier weight and the bias for the class i. Backbone: To facilitate the following interaction be- tween visual features and semantic prompts, we adopt the Vision Transformers as the image feature extractor f. Specifically, an input image x ∈ RH×W×C is first divided to a sequence of M image patches X = {x1 p, x2 p, ...,xM p } where xi p ∈ RP×P×C is an image patch and P is the patch size. Then, each patch is mapped into an embedding vector and added with a learnable position embedding. The pre- processed image patches for the Transformer input can be written as: Z0 = [z1 0 , z2 0 , ...,zM 0 ], where zi 0 ∈ RCz is the patch token at the position i and Cz is the number of chan- nels of each token.Pre-trained  Text Encoder A photo of a unicycle. Patch Embedding & Transformer Layers Projector Spatial and Channel Interaction Transformer Layers DotProduct & SoftMax Weighted Sum 𝒛𝟎 𝒛𝟏 𝒒𝟎 𝒌𝟎 𝒗𝟎 𝒒1 𝒌1 𝒗1 Average MLP 𝒛𝟎 𝒛𝟏 𝒛𝑴 Spatial Interaction Channel Interaction Figure 2. Framework of the proposed Semantic Prompt approach. The support image is split into small patches and fed into Transformer layers to extract visual features, which however may contain both class-specific features and other clutters. To address this problem, we leverage textual features extracted from class names as semantic prompts to adaptively tune the visual feature extraction. The semantic prompts can interact with visual features along the spatial and the channel dimensions, and guide the feature extractor to capture the intrinsic discriminative features about the new class. The patch tokens are fed into L Transformer layers to extract visual features, each of which consists of multihead self-attention (MSA), a MLP block, Layernorm (LN), and residual connections. (Please refer to the appendix for more details.) At the top layer L, we average all embedding vec- tors in the sequence as the extracted image features: f(x) = 1 M MX i=1 zi L, (2) where zi L is the ith embedding vector at the layer L. Note that self-attention has quadratic computation costs with respect to the sequence length. To reduce compu- tation costs, we adopt the Visformer [7], a variant of the original ViT [11], in our implementation, which replaces the first seven Transformer layers with convolutional blocks and adopts pooling among stages to reduce the sequence length. 4.2. Semantic Prompt After pre-trained on the base dataset, the feature extrac- tor f can extract substantial visual features from the input images. However, due to the semantic shift between novel classes and the base dataset, the feature extractor is limited in its ability to generalize the knowledge to novel concepts with only a few labeled examples, especially when spurious correlations appear in novel class images [3,50]. For exam- ple, given an image of an unseen bird standing in a tree, the model may treat both bird features and other visual features (e.g. leaves, twigs) to represent the concept of the bird, and fails to recognize the bird in other environments. To mitigate this problem, we explore additional semantic information as prompts to guide the visual feature network to obtain intrinsic and discriminative class prototypes under rare support samples, so that query images can be classi- fied easily in terms of their distances to theses prototypes. Specifically, textual data of class names is adopted as prior knowledge for novel classes, due to its strong ability to de- scribe semantics. Moreover, we use the NLP models with large-scale pre-training [33, 35, 38] to extract textual fea- tures. The prior knowledge from a large bank of pre-trained NLP models benefits textual feature extraction from class names. To accommodate the model to semantic prompts, we adopt the meta-training strategy [49] to fine-tune the fea- ture extractor associated with semantic prompts on a series of training episodes. The framework of our approach is il- lustrated in Figure 2. Specifically, given a support image xs in a training episode, we feed its class name ytext into a pre-trained language model g(·) to extract semantic fea- tures i.e., g(ytext). The semantic features are used to mod- ulate the feature extraction for the rare support samples. We denote fg(xs) =f(xs|g(ytext)) as the conditional feature extraction process, which will be described in the following section. The obtained support features are averaged within each class to compute class prototypes. Let pi denote the prototype for the class i, then pi = 1 K KX j=1 fg(xs j), (3) where xs j is the jth support image of the class i.During meta-training, we freeze the text encoderg(·) and fine-tune other parameters by maximizing the feature simi- larities between query samples and their prototypes with a cross-entropy loss: Lmeta = −ES,QExq log exp(s(f(xq), pyq )/τ)PN i=1 exp(s(f(xq), pi)/τ) , (4) where s denotes the cosine similarity, pyq is the prototype of the class yq, and τ is a temperature hyper-parameter. 4.2.1 Interaction on the spatial dimension We first take inspiration from the prompt methods in NLP [5, 34] to concatenate prompt vectors with the input se- quence and feed them together into Transformer layers. Given the semantic featuresg(ytext) and the input sequence of patch embeddings Zl−1 = [ z1 l−1, z2 l−1, ...,zM l−1] ∈ RM×Cz at the layer l, we obtain a new sequence ˆZl−1 ∈ R(M+1)×Cz by extending Zl−1 with the projected semantic features : ˆZl−1 = [z0, z1 l−1, ...,zM l−1], (5) where z0 = hs(g(ytext)) ∈ RCz is the projected semantic embedding for spatial interaction andhs is the projector that keeps the dimension of the semantic embedding to be the same as the patch embeddings. Then, the extended sequence ˆZl−1 is fed into the re- maining Transformer layers, which contain multihead self- attention modules (MSA) to allow the interaction between semantic prompts and patch tokens along the spatial dimen- sion. Specifically, letting ˆZl−1 be the input sequence to a MSA module at the layer l, MSA first maps each token into three vectors, q, k, v ∈ RNh×(M+1)×Ch , with linear pro- jection parameterized by Wqkv, i.e., [q, k, v] = ˆZl−1Wqkv, (6) where Nh is the number of heads and Ch is the number of channels for each head. It then computes the attention weights A ∈ RNh×(M+1)×(M+1) by taking the inner prod- uct between q and k and performing softmax along the spa- tial dimension: A = softmax(qkT /C 1 4 h ). (7) The attention weights are used to choose and aggregate in- formation from different positions. The final output is ob- tained by concatenating outputs of all heads and performing linear projection parameterized by Wout: MSA ( ˆZl−1) = (Av)Wout. (8) 4.2.2 Interaction on the channel dimension Besides spatial interaction via MSA, we propose another in- teraction mechanism that allows modulating and augment- ing visual features channel-by-channel according to the in- put semantic prompts. Given the input sequence of patch embeddings Zl−1 = [z1 l−1, z2 l−1, ...,zM l−1] ∈ RM×Cz at the layer l, we first obtain a global visual context vector zc l−1 ∈ RCz by averaging all patch tokens: zc l−1 = 1 M MX i=1 zi l−1. (9) The visual context zc l−1 is then concatenated with the projected semantic vector z0 = hc(g(ytext)) ∈ RCz , and fed into a 2-layer MLP module to obtain a modulating vec- tor βl−1 ∈ RCz : βl−1 = σ(W2 σ(W1 [z0; zc l−1] +b1) +b2), (10) where W1, b1, W2, b2 are the parameters of the MLP mod- ule, σ is the sigmoid activation function, and hc is the pro- jector for the channel interaction. We finally add the modulating vector to all patch tokens such that it can tune the visual features at each channel. The modulated sequence ˜Zl−1 ∈ RM×Cz can be written as: ˜Zl−1 = [zi l−1 + βl−1, ] i = 1, 2, ..., M. (11) 5. Experiments 5.1. Datasets and implementation details miniImageNet and tieredImageNet. The miniImageNet dataset is proposed in [49] to bench- mark the few-shot learning problem. It contains a subset of 100 classes in the ImageNet [41] dataset, where 64 classes are used as base classes for pre-training and meta-training, 16 classes are used for validation, and 20 classes are used for testing. The tiredImageNet dataset [39] is also derived from ImageNet and contains more classes: 351 classes used for training, 97 classes used for validation, and 160 classes used for testing. The semantic difference between base classes and novel classes in the tieredImageNet dataset is much larger than miniImageNet. CIFAR-FS and FC100. These two datasets are derived from the CIFAR-100 [20] dataset with different partition modes. CIFAR-FS [22] randomly splits 100 classes into 64 training classes, 16 validation classes and 20 testing classes. In contrast, FC100 [31] divides classes based on their se- mantic superclasses, where 60 classes from 20 superclasses are used for training, 20 classes from 4 superclasses are used for validation, 20 classes from 4 superclasses are used for testing. The large semantic gap makes FC100 more difficult than CIFAR-FS.miniImageNet 5-way tieredImageNet 5-way Method Backbone Params/FLOPS 1-shot 5-shot 1-shot 5-shot LEO [42] WRN-28-10 36.5M/ 3.7 × 1010 61.76±0.08 77.59 ±0.12 66.33 ±0.05 81.44 ±0.09 CC+rot [14] WRN-28-10 36.5M/ 3.7 × 1010 62.93±0.45 79.87 ±0.33 70.53 ±0.51 84.98 ±0.36 Align [1] WRN-28-10 36.5M/ 3.7 × 1010 65.92±0.60 82.85 ±0.55 74.40±0.68 86.61±0.59 MetaOptNet [22] ResNet-12 12.5M/ 3.5 × 109 62.64±0.61 78.63 ±0.46 65.99 ±0.72 81.56 ±0.53 Meta-Baseline [6] ResNet-12 12.5M/ 3.5 × 109 63.17±0.23 79.26 ±0.17 68.62 ±0.27 83.74 ±0.18 DeepEMD [56] ResNet-12 12.5M/ 3.5 × 109 65.91±0.82 82.41 ±0.56 71.16 ±0.87 86.03 ±0.58 RE-Net [17] ResNet-12 12.5M/ 3.5 × 109 67.60±0.44 82.58 ±0.30 71.61 ±0.51 85.28 ±0.35 TPMM [51] ResNet-12 12.5M/ 3.5 × 109 67.64±0.63 83.44±0.43 72.24±0.70 86.55 ±0.63 SetFeat [2] ResNet-12 12.5M/ 3.5 × 109 68.32±0.62 82.71±0.46 73.63 ±0.88 87.59±0.57 SUN [10] Visformer-S 12.4M/ 1.7 × 108 67.80±0.45 83.25 ±0.30 72.99 ±0.50 86.74 ±0.33 KTN [32] ResNet-12 12.5M/ 3.5 × 109 61.42±0.72 74.16 ±0.56 - - AM3 [52] ResNet-12 12.5M/ 3.5 × 109 65.30±0.49 78.10 ±0.36 69.08 ±0.47 82.58 ±0.31 TRAML [24] ResNet-12 12.5M/ 3.5 × 109 67.10±0.52 79.54±0.60 - - DeepEMD-BERT [53] ResNet-12 12.5M/ 3.5 × 109 67.03±0.79 83.68±0.65 73.76 ±0.72 87.51 ±0.75 Pre-train (Ours) Visformer-T 10.0M/ 1.3 × 109 65.16±0.44 81.22 ±0.32 72.38 ±0.50 86.74 ±0.34 SP-CLIP (Ours) Visformer-T 10.0M/ 1.3 × 109 72.31±0.40 83.42±0.30 78.03±0.46 88.55±0.32 SP-SBERT (Ours) Visformer-T 10.0M/ 1.3 × 109 70.70±0.42 83.55±0.30 73.31±0.50 88.56 ±0.32 SP-GloVe (Ours) Visformer-T 10.0M/ 1.3 × 109 70.81±0.42 83.31 ±0.30 74.68 ±0.50 88.64±0.31 Table 1. Comparison with previous work onminiImageNet and tieredImageNet. Methods in the top rows do not use semantic information, and methods in the middle rows leverage semantic information from class names [24, 32, 52] or descriptions [53]. Accuracies are reported with 95% confidence intervals. Text encoders. To extract rich semantic features from class names, we adopt three types of text encoders, i.e., CLIP [35], SBERT [38], and GloVe [33], which are pre- trained on large-scale corpora and are available for public use. For CLIP, we only use its text encoder, and extend the input class name with a text template: A photo of a {class name}. For SBERT and Glove, we directly feed class names into their encoders and average the output word vectors if there are multiple words in a name. Implementation details. We adopt Visformer-Tiny [7] as the feature extractor and resize the input image into 224×224 by default. Other input resolutions are validated in Section 5.3.5. Images are augmented with RandomRe- sizedCrop, RandAug [8] and RepeatAug [4]. During pre- training, we use the AdamW optimer [29] with a learning rate of 5e-4 and a weight decay of 5e-2. We pre-train the model for 800 epochs on miniImageNet, CIFAR-FS and FC100, and for 300 epochs on tieredImageNet. During meta-training, we reduce the learning rate of the feature ex- tractor to 1e-6 and set the learning rate of the projectors as 5e-4. The model is meta-trained for 100 epochs on all datasets. The hyper-parameter τ is set as 0.2 according to validation accuracy. We conduct experiments with a TITAN Xp server and training can be done with one GPU. During evaluation, we randomly sample 2,000 test episodes from the novel classes. For 1-shot learning, we use the cosine classifier for prediction as in Eq.4. For 5- shot learning, we adopt logistic regression classifiers with random crop augmentation. We finally report the average accuracy with 95% confidence intervals. 5.2. Comparison with the state-of-the-art To evaluate the effectiveness of our approach, we con- duct extensive experiments on four datasets , and compare the results with previous state-of-the-art methods in Table 1 and Table 2. Compared with previous methods that leverages seman- tic information (KTN [32], AM3 [52], TRAML [24], Deep- BERT [53]), our method improves 1-shot accuracy by 5.21% on miniImageNet and by 4.27% on tieredImageNet. DeepEMD-BERT achieves better 5-shot accuracy than ours on miniImageNet, but requires multiple forward passes and additional inner optimization step to obtain reliable local feature similarities. Note that previous methods usually adopts CNN as the backbone, except a recently proposed method SUN [10] that also adopts the Visformer backbone. Nevertheless, our method outperforms SUN by 2.46% on average over three datasets. When using different text encoders to extract semantic features, the proposed SP presents consistent improvements over the pre-training baseline. Specifically, we can see that SP with CLIP achieves better on 1-shot than SBERT and GloVe, probably because CLIP’s multi-modal pre-training results in better alignment of semantic embeddings with vi- sual concepts. In 5-shot, the performance difference de- creases as the model performance is dominated by visual features when support images are sufficient. In the follow- ing experiments, we use CLIP as the default text encoder.CIFAR-FS 5-way FC100 5-way Method Backbone Params/FLOPs 1-shot 5-shot 1-shot 5-shot PN+rot [14] WRN-28-10 36.5M/ 3.7 × 1010 69.55±0.34 82.34 ±0.24 - - Align [1] WRN-28-10 36.5M/ 3.7 × 1010 - - 45.83±0.48 59.74±0.56 ProtoNet [45] ResNet-12 12.5M/ 3.5 × 109 72.2±0.7 83.5 ±0.5 37.5 ±0.6 52.5 ±0.6 MetaOptNet [22] ResNet-12 12.5M/ 3.5 × 109 72.6±0.7 84.3 ±0.5 41.1 ±0.6 55.5 ±0.6 MABAS [18] ResNet-12 12.5M/ 3.5 × 109 73.51±0.92 85.49 ±0.68 42.31 ±0.75 57.56 ±0.78 Distill [47] ResNet-12 12.5M/ 3.5 × 109 73.9±0.8 86.9 ±0.5 44.6 ±0.7 60.9±0.6 RE-Net [17] ResNet-12 12.5M/ 3.5 × 109 74.51±0.46 86.60 ±0.32 - - infoPatch [27] ResNet-12 12.5M/ 3.5 × 109 - - 43.8 ±0.4 58.0 ±0.4 SUN [10] Visformer-S 12.4M/ 1.7 × 108 78.37±0.46 88.84 ±0.32 - - Pre-train (Ours) Visformer-T 10.0M/ 1.3 × 109 71.99±0.47 85.98 ±0.34 43.77 ±0.39 59.48 ±0.39 SP-CLIP (Ours) Visformer-T 10.0M/ 1.3 × 109 82.18±0.40 88.24±0.32 48.53±0.38 61.55 ±0.41 SP-SBERT (Ours) Visformer-T 10.0M/ 1.3 × 109 81.32±0.40 88.31 ±0.32 47.03 ±0.40 61.03 ±0.40 SP-GloVe (Ours) Visformer-T 10.0M/ 1.3 × 109 81.62±0.41 88.32±0.32 46.69±0.41 61.18 ±0.41 Table 2. Comparison with previous work on CIFAR-FS [22] and FC100 [31]. Aug SI CI Mini Tiered CIFAR-FS FC100 × × × 61.96 71.91 68.84 40.78 ✓ × × 65.15 72.38 71.99 43.77 ✓ ✓ × 71.59 76.20 81.19 47.83 ✓ × ✓ 70.48 77.62 79.80 47.10 ✓ ✓ ✓ 72.31 78.03 82.18 48.53 Table 3. Ablation study on four datasets under the 1-shot setting. SI means spatial interaction, and CI means channel interaction. 5.3. Model analysis 5.3.1 Ablation study The ablation study results are shown in Table 3. By ex- tending the standard RandomResizedCrop with RandAug and RepeatAug, the 1-shot accuracy of the pre-trained fea- ture extractor is improved by 2.45% on average over four datasets. To validate the effectiveness of SP, we fine-tune the feature extractor with three different interaction mech- anisms, including SI (spatial interaction), CI (channel in- teraction) and SI+CI. As shown in Table 3, both SI and CI are very effective, improving average 1-shot accuracy on 4 datasets by 5.89% and 5.43%, respectively. Furthermore, by combing them together, the 1-shot learning accuracy is further improved on all four datasets. These results indicate that the proposed SP is an effective approach to leveraging semantic information for few-shot learning. 5.3.2 Layer selection Theoretically, the semantic prompt in this work can be in- serted into the feature extractor at any layer. However, we find that the layer selection has a significant impact on the performance. In Figure 3, we can see that inserting prompts at higher layers improves accuracies, while insert- ing prompts at lower layers leads to performance drop. Con- (a)  (b) Figure 3. Accuracy vs. different layers to inset prompts. We report 5-way 1-shot accuracy (%) on the validation set of miniImageNet and CIFAF-FS along the meta-training process. The feature ex- tractor has three stages and multiple Transformer layers in each stage. sidering that prompt vectors are class-specific, these results indicate that class-specific features should be extracted at higher network layers, while features at lower layers should better be shared among classes. When looking into the per- formance of each layer, we can see that while the optimal layer selection varies slightly for different datasets, SP at all layers of the third stage improves accuracy consistently. To simplify architecture design, we choose the layer3-2 as default in our experiments. 5.3.3 The backbone and classifier architectures In Table 4, we re-implement three baseline methods with the same Visformer backbone as ours, and compare the re- sults with different backbones under the miniImageNet 1- shot setting. It can be seen that simply replacing ResNet- 12 with Visformer can not obtain significant improvement. Instead, using semantic prompt can improves 1-shot perfor- mance over these baselines when equipped with the same Visformer backbone. In Tab.5, we compare the LR and NN classifiers over all datasets. The simple NN classifier performs as well as theBackbone ProtoNet [45] MetaOptNet [22] Meta-Baseline [6] Ours ResNet-12 63.28 63.29 64.36 - Visformer-T 63.16 64.39 63.32 72.31 Table 4. Comparison with different backbones. Mini Tiered CIFAR-FS FC100 Classifier 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot NN 72.31 82.86 78.03 87.74 82.18 88.04 48.53 61.10 LR 72.37 83.42 78.11 88.64 82.17 88.24 48.61 61.55 Table 5. Comparison of classifiers. NN: cosine-distance nearest prototype classifier. LR: linear logistic regression classifier. Projector Pooling strategy Linear MLP Head Patches All 1-shot 72.31 72.70 66.48 72.29 72.31 5-shot 83.42 83.56 72.70 83.39 83.42 Table 6. Choice of the projector, and the pooling strategy for the output sequence. ‘ Head’ means selecting the output at the posi- tion of the prompt vector; ‘ Patches’ means averaging the output features of all patches; ‘All’ means averaging all feature vectors in the output sequence. LR classifier for 1-shot, while the LR benefits from more training examples and outperforms the NN by 0.53% for 5-shot. 5.3.4 Projector structure and pooling strategy As shown in Table 6, the projector design has little effect on performance: both linear and MLP projectors work well and the MLP has slight advantage. In contrast, the pool- ing strategy has much more effect on performance. When adopting the ‘Head’ strategy, both 1-shot and 5-shot learn- ing accuracies are very poor. This indicates that the output at the position of the prompt vector is easy to overfit on semantic features and neglect rich visual features in image patches. Adopting average on all output features can ad- dress this problem and achieve better results. 5.3.5 Image size and stem design In Table 7, we experiment with a smaller input size, 84×84, to validate the influence of image size. It can be seen that directly changing the input size to 84 ×84 leads to evident performance drop on all datasets. We suppose that this is because the kernel size and the stride of the stem is too large to capture the detailed visual features when the in- put image gets small. To address this problem, we reduce the kernel size and the stride of the stem accordingly. After this change, the 1-shot learning performance under 84 ×84 improves significantly, and gets comparable results with the 224×224 resolution on all datasets. Input size 224 ×224 84 ×84 84 ×84 Stem Ks=7, Stride=2 Ks=7, Stride=2 Ks=3, Stride=1 MiniImageNet 72.31 ±0.40 68.09 ±0.38 72.16 ±0.40 TieredImageNet 78.03 ±0.46 72.14 ±0.47 77.28 ±0.46 CIFAR-FS 82.18 ±0.40 77.26 ±0.42 82.00 ±0.41 FC100 48.53 ±0.38 46.44 ±0.40 48.52 ±0.40 Table 7. The effect of input size and stem design. ‘Ks’ means the kernel size of the first convolution layer (stem), and ‘Stride’ means its stride. 5-way 1-shot accuracy is reported on four datasets with 95% confidence intervals. Prompt with harvestman Prompt with spider webPre-training baseline Input image with  harvestman and  spider web Figure 4. Visualization of attention maps when prompting with different class labels. 5.3.6 Visualization In Figure 4, we visualize the attention maps by computing the dot product between the output feature and the feature vector at each location. It can be seen that the visual features of the pre-training baseline are cluttered with background information, but our method can focus on semantic-level visual features according to the given text prompt. For ex- ample, given the text prompt of harvestman, the model will attend to the features of the harvest rather than spider web or background clutters. 6. Conclusion In this paper, we propose a novel Semantic Prompt (SP) approach for FSL, which adaptively tunes the fea- ture extraction with the semantic features derived from class names. The proposed approach is evaluated on four benchmark datasets, and achieves significant improvements against previous methods. More in-depth analysis demon- strates that SP encourages the model to extract more class- specific features and is robust to different text encoders and model designs. Acknowledgement This work was supported in part by the National Natu- ral Science Foundation of China under Grants 61721004, 61976214, 62076078, 62176246 and National Key R&D Program of China (2022ZD0117901).References [1] Arman Afrasiyabi, Jean-Franccois Lalonde, and Christian Gagn´e. Associative alignment for few-shot image classifi- cation. In ECCV, 2020. 6, 7 [2] Arman Afrasiyabi, Hugo Larochelle, Jean-Franc ¸ois Lalonde, and Christian Gagn´e. Matching feature sets for few-shot im- age classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9014– 9024, 2022. 6 [3] Afra Feyza Aky ¨urek, Ekin Aky ¨urek, Derry Wijaya, and Ja- cob Andreas. Subspace regularizers for few-shot class incre- mental learning. In International Conference on Learning Representations, 2022. 2, 4 [4] Maxim Berman, Herv ´e J ´egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a unified im- age embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019. 6 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 3, 5 [6] Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xiaolong Wang. Meta-baseline: exploring simple meta- learning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 9062–9071, 2021. 3, 6, 8 [7] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The vision-friendly transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 589–598, 2021. 4, 6 [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 6 [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2, 3 [10] Bowen Dong, Pan Zhou, Shuicheng Yan, and Wangmeng Zuo. Self-promoted supervision for few-shot transformer. arXiv preprint arXiv:2203.07057, 2022. 6, 7 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 4 [12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 1, 2 [13] Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and Shaogang Gong. Recent advances in zero- shot recognition: Toward data-efficient understanding of vi- sual content. IEEE Signal Processing Magazine, 35(1):112– 125, 2018. 1 [14] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P´erez, and Matthieu Cord. Boosting few-shot visual learning with self-supervision. In ICCV, 2019. 6, 7 [15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 9729–9738, 2020. 3 [16] Hongwei Huang, Zhangkai Wu, Wenbin Li, Jing Huo, and Yang Gao. Local descriptor-based multi-prototype network for few-shot learning. PR, 116:107935, 2021. 1, 2 [17] Dahyun Kang, Heeseung Kwon, Juhong Min, and Minsu Cho. Relational embedding for few-shot classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8822–8833, 2021. 6, 7 [18] Jaekyeom Kim, Hyoungseok Kim, and Gunhee Kim. Model- agnostic boundary-adversarial sampling for test-time gener- alization in few-shot learning. In ECCV, 2020. 7 [19] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision , pages 491–507. Springer, 2020. 3 [20] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2019. 5 [21] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual con- cepts. In CogSci, 2011. 1 [22] Kwonjoon Lee, Subhransu Maji, A. Ravichandran, and Ste- fano Soatto. Meta-learning with differentiable convex opti- mization. In CVPR, 2019. 5, 6, 7, 8 [23] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.arXiv preprint arXiv:2104.08691, 2021. 3 [24] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot learning with adaptive margin loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12576– 12584, 2020. 2, 6 [25] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz- ing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 3 [26] Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, and Yueting Zhuang. Zero-shot recognition using dual visual- semantic mapping paths. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 3279–3287, 2017. 1 [27] Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, and Li Zhang. Learning a few-shot embed- ding model with contrastive learning. In AAAI, 2021. 7 [28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021. 3 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6[30] Martin Maier and Rasha Abdel Rahman. No matter how: Top-down effects of verbal and semantic category knowl- edge on early visual perception. Cognitive, Affective, & Be- havioral Neuroscience, 19(4):859–876, 2019. 2 [31] Boris Oreshkin, Pau Rodr ´ıguez L ´opez, and Alexandre La- coste. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information process- ing systems, 31, 2018. 2, 5, 7 [32] Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, and Jinhui Tang. Few-shot image recognition with knowl- edge transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 441–449, 2019. 1, 2, 3, 6 [33] Jeffrey Pennington, Richard Socher, and Christopher D Man- ning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) , pages 1532–1543, 2014. 4, 6 [34] Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019. 3, 5 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning , pages 8748–8763. PMLR, 2021. 3, 4, 6 [36] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. 2, 3 [37] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017. 1, 2 [38] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 4, 6 [39] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few- shot classification. In ICLR, 2018. 5 [40] Bernardino Romera-Paredes and Philip Torr. An embarrass- ingly simple approach to zero-shot learning. InInternational conference on machine learning, pages 2152–2161. PMLR, 2015. 1 [41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. IJCV, 2015. 5 [42] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Had- sell. Meta-learning with latent embedding optimization. In ICLR, 2019. 1, 2, 6 [43] Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, and Yuji Matsumoto. Ridge regression, hubness, and zero-shot learning. In Joint European conference on ma- chine learning and knowledge discovery in databases, pages 135–151. Springer, 2015. 1 [44] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng Y AN. Inception transformer. In Ad- vances in Neural Information Processing Systems, 2022. 3 [45] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017. 1, 2, 7, 8 [46] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Re- lation network for few-shot learning. In CVPR, 2018. 1, 2 [47] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen- baum, and Phillip Isola. Rethinking few-shot image classi- fication: a good embedding is all you need? In European Conference on Computer Vision , pages 266–282. Springer, 2020. 3, 7 [48] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es- lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200–212, 2021. 3 [49] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In NeurIPS, 2016. 1, 2, 3, 4, 5 [50] Zhenhailong Wang, Hang Yu, Manling Li, Han Zhao, and Heng Ji. Model-agnostic multitask fine-tuning for few-shot vision-language transfer learning. arXiv preprint arXiv:2203.04904, 2022. 4 [51] Jiamin Wu, Tianzhu Zhang, Yongdong Zhang, and Feng Wu. Task-aware part mining network for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8433–8442, 2021. 6 [52] Chen Xing, Negar Rostamzadeh, Boris Oreshkin, and Pe- dro O O Pinheiro. Adaptive cross-modal few-shot learn- ing. Advances in Neural Information Processing Systems , 32, 2019. 1, 2, 3, 6 [53] Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, and Steven Schockaert. Aligning visual prototypes with bert em- beddings for few-shot learning. In Proceedings of the 2021 International Conference on Multimedia Retrieval , pages 367–375, 2021. 1, 6 [54] Kun Yan, Chenbin Zhang, Jun Hou, Ping Wang, Zied Bouraoui, Shoaib Jameel, and Steven Schockaert. Infer- ring prototypes for multi-label few-shot image classification with word vector guided attention. Proceedings of the AAAI Conference on Artificial Intelligence, 36(3):2991–2999, Jun. 2022. 3 [55] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Ad- vances in neural information processing systems , 32, 2019. 3 [56] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with differentiable earth mover’s distance and structured classifiers. InProceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12203–12213, 2020. 2, 6 [57] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differen- tiable prompt makes pre-trained language models better few- shot learners. arXiv preprint arXiv:2108.13161, 2021. 3[58] Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejew- ski. Graph convolutional networks: a comprehensive review. Computational Social Networks, 6(1):1–23, 2019. 3 [59] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan ¨O Arik, and Tomas Pfister. Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual un- derstanding. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, volume 36, pages 3417–3425, 2022. 3",
      "references": [],
      "meta_data": {
        "arxiv_id": "2303.14123v1",
        "authors": [
          "Wentao Chen",
          "Chenyang Si",
          "Zhang Zhang",
          "Liang Wang",
          "Zilei Wang",
          "Tieniu Tan"
        ],
        "published_date": "2023-03-24T16:32:19Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "We propose Semantic Prompt (SP), a novel approach for few-shot image recognition that uses semantic information from class names as prompts to adaptively tune the visual feature extractor. The core idea is cognitive penetrability: semantic prompts guide the model to focus on intrinsic, class-specific features when only a few support examples exist. Two complementary mechanisms are introduced to inject semantic prompts: (1) spatial interaction by concatenating prompts with patch embeddings and processing them through Transformer self-attention to enable interaction along spatial dimensions; (2) channel interaction by combining prompts with global visual context and modulating patch features via a two-layer MLP, then adding the resultant modulating vector to each patch token. The prompts are derived from text encoders (e.g., CLIP, SBERT, GloVe) applied to class names. The approach uses a two-stage training: pre-training a feature extractor on a base dataset with a linear head, then meta-training with SP where the text encoder is frozen and the rest of the network is fine-tuned. Prototypes for novel classes are formed by averaging the semantic-conditioned features of support images, and classification is performed by cosine similarity to prototypes with temperature normalization. The method achieves consistent improvements on four benchmarks, reporting an average 1-shot accuracy gain of 3.67% over baselines. ",
        "methodology": "The method comprises two main stages. (1) Pre-training: A Vision Transformer-based backbone (Visformer) is trained on the base dataset with a linear classifier over image features. Images are split into patches, embedded, and processed by Transformer blocks. The output feature f(x) is the average of patch tokens at the top layer. (2) Semantic Prompt (SP) fine-tuning: A pre-trained language model g(·) encodes the class name text ytext into semantic features g(ytext). The conditional feature extractor is f(xs|g(ytext)) = fg(xs) used to compute class prototypes pi as the average of fg(xs) over K support samples. During meta-training, the text encoder g(·) is frozen, and network parameters are updated to maximize the cosine similarity between query features f(xq) and the corresponding prototype, with temperature τ. Two interaction pathways integrate semantic prompts: (a) Spatial interaction: prompts are projected to match patch embedding dimensionality and prepended to the patch sequence, then processed by Transformer multi-head self-attention layers so prompts influence attention over spatially distributed patch features; (b) Channel interaction: a global visual context vector is computed by averaging patch embeddings; the semantic vector and this context are concatenated and fed to a 2-layer MLP to produce a modulating vector β. This β is added to each patch embedding, modulating features channel-wise. The final feature from the Lth Transformer layer is averaged over patches to produce f(x). The approach uses three text encoders (CLIP, SBERT, GloVe) and experiments with Visformer-Tiny backbone, RandAug/RepeatAug, AdamW optimization, and a cosine classifier for 1-shot. ",
        "experimental_setup": "Datasets: miniImageNet (64 base, 16 val, 20 test), tieredImageNet (351 train, 97 val, 160 test), CIFAR-FS (64 train, 16 val, 20 test), FC100 (60 train, 20 val, 20 test). Text encoders: CLIP (text portion), SBERT, GloVe. Backbone: Visformer-Tiny; input resolution 224x224; data augmentation: RandomResizedCrop, RandAug, RepeatAug. Pre-training: AdamW, lr 5e-4, weight decay 5e-2; pre-train epochs: 800 (miniImageNet, CIFAR-FS, FC100) and 300 (tieredImageNet). Meta-training: LR for feature extractor at 1e-6, projector lr 5e-4; meta-training for 100 epochs. Evaluation: 2000 test episodes; 1-shot uses cosine similarity classifier, 5-shot uses logistic regression. Hyperparameter τ=0.2. Hardware: single GPU (Titan Xp). Text encoders choices: CLIP, SBERT, GloVe with templates for CLIP (A photo of the {class name}). ",
        "limitations": "Relies on quality of semantic prompts from external language models; performance sensitive to layer where prompts are inserted (best at higher layers, layer3-2 default). Additional computational overhead due to Transformer-based spatial interaction and channel modulation, though with Visformer mitigated. Requires base dataset with disjoint novel classes for meta-training; evaluation mostly on standard FSL benchmarks; possible limitations in domains where class-name semantics do not align well with visual discriminative features; ablation shows gains but may be task-dependent; 1-shot gains may rely on appropriate prompt alignment (text encoder choice). ",
        "future_research_directions": "Explore more modalities or prompts beyond text (e.g., attributes, descriptions, knowledge graphs) as prompts to tune visual representations; study dynamic or learned prompt length and content, including prompting strategies that adapt during meta-training; investigate efficiency improvements for prompt insertion and attention (sparse attention, token pruning); extend SP to other tasks such as segmentation or video few-shot learning; evaluate across more diverse domains and language settings; jointly train text encoder with minimal freezing to allow adaptation to novel domains; explore multi-prompt ensembles or prompt regularization to reduce overfitting.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "SLICE: Stabilized LIME for Consistent Explanations for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Towards Consistency in Adversarial Classification",
      "full_text": "Towards Consistency in Adversarial Classiﬁcation Laurent Meunier1,2 Raphaël Ettedgui2 Rafael Pinot3 Yann Chevaleyre2 Jamal Atif2 1 Meta AI Research, Paris, France 2Miles Team, LAMSADE, Université Paris-Dauphine, Paris, France 3 Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland Abstract In this paper, we study the problem of consistency in the context of adversarial examples. Speciﬁcally, we tackle the following question: Can surrogate losses still be used as a proxy for minimizing the0/1 loss in the presence of an adversary that alters the inputs at test-time? Diﬀerent from the standard classiﬁcation task, this question cannot be reduced to a point-wise mini- mization problem, and calibration needs not to be suﬃcient to ensure consistency. In this paper, we expose some pathological behaviors speciﬁc to the adversarial problem, and show that no convex surro- gate loss can be consistent or calibrated in this context. It is therefore necessary to design another class of surrogate functions that can be used to solve the adversarial consistency issue. As a ﬁrst step towards designing such a class, we identify suﬃcient and necessary conditions for a surrogate loss to be calibrated in both the adversarial and standard settings. Finally, we give some directions for building a class of losses that could be consistent in the adversarial framework. 1 Introduction State-of-the-artmachinelearningclassiﬁersareknowntobevulnerabletoadversarialexampleattacks[Biggio et al., 2013, Szegedy et al., 2014], i.e., perturbation of the input data at test time that, while imperceptible, can signiﬁcantly inﬂuence the classiﬁer’s output. This can have extreme consequences in real-life scenarios such as autonomous cars [Yao et al., 2020]. Therefore, it is necessary to designrobust classiﬁers that present worst-case guarantees against a range of possible perturbations. To account for the possibility of an ad- versary manipulating the inputs at test time, we need to revisit the standard risk minimization problem by penalizing any classiﬁcation model that might change its decision when the point of interest is slightly changed. Essentially, this is done by replacing the standard (pointwise)0/1 loss with an adversarial version that mimics its behavior locally but also penalizes any error in a given region around the point on which it is evaluated. Yet, just like the0/1 loss, its adversarial counterpart is not convex, which renders the risk minimization diﬃcult. To circumvent this limitation, we take inspiration from the standard learning theory approach which consists in solving a simpler optimization problem where the non-convex loss function is replaced by a convex surrogate. In general, the surrogate loss is chosen to have a property calledconsistency [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007], which essentially guarantees that any sequence of classiﬁers that minimizes the surrogate objective must also be a sequence that minimizes the Bayes risk. In the context of standard classiﬁcation, a large family of convex losses, calledclassiﬁer-consistent, exhibits this property. This class notoriously includes the hinge loss, the logistic loss and the square loss. However, the adversarial version of these surrogate losses need not to exhibit the same consistency properties with respect to the adversarial0/1 loss. In fact, most existing results in the standard framework relyonareductionoftheglobalconsistencyproblemtoapoint-wiseproblem, called calibration. However, this 1 arXiv:2205.10022v1  [cs.LG]  20 May 2022approach is not feasible in the adversarial setting, because the new losses are by nature non-point-wise: the optimum for a given input may depend on yet a whole other set of inputs [Awasthi et al., 2021a,c]. Studying the concepts of calibration and consistency in an adversarial context remains an open and understudied issue. Furthermore, this is a complex and technical area of research, that requires a rigorous analysis, since small tweaks in deﬁnitions can quickly make results meaningless or inaccurate. This diﬃculty is illustrated in the literature, wherearticlespublishedinhighproﬁleconferencestendtocontradictorrefuteeachotherBao et al. [2020], Awasthi et al. [2021a,c]. Objective & Contributions. The objective of our work is to try and identify possible sources of confusion that may hinder the understanding of the concepts of calibration and consistency in the adversarial setting. In particular, we ﬁrst come back in Section 2 on the problem of consistency and calibration in the standard setting and carefully deﬁne their adversarial counterpart. In doing so, we note that previous papers studying adversarial surrogate losses [Bao et al., 2020, Awasthi et al., 2021a,c] did not use the same0/1-loss as the one used in seminal papers on consistency [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007]. This diﬀerence might be crucial since it may lead to inaccurate results (see Section 5). We then study in Section 3, the problem of calibration in the adversarial setting and provide both necessary and suﬃcient conditions for a loss to be calibrated in this setting. It also worth noting that our results are easily extendable toH- calibration (see Appendix L). One on the main takeaway of our analysis is that no convex surrogate loss can be calibrated in the adversarial setting. We however characterize a set of non-convex loss functions, namely shifted odd functionsthat solve the calibration problem in the adversarial setting. Finally, we focus on the problem of consistency in the adversarial setting in Section 4. Based on min-max arguments, we provide insights that might help paving a way to prove consistency of shifted odd functions in the adversarial setting. Speciﬁcally, we prove strong duality results for these losses and show tight links with the0/1-loss. From these insights, we are able to provide a close but weaker property to consistency. 2 Notions of Calibration and Consistency Let us consider a classiﬁcation task with input spaceX and output spaceY= {−1,+1}. Let (X,d) be a proper Polish (i.e. completely separable) metric space representing the inputs space. For allx ∈X and δ >0, we denoteBδ(x) the closed ball of radiusδand centerx. We also assume that for allx∈X and δ >0, Bδ(x) contains at least two points1. Let us also endowYwith the trivial metricd′(y,y′) = 1y̸=y′. Then the space (X×Y,d⊕d′) is a proper Polish space. For any Polish spaceZ, we denoteM1 +(Z) the Polish space of Borel probability measures onZ. We will denoteF(Z) the space of real valued Borel measurable functions on Z. Finally, we denote¯R := R ∪{∞,+∞}. 2.1 Notations and Preliminaries The 0/1-loss is both non-continuous and non-convex, and its direct minimization is a diﬃcult problem. The concepts of calibration and consistency aim at identifying the properties that a loss must satisfy in order to be a good surrogate for the minimization of the0/1-loss. In this section, we deﬁne these two concepts and explain the diﬀerence between them. First of all, we need to give a general deﬁnition of a loss function. Deﬁnition 2.1(Loss function). A loss function is a functionL: X×Y×F (X) →R such thatL(·,·,f) is Borel measurable for allf ∈F(X). Note that this deﬁnition is not speciﬁc to the standard or adversarial case. In general, the loss at point (x,y) can either depend only onf(x), or on other points related tox(e.g. the set of points within a distance ε of x). We now recall the deﬁnition of the risk associated with a lossL and a distributionP. Deﬁnition 2.2(L-risk of a classiﬁer). For a given loss functionL, and a Borel probability distributionP over X×Y we deﬁne the risk of a classiﬁerf associated with the lossL and a distributionP as RL,P(f) := E(x,y)∼P [L(x,y,f )] . 1For instance, for any norm∥·∥, (Rd, ∥·∥) is a Polish metric space satisfying this property. 2We also deﬁne the optimal risk associated with the lossL as R⋆ L,P := inf f∈F(X) RL,P(f) Essentially, the risk of a classiﬁer is deﬁned as the average loss over the distributionP. When the lossL is diﬃcult to optimize in practice (e.g when it is non-convex or non-diﬀerentiable), it is often preferred to optimize a surrogate loss function instead. In the literature [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007], the notion of surrogate losses has been studied as a consistency problem. In a nutshell, a surrogate loss is said to be consistent if any minimizing sequence of classiﬁers for the risk associated with the surrogate loss is also one for the risk associated withL. Formally, the notion of consistency is as follows. Deﬁnition 2.3(Consistency). Let L1 and L2 be two loss functions. For a givenP ∈M+ 1 (X×Y ), L2 is said to be consistent forP with respect toL1 if for all sequences(fn)n ∈F(X)N : RL2,P(fn) →R⋆ L2,P =⇒ RL1,P(fn) →R⋆ L1,P (1) Furthermore,L2 is said consistent with respect to a lossL1 the above holds for any distributionP. Consistency is in general a diﬃcult problem to study because of its high dependency on the distribution P at hand. Accordingly, several previous works [Zhang, 2004, Bartlett and Mendelson, 2002, Steinwart, 2007] introduced a weaker notion to study a pointwise version consistency. This simpliﬁed notion is called calibrationand corresponds to consistency whenP is a combination of Dirac distributions. The main building block in the analysis of the calibration problem is the calibration function, deﬁned as follows. Deﬁnition 2.4(Calibration function). Let L be a loss function. The calibration functionCL is CL(x,η,f ) := ηL(x,1,f) + (1−η)L(x,−1,f), for anyη∈[0,1], x∈X and f ∈F(X). We also deﬁne the optimal calibration function as C⋆ L(x,η) := inf f∈F(X) CL(x,η,f ). Note that for any x ∈ Xand η ∈ [0,1], CL(x,η,f ) = RL,P(f) with P = ηδ(x,+1) + (1 −η)δ(x,−1). The calibration function thus corresponds then to a pointwise notion of the risk, evaluated at pointx. η corresponds in this case to the conditional probability ofy = 1 given x. We now deﬁne the calibration property of a surrogate loss. Deﬁnition 2.5 (Calibration). Let L1 and L2 be two loss functions. We say that L2 is calibrated with regards toL1 if for everyξ >0, η∈[0,1] and x∈X, there existsδ >0 such that for allf ∈F(X), CL2 (x,η,f )−C⋆ L2 (x,η) ≤δ =⇒ CL1 (x,η,f ) −C⋆ L1 (x,η) ≤ξ. Furthermore, we say thatL2 is uniformly calibrated with regards toL1 if for everyξ >0, there exists δ >0 such that for allη∈[0,1], x∈X and f ∈F(X) we have CL2 (x,η,f ) −C⋆ L2 (x,η) ≤δ =⇒ CL1 (x,η,f ) −C⋆ L1 (x,η) ≤ξ. Connection between calibration and consistency.It is always true that calibration is a necessary condition for consistency. Yet there is no reason, in general, for the converse to be true. However, in the speciﬁc context usually studied in the literature (i.e., the standard classiﬁcation with a well-deﬁned0/1-loss), the notions of consistency and calibration have been shown to be equivalent. [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007]. In the next section, we come back on existing results regarding calibration and consistency in this speciﬁc (standard) classiﬁcation setting. 32.2 Existing Results in the Standard Classiﬁcation Setting Classiﬁcation is a standard task in machine learning that consists in ﬁnding a classiﬁcation functionh: X→ Ythat maps an inputx to a labely. In binary classiﬁcation,h is often deﬁned as the sign of a real valued function f ∈F(X). The loss usually used to characterize classiﬁcation tasks corresponds to the accuracy of the classiﬁerh. When h is deﬁned as above, this loss is deﬁned as follows. Deﬁnition 2.6(0/1 loss). Let f ∈F(X). We deﬁne the0/1 loss as follows l0/1(x,y,f ) = 1y×sign(f(x))≤0 with a convention for the sign, e.g. sign(0) = 1 . We will denote RP(f) := Rl0/1,P(f), R⋆ P := R⋆ l0/1,P, C(x,η,f ) := Cl0/1 (x,η,f ) and C⋆(x,η) := C⋆ l0/1 (x,η). Note that this0/1-loss is diﬀerent from the one introduced by Bao et al. [2020], Awasthi et al. [2021a,c]: theyused 1y×f(x)≤0 whichisausual 0/1 lossbutunadaptedtoconsistencyandcalibratedstudy(seeSection5 for details). Some of the most prominent works [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007] among them focus on the concept of margin losses, as deﬁned below. Deﬁnition 2.7 (Margin loss). A loss Lφ is said to be amargin loss if there exists a measurable function φ: R →R+ such that: Lφ(x,y,f ) = φ(yf(x)) For simplicity, we will say thatφ is a margin loss function and we will denoteRφ and Cφ the risk associated with the margin lossφ. Notably, it has been demonstrated in several previous works [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007] that, for a margin lossφ, we have always have C⋆ φ(x,η) = infα∈R ηφ(α)+(1 −η)φ(−α). This is in particular one of the main observation allowing to show the following strong result about the connection between consistency and calibration. Theorem 2.1 (Zhang [2004], Bartlett et al. [2006], Steinwart [2007]). Let φ : R →R+ be a continuous margin loss. Then the three following assertions are equivalent: (i)φ is calibrated with regards tol0/1, (ii)φ is uniformly calibratedl0/1, (iii)φ is consistent with regards tol0/1. Moreover, ifφ is convex and diﬀerentiable at0, thenφ is calibrated if and onlyφ′(0) <0. The Hinge lossφ(t) = max(1 −t,0) and the logistic lossφ(t) = log(1 + e−t) are classical examples of convex consistent losses. Convexity is a desirable property for faster optimization of the loss, but there exist other non-convex losses that are calibrated as the ramp loss (φ(t) = max(1 −t,0) + max(1 + t,0)) or the sigmoid loss (φ(t) = (1 + et)−1). In the next section, we present the adversarial classiﬁcation setting for which Theorem 2.1 may not hold anymore. Remark 1. The equivalence between calibration and consistency is a consequence from the fact that, over the large space of measurable functions, minimizing the loss pointwisely in the input by desintegrating with regards tox is equivalent to minimize the whole risk over measurable functions. This result is very powerful and simplify the study of calibration in the standard setting. 2.3 Calibration and Consistency in the Adversarial Setting. We now consider the adversarial classiﬁcation setting where an adversary tries to manipulate the inputs at test time. Givenε> 0, they can move each pointx∼P to another pointx′which is at distance at mostε from x2. The goal of this adversary is to maximize the0/1 risk the shifted points fromP. Formally, the loss associated to adversarial classiﬁcation is deﬁned as follows. 2Note that after shiftingx to x′, the point need not be in the support ofP anymore. 4Deﬁnition 2.8(Adversarial 0/1 loss). Let ε≥0. We deﬁne the adversarial0/1 loss of levelε as: l0/1,ε(x,y,f ) = sup x′∈Bε(x) 1ysign(f(x))≤0 We will denote Rε,P(f) := Rl0/1,ε,P(f), R⋆ ε,P := R⋆ l0/1,ε,P, Cε(x,η,f ) := Cl0/1,ε(x,η,f ) and C⋆ ε(x,η) := C⋆ l0/1,ε(x,η) for everyP, x, f and η. Speciﬁcity of the adversarial caseThe adversarial risk minimization problem is much more challenging than its standard counterpart because an inner supremum is added to the optimization objective. With this inner supremum, it is no longer possible to reduce the distributional problem to a pointwise minimization as it is usually done in the standard classiﬁcation framework. In fact, the notions of consistency and calibration are signiﬁcantly diﬀerent in the adversarial setting. This means that the results obtained in the standard classiﬁcation may no longer be valid in the adversarial setting (e.g., the calibration need not be suﬃcient for consistency), which makes the study of consistency much more complicated. As a ﬁrst step towards analyzing the adversarial classiﬁcation problem, we now adapt the notion of margin loss to the adversarial setting. Deﬁnition 2.9 (Adversarial margin loss). Let φ : R →R+ be a margin loss andε ≥0. We deﬁne the adversarial loss of levelε associated withφ as: φε(x,y,f ) = sup x′∈Bε(x) φ(yf(x′)) We say that φ is adversarially calibrated (resp. uniformly calibrated, resp. consistent) at levelε if φε is calibrated (resp. uniformly calibrated, resp. consistent) wrtl0/1,ε. Note that a ﬁrst important sanity check to make is verify thatφε and l0/1,ε are indeed measurable and well deﬁned. The arguments are not trivial since it uses advanced arguments from measure theory, but it is necessary to establish measurability before going further on. Proposition 2.1 states the measurability ofφε and l0/1,ε. We prove this result in Appendix B. Proposition 2.1. Let φ: R ×Y→ R be a measurable function andε≥0. For everyf ∈F(X), (x,y) ↦→ φε(x,y,f ) and (x,y) ↦→l0/1,ε(x,y,f ) are universally measurable. Now that, we proved that the adversarial setting is properly deﬁned, we can make a ﬁrst observation: the calibration functions forφand φε are actually equal. This property might seem counter-intuitive at ﬁrst sight as the adversarial risk is most of the time strictly larger than its standard counterpart. However, the calibration functions are only pointwise dependent, hence having the same prediction for any element of the ball Bε(x) suﬃces to reach the optimal calibrationC⋆ φ(x,η). Proposition 2.2. Let ε> 0. Let φ be a continuous classiﬁcation margin loss. For allx∈X and η∈[0,1], we have C⋆ φε(x,η) = inf α∈R ηφ(α) + (1−η)φ(−α) = C⋆ φ(x,η) . The last equality also holds for the adversarial0/1 loss. The proof of this result is available in Appendix C 3 Solving Adversarial Calibration In this section, we study the calibration of adversarial margin losses with regard to the adversarial0/1 loss. Weﬁrstprovidenecessaryandsuﬃcientconditionsunderwhichmarginlossesareadversariallycalibrated. We then show that a wide range of surrogate losses that are calibrated in the standard setting are not calibrated in the adversarial setting. Finally we propose a class of losses that are calibrated in the adversarial setting, namely theshifted odd losses. 53.1 Necessary and Suﬃcient Conditions for Calibration One of our main contributions is to ﬁnd necessary and suﬃcient conditions for calibration in the adversarial setting. In a brief, we identify that for studying calibration it is central to understand the case where there might be indecision for classiﬁers (i.e.η= 1/2). Indeed, in this case, either labelling positively or negatively the inputx would lead the same loss forx. Next result provides a necessary condition for calibration. Theorem 3.1 (Necessary condition for Calibration). Let φ be a continuous margin loss andε >0. If φ is adversarially calibrated at levelε, then φ is calibrated in the standard classiﬁcation setting and0 ̸∈ argminα∈¯R 1 2 φ(α) + 1 2 φ(−α). We proof this theorem in Appendix D. While the condition of calibration in the standard classiﬁcation setting seems natural, we need to understand why0 ̸∈argminα∈¯R 1 2 φ(α) + 1 2 φ(−α). The intuition behind this result is that a sequence of functions simply converging towards0 in the ball of radiusε around some x can take positive and negative values thus leading to suboptimal0/1 adversarial risk. It turns out that, given an additional mild assumption, this condition is actually suﬃcient to ensure calibration. Theorem 3.2 (Suﬃcient condition for Calibration). Let φ be a continuous margin loss andε > 0. If φ is decreasing and strictly decreasing in a neighbourhood of0 and calibrated in the standard setting and 0 ̸∈argminα∈¯R 1 2 φ(α) + 1 2 φ(−α), thenφ is adversarially uniformly calibrated at levelε. The proof of this theorem is available in Appendix E. Remark 2(Decreasing hypothesis). For the reciprocal, the additional assumption thatφ is decreasing and strictly decreasing in a neighborhood of0 is not restrictive for usual losses. In Theorem 2.1, this assumption is stated as a necessary and suﬃcient condition for convex losses to be calibrated. 3.2 Negative results Thanks to Theorem 3.1, we can present two notable corollaries invalidating the use of two important classes of surrogate losses in the standard setting. The ﬁrst class of losses are convex margin losses. These losses are maybe the most widely used in modern day machine learning as they comprise the logistic loss or the margin loss that are the building block of most classiﬁcation algorithms. Corollary 3.1. Let ε> 0. Then no convex margin loss can be adversarially calibrated at levelε. A convex loss satisﬁes1 2 φ(α) + 1 2 φ(−α) ≥φ(0), hence0 ∈argminα∈R φ(α) + φ(−α). From Theorem 3.1, we deduce the result. Then,φ is not adversarially calibrated at levelε. This result seems counter-intuitive and highlights the diﬃculty of optimizing and understanding the adversarial risk. Since convex losses are not adversarially calibrated, one may hope to rely on famous non-convex losses such as sigmoid and ramp losses. But, unfortunately, such losses are not calibrated either. Corollary 3.2. Let ε >0. Let λ ∈R and ψ be a lower-bounded odd function such that for allα ∈R, ψ >−λ. We deﬁneψ as φ(α) = λ+ ψ(α). Then φ is not adversarially calibrated at levelε. Indeed, 1 2 φ(α) + 1 2 φ(−α) = λ, so thatargminα∈R 1 2 φ(α) + 1 2 φ(−α) = R. Thanks to Theorem 3.1,φis not adversarially calibrated at levelε. 3.3 Positive results Theorem 3.2 also gives suﬃcient conditions forφ to be adversarially calibrated. Leveraging this result, we devise a class of margin losses that are indeed calibrated in the adversarial settings. We call this classshifted odd losses, and we deﬁne it as follows. Deﬁnition 3.1(Shifted odd losses). We say thatφ is ashifted odd margin lossif there existsλ≥0, τ >0, and a continuous lower bounded decreasing odd functionψ that is strictly decreasing in a neighborhood of0 such that for allα∈R, ψ(α) ≥−λ and φ(α) = λ+ ψ(α−τ). 610.0  7.5  5.0  2.5  0.0 2.5 5.0 7.5 10.0 0.0 0.2 0.4 0.6 0.8 1.0( ) Sigmoid Loss Shifted Sigmoid Loss 10.0  7.5  5.0  2.5  0.0 2.5 5.0 7.5 10.0 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 1 2 ( ) + 1 2 ( ) Sigmoid Loss Shifted Sigmoid Loss Figure1: Illustrationoftheacalibratedlossintheadversarialsetting. Thesigmoidlosssatisfythehypothesis for ψ. Its shifted version is then calibrated for adversarial classiﬁcation. The key diﬀerence between a standard odd margin loss and a shifted odd margin loss is the variations of the functionα↦→1 2 φ(α) + 1 2 φ(−α). The primary diﬀerence is that, in the standard case the optima of this function are located at0 while they are located in−∞and +∞in the adversarial setting. Let us give some examples of margin shifted odd losses below. Example (Shifted odd losses). For everyε> 0 and everyτ >0, the shifted logistic loss, deﬁned as follows, is adversarially calibrated at levelε: φ: α↦→(1 + exp (α−τ))−1 This loss is plotted on left in Figure 1. We also plotted on right in Figure 1α↦→1 2 φ(α) + 1 2 φ(−α) to justify that0 ̸∈argminα∈¯R 1 2 φ(α) + 1 2 φ(−α). Also note that the shifted ramp loss also satisﬁes the same properties. A consequence of Theorem 3.2 is that shifted odd losses are adversarially calibrated, as demonstrated in Proposition 3.1 stated below. Proposition 3.1. Let φ be a shifted odd margin loss. For everyε> 0, φ is adversarially calibrated at level ε. We proof this proposition in Appendix F. 4 Towards Adversarial Consistency We focus our study now on the problem of adversarial consistency. In a ﬁrst part, taking inspiration from Long and Servedio [2013], Awasthi et al. [2021a], we study theε-realisable case, i.e. the case where the adversarial risk at levelεequals zero. In a second part, we analyze the behavior of a candidate class of losses, namely the0/1-like margin losses. 4.1 The Realizable Case The realizable case is important since there are no possible adversaries for the Bayes optimal classiﬁer. Formally, this means that the adversarial risk equals0, as stated in the following deﬁnition. Deﬁnition 4.1(ε-realisability). Let P be a Borel probability distribution onX×Y and ε≥0. We say that P is ε-realisable if R⋆ ε,P = 0. In the case of realizable probability distribution, calibrated (and consequently consistent) margin losses in the standard classiﬁcation setting are also calibrated and consistent in the adversarial case. Proposition 4.1. Let ε >0. Let P be anε-realisable distribution andφ be a calibrated margin loss in the standard setting. Thenφ is adversarially consistent at levelε. 7We give a proof of this result in Appendix G. The intuition behind this result is that if a probability distribution isε-realisable, the marginal distributions are suﬃciently separated, so that there are no possible adversarial attacks, each point in theε-neighbourhood of the support of the distribution can be classiﬁed independently of each other. 4.2 Towards the General Case In this section, we seek to pave the way towards proving the consistency of shifted odd losses. We will observe that their behavior is actually very similar to that of the0/1 loss, which makes them good candidates to be consistent losses. To this end, we ﬁrst add an extra hypothesis to the odd shifted losses in order to simplify our technical analysis. Deﬁnition 4.2 (0/1-like margin losses). φ is a 0/1-like margin lossif there existsλ ≥0, τ ≥0, and a continuous lower bounded strictly decreasing odd functionψ in a neighbourhood of0 such that for allα∈R, ψ(α) ≥−λ and φ(α) = λ+ ψ(α−τ) and lim t→−∞ φ(t) = 1 and lim t→+∞ φ(t) = 0 Note here that the losses here are not necessarily shifted becauseτ might equal0, making this condition weaker. Consequently, we cannot hope that such losses are consistent neither calibrated, but they might help in ﬁnding the path towards consistency. Note also that ifφ is an odd or shifted odd loss, one can always ﬁnd a rescaling ofφsuch thatφbecomes a0/1-like margin loss. Note also that such a rescaling does neither change the notion of consistency and calibration forφ nor for its rescaled version. Based on min-max arguments, we provide below some results better characterizing0/1-like margin loss functions in the adversarial setting. Let us ﬁrst recall the notions ofmidpoint property and adversarial distributions set that will be useful from now on as well as an important existing result from Pydi and Jog [2021]. Deﬁnition 4.3.Let (X,d) be a proper Polish metric space. We say thatXsatisfy themidpoint propertyif for allx1,x2 ∈X there existx∈X such thatd(x,x1) = d(x,x2) = d(x1,x2) 2 . We recall also the setAε(P) of adversarial distributions introduced in Meunier et al. [2021]. Deﬁnition 4.4. Let P be a Borel probability distribution andε > 0. We deﬁne the set of adversarial distributions Aε(P) as: Aε(P) := { Q ∈M+ 1 (X×Y ) |∃γ ∈M+ 1 ( (X×Y )2) , d(x,x′) ≤ε, y= y′ γ-a.s., Π1♯γ = P, Π2♯γ = Q} where Πi denotes the projection on thei-th component. Theorem 4.1(Pydi and Jog [2021]). Let Xbe a Polish space satisfying the midpoint property. Then strong duality holds: R⋆ ε,P = inf f∈F(X) sup Q∈Aε(P) RQ(f) = sup Q∈Aε(P) inf f∈F(X) RQ(f) Moreover the supremum of the right-hand term is attained. Note that in the original version of the theorem, Pydi and Jog [2021] did not prove that the supremum is attained. We add a proof of this in Appendix H. 8Connections between0/1-like margin loss and0/1 loss: a min-max viewpoint. Thanks the the above concepts, we can now present some results identifying the similarity and the diﬀerences between the 0/1 loss and 0/1-like margin losses. We ﬁrst show that for a given ﬁxed probability distributionP, the adversarial optimal risk associated with a0/1-like margin loss and the0/1 loss are equal. The proof of this result is available in Appendix I Theorem 4.2.Let Xbe a Polish space satisfying the midpoint property. Letε≥0, P be a Borel probability distribution overX×Y , andφ be a0/1-like margin loss. Then, we have: R⋆ φε,P = R⋆ ε,P In particular, we note that this property holds true for the standard risk. From this result, we can derive two interesting corollaries about0/1-like margin losses. First, strong duality holds for the risk associated with φ. Corollary 4.1(Strongdualityfor φ). Let us assume thatXis a Polish space satisfying the midpoint property. Let ε≥0, P be a Borel probability distribution overX×Y , andφ be a0/1-like margin loss. Then, we have: inf f∈F(X) sup Q∈Aε(P) Rφ,Q(f) = sup Q∈Aε(P) inf f∈F(X) Rφ,Q(f) Moreover the supremum is attained. Note that there is no reason that the inﬁmum is attained. A second interesting corollary is the equality of the set of optimal attacks, i.e. distributions ofAε(P) that maximize the dual problem: an optimal attack for the0/1 loss is also an optimal attack for a0/1-like margin, and vice versa. Corollary 4.2 (Optimal attacks). Let assume thatX be a Polish space satisfying the midpoint property. Let ε≥0 and P be a Borel probability distribution overX×Y . Then, an optimal attackQ⋆ of levelε exists for both the0/1 loss andφ. Moreover, forQ ∈Aε(P). Q is an optimal attack for the lossφ if and only if it is an optimal attack for the0/1 loss. The proof of these two corollaries is available in Appendix J. A step towards consistency. From the previous results, we are able to prove a ﬁrst result toward the demonstration of consistency. This result is much weaker than consistency result, but it guarantees that if a sequence minimizes the adversarial risk, then it minimizes the risk for optimal attacks, i.e. in a game where the attacker plays before the classiﬁer. The proof of this result is in Appendix K Proposition 4.2. Let us assume thatX be a Polish space satisfying the midpoint property. Letε≥0 and P be a Borel probability distribution overX×Y . Let Q⋆ be an optimal attack of levelε. Let (fn)n∈N be a sequence ofF(X) such thatRφε,P(fn) →R⋆ φε,P. Then RQ⋆(fn) →R⋆ ε,P. We hope this result and its proof may lead to a full proof of consistency. This result is signiﬁcantly weaker than consistency as stated in the following remark. In the proof of the previous results, we did not use the assumptions that losses are shifted. In our opinion, it is the key element that we miss and that we need to use to conclude on the consistency of this family of losses. The shift in the loss would force the classiﬁer to goes to±∞on aεneighborhood support of the distribution ofP and then the risk would equal the adversarial0/1-loss risk. However, we did not succeed in showing the appropriate result: we believe this question is complicated and is left as further work. 5 Related Work and Discussions WenowexplainthediﬀerencesbetweenourapproachandtheoneproposedbyBao et al.[2020],Awasthi et al. [2021a,c]. The two main diﬀerences are the choice of the0/1 loss and the studied notion of consistency and calibration. 9Alternative 0/1 loss An alternative 0/1 loss would the following: l≤(f(x),y) = 1yf(x)≤0. This loss penalizes indecision: i.e. predicting0 would lead to a pointwise risk of1 for y= 1 and y= −1 while the0/1 loss l0/1 returns 1 for y = 1 and 0 for y = −1. This deﬁnition was used by Bao et al. [2020], Awasthi et al. [2021a,c] to prove their calibration and consistency results. While Bartlett et al. [2006] was not explicit on the choice for the0/1 loss, Steinwart [2007] explicitly mentions that the0/1 loss is not a margin loss. The use of this loss is not suited for studying consistency and leads to inaccurate results as shown in the following counterexample. On X = R, let P deﬁned as P = 1 2 (δx=0,y=1 + δx=0,y=−1) and φ : R →R be a margin based loss. Theφ-risk minimization problem writesinfα 1 2 (φ(α) + φ(−α)). For any convex functionalφ the optimum is attained forα= 0. fn : x↦→0 is a minimizing sequence for theφ-risk. HoweverRl≤(fn) = 1 for all nand R∗ l≤ = 1 2 . Then we deduce that no convex margin based loss is consistent wrtl≤. Consequently, the 0/1 loss to be used in adversarial consistency needs to bel0/1,ε(x,y,f ) = supx′∈Bε(x) 1ysign(f(x))≤0, otherwise the obtained results might be innacurate. H-consistencyand H-calibration Bao et al.[2020],Awasthi et al.[2021a,c]proposedtostudy H-calibration and H-consistency in the adversarial setting, i.e. calibration and consistency when minimizing sequences are in H. However, even in the standard classiﬁcation setting, the link between both notions in this extended setting is not clear at all since a pointwise minimization of the risk cannot be done. To our knowledge, there is only one research paper [Long and Servedio, 2013] that focuses on this notion in standard setting. They do it in the restricted case of realisability, i.e. when the standard optimal risk associated with the 0/1 loss equals0. We believe that studyingH-consistency andH-calibration in the adversarial setting is a bit anticipated. For these reasons, we focus only on calibration and consistency on the space of measurable functions F(X). However, note that many of our results can be adapted toH-calibration: we propose, in Appendix L, conditions on classesHso that Theorems 3.1 and 3.2 still holds. About the Adversarial Bayes Risk and Game Theory. A recent trend of work has focused on analyzing the adversarial risk from multiple point of views. Bhagoji et al. [2019] as well as Pydi and Jog [2020, 2021] showed that the adversarial optimal Bayes classiﬁer can be written as optimal transport for a well chosen cost. Another line of work [Pinot et al., 2020, Meunier et al., 2021, Pydi and Jog, 2021] have focused on a game theoretic approach for analyzing the adversarial risk having interest in the nature of equilibria between the classiﬁer and the attacker. Recently, some researchers [Awasthi et al., 2021b, Bungert et al., 2021] proved encouraging results on the existence of an optimal Bayes classiﬁer in the adversarial setting under mild assumptions. 6 Conclusion In this paper, we set some solid theoretical foundations for the study of adversarial consistency. We high- lighted the importance of the deﬁnition of the 0/1 loss, as well as the nuance between calibration and consistency that is speciﬁc to the adversarial setting. Furthermore, we solved the adversarial calibration problem, by giving a necessary and suﬃcient condition for decreasing, continuous margin losses to be adver- sarially calibrated. Since this is a necessary condition for consistency, an important consequence of this result is that no convex margin loss can be consistent. This rules out most of the commonly used surrogates, and spurs the need for new families of consistent, yet diﬀerentiable families of losses. We provide ﬁrst insights into which losses may be consistent, by showing that translations of odd loss functions are calibrated. 10References Pranjal Awasthi, Natalie Frank, Anqi Mao, Mehryar Mohri, and Yutao Zhong. Calibration and consistency of adversarial surrogate losses.Advances in Neural Information Processing Systems, 34, 2021a. Pranjal Awasthi, Natalie S Frank, and Mehryar Mohri. On the existence of the adversarial bayes classiﬁer (extended version).arXiv preprint arXiv:2112.01694, 2021b. Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. A ﬁner calibration analysis for adversarial robustness. arXiv preprint arXiv:2105.01550, 2021c. Han Bao, Clay Scott, and Masashi Sugiyama. Calibrated surrogate losses for adversarially robust classi- ﬁcation. In Jacob Abernethy and Shivani Agarwal, editors,Proceedings of Thirty Third Conference on Learning Theory, volume 125 ofProceedings of Machine Learning Research, pages 408–451. PMLR, 09–12 Jul 2020. URLhttp://proceedings.mlr.press/v125/bao20a.html. Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. Peter L Bartlett, Michael I Jordan, and Jon D McAuliﬀe. Convexity, classiﬁcation, and risk bounds.Journal of the American Statistical Association, 101(473):138–156, 2006. Dimitir P Bertsekas and Steven Shreve.Stochastic optimal control: the discrete-time case. 2004. Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. Advances in Neural Information Processing Systems, 32, 2019. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. InJoint European conference on machine learning and knowledge discovery in databases, pages 387–402. Springer, 2013. Leon Bungert, Nicolás García Trillos, and Ryan Murray. The geometry of adversarial training in binary classiﬁcation. arXiv preprint arXiv:2111.13613, 2021. Phil Long and Rocco Servedio. Consistency versus realizable h-consistency for multiclass classiﬁcation. In International Conference on Machine Learning, pages 801–809. PMLR, 2013. Laurent Meunier, Meyer Scetbon, Rafael B Pinot, Jamal Atif, and Yann Chevaleyre. Mixed nash equilibria in the adversarial examples game. InInternational Conference on Machine Learning, pages 7677–7687. PMLR, 2021. Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif. Randomization matters. how to defend against strong adversarial attacks.International Conference on Machine Learning, 2020. Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings. In International Conference on Machine Learning. 2020. Muni Sreenivas Pydi and Varun Jog. The many faces of adversarial risk.Advances in Neural Information Processing Systems, 34, 2021. Ingo Steinwart. How to compare diﬀerent loss functions and their risks.Constructive Approximation, 26(2): 225–287, 2007. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. InInternational Conference on Learning Representations, 2014. 11Deng Yao, Zheng Xi, Zhang Tianyi, Chen Chen, Lou Guannan, and Kim Miryung. An analysis of adversarial attacks and defenses on autonomous driving models. In18th Annual IEEE International Conference on Pervasive Computing and Communications. IEEE, 2020. Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32(1):56–85, 2004. 12Appendix A Equivalent deﬁnitions for calibration and consistency Consistency. A lossL2 is consistent wrt. a lossL1 if and only if for everyξ >0, there existsδ >0 such that for everyf ∈F(X), RL2,P(f) −R⋆ L2,P ≤δ =⇒ RL1,P(f) −R⋆ L1,P ≤ξ Calibration. L2 is calibrated with regards toL1 if for allη∈[0,1], x∈X, for all(fn)n ∈F(X)N: CL2 (x,η,f ) −C⋆ L2 (x,η) −−−−→ n→∞ 0 =⇒ CL1 (x,η,f ) −C⋆ L1 (x,η) −−−−→ n→∞ 0 . Also, L2 is uniformly calibratedwith regards toL1 if for all(fn)n ∈F(X)N: sup η∈[0,1],x∈X CL2 (x,η,f ) −C⋆ L2 (x,η) −−−−→ n→∞ 0 =⇒ sup η∈[0,1],x∈X CL1 (x,η,f ) −C⋆ L1 (x,η) −−−−→ n→∞ 0 . B Proof of Proposition 2.1 Proposition. Let φ : R ×Y →R be a measurable function andε ≥0. For every f ∈F (X), (x,y) ↦→ φε(x,y,f ) and (x,y) ↦→l0/1,ε(x,y,f ) are universally measurable. Proof. Let φ: R →R+ be a continuous function. We deﬁneφε(x,y,f ) = supx′∈Bε(x) φ(yf(x)). We have : φε(x,y,f ) = sup (x′,y′)∈X×Y φ(y′f(x′)) −∞× 1{d(x′,x) ≥ε or y′̸= y} We have that ((x,y),(x′,y′)) ↦→φ(y′f(x′)) −∞× 1{d(x′,x) ≥ε or y′̸= y} deﬁnes a measurable, hence upper semi-analytic function. Using [Bertsekas and Shreve, 2004, Proposition 7.39, Corollary 7.42], we get that for allf ∈F(X), (x,y) ↦→φε(x,y,f ) is a universally measurable function. C Proof of Proposition 2.2 Proposition. Let ε> 0. Let φ be a continuous classiﬁcation margin loss. For allx∈X and η∈[0,1], C⋆ φε(x,η) = inf f∈F(X) Cφε(x,η,f ) = inf α∈R ηφ(α) + (1−η)φ(−α) = C⋆ φ(x,η) . The last equality also holds for the0/1 loss. Proof. For anyf ∈F(X), we have: Cφε(x,η,f ) = η sup x′∈Bε(x) φ(f(x′)) + (1−η) sup x′∈Bε(x) φ(−f(x′)) ≥ηφ(f(x)) + (1−η)φ(−f(x)) ≥inf α∈R ηφ(α) + (1−η)φ(−α) . 13Then we deduce thatinff∈F(X) Cφε(x,η,f ) ≥infα∈R ηφ(α) + (1−η)φ(−α). On the other side, let(αn)n be a minimizing sequence such thatηφ(αn) + (1 −η)φ(−αn) −−−−→ n→∞ infα∈R ηφ(α) + (1 −η)φ(−α). We set fn : x↦→αn for allx. Then we have: inf f∈F(X) Cφε(x,η,f ) ≤Cφε(x,η,f n) = ηφ(αn) + (1−η)φ(−αn) −−−−→ n→∞ inf α∈R ηφ(α) + (1−η)φ(−α). Then we conclude that:C⋆ φε(x,η) = infα∈R ηφ(α) + (1−η)φ(−α). D Proof of Theorem 3.1 Theorem (Necessary condition for Calibration). Let φ be a continuous margin loss and ε > 0. If φ is adversarially calibrated at level ε, then φ is calibrated in the standard classiﬁcation setting and 0 ̸∈ argminα∈¯R 1 2 φ(α) + 1 2 φ(−α). Proof. Let us show that if0 ∈argminα∈¯R φ(α) +φ(−α) then φis not calibrated for the adversarial problem. For that, letx∈X and we ﬁxη = 1 2 . For n≥1, we deﬁnefn(u) = 1 n for u̸= x and −1 n for u= x. Since |Bε(x)|≥ 2, we have Cφε(x,1 2,fn) = max ( φ( 1 n),φ(−1 n) ) −−−−→ n→∞ φ(0) As,φ(0) = infα∈¯R 1 2 (φ(α) + φ(−α)), theabovemeansthat (fn)n isaminimizingsequencefor α↦→1 2 (φ(α) + φ(−α)). Then thanks to Proposition 2.2,(fn)n is also a minimizing sequence forf ↦→Cφε(x,1 2 ,f). However, for every integer n, we haveCε(x,1 2 ,fn) = 1 ̸= 1 2 . As inff∈F(X) Cε(x,1 2 ,f) = 1 2 , φ is not calibrated with regard to the 0/1 loss in the adversarial setting at levelε. We also immediately notice that ifφ is calibrated with regard to 0/1 loss in the adversarial setting at levelε then φ is calibrated in the standard setting. E Proof of Theorem 3.2 Theorem (Suﬃcient condition for Calibration). Let φ be a continuous margin loss andε >0. If φ is decreasing and strictly decreasing in a neighbourhood of0 and calibrated in the standard setting and0 ̸∈ argminα∈¯R 1 2 φ(α) + 1 2 φ(−α), thenφ is adversarially uniformly calibrated at levelε. Proof. Let ξ ∈(0,1 2 ). Thanks to Theorem 2.1,φ is uniformly calibrated in the standard setting, then there exists δ >0, such that for allx∈X, η∈[0,1], f ∈F(X): Cφ(x,η,f ) −C⋆ φ(x,η) ≤δ =⇒ C(x,η,f ) −C⋆(x,η) ≤ξ. Case η̸= 1 2 : Let x∈X and f ∈F(X) such that: Cφε(x,η,f ) −C⋆ φε(x,η) = sup u,v∈Bε(x) ηφ(f(u)) + (1−η)φ(−f(v)) −C⋆ φε(x,η) ≤δ We recall thanks to Proposition 2.2 that for everyu,v ∈X, C⋆ φε(u,η) = C⋆ φ(v,η) = inf α∈R ηφ(α) + (1−η)φ(−α) . Then in particular, for allx′∈Bε(x), we have: Cφ(x′,η,f ) −C⋆ φ(x′,η) ≤ sup u,v∈Bε(x) ηφ(f(u)) + (1−η)φ(−f(v)) −C⋆ φε(x,η) ≤δ . 14Then since φ is calibrated for standard classiﬁcation, for allx′ ∈Bε(x), C(x′,η,f ) −C⋆(x′,η) ≤ξ. Since, ξ <1 2 , we haveC(x′,η,f ) = C⋆(x′,η) and then for allx′∈Bε(x), f(x′) <0 if η <1/2 or f(x′) ≥0 if η >1/2. We then deduce that Cε(x,η,f ) = η sup x′∈Bε(x) 1f(x′)≤0 + (1 −η) sup x′∈Bε(x) 1f(x′)>0 = min(η,1 −η) = C⋆ ε(x,η) Then we deduce,Cε(x,η,f ) −C⋆ ε(x,η) ≤ξ. Case η = 1 2 : This shows us that calibration problems will only arise whenη = 1 2 , i.e. on points where the Bayes classiﬁer is indecise. For this case, we will reason by contradiction: we can construct a sequence of pointsαn and βn, whose risks converge to the same optimal value, while one sequence remains close to some positive value, and the other to some negative value. Assume that for alln, there existfn ∈F(X) and xn ∈X such that Cφε(xn,1 2,fn) −C⋆ φε(xn,1 2) ≤1 n and there existsun,vn ∈Bε(xn), such that fn(un)fn(vn) ≤0 Let us denoteαn = fn(un) and βn = fn(vn). Moreover, we have thanks to Proposition 2.2: 0 ≤1 2φ(αn) + 1 2φ(−αn) −inf u∈R [1 2φ(u) + 1 2φ(u) ] ≤Cφε(x,1 2,fn) −C⋆ φε(x,1 2) ≤1 n Then we deduce that(αn)n is a minimizing sequence foru↦→1 2 φ(u) + 1 2 φ(−u) and similarly(βn)n is also a minimizing sequence foru↦→1 2 φ(u) + 1 2 φ(−u) . Now note that there always existα,β ∈¯R such that, up to an extraction of a subsequence, we haveαn −−−−→ n→∞ α and βn −−−−→ n→∞ β. Furthermore by continuity ofφ and since 0 ̸∈argmin φ(u) +φ(−u), α̸= 0 and β ̸= 0. Without loss of generality one can assume thatα< 0 <β , then forn suﬃciently large, αn <0 <βn. Moreover we have 0 ≤1 2 max (φ(αn),φ(βn)) + 1 2 max (φ(−αn),φ(−βn)) −C⋆ φε(x,1 2) ≤Cφε(x,1 2,fn) −C⋆ φε(x,1 2) ≤1 n so that we deduce: 1 2 max (φ(αn),φ(βn)) + 1 2 max (φ(−αn),φ(−βn)) −→inf u∈R [1 2φ(u) + 1 2φ(u) ] (2) Since, fornsuﬃciently large, αn <0 <βn and φis decreasing and strictly decreasing in a neighbourhood of 0, we have that: max (φ(αn),φ(βn)) = φ(αn) and max (φ(−αn),φ(−βn)) = φ(−βn). 15Moreover, there existsλ >0 such that forn suﬃciently large φ(αn) −φ(βn) ≥λ. Then for n suﬃciently large: 1 2 max (φ(αn),φ(βn)) + 1 2 max (φ(−αn),φ(−βn)) = 1 2φ(αn) + 1 2φ(−βn) = 1 2 (φ(αn) −φ(βn)) + 1 2φ(−βn) + 1 2 + φ(βn) ≥1 2λ+ inf u∈R [1 2φ(u) + 1 2φ(u) ] which leads to a contradiction with Equation 2. Then there exists a non zero integern0 such that for all f ∈F(X), x∈X Cφε(x,1 2,f) −C⋆ φε(x,1 2) ≤ 1 n0 =⇒ ∀u,v ∈Bε(x), f(u) ×f(v) >0. The right-hand term is equivalent to: for allu∈Bε(x), f(u) >0 or for allu∈Bε(x), f(u) <0. Then Cε(x,η,f ) = 1 2 and thenCε(x,η,f ) = C⋆ ε(x,η) Putting all that together, for allx∈X, η∈[0,1], f ∈F(X): Cφε(x,η,f ) −C⋆ φε(x,η) ≤min(δ, 1 n0 ) =⇒ Cε(x,η,f ) −C⋆ ε(x,η) ≤ξ. Then φ is adversarially uniformly calibrated at levelε F Proof of Proposition 3.1 Proposition. Let φ be a shifted odd margin loss. For everyε> 0, φ is adversarially calibrated at levelε. Proof. Let λ> 0, τ >0 and φbe a strictly decreasing odd function such that˜φdeﬁned as˜φ(α) = λ+φ(α−τ) is non-negative. Proving that 0 /∈argmint∈¯R 1 2 ˜φ(t) + 1 2 ˜φ(−t). φ is clearly strictly decreasing and non-negative then it admits a limitl:= −limt→+∞˜φ(t) ≥0. Then we have: lim t→+∞ ˜φ(t) = λ+ l and lim t→−∞ ˜φ(t) = λ−l Consequently we have: lim t→∞ 1 2 ˜φ(t) + 1 2 ˜φ(−t) = λ On the other side ˜φ(0) = λ+ φ(−τ) > λ+ φ(0) = λ since τ >0 and φ is strictly decreasing. Then 0 /∈argmint∈¯R 1 2 ˜φ(t) + 1 2 ˜φ(−t). Proving that˜φ is calibrated for standard classiﬁcation.Let ξ >0, η∈[0,1], x∈X. If η= 1 2 , then for allf ∈F(X), C(x,1 2 ,f) = C⋆(x,1 2 ) = 1 2 . Let us now assume thatη̸= 1 2 , we have for allf ∈F(X): C˜φ(x,η,f ) = λ+ ηφ(f(x) −τ) + (1−η)φ(−f(x) −τ) = λ+ (η−1 2) (φ(f(x) −τ) −φ(−f(x) −τ)) + 1 2 (φ(f(x) −τ) + φ(−f(x) −τ)) 16Let us show thatargmint∈¯R 1 2 ˜φ(t) + 1 2 ˜φ(−t) = {−∞,+∞}. We have for allt: 1 2 ˜φ(t) + 1 2 ˜φ(−t) = λ+ 1 2 (φ(t−τ) + φ(−t−τ)) = λ+ 1 2 (φ(t−τ) −φ(t+ τ)) >λ since t−τ < t+ τ and φ is strictly decreasing. Hence by continuity ofφ the optimum are attained when t→∞ or t→−∞. Then argmint∈¯R 1 2 ˜φ(t) + 1 2 ˜φ(−t) = {−∞,+∞}. Without loss of generality, letη >1/2, then t↦→(η−1 2) (φ(t−τ) −φ(−t−τ)) is strictly decreasing andargmint∈¯R 1 2 (φ(t−τ) + φ(−t−τ)) = {−∞,+∞}, then we have argmin t∈¯R λ+ (η−1 2) (t−τ) −φ(−t−τ)) + 1 2 (φ(t−τ) + φ(−t−τ)) = {+∞} . By continuity ofφ, we deduce that forδ >0 suﬃciently small: C˜φ(x,η,f )−C⋆ ˜φ(x,η) ≤δ =⇒ f(x) >0 The same reasoning holds forη <1 2 . Then we deduce that˜φ is calibrated for standard classiﬁcation. Finally, we obtain that˜φ is calibrated for adversarial classiﬁcation for everyε> 0. G Proof of Proposition 4.1 Proposition. Let ε >0. Let P be an ε-realisable distribution and φ be a calibrated margin loss in the standard setting. Thenφ is adversarially consistent at levelε. To formally prove this result, we need a preliminary lemma. Lemma G.1.Let P be anε-realisable distribution andφ be a calibrated margin loss in the standard setting. Then R⋆ φε,P = infα∈R φ(α). Proof. Let a ∈R be such that φ(a) −infα∈R φ(α) ≤ξ. P being ε-realisable, there exists a measurable function f such that: Rε,P(f) = EP [ sup x′∈Bε(x) 1ysign(f(x))≤0 ] = P[∃x′∈Bε(x),sign(f(x′)) ̸= y] ≤ξ′:= ξ max(1,φ(−a)). Denoting p= P(y= 1), P1 = P[·|y= 1] and P−1 = P[·|y= −1], we have: p×P1 [∃x′∈Bε(x),f(x′) <0] ≤ξ′ and (1 −p) ×P−1 [∃x′∈Bε(x),f(x′) ≥0] ≤ξ′ . 17Let us now deﬁneg as: g(x) = { a if f(x) ≥0 −a if f(x) <0 We have: Rφε,P(g) = EP [ sup x′∈Bε(x) φ(yg(x)) ] = p×EP1 [ sup x′∈Bε(x) φ(g(x)) ] + (1 −p) ×EP−1 [ sup x′∈Bε(x) φ(−g(x)) ] We have: p×EP1 [ sup x′∈Bε(x) φ(g(x)) ] ≤p×EP1 [ sup x′∈Bε(x) φ(g(x))1f(x′)<0 ] + p×EP1 [ sup x′∈Bε(x) φ(g(x))1f(x′)≥0 ] = φ(−a) ×p×P1 [∃x′∈Bε(x),f(x′) <0] + φ(a) ×p×(1 −P1 [∃x′∈Bε(x),f(x′) <0]) ≤φ(−a)ξ′+ p×φ(a) ≤p×inf α∈R φ(α) + 2ξ Similarly, we have: (1 −p) ×EP−1 [ sup x′∈Bε(x) φ(−g(x)) ] ≤(1 −p) ×inf α∈R φ(α) + 2ξ We get:Rφε,P(g) ≤infα∈R φ(α) + 4ξ and, henceR⋆ φε,P = infα∈R φ(α). We are now ready to prove the result of consistency in the realizable case. Proof. Let 0 < ξ <1. Thanks to Theorem 2.1,φ is uniformly calibrated for standard classiﬁcation, then, there existsδ >0 such that for allf ∈F(X) and for allx: φ(yf(x)) −inf α∈R φ(α) ≤δ =⇒ 1ysignf(x)≤0 = 0 Let nowf ∈F(X) be such thatRφε,P(f) ≤R⋆ φε,P + δξ. Thanks to Lemma G.1, we have: Rφε,P(f) −R⋆ φε,P = EP [ sup x′∈Bε(x) φ(yf(x)) −inf α∈R φ(α) ] ≤δξ Then by Markov inequality: P [ sup x′∈Bε(x) φ(yf(x)) −inf α∈R φ(α) ≥δ ] ≤ EP [ supx′∈Bε(x) φ(yf(x)) −infα∈R φ(α) ] δ ≤ξ 18So we haveP[∀x′∈Bε(x),φ(yf(x)) −infα∈R φ(α) ≤δ] ≥1 −ξ and then P [ ∀x′∈Bε(x),1ysign(f(x))≤0 = 0 ] ≥1 −ξ . Since P is ε-realisable, we haveR⋆ ε,P = 0 and: Rε,P(f) −R⋆ ε,P = Rε,P(f) = P[∃x′∈Bε(x),sign(f(x′)) ̸= y] ≤ξ which concludes the proof. H Proof of the existence of a maximum in Theorem 4.1 Theorem (Pydi and Jog [2021]). Let X be a Polish space satisfying the midpoint property. Then strong duality holds: R⋆ ε(P) = inf f∈F(X) sup Q∈Aε(P) RQ(f) = sup Q∈Aε(P) inf f∈F(X) RQ(f) Moreover the supremum of the right-hand term is attained. We give a proof for the existence of the supremum. Proof. To prove that, note that for every Borel probability distributionQ over X×Y , inf f∈F(X) RQ(f) = (1 −q) + inf f∈C(X), 0≤f≤1 ∫ fd(qQ1 + (q−1)Q−1) where C(X) denotes the space of continuous functions onX, q= Q[y= 1] and Qi = Q[·| y= i]. When f is continuous and bounded, the function: µ∈M(X) ↦→ ∫ fdµ is continuous for the weak topology of measures, then: µ∈M(X) ↦→ inf f∈C(X), 0≤f≤1 ∫ fdµ is upper semi continuous for the weak topology of measures, as it is the inﬁnum of continuous functions. Then using the compacity ofAε(P), we deduce that the supremum is attained. I Proofs of Theorem 4.2 Theorem. Let X be a Polish space satisfying the midpoint property. Letε ≥0, P be a Borel probability distribution overX×Y , andφ be a0/1-like margin loss. Then, we have: R⋆ φε,P = R⋆ ε,P To prove this result, we need the following lemma. Lemma I.1. Let Q be a Borel probability distribution overX×Y and φ be a 0/1-like margin loss, then: R⋆ φ,Q = R⋆ Q. 19Proof. Bartlett et al. [2006], Steinwart [2007] proved that: for every margin lossesφ, R⋆ φ,Q = inf f∈F(X) E(x,y)∼Q [φ(yf(x))] = Ex∼Qx [ inf α∈R [Q(y= 1|x)φ(α) + (1−Q(y= −1|x))φ(−α)] ] = Ex∼Qx [ C⋆ φ(Q(y= 1|x),x) ] We also haveR⋆ Q = Ex∼Qx [C⋆(Q(y= 1|x),x)]. Moreover, ifφ is a0/1-like margin loss, one can prove easily that for every x ∈ Xand η ∈ [0,1], C⋆ φ(η,x) = min( η,1 −η) = C⋆(η,x). We can then conclude that R⋆ φ,Q = R⋆ Q. We can now prove Theorem 4.2. Proof. Let ξ >0 and P be a Borel probability distribution overX×Y . Let f such thatRε,P(f) ≤R⋆ ε,P + ξ. Let a> 0 such thatφ(a) ≤ξ and φ(−a) ≥1 −ξ. We deﬁneg as: g(x) = { a if f(x) ≥0 −a if f(x) <0 We haveφ(yg(x)) = φ(a)1ysign(f(x))>0 + φ(−a)1ysign(f(x))≤0. Then Rφε,P(g) = EP [ sup x′∈Bε(x) φ(yg(x)) ] = EP [ sup x′∈Bε(x) φ(−a)1ysign(f(x′))≤0 + φ(a)1ysign(f(x′))>0 ] ≤EP [ sup x′∈Bε(x) 1ysign(f(x′))≤0 ] + φ(a) ≤R⋆ ε,P + 2ξ . Then we haveR⋆ φε,P ≤R⋆ ε,P. On the other side, we have: R⋆ φε,P ≥ sup Q∈Aε(P) inf f∈F(X) Rφ,Q(f) = sup Q∈Aε(P) R⋆ φ,Q = sup Q∈Aε(P) R⋆ Q from Lemma I.1 = sup Q∈Aε(P) inf f∈F(X) RQ(f) = inf f∈F(X) sup Q∈Aε(P) RQ(f) = R⋆ ε,P The last step is a consequence of Theorem 4.1. Then ﬁnally we get thatR⋆ φε,P = R⋆ ε,P. 20J Proofs of Corollaries 4.1 and 4.2 Corollary (Strong duality forφ). Let us assume thatX is a Polish space satisfying the midpoint property. Let ε≥0, P be a Borel probability distribution overX×Y , andφ be a0/1-like margin loss. Then, we have: inf f∈F(X) sup Q∈Aε(P) Rφ,Q(f) = sup Q∈Aε(P) inf f∈F(X) Rφ,Q(f) Moreover the supremum is attained. Corollary (Optimal attacks). Let assume thatX be a Polish space satisfying the midpoint property. Let ε≥0 and P be a Borel probability distribution overX×Y . Then, an optimal attackQ⋆ of levelε exists for both the0/1 loss andφ. Moreover, forQ ∈Aε(P). Q is an optimal attack for the lossφ if and only if it is an optimal attack for the0/1 loss. Proof. We have: inf f∈F(X) sup Q∈Aε(P) Rφ,Q(f) = R⋆ φε,P = R⋆ ε,P by Theorem 4.2 = inf f∈F(X) sup Q∈Aε(P) RQ(f) = sup Q∈Aε(P) inf f∈F(X) RQ(f) = sup Q∈Aε(P) R⋆ Q(f) = sup Q∈Aε(P) R⋆ φ,Q(f) by Lemma I.1 = sup Q∈Aε(P) inf f∈F(X) Rφ,Q(f) Q ↦→inff∈F(X) Rφ,Q(f) = inf f∈F(X) RQ(f) is upper semi-continuous for the weak topology of measures. Moreover, Aε(P) is compact for the weak topology of measures, thenQ ↦→inff∈F(X) Rφ,Q(f) admits a maximum overAε(P). AndQ is an optimal attack for the lossφif and only if it is an optimal attack for the 0/1 loss. K Proof of Proposition 4.2 Proposition. Let us assume thatX be a Polish space satisfying the midpoint property. Letε≥0 and P be a Borel probability distribution overX×Y. LetQ⋆ be an optimal attack of levelε. Let(fn)n∈N be a sequence of F(X) such thatRφε,P(fn) →R⋆ φε,P. Then RQ⋆(fn) →R⋆ ε,P. Proof. Let (fn)n∈N be a sequence ofF(X) such thatRφε,P(fn) →R⋆ φε,P. Let Q⋆ be an optimal attack of level ε. From Corollary 4.1, we get that: R⋆ φε,P = R⋆ φ,Q⋆ . Then we get 0 ≤Rφ,Q⋆(fn) −R⋆ φ,Q⋆ ≤Rφε,P(fn) −R⋆ φε,P from which we deduce that:Rφ,Q⋆(fn) →R⋆ φ,Q⋆. Since φ is consistent in the standard classiﬁcation setting, we then have RQ⋆(fn) →R⋆ Q⋆ . 21L About H-calibration Our results naturally extend toH-calibration. With mild assumptions onH, it is possible to recover all the results made on calibration onF(X). First, it is worth noting that, ifHcontains all constant functions, then most results about calibration in the adversarial setting extend. Proposition 2.2 naturally extends to H-calibration as long asHcontains all constant functions. Proposition. Let H⊂F (X). Let us assume thatHcontains all constant functions. Letε> 0 and φ be a continuous classiﬁcation margin loss. For allx∈X and η∈[0,1], we have C⋆ φε,H(x,η) = C⋆ φ,H(x,η) = inf α∈R ηφ(α) + (1−η)φ(−α) = C⋆ φε(x,η) = C⋆ φ(x,η) . The last equality also holds for the adversarial0/1 loss. The proof is exactly the same as for Proposition 2.2 since we used a constant function to prove the equality. Under the same assumptions, the notion ofH-calibration and uniformH-calibration are equivalent in the standard setting. Proposition. Let H⊂F (X). Let us assume thatHcontains all constant functions. Letφ be a continuous classiﬁcation margin loss.φis uniformlyH-calibrated for standard classiﬁcation if and only ifφis uniformly calibrated for standard classiﬁcation. It also holds for non-uniform calibration. Proof. Let us assume thatφis a continuous classiﬁcation margin loss and thatφis uniformly calibrated. Let ξ >0. There existsδ >0 such that, for allη∈[0,1], x∈X and f ∈F(X): Cφ(x,η,f ) −C⋆ φ(x,η) ≤δ =⇒ C(x,η,f ) −C⋆(x,η) ≤ξ . Let η ∈[0,1], x ∈ Xand f ∈ Hsuch that Cφ(x,η,f ) −C⋆ φ,H(x,η) ≤δ. Thanks to Proposition L, C⋆ φ,H(x,η) = C⋆ φ(x,η), andf ∈F(X), thenCφ(x,η,f ) −C⋆ φ(x,η) ≤δ and then: C(x,η,f ) −C⋆ H(x,η) = C(x,η,f ) −C⋆(x,η) ≤ξ Then φ is uniformlyH-calibrated in standard classiﬁcation. Reciprocally, let us assume that φ is a continuous classiﬁcation margin loss and thatφ is uniformly H-calibrated. Let ξ >0. There existsδ >0 such that, for allη∈[0,1], x∈X and f ∈H: Cφ(x,η,f ) −C⋆ φ,H(x,η) ≤δ =⇒ C(x,η,f ) −C⋆ H(x,η) ≤ξ . Let η ∈[0,1], x ∈X and f ∈H such that Cφ(x,η,f ) −C⋆ φ,H(x,η) ≤δ. Cφ(x,η,f ) = ηφ(f(x)) + (1 − η)φ(−f(x)). Let ˜f : u ↦→f(x) for all u ∈X , then ˜f ∈H since ˜f is constant, Cφ(x,η,f ) = Cφ(x,η, ˜f) and C(x,η,f ) = C(x,η, ˜f). Thanks to the previous proposition,C⋆ φ,H(x,η) = C⋆ φ(x,η). Then: Cφ(x,η, ˜f) − C⋆ φ,H(x,η) ≤δ and then: C(x,η,f ) −C⋆ φ,H(x,η) = C(x,η, ˜f) −C⋆ φ(x,η) ≤ξ Then φ is uniformly calibrated in standard classiﬁcation. We can now obtain the necessary and suﬃcient conditions as follows. They are really similar to the adversarial calibration ones. Proposition (Necessary conditions forH-Calibration of adversarial losses). Let ε> 0. LetH⊂F (X). Let us assume that Hcontains all constant functions and that there existsx ∈X and (fn)n ∈HN such that fn(u) →0 for all u ∈Bε(x) and for alln ∈N, supu∈Bε(x) fn(u) > 0 and infu∈Bε(x) fn(u) < 0 Let φ be a continuous margin loss . Ifφis adversarially uniformlyH-calibrated at levelε, thenφis uniformly calibrated in the standard classiﬁcation setting and0 ̸∈argminα∈¯R 1 2 φ(α) + 1 2 φ(−α). 22Proposition (Suﬃcient conditions forH-Calibration of adversarial losses). Let H⊂F (X). Let us assume that Hcontains all constant functions. Letφ be a continuous strictly decreasing margin loss andε> 0. If φ is calibrated in the standard classiﬁcation setting and0 ̸∈argminα∈¯R 1 2 φ(α)+ 1 2 φ(−α), thenφis adversarially uniformly H-calibrated at levelε.x The proofs are the same as for the adversarial calibration setting. Note however that the assumptions on Hare very weak: for instance, the set of linear classiﬁers H= { x↦→⟨w,x⟩+ b|w∈Rd,b ∈R } satisﬁes the existence ofx ∈X and (fn)n ∈HN such thatfn(u) →0 for all u ∈Bε(x) and for alln ∈N, supu∈Bε(x) fn(u) >0 and infu∈Bε(x) fn(u) <0. 23",
      "references": [],
      "meta_data": {
        "arxiv_id": "2205.10022v1",
        "authors": [
          "Laurent Meunier",
          "Raphaël Ettedgui",
          "Rafael Pinot",
          "Yann Chevaleyre",
          "Jamal Atif"
        ],
        "published_date": "2022-05-20T08:30:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper investigates the problem of consistency in adversarial classification, i.e., whether surrogate losses can proxy the 0/1 loss when inputs can be adversarially perturbed at test time. It shows that, unlike standard classification, no convex margin loss can be calibrated or consistent in the adversarial setting. It identifies necessary and sufficient conditions for a surrogate to be calibrated in both adversarial and standard frameworks, and introduces shifted odd losses as a new class of calibrated surrogates for adversarial robustness. The authors analyze both the ε-realisable (realizable) case and the general case via min–max duality, establishing strong duality results and connections to the 0/1 loss, and outline directions for constructing adversarially consistent surrogates.",
        "methodology": "The work develops formal definitions of adversarial losses (e.g., adversarial 0/1 loss and adversarial margin losses) and the concepts of calibration and consistency in the adversarial setting. It proves necessary and sufficient conditions for calibration (Theorems 3.1 and 3.2), shows that common convex losses (and many non-convex ones) fail to be adversarially calibrated (Corollaries 3.1–3.2), and introduces shifted odd margin losses (Definition 3.1) that are provably adversarially calibrated (Proposition 3.1). It analyzes realizable versus general cases (Proposition 4.1, Theorem 4.1), uses min–max duality with adversarial distributions Aε(P) (Pydi & Jog foundations), and establishes connections between 0/1-like margin losses and the 0/1 loss (Theorem 4.2, Corollaries 4.1, 4.2). The paper also discusses H-calibration extensions and provides proofs in the appendices.",
        "experimental_setup": "This is a theoretical paper; there are no empirical datasets or experiments. All results are established via mathematical definitions, theorems, and proofs, with illustrative figures (e.g., Figure 1) and extensive appendices containing supplementary proofs (Appendices B–L).",
        "limitations": "The results rely on a set of structural assumptions (continuous margin losses, the midpoint property of X, ε-ball perturbations, and the chosen adversarial 0/1 loss variant). The key takeaway that no convex margin loss is adversarially calibrated does not easily translate into practical training algorithms, and even the favorable shifted odd losses require non-convex optimization and careful tuning. While the realizable case yields stronger consistency results, a full general, non-realisable consistency proof for shifted odd losses remains open. Extensions to H-calibration are provided but require additional conditions (e.g., H containing constants).",
        "future_research_directions": "Develop and analyze a broader class of adversarially consistent surrogates beyond shifted odd losses; aim to prove full adversarial consistency for shifted odd losses and related non-convex families; extend calibration and consistency results to H-calibration with weaker assumptions; design practical optimization strategies for non-convex adversarial surrogates and study their computational properties; empirically validate robustness on standard adversarial benchmarks; explore connections with adversarial Bayes via optimal transport and game-theoretic frameworks; investigate extensions to more general perturbation models and higher‑order adversaries.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization",
      "full_text": "Domain Generalization for Medical Imaging Classiﬁcation with Linear-Dependency Regularization Haoliang Li1 YuFei Wang1 Renjie Wan1 Shiqi Wang2 Tie-Qiang Li3,4 Alex C. Kot1 1Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore 2Department of Computer Science, City University of Hong Kong, China 3Department of Clinical Science, Intervention, and Technology, Karolinska Institute, Sweden 4Department of Medical Radiation and Nuclear Medicine, Karolinska University Hospital, Sweden {lihaoliang,yufei001,rjwan,eackot}@ntu.edu.sg shiqwang@cityu.edu.hk tie-qiang.li@ki.se Abstract Recently, we have witnessed great progress in the ﬁeld of medical imaging classiﬁ- cation by adopting deep neural networks. However, the recent advanced models still require accessing sufﬁciently large and representative datasets for training, which is often unfeasible in clinically realistic environments. When trained on limited datasets, the deep neural network is lack of generalization capability, as the trained deep neural network on data within a certain distribution (e.g. the data captured by a certain device vendor or patient population) may not be able to gener- alize to the data with another distribution. In this paper, we introduce a simple but effective approach to improve the generalization capability of deep neural networks in the ﬁeld of medical imaging classiﬁcation. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a representative feature space through variational encoding with a novel linear-dependency regularization term to capture the shareable information among medical data collected from different domains. As a result, the trained neural network is expected to equip with better generalization capability to the “unseen\" medical data. Experimental results on two challenging medical imaging classiﬁca- tion tasks indicate that our method can achieve better cross-domain generalization capability compared with state-of-the-art baselines. 1 Introduction Due to the breakthrough in machine learning and deep learning, recent years have witnessed numerous signiﬁcant successes in various medical imaging tasks. However, one of the limitations of deep learning is that it lacks generalization capability when the number of training data is not sufﬁcient [44]. In practice, it is often the case that the testing data (a.k.a. target domain) can be dissimilar to the training data (a.k.a. source domain) in terms of many factors, such as imaging protocol, device vendors and patient populations. Such domain shift problem can lead to a signiﬁcantly negative impact on the performance of medical imaging classiﬁcation. To tackle such domain shift problem, domain adaptation [ 31] aims to transfer the knowledge from a source domain to a different but relevant target domain. Recently, many studies have been conducted to improve the transferable capability in the ﬁeld of medical imaging classiﬁcation with domain adaptation by assuming that target domain data are accessible [43, 8]. In many cases, requiring to access the target domain data in advance may not be feasible. For example, in the real-time clinical application scenario, it is difﬁcult to collect sufﬁcient target domain data to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2009.12829v3  [cs.CV]  29 Oct 2020help with network training. For another example, it is also difﬁcult to access the target domain data as many medical data are protected by privacy regulation. Thus, it is natural to ask whether we can still learn a generalized deep neural network without any prior knowledge regarding the target domain. Domain generalization has been proposed to tackle this problem by assuming to have no access to the target information but utilizing multiple source domains’ information to better generalize to the “unseen\" new domain for testing. Generally speaking, current research regarding domain generalization in the ﬁeld of medical imaging classiﬁcation can be categorized into two streams. The ﬁrst stream aims at conducting data aug- mentation based on medical imaging data in terms of image quality, image appearance and spatial shape [42]. Although the variation of medical images turns out to be more compact as the capturing environment can be ﬁxed in advanced compared with the images captured in our daily life, it may be difﬁcult to choose suitable augmentation types and magnitudes for clinical deployment purposes in a certain environment. The other stream leverages the advantage of domain alignment or meta-learning methods for feature representation learning [39, 7]. However, the learned feature representation may still suffer from the overﬁtting problem, as the feature representations are only shareable among multiple source domains which may not be able to generalize to target. In this work, we propose to marriage the advantage of data augmentation and domain alignment to tackle the domain generalization problem for medical imaging classiﬁcation. Instead of directly conducting augmentation in the image domain through some linear transformations with pre-deﬁned parameters [42], we assume that there exists linear dependency in a latent space among various domains. To model such linear dependency, we propose to train a deep neural network with a novel rank regularization term on latent feature space by setting the rank of latent feature to be the number of categories. Meanwhile, we also propose to restrict the distribution of latent features to follow a pre- deﬁned prior distribution through variational encoding. We theoretically prove that an upper bound on the empirical risk of any “unseen\" but related target domain can be achieved under our formulation, such that the overﬁtting problem can be alleviated. Experimental results on two challenging medical imaging classiﬁcation tasks, including imbalanced-category based skin lesion classiﬁcation as well as spinal cord gray matter segmentation (which can be treated as pixel-wise classiﬁcation), indicate that our proposed method can achieve much better generalization capability compared with other state-of-the-art baselines. The code is available at https://github.com/wyf0912/LDDG. 2 Related Works Domain Adaptation and Generalization. To tackle the domain-shift problem between source and target domain data, traditional domain adaptation approaches focused on either subspace learning or instance re-weighting [16, 30, 41, 11]. Deep learning methods are also proved to be effective for domain adaptation task through either distribution alignment (e.g. Maximum Mean Discrepancy) [26] or adversarial learning through feature level [10, 35] or pixel level [3]. Recently, it has been shown that by considering pixel level adaptation and feature level adaptation together, better adaptation performance can be achieved [15, 23]. Compared with domain adaptation, domain generalization is much more challenging, as we assume that we have no access to the target domain. Instead, we aim to train a model that is expected to be generalized to the “unseen\" target by assuming that only multiple source domains are available. For example, Yang and Gao [38] proposed to leverage Canonical Correlation Analysis (CCA) to extract shareable information among domains. Muandet et al. [29] proposed a Domain Invariant Component Analysis (DICA) algorithm to learn an empirical mapping based on multiple source- domain data where the distribution mismatch across domains was minimized. This idea was further extended by [22, 24] in an autoencoder framework with distribution regularization on latent space. In [37, 20], the low-rank regularization based on classiﬁer and model parameters were explored to extract universal feature representation. Ghifary et al. [12] proposed a multi-task autoencoder to learn domain invariant features by reconstructing the latent representation of a given sample from one domain to another domain. Motiian et al. [28] proposed to minimize the semantic alignment loss as well as the separation loss based on deep learning models. Carlucci et al. [4] proposed to shufﬂe the image patch to learn generalized feature representation. Recently, Wang et al. [36] proposed to extend MixUp [40] to the settings of multiple domains for heterogeneous domain generalization task. As for meta-learning based techniques, Li et al. [21] proposed to transfer the idea in [9] to the “unseen\" target domain setting by randomly constructing meta-train and meta-test set, which was 2further extended by Balaji et al. [1] with a scheme to learn a regularization network to improve the scalability of domain generalization. Cross-Domain Medical Imaging Classiﬁcation. Due to the various imaging protocols, device vendors and patient populations, we may also encounter the problem of distribution shift in clinical practice. To tackle such domain shift problem, image synthesis can be adopted through Generative Adversarial Networks [13, 45] to mitigate the domain shift problem. For example, Zhang et al. [43] proposed to leverage CycleGAN for medical imaging problem to transfer the knowledge from CT images to X-ray images. Chen et al. [5] conducted domain translation from MR to CT domain for heart segmentation problem. With few label information available in target domain, Zhang et al. [44] proposed to conduct segmentation and data synthesis jointly to segment heart chambers in both CT and MR domain. Dou et al. [8] proposed a two parallel domain-speciﬁc encoders and a decoder where the weights are shared between domains to boost the performance of training on both single domain and cross-domain scenario. When target domain data are not available, Zhang et al. [42] proposed to conduct data augmentation on source domain to achieve better generalization capability for medical imaging classiﬁcation task. Yoon et al. [39] proposed to learn generalized feature representation through classiﬁcation and contrastive semantic alignment technique [28]. More recently, Dou et al. [7] proposed to conduct meta-learning with global class alignment as well as local sample clustering regularization for medical imaging classiﬁcation task. 3 Methodology Preliminary. We denote the training samples from multiple source domains on a joint space X×Y as D= {(xk i,yk i)}Nk i=1,k ∈{1,2,...,K }, where xk i denotes the ith training sample from the kth source domain, Nk is the number of samples in the kth domain, and yk i is the corresponding label groundtruth. The goal of domain generalization is that given a sample xT from an unseen domain, we aim to predict its output ˆyT through a trained classiﬁer. We provide a framework named Linear-Dependency Domain Generalization (LDDG) that improves the generalization capability of medical imaging classiﬁcation. By assuming that there exists linear dependency in the latent feature space among various domains based on a certain task, we propose to regularize the latent feature space by modeling intra-class variation among multiple source domains through rank constraint meanwhile matching the distribution of latent features extracted from multiple source domains to a pre-deﬁned distribution prior, such that the shareable information among domains can be learned. The details of our proposed method are introduced below. Linear-Dependency Modeling. Directly training a classiﬁcation network with a task-speciﬁc loss (e.g. cross-entropy loss) may not be feasible, as in the ﬁeld of medical imaging, it is difﬁcult to collect large and diverse datasets, which can lead to poor generalization on a new and “unseen\" domain. To improve the generalization capability of medical imaging classiﬁcation, in [42], based on the observation that medical image domain variability is more compact compared with other image data, a data augmentation approach was proposed based on three different aspects: image quality (e.g., blurriness), image appearance (e.g., brightness) and spatial shape (e.g., rotation, scaling), by assuming other characteristics are supposed to be more consistent. However, we empirically ﬁnd that it is challenging to choose a suitable augmentation type as well as its magnitude for a speciﬁc medical imaging classiﬁcation task. Inspired by the observation that most of the aforementioned augmentation processes can be con- ducted through linear transformation, we assume that there exists linear dependency on the latent feature space. To be more speciﬁc, by assuming that we have a medical image batch collected from K different domains with label cas {x1 i1,c,x2 i2,c,...,x K iK,c}, there exists a set of parameters {α1,α2,...,α K}such that the corresponding latent features {z1 i1,c,z2 i2,c,...,z K iK,c}hold the property that zj ij,c = α1z1 i1,c + α2z2 i2,c + ...+ αj−1zj−1 ij−1,c + αj+1zj+1 ij+1,c + ...+ αKzK iK,c for different j. In other words, there exists a dominant eigenvalue capturing the category information of the matrix [z1 i1,c,z2 i2,c,...,z K iK,c]. Therefore, given a sample mini-batch denoted by X= {xk i}, we can obtain the corresponding latent features as Zthrough a posterior q(z|x) parameterized by an encoder. By further conducting mode-1 ﬂattening Zas Z1, our proposed rank regularization can be given as rank(Z) =C, where Cis the number of categories of a speciﬁc task. 1We assume that the ﬁrst dimension is associated with sample index. 3Setting the rank of Z to Cis equivalent to minimize the (C+ 1)th singular value of Z. By denoting it as σC+1, we can reformulate the rank loss and compute its sub-gradient as Lrank = σC+1, ∂σC+1 ∂Z = U:,C+1V⊤ :,C+1, (1) where U and V are obtained through SVD as Z = UΣV⊤. Noted that it is quite common to impose low-rank regularization in the ﬁnal objective for domain generalization task [37, 20]. Our proposed method is different from these methods in two folds, 1) we impose rank regularization based on the latent feature space while the existing works imposed low-rank regularization on classiﬁer parameters, which are not computational efﬁciency; 2) we set the rank to be a speciﬁc number instead of simply conducting low-rank regularization, we show in the experimental section that it can lead to better performance. Distribution Alignment. In addition to modeling the linear dependency in latent feature space, we further propose to extract shareable information among multiple domains, such that a more transferable feature representation can be learned which can beneﬁt generalization capability of deep neural networks. Existing techniques aimed to either minimize domain variance through distribution alignment between domain pairs [29, 22] or conduct local sample clustering through contrastive loss or triplet loss [28, 7]. However, based on our empirical analysis, the aforementioned technique may suffer from overﬁtting problem to the source domains, which is not surprising as the distribution of “unseen\" target domain may not match the distribution of multiple source domains. Thus, simply minimizing domain variance or local sample cluttering on source domains may not be able to generalize well to the “unseen\" one. To this end, we propose to conduct variational encoding [18] by adopting Kullback-Leibler (KL) divergence, which aims to match the latent features from multiple source domains to a pre-deﬁned prior distribution. In our work, we adopt Gaussian distribution N∼ (0,1) as the prior distribution, which is computationally tractable through reparameterization trick [18]. The KL divergence can be formulated as KL(q(Z|X)||N∼ (0,1)), where Zis the latent features deﬁned in the previous section. We show in the next section that by jointly conducting linear-dependency modeling and distribution regularization through KL divergence can lead to an upper bound of empirical risk from any “unseen\" but related domains. Theoretical Analysis. In this section, we provide the theoretical analysis of our proposed framework. In particular, We show that our framework can lead to an upper bound of expected loss on “unseen\" but related target domain. We ﬁrst make the following assumptions: Assumption 1. For any latent feature belong to domain T with label c, it can be represented by data from other related domains, i.e., q(zT iT ,c|xT iT ,c) =∑K j=1 βjq(zj ij,c|xj ij,c), where βj >= 0and ∥β∥≤ M , {x1 i1,c,x2 i2,c,...,x K iK,c}belong to the same category as xT. Noted that Assumption 1 is a mild assumption in the ﬁeld of medical imaging classiﬁcation task and is also reasonable in our setting as we restrict the rank of latent features to be the number of category, such that there exists linear dependency based on the latent features belonging to the same category. We further make assumption on the loss function Lbased on the output of classiﬁer. Assumption 2. (1) L is non-negative and bounded. (2) L is convex: L(∑ jλjyj,y) ≤∑ jλjL(yj,y), where λj ≥0 and ∑ jλj = 1. Note that this assumption is easy to be satisﬁed for several standard loss functions (e.g. cross-entropy loss). Under these assumptions, we have the following theorems. Theorem 1. Given a sample xT iT ,c from target domain T where the distribution of its latent variable is represented as q(zT iT ,c|xT iT ,c) =∑K j=1 βjq(zj ij,c|xj ij,c), its latent variable is within the manifold of N∼ (0,1). 4Proof. For simplicity, we ﬁrst denote the distribution on latent variables asqi(z),i = {1,2,...,K,T } as well as the Gaussian prior N∼ (0,1) as q∗(z). We can obtain the following upper bound, KL(qT(z)||q∗(z)) = K∑ j=1 βj ∫ z qj(z) logqT(z) q∗(z) dz = K∑ j=1 βj ∫ z qj(z) logqj(z)[1 + (qT(z)/qj(z) −1)] q∗(z) dz≤ K∑ j=1 βjKL(qj(z)∥q∗(z)), (2) where we use log(1 +x) ≤xand ∫ qT(z)dz= ∫ qj(z)dz= 1. As KL(qj(z)∥q∗(z)) is minimized according to our proposed distribution alignment, KL(qT(z)||q∗(z)) is then minimized. This completes the proof. Theorem 1 shows that the latent feature of any unseen but related domain lies in the manifold of pre-deﬁned prior. With the help of Theorem 1, we can further derive the upperbound of empirical risk of target domain. Theorem 2. Given data from Ksource domains, where the empirical risk of domain jis given as L(ˆyj,y) =ϵj ≤ϵ, the expected loss L(ˆyT,y) is at most Mϵ + logC, where Cdenotes the number of category given a task, if the classiﬁcation layer is linear with softmax normalization trained by L which is a cross-entropy loss. Proof. Based on Theorem 1, we have qT(z) =q∗(z). Thus, we have the following upper bound, ∫ z L(ˆyT,y)qT(z)dz= ∫ z L( K∑ j=1 βjˆyj,y)q∗(z)dz= ∫ z L(∥β∥ K∑ j=1 βj ∥β∥ˆyj,y)q∗(z)dz ≤ K∑ j=1 βj ∥β∥ ∫ z L(∥β∥ˆyj,y)qj(z)dz≤ K∑ j=1 βj ∥β∥Mϵ + logC = Mϵ + logC, (3) where L(·) denotes the cross-entropy loss with softmax operation. Noted that we only adopt a linear layer for classiﬁer, thus ˆyT = ∑K j=1 βjˆyj holds based on Assumption 1. We also utilize the bounds of Log-Sum-Exp function f(a) ≤max{a1,a2,...,a n}+ logn, where f(a) = log∑n i=1 exp(ai). In our work, {a1,a2,...an}corresponds to the softmax output of ndifferent nodes. Thus, the second line of proof holds. This completes the proof. Theorem 2 shows that our proposed method has a good generalization capability. If the empirical risks on source domains are small, the empirical risk on target domain is also expected to be small. Model Training. Our proposed architecture consists of three part, a feature extractor Qθ, a variational encoding network Fω, and a classiﬁcation network Tφ. Regarding the classiﬁcation network, we only adopt a linear module (e.g. convolutional layer, linear layer) without any non-linear processing, such that our assumption can be satisﬁed. Images X= {xk i}are ﬁrst fed into the feature extractor Qθ to obtain the latent features, and then the latent features are resampled [ 18] through variational encoding network Fω, ﬁnally the classiﬁcation network Tφ outputs the corresponding prediction {ˆyk i}. A cross-entropy loss together with rank and distribution regularization to penalize the difference between {ˆyk i}and the groundtruth label {yk i}, the distribution difference between latent features and the Gaussian prior as well as the rank of the latent features. In summary, our model can be trained by minimizing the following objective as Lobj = ∑ i,k Lc(ˆyk i,yk i) +λ1Lrank + λ2KL(q(Z|X)||N∼ (0,1)), (4) where Lc(ˆyk i,yk i) denotes the cross-entropy loss with softmax operation, Lrank is the rank loss deﬁned in Equation 1. 5Table 1: Domain generalization results on the skin lesion classiﬁcation task. We repeat experiment for 5 times for each technique and report the mean value and standard deviation. Target DeepAll MASF [7] MLDG [21] CCSA [39] LDDG (Ours) DMF 0.2492±0.0127 0.2692±0.0146 0.2673±0.0452 0.2763±0.0263 0.2793±0.0244 D7P 0.5680±0.0181 0.5678±0.0361 0.5662±0.0212 0.5735±0.0227 0.6007±0.0208 MSK 0.6674±0.0083 0.6815±0.0122 0.6891±0.0167 0.6826±0.0131 0.6967±0.0193 PH2 0.8000±0.0167 0.7833±0.0101 0.8016±0.0096 0.7500±0.0419 0.8167±0.0096 SON 0.8613±0.0296 0.9204±0.0227 0.8817±0.0198 0.9045±0.0128 0.9272±0.0117 UDA 0.6264±0.0312 0.6538±0.0196 0.6319±0.0284 0.6758±0.0138 0.6978±0.0110 Avg 0.6287 0.6460 0.6396 0.6438 0.6697 4 Experiments In this section, we evaluate our proposed method based on two different medical imaging classiﬁcation tasks: skin lesion classiﬁcation task and gray matter segmentation task of spinal cord. The detail of architectures and experimental settings can be found in supplementary materials. 4.1 Skin lesion classiﬁcation We adopt seven public skin lesion datasets, including HAM10000 [34], Dermoﬁt (DMF) [2], Derm7pt (D7P) [17], MSK [6], PH2 [27], SONIC (SON) [6], and UDA [6], which contain skin lesion images collected from different equipments. We follow the protocol in [ 39] by choosing seven-category subset from these datasets, including melanoma (mel), melanocytic nevus (nv), dermatoﬁbroma (df), basal cell carcinoma (bcc), vascular lesion (vasc), benign keratosis (bkl), and actinic keratosis (akiec). Each dataset is randomly divided into 50% training set, 20% validation set and 30% testing set, where the relative class proportions are maintained across dataset partitions. As suggested in [39], for each setting, we use one dataset from DMF, D7P, MSK, PH2, SON and UDA as target domain and the remaining datasets together with HAM10000 as source domains. We use a ResNet18 model [ 14] pretrained on ImageNet as the backbone for our proposed method as well as other baselines. Results: We compare our method with state of the art domain generalization methods, including MASF [7], MLDG [ 21], and CCSA [ 39], which have shown the capability to generalize across domains in the ﬁeld of medical imaging. We report the baseline results by tuning the hyper-parameters in a wide range. We also adopt the baseline by directly training the model with the classiﬁcation loss, which is referred as “DeepAll\". The results are shown in Table 1. As we can see, all the domain generalization based techniques can outperform the DeepAll by directly training on source domains with classiﬁcation loss. Among the domain generalization methods, MASF can achieve relatively better performance compared with MLDG and CCSA, as it is built upon meta-learning mechanism by considering both global and local based domain alignment. Compared with all baselines, our proposed algorithm can achieve better performance in a clear margin, which is reasonable as our proposed training mechanism leverage the advantage of both data augmentation and domain alignment, which is less likely to suffer from overﬁtting problem. We can also observe that in some cases, all algorithms can perform relatively well, which we conjecture that the domain gap between source and target domain is relatively small. However, the performances are not desired in some cases (e.g. when using DMF as target domain), which may be due to the large domain gap between source and target domain. This observation is also consistent with the results reported in [39]. We also experiment using the data augmentation based technique BigAug [42] by considering a wide-range of augmentation spaces but ﬁnd it cannot yield competitive performance and the results are omitted here for brevity. We conjecture the reason that the augmentation types may not be suitable for skin lesion classiﬁcation task. In practice, it is also likely that we only have the data from one single domain during training. To further analyze the effectiveness of our proposed method under this scenario, we consider HAM10000 for training and the others for testing for skin lesion classiﬁcation task. Besides directly training with classiﬁcation loss (DeepAll), we also compare with CCSA [39] and MixUp [40]. Noted that other domain generalization baselines are not applicable in this case, as they require multiple domains available to simulate domain shift. The results are shown in Table 2. In most of the cases, our proposed method can outperform the DeepAll, CCSA as well as MixUp. For CCSA, as the contrastive loss is applied on only one source domain while target domain is unseen, it is likely to suffer from overﬁtting 6Table 2: Domain generalization results with HAM10000 as source domain. DMF D7P MSK PH2 SON UDA Average DeepAll 0.3003 0.4972 0.1667 0.4945 0.5025 0.4945 0.4093 CCSA [39] 0.2762 0.5082 0.4652 0.4667 0.5275 0.5055 0.4582 MixUp [40]0.3514 0.4029 0.3000 0.4333 0.6296 0.4615 0.4298 Ours 0.2943 0.5191 0.5087 0.5500 0.6949 0.5714 0.5231 Table 3: Domain generalization results on gray matter segmentation task. (a) DeepAll source target DSC CC JI TPR ASD 2,3,4 1 0.8560 65.34 0.7520 0.8746 0.08091,3,4 2 0.7323 26.21 0.5789 0.8109 0.09921,2,4 3 0.5041 -209 0.3504 0.4926 1.86611,2,3 4 0.8775 71.92 0.7827 0.8888 0.0599 Average 0.7425 -11.4 0.6160 0.7667 0.5265 (b) Probabilistic U-Net [19] source target DSC CC JI TPR ASD 2,3,4 1 0.8387 59.94 0.7276 0.8943 0.18201,3,4 2 0.8067 51.53 0.6778 0.7555 0.05801,2,4 3 0.5113 -188 0.3550 0.5638 2.08661,2,3 4 0.8782 72.18 0.7833 0.8910 0.2183 Average 0.7587 -1.09 0.6359 0.7762 0.6362 (c) MASF [7] source target DSC CC JI TPR ASD 2,3,4 1 0.8502 64.22 0.7415 0.8903 0.22741,3,4 2 0.8115 53.04 0.6844 0.8161 0.08261,2,4 3 0.5285 -99.3 0.3665 0.5155 1.85541,2,3 4 0.8938 76.14 0.8083 0.89910.0366 Average 0.7710 23.52 0.6502 0.7803 0.5505 (d) MLDG [21] source target DSC CC JI TPR ASD 2,3,4 1 0.8585 64.57 0.7489 0.8520 0.05731,3,4 2 0.8008 49.65 0.6696 0.7696 0.07451,2,4 3 0.5269 -108 0.3668 0.5066 1.77081,2,3 4 0.8837 73.60 0.7920 0.8637 0.0451 Average 0.7675 19.96 0.6443 0.7480 0.4869 (e) CCSA [39] source target DSC CC JI TPR ASD 2,3,4 1 0.8061 50.15 0.6801 0.8703 0.16781,3,4 2 0.8009 50.04 0.6687 0.8141 0.09391,2,4 3 0.5012 -112 0.3389 0.5444 1.54801,2,3 4 0.8686 69.61 0.7684 0.8926 0.0449 Average 0.7442 14.45 0.6140 0.7804 0.4637 (f) LDDG (Ours) source target DSC CC JI TPR ASD 2,3,4 1 0.8708 69.29 0.7753 0.8978 0.04111,3,4 2 0.8364 60.58 0.7199 0.8485 0.04161,2,4 3 0.5543 -71.6 0.3889 0.5923 1.51871,2,3 4 0.8910 75.46 0.8039 0.88440.0289 Average 0.7881 33.43 0.6720 0.8058 0.4076 problem. For MixUp, the combination is conducted only in a convex manner, which may not be able to generalize well to the out-of-distribution target domain. 4.2 Spinal cord gray matter segmentation We then consider the task of gray matter segmentation of spinal cord based on magnetic resonance imaging (MRI) to evaluate our proposed method. In particular, we adopt the data from spinal cord gray matter segmentation challenge [32], which are collected from four different medical centers with different MRI systems (Philips Achieva, Siemens Trio, Siemens Skyra). The voxel size resolutions are ranging from 0.25 ×0.25 ×2.5mm to 0.5 ×0.5 ×5mm. To evaluate the generalization capability of our proposed method, we consider the data collected from one medical center as a domain, which leads to four different domain, namely \"site1\", \"site2\", \"site3\" and \"site4\", where one domain is adopted as target domain and the remaining are considered as source domains. We adopt 2D-UNet [33] as the backbone network by considering the MRI axial slice as input2. Due to the imbalance of the number of voxels belonging to spinal cord gray matter and background in the MRI image, we follow [32] to consider a two-stage strategy in a coarse-to-ﬁne manner: 1) segment the spinal cord area (where the groundtruth of spinal cord is available), 2) segment the gray matter area from the output of 1) for our proposed method as well as baselines for comparison. Results: We compare our method with state of the art domain generalization methods, including MASF [7], MLDG [ 21], CCSA [ 39], by tuning the baseline hyper-parameters in a wide range. Moreover, we also compare with the Probabilistic U-Net [19] which automatically learned a prior for medical imaging segmentation task. For quantitative evaluation, we use a number of metrics to validate the effectiveness of our method. In particular, the metrics include three overlapping metrics: Dice Similarity Coefﬁcient (DSC), Jaccard Index (JI) and Conformity Coefﬁcient (CC); One 2Noted that we have tried 3D-UNet as suggested in [32] but ﬁnd the performances are similar to 2D-UNet in cross-domain scenario. 7Figure 1: Qualitative comparisons. Each row represents a sample from a speciﬁc domain. Each column denotes the input, ground truth (gt) or different methods including DeepAll, CCSA [ 39], MLDG [21], MASF [7], Probabilistic U-Net [19] (abbreviated as PROB here), respectively. As the area of interest in the original samples is very small, all the samples are center cropped for better visualization. statistical based metrics: Sensitivity (a.k.a. True Positive Rate (TPR))3; One distance based metric: Average surface distance (ASD), which are all performed in 3D. The results are shown in Table 5. As we can observe, our method achieve the best results when using “site1\", “site2\" and “site3\" as target domain based on all metrics, and achieve the best results in “site4\" under ASD, which shows the effectiveness of our proposed method. Among all other domain generalization based methods, MASF can achieve better performance compared with CCSA and MLDG. Such results are consistent with the performance for skin lesion classiﬁcation task. We also observe that probabilistic U-Net can achieve relatively better performance compared with the “DeepAll\" baseline by directly training on source domain with classiﬁcation loss. However, it may still suffer from overﬁtting problem as the learned prior may not generalize well to “unseen\" target domain. Again, we also evaluate the method proposed in [42] by considering a wide range of data augmentation parameters but ﬁnd the results are not desired. We conjecture that the default augmentation types are not suitable for this task. Some results of adopting [42] can be found in the supplementary materials. We further show some qualitative results in Figure 1. As we can see, while the “DeepAll\" baseline as well as other domain generalization based methods fail to segment the gray matter (e.g. when using “site2\" as target domain) or over-segment a large portion of gray matter by extending the segmentation maps to the white matter (e.g. when using “site3\" as target domain), our proposed method can generally achieve better performance compared with all the methods for comparison. 4.3 Ablation Study we ﬁrst conduct experiments on skin lesion classiﬁcation task to understand the impact of different components of our proposed algorithm by considering UDA as target domain. The results are shown in Table 4, where “Rank\" and “KL\" denote our proposed rank regularization and distribution alignment through KL divergence minimization, respectively. “LR\" denotes the rank regularization with nuclear norm minimization, which is a popular way to conduct low-rank constraint. We have the following observations: 1) both rank regularization and distribution alignment through KL can beneﬁt the generalization capability for medical imaging classiﬁcation task, which is reasonable as adopting these two terms jointly can theoretically lead to a empirical risk upper bound on target domain; 2) our proposed rank regularization can outperform the low-rank regularization by directly 3We omit True Negative Rate in our case due to the imbalance of the number of voxels belonging to spinal cord gray matter and background in the MRI image. 8Rank - LR LR - ✓ ✓KL - - ✓ ✓ - ✓ accuracy0.6264 0.6319 0.6703 0.6703 0.6813 0.6978 Table 4: Ablation study on key components of our method. We choose the skin lession classiﬁcation task and use UDA as the target domain. 1 3 5 7 9 11 13 15 rank(Z) 0.62 0.64 0.66 0.68 0.7 average accuracy Figure 2: The model performance with differ- ent rank(Z). UDA from skin lesion classiﬁ- cation task is selected as the target domain. minimizing the nuclear norm, which is reasonable as the discriminative category speciﬁc information can be explored by enforcing the value of rank to be the number of category. We then evaluate the effectiveness of our proposed rank regularization by varying the rank values of latent features by considering UDA as target domain. The results are shown in Figure 2. As we can observe, the average classiﬁcation accuracy has an ascending trend ﬁrst, and then drop. In particular, the accuracy reaches its peak when rank(Z) = 7, which is also the number of category in our task. However, we also observe that the performance drops when rank(Z) gets larger, which is reasonable as increasing the value of rank(Z) may leads to noise information which can have negative impact to the task. Noted that we can still achieve better performance compared with only using low-rank regularization with nuclear norm as shown in Table 4, which is reasonable as the nuclear norm does not take category information into consideration. Finally, we are interested in the singular values of our proposed method, which can be computed through SVD. In particular, we conduct experiments on segmentation task for stage 1 and stage 2 by considering “site1\" as the target domain. We show the convergence results of singular values in Figure 3. As we can see, our proposed method can converge in 100 epochs for both stage 1 and 2 despite the fact that deep neural networks are highly nonlinear. Regarding the singular value, we ﬁnd that the magnitude for σ1 and σ2 are relatively large while other values are much smaller, which is reasonable as we aim to explore the category speciﬁc information. Last but not the least, we ﬁnd that compared with σ2, the value of σ1 is much larger, which we conjecture the reason that the area of gray matter (or spinal cord) is much smaller than others. 50 100 150 200  epoch (a) 10 0 10 1 10 2 singular value  1 2 3 4 5 6 7 8 0 50 100 150 epoch  (b) 10 2 singular value  1 2 3 4 5 6 7 8 Figure 3: Analysis of singular values, (a) singular values in spinal cord segmentation stage (stage 1), (b) singular values of in gray matter segmentation stage (stage 2). 5 Conclusion In this paper, we tackle the generalization problem in medical imaging classiﬁcation task. Our proposed method takes the advantage of both linear-dependency modeling and domain alignment. In particular, we propose to learn a representative feature space for medical imaging classiﬁcation task through variational encoding with linear-dependency regularization with a novel rank regularization term. Our theoretical analysis shows that an empirical risk upper bound on target domain can be achieved under our formulation. Experimental results on skin lesion classiﬁcation task and spinal cord gray matter segmentation task show the effectiveness of our proposed method. 9Broader Impact. Our proposed method shows reasonable potential in the application of clinically realistic environments especially under the scenarios where only limited training samples are available and the capturing vendors and environments are diverse. In the short-term, the potential beneﬁciary of the proposed research lies in that it could signiﬁcantly alleviate the domain shift problem in medical image analysis, as evidenced in this paper. In the long term, it is expected that the principled methodology could offer new insights in intelligent medical diagnostic systems. One concrete example is that the medical imaging classiﬁcation functionality can be incorporated into different types of smartphones (with different capturing sensors, resolutions, etc.) to assess risk of skin disease (e.g. skin cancer in suspicious skin lesions) such that the terminal stage of skin cancer can be avoided. However, the medical data can be protected by privacy regulation such that the protected attributes (e.g. gender, ethnicity) may not be released publicly for training purpose. In this sense, the trained model may lack of fairness, or worse, may actively discriminate against a speciﬁc group of people (e.g. ethnicity with relatively small proportion of people). In the future, the proposed methodology can be feasibly extended to improve the algorithm fairness for numerous medical image analysis tasks and meanwhile guarantee the privacy of the protected attributes. Acknowledgement. The research work was done at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University. This research is supported in part by the Wallenberg-NTU Presidential Postdoctoral Fellowship, the NTU-PKU Joint Research Institute, a collaboration between the Nanyang Technological University and Peking University that is sponsored by a donation from the Ng Teng Fong Charitable Foundation, the Science and Technology Foundation of Guangzhou Huangpu Development District under Grant 2017GH22 and 201902010028, and Sino-Singapore International Joint Research Institute (Project No. 206-A017023 and 206-A018001). References [1] Y . Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems , pages 998–1008, 2018. [2] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. A color and texture based hierarchical k-nn approach to the classiﬁcation of non-melanoma skin lesions. In Color Medical Image Analysis, pages 63–86. Springer, 2013. [3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, page 7, 2017. [4] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tommasi. Domain generalization by solving jigsaw puzzles. arXiv preprint arXiv:1903.06864, 2019. [5] C. Chen, Q. Dou, H. Chen, J. Qin, and P.-A. Heng. Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 865–872, 2019. [6] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), pages 168–172. IEEE, 2018. [7] Q. Dou, D. C. de Castro, K. Kamnitsas, and B. Glocker. Domain generalization via model- agnostic learning of semantic features. In Advances in Neural Information Processing Systems, pages 6447–6458, 2019. [8] Q. Dou, C. Ouyang, C. Chen, H. Chen, B. Glocker, X. Zhuang, and P.-A. Heng. Pnp-adanet: Plug-and-play adversarial domain adaptation network with a benchmark at cross-modality cardiac segmentation. arXiv preprint arXiv:1812.07907, 2018. [9] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126–1135, 2017. [10] Y . Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V . Lempitsky. Domain-adversarial training of neural networks.Journal of Machine Learning Research, 17(59):1–35, 2016. 10[11] M. Ghifary, D. Balduzzi, W. B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed framework for domain adaptation and domain generalization. IEEE Transactions on Pattern Analysis & Machine Intelligence, (1):1–1, 2017. [12] M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pages 2551–2559, 2015. [13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Generative adversarial nets. InAdvances in neural information processing systems, pages 2672–2680, 2014. [14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [15] J. Hoffman, E. Tzeng, T. Park, J.-Y . Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018. [16] J. Huang, A. Gretton, K. M. Borgwardt, B. Schölkopf, and A. J. Smola. Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing Systems, 2006. [17] J. Kawahara, S. Daneshvar, G. Argenziano, and G. Hamarneh. Seven-point checklist and skin lesion classiﬁcation using multitask multimodal neural nets. IEEE journal of biomedical and health informatics, 23(2):538–546, 2018. [18] D. P. Kingma and M. Welling. Auto-encoding variational bayes.arXiv preprint arXiv:1312.6114, 2013. [19] S. Kohl, B. Romera-Paredes, C. Meyer, J. De Fauw, J. R. Ledsam, K. Maier-Hein, S. A. Eslami, D. J. Rezende, and O. Ronneberger. A probabilistic u-net for segmentation of ambiguous images. In Advances in Neural Information Processing Systems, pages 6965–6975, 2018. [20] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain gener- alization. In Proceedings of the IEEE International Conference on Computer Vision, pages 5542–5550, 2017. [21] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [22] H. Li, S. Jialin Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400–5409, 2018. [23] H. Li, R. Wan, S. Wang, and A. C. Kot. Unsupervised domain adaptation in the wild via disentangling representation learning. International Journal of Computer Vision, pages 1–17, 2020. [24] H. Li, S. Wang, R. Wan, and A. K. Chichung. Gmfad: Towards generalized visual recognition via multi-layer feature alignment and disentanglement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [25] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. [26] M. Long, Y . Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015. [27] T. Mendonça, P. M. Ferreira, J. S. Marques, A. R. Marcal, and J. Rozeira. Ph 2-a dermoscopic image database for research and benchmarking. In 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC) , pages 5437–5440. IEEE, 2013. [28] S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Uniﬁed deep supervised domain adaptation and generalization. In ICCV, 2017. [29] K. Muandet, D. Balduzzi, and B. Schölkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pages 10–18, 2013. [30] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2011. 11[31] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359, 2010. [32] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio, M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener, et al. Spinal cord grey matter segmentation challenge. Neuroimage, 152:312–329, 2017. [33] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015. [34] P. Tschandl, C. Rosendahl, and H. Kittler. The ham10000 dataset, a large collection of multi- source dermatoscopic images of common pigmented skin lesions. Scientiﬁc data, 5:180161, 2018. [35] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7167–7176, 2017. [36] Y . Wang, H. Li, and A. C. Kot. Heterogeneous domain generalization via domain mixup. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3622–3626. IEEE, 2020. [37] Z. Xu, W. Li, L. Niu, and D. Xu. Exploiting low-rank structure from latent domains for domain generalization. In ECCV. 2014. [38] P. Yang and W. Gao. Multi-view discriminant transfer learning. In IJCAI, 2013. [39] C. Yoon, G. Hamarneh, and R. Garbi. Generalizable feature learning in the presence of data bias and domain class imbalance with application to skin lesion classiﬁcation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 365–373. Springer, 2019. [40] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk mini- mization. arXiv preprint arXiv:1710.09412, 2017. [41] K. Zhang, M. Gong, and B. Schölkopf. Multi-source domain adaptation: A causal view. In AAAI, volume 1, pages 3150–3157, 2015. [42] L. Zhang, X. Wang, D. Yang, T. Sanford, S. Harmon, B. Turkbey, H. Roth, A. Myronenko, D. Xu, and Z. Xu. Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation. IEEE Transactions on Medical Imaging, 2020. [43] Y . Zhang, S. Miao, T. Mansi, and R. Liao. Task driven generative modeling for unsupervised domain adaptation: Application to x-ray image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 599–607. Springer, 2018. [44] Z. Zhang, L. Yang, and Y . Zheng. Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern Recognition, pages 9242–9251, 2018. [45] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223–2232, 2017. A Detail of Architectures and Experimental Settings A.1 Experimental Setting for Skin Lesion Classiﬁcation Task We use a ResNet18 model [14] pretrained on ImageNet without the FC layer as the feature extractor Qθ with the input size 224 ×224 for our proposed method as well as other baselines. For our method, the network before average pooling is used as the feature extractor. We insert a variational encoding network between the feature extractor and the ﬁnal fully connected layer which acts as the classiﬁer Tφ. The variational encoding network Fω is implemented using two separated networks with the same architecture, including a fully connected layer with the output dimension as 512, a Relu activation layer and a fully connected layer with the output dimension as 80. We then conduct 12reparameterization trick to obtain the output of variational encoding network. The classiﬁcation network Tφ is a fully connected layer with the output size as 7. For the hyperparameters, we choose λ1 = 0.001 and λ2 = 0.4 for all settings. Due to the class imbalance within and across datasets, we adopt the focal loss [25] as the classiﬁcation objective for our proposed method as well as other baseline techniques. For implementation, the alternate form proposed in [25] is adopted as it is an extension of cross-entropy loss and is also bounded and convex, which satisﬁes our assumption in the manuscript. During training, the Adam optimizer is used with learning rate as 0.0001, weight decay as 0.001 and the size of minibatch as 32. We train the models for 200 epochs and the learning rate is decreased by a factor 10 after every 80 epochs. For evaluation on testing set, we use the best performing model on the validation set. A.2 Experimental Setting for Spinal Cord Gray Matter Segmentation Task We adopt 2D-UNet [33] (without the last 1 ×1 convolutional layer) as the backbone network by considering the MRI axial slice as input. The variational encoding network Fω is implemented using two separated network with an identical architecture which includes a latent layer using 1 ×1 convolutional layer with output channel as 64, a Relu activation layer and a 1 ×1 convolution layer to predict mean and standard deviation layers of the distribution with output channel as 8 . We then conduct reparameterization trick to obtain the output of variational encoding network. We further adopt a 1×1 convolution layer as the classiﬁcation network Tφ with the output channel size as 2 for segmentation purpose. For the hyperparameters, we use λ1 = 0.001 and λ2 = 0.01. We adopt the weighted binary cross- entropy loss for classiﬁcation, where the weight of a positive sample is set to the reciprocal of the positive sample ratio in the region of interest. We use Adam algorithm with learning rate as 1e-4, weight decay as 1e-8 and the batch size as 8 for each domain for training. We train the model for 200 epochs, where the learning rate is decreased every 80 epoch with a factor of 10. For data processing, the 3D MRI data is ﬁrst sliced into 2D in axial slice view and then center cropped to 160 ×160. We further conduct random cropping which leads to the size as 144 ×144 for training. B BigAug [42] for Segmentation We present here by considering the data-augmentation based domain generalizing method BigAug [42], which stacked different types of transformations, including sharpness, blurriness, noise, bright- ness, contrast, rotation, scaling, etc., by considering 2D-UNet for spinal cord gray matter segmentation task [32]. The results are shown in Table 1 (a). As we can observed, the performances are not desired by directly adopting the default parameters for augmentation in [42], which are even worse than the “DeepAll\" baseline in terms of DSC and JI. To understand the reason why BigAug [42] with default parameter setting leads to negative transfer, we visualize in Figure 4 some examples of the transformed input and groundtruth pairs by considering both the groundtruth of spinal cord and gray matter. As we can see, by conducting the augmentation with default parameters in [42], the quality of input deteriorates and the boundary can be oversmooth, which may further lead to more discrepancy between source and target domain. We further consider to adopt the same augmentation in [42] by tuning the parameters in a wide range to report the best segmentation performance for this task. The results are shown in Table 1 (b). We observe that there exists some improvement compared with “DeepAll\" baseline, but our proposed method can still outperform [42] with parameter tuning, which is reasonable as it is difﬁcult to choose a suitable augmentation type and magnitude for different medical imaging classiﬁcation tasks. 13Table 5: Domain generalization results on gray matter segmentation task using BigAug [42]. (a) Default Parameters [42] source target DSC CC JI TPR ASD 2,3,4 1 0.7675 38.47 0.6250 0.7798 0.12861,3,4 2 0.7542 34.50 0.6061 0.9187 0.10131,2,4 3 0.5468 -76.2 0.3809 0.6381 1.90131,2,3 4 0.8706 70.18 0.7712 0.9232 0.0437 Average 0.7348 16.74 0.5958 0.8150 0.5437 (b) Tuned Parameters source target DSC CC JI TPR ASD 2,3,4 1 0.8438 62.02 0.7334 0.8600 0.16131,3,4 2 0.7703 40.17 0.6269 0.8866 0.18021,2,4 3 0.5556 -73.7 0.3905 0.6282 1.55601,2,3 4 0.8891 74.94 0.8009 0.8827 0.0362 Average 0.7647 25.86 0.6379 0.8144 0.4834 Figure 4: The samples of input and ground truth pairs generated from BigAug using the default hyper parameters. The ﬁrst row shows the input, the second row shows the groundtruth of spinal cord, and the last row shows the groundtruth gray matters. 14",
      "references": [],
      "meta_data": {
        "arxiv_id": "2009.12829v3",
        "authors": [
          "Haoliang Li",
          "YuFei Wang",
          "Renjie Wan",
          "Shiqi Wang",
          "Tie-Qiang Li",
          "Alex C. Kot"
        ],
        "published_date": "2020-09-27T12:30:30Z",
        "github_url": "https://github.com/wyf0912/LDDG"
      },
      "llm_extracted_info": {
        "main_contributions": "We address domain generalization in medical imaging classification with Linear-Dependency Domain Generalization (LDDG). The approach learns a representative latent feature space by imposing a rank constraint to capture linear dependency across multiple source domains (rank(Z)=C, C = number of classes) and by aligning latent distributions to a Gaussian prior via variational encoding. Theoretical analysis shows an upper bound on the empirical risk for unseen but related target domains, and empirical results demonstrate improved cross-domain generalization on skin lesion classification and spinal cord gray matter segmentation, outperforming state-of-the-art baselines. Code is publicly available.",
        "methodology": "The framework consists of a three-part model: a feature extractor Qθ, a variational encoder Fω that samples latent Z, and a linear classifier Tφ. Latent features from a batch drawn from multiple domains are arranged as a matrix Z. A rank regularization term Lrank enforces rank(Z)=C by flattening Z and penalizing the (C+1)th singular value (via SVD). Simultaneously, a distribution-alignment term KL(q(Z|X) || N(0,1)) encourages latent features to follow a standard Gaussian prior using a reparameterization trick. The overall objective is Lobj = sum Lc(ˆy, y) + λ1 Lrank + λ2 KL(...), where Lc is cross-entropy (or focal loss in imbalanced tasks). The rank term targets the shareable class-relevant information, while KL regularization promotes a domain-invariant latent space. Theoretical results show (1) latent variables from unseen but related domains lie near the Gaussian prior (Theorem 1) and (2) the target-domain risk is upper-bounded by Mε + log C under a linear classifier (Theorem 2).",
        "experimental_setup": "Skin lesion classification: uses seven public datasets (HAM10000, DMF, Derm7pt, MSK, PH2, SON, UDA) with a seven-category subset (mel, nv, df, bcc, vasc, bkl, akiec). Each dataset is split 50/20/30 for train/val/test; in each setting one dataset is target and the rest plus HAM10000 are sources. Backbone: ResNet18 pretrained on ImageNet; latent features go through a two-layer variational encoder producing 80-d latent, followed by a linear classifier for 7 classes. Hyperparameters: λ1=0.001, λ2=0.4; batch=32; training for 200 epochs; focal loss used for imbalance. HAM10000 as source with others as targets is also evaluated (Table 2), comparing DeepAll, CCSA, MixUp, and our method. Spinal cord gray matter segmentation: data from four centers (sites 1–4) with different MRI systems. A 2D-UNet backbone processes axial slices; a two-stage coarse-to-fine approach segments the spinal cord and then gray matter. The variational encoder outputs 8-d mean/std, with KL regularization λ2=0.01 and L1 regularization λ1=0.001. Training uses Adam (LR 1e-4), batch size 8, 200 epochs. Evaluation uses DSC, JI, CC, TPR (sensitivity), ASD; ablation and singular-value analyses accompany the experiments.",
        "limitations": "Limitations include reliance on the latent-space linear-dependency assumption and the fixed target rank C, which may not hold for all tasks. The method introduces computational overhead due to SVD-based rank loss and KL divergence, and results are demonstrated on two tasks with limited diversity. The domain generalization guarantees depend on assumptions about related-domain representations (Assumptions 1–2) and may be sensitive to hyperparameters (λ1, λ2) and the chosen prior. Also, fairness and privacy considerations in medical data imply potential bias if protected attributes are unavailable for training.",
        "future_research_directions": "Extend LDDG to additional medical imaging modalities and 3D data; explore adaptive or task-dependent rank constraints beyond a fixed C; investigate alternative priors or more expressive distribution-alignment strategies; combine with other DG approaches (e.g., meta-learning, adversarial alignment) for robustness; study fairness and privacy-preserving extensions; apply to real-time or mobile medical imaging pipelines (e.g., smartphone skin-cancer risk assessment) and evaluate on larger, more diverse public and clinical datasets.",
        "experimental_code": "# LowRank Implementation (as used for LDDG)\\nclass LowRank(Function):\\n    @staticmethod\\n    def forward(ctx, x):\\n        U, S, V = torch.svd(x)\\n        ctx.save_for_backward(x, U, V)\\n        return torch.sum(S)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        data = ctx.saved_tensors\\n        grad = torch.mm(data[1], data[2].t())\\n        return grad_output * grad\\n\\n# Training-time usage to enforce low-rank latent representation (rank constraint)\\n# The latent feature that is sampled and regularized is taken from the variational latent (mu/logvar/feature) produced by UNetOurs.\\n# spinal cord branch\\nfeature = mu_logvar[:, 2].permute(0, 2, 3, 1).contiguous().view(-1, net_spinal_cord.feature_dim)\\nfeature = feature[torch.randperm(len(feature))]\\nU, S_spinal, V = torch.svd(feature[0:2000])\\nlow_rank_loss_spinal = S_spinal[2]\\n\\n# total loss for spinal cord branch (spinal + kl + low-rank)\\ntotal_loss_spin = loss_spinal_cord + loss_kl_spinal * args.kl_weight * 2 + low_rank_loss_spinal * args.low_rank_tradeoff\\n\\n# spinal cord feature extraction and rank constraint applied to GM branch as well\\nfeature = mu_logvar[:, 2].permute(0, 2, 3, 1).contiguous().view(-1, net_gm.feature_dim)\\n# select a subset of features corresponding to spinal mask region for GM branch\\nspinal_mask_local = spinal_mask_pred.permute(0, 2, 3, 1).view(-1).nonzero()[:, 0]\\nfeature = feature[spinal_mask_local][torch.randperm(len(spinal_mask_local))]\\nU, S_gm, V = torch.svd(feature[0:min(2000, len(spinal_mask_local))])\\nlow_rank_loss_gm = S_gm[2]\\n\\ntotal_loss_gm = args.kl_weight * loss_kl_gm + loss_gm + low_rank_loss_gm * args.low_rank_tradeoff\\n\\n# Reparameterization in UNetOurs (latent sampling for training)\\nclass UNetOurs(nn.Module):\\n    def __init__(self, n_channels, n_classes, bilinear=True, feature_dim=8):\\n        super(UNetOurs, self).__init__()\\n        self.n_channels = n_channels\\n        self.n_classes = n_classes\\n        self.bilinear = bilinear\\n        self.feature_dim = feature_dim\\n        ...  # encoder/decoder layers as in standard 2D U-Net\\n        self.mu = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),\\n                                nn.Conv2d(64, feature_dim, kernel_size=1))\\n        self.logvar = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),\\n                                    nn.Conv2d(64, feature_dim, kernel_size=1))\\n        self.outc = OutConv(feature_dim, n_classes)\\n\\n    def forward(self, x):\\n        x1 = self.inc(x)\\n        x2 = self.down1(x1)\\n        x3 = self.down2(x2)\\n        x4 = self.down3(x3)\\n        x5 = self.down4(x4)\\n        x = self.up1(x5, x4)\\n        x = self.up2(x, x3)\\n        x = self.up3(x, x2)\\n        x = self.up4(x, x1)\\n        mu = self.mu(x)\\n        logvar = self.logvar(x)\\n        if self.training:\\n            feature = self.reparameterization(mu, logvar)  # b*c*w*h\\n            logits = self.outc(feature)\\n            return logits, torch.stack([mu, logvar, feature], dim=1)\\n        else:\\n            feature = mu\\n            logits = self.outc(feature)\\n            return logits, mu\\n\\n    def reparameterization(self, mu, logvar):\\n        std = torch.exp(logvar / 2)\\n        sampled_z = torch.normal(mu, std)\\n        z = sampled_z * std + mu\\n        return z\\n\\n# Objective: Lobj = Lc + lambda1*Lrank + lambda2*KL (as implemented by combining cross-entropy loss with low-rank and KL terms)\\n# The repository ties these components together in train_lddg.py using the total_loss_spin and total_loss_gm expressions shown above.",
        "experimental_info": "- Task and model: Domain Generalization for medical imaging using a variational latent space within a U-Net (UNetOurs) with a dedicated variational encoder, producing mu and logvar (and a latent feature). - Rank constraint: Implements a rank-regularization-like term by taking the latent features, performing an SVD, and using the trailing singular value as a low-rank penalty. Specifically, for the spinal branch, features are collected as feature = mu_logvar[:, 2]..., then reduced to the first 2000 samples, SVD is performed, and low_rank_loss_spinal = S_spinal[2] is used in total_loss_spin. The same approach is applied to the GM branch with a mask-based subset: low_rank_loss_gm = S_gm[2]. - Variational sampling: The UNetOurs forward uses a reparameterization trick to sample latent z from mu and logvar during training (and uses mu directly during inference). - Loss formulation (code reflects LDDG):\\n  - loss_spinal_cord: binary cross-entropy with logits for spinal predictions;\\n  - loss_kl_spinal: KL divergence between posterior (mu, logvar) and standard Gaussian;\\n  - low_rank loss terms: low_rank_loss_spinal (for spinal) and low_rank_loss_gm (for GM);\\n  - total losses: total_loss_spin = loss_spinal_cord + loss_kl_spinal * kl_weight * 2 + low_rank_loss_spinal * low_rank_tradeoff; total_loss_gm = kl_weight * loss_kl_gm + loss_gm + low_rank_loss_gm * low_rank_tradeoff. - Hyperparameters (as used in training script): latent_dim (default 8); kl_weight (default 0.01); low_rank_tradeoff (default 0.001); p_weight1 (for spinal class weighting, default 2); learning rate 1e-4; batch size 24; epochs 200. - Data/augmentation: uses SynchronousTransforms with CenterCrop, RandomCrop, etc., to augment data for training. - Data/task context: Spinal cord challenge, two-branch segmentation (spinal cord and gray matter), cross-domain adaptation across multiple sites. - Training details: Optimizers (Adam) for two networks, per-branch losses, and periodic evaluation as per train script."
      }
    },
    {
      "title": "Adversarial AutoAugment",
      "full_text": "arXiv:1912.11188v1  [cs.CV]  24 Dec 2019 Published as a conference paper at ICLR 2020 ADV E R S A R IA L AU TO AU G M E N T Xinyu Zhang Qiang W ang Huawei Huawei zhangxinyu10@huawei.com wangqiang168@huawei.com Jian Zhang Zhao Zhong Huawei Huawei zhangjian157@huawei.com zorro.zhongzhao@huawei.com ABSTRACT Data augmentation (DA) has been widely utilized to improve g eneralization in training deep neural networks. Recently, human-designed d ata augmentation has been gradually replaced by automatically learned augmenta tion policy. Through ﬁnding the best policy in well-designed search space of data augmentation, Au- toAugment (Cubuk et al., 2018) can signiﬁcantly improve val idation accuracy on image classiﬁcation tasks. However, this approach is not co mputationally practi- cal for large-scale problems. In this paper, we develop an ad versarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object an d augmentation pol- icy search loss. The augmentation policy network attempts t o increase the train- ing loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we re use the computation in target network training for policy evaluation, and dispe nse with the retraining of the target network. Compared to AutoAugment, this leads t o about 12× reduc- tion in computing cost and 11× shortening in time overhead on ImageNet. W e show experimental results of our approach on CIF AR-10/CIF A R-100, ImageNet, and demonstrate signiﬁcant performance improvements over state-of-the-art. On CIF AR-10, we achieve a top-1 test error of 1.36%, which is the currently best per- forming single model. On ImageNet, we achieve a leading perf ormance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data. 1 I NTRODUC TI ON Massive amount of data have promoted the great success of dee p learning in academia and industry. The performance of deep neural networks (DNNs) would be impr oved substantially when more su- pervised data is available or better data augmentation meth od is adapted. Data augmentation such as rotation, ﬂipping, cropping, etc., is a powerful technique to increase the amount and diversit y of data. Experiments show that the generalization of a neural networ k can be efﬁciently improved through manually designing data augmentation policies. However, t his needs lots of knowledge of human expert, and sometimes shows the weak transferability acros s different tasks and datasets in practi- cal applications. Inspired by neural architecture search ( NAS)(Zoph & Le, 2016; Zoph et al., 2017; Zhong et al., 2018a;b; Guo et al., 2018), a reinforcement lea rning (RL) (Williams, 1992) method called AutoAugment is proposed by Cubuk et al. (2018), which can automatically learn the aug- mentation policy from data and provide an exciting performa nce improvement on image classiﬁca- tion tasks. However, the computing cost is huge for training and evaluating thousands of sampled policies in the search process. Although proxy tasks, i.e., smaller models and reduced datasets, are taken to accelerate the searching process, tens of thousand s of GPU-hours of consumption are still required. In addition, these data augmentation policies op timized on proxy tasks are not guaranteed to be optimal on the target task, and the ﬁxed augmentation po licy is also sub-optimal for the whole training process. 1Published as a conference paper at ICLR 2020 Policy Network  Target Network  Network  Training Dataset  Pre-process Large Batch  Policy  Search  Minimize  Training Loss  Maximize  Training Loss  Training  Losses  Moving Average  & Normalize  {߬ଵ,߬ଶ,߬ଷ … ,߬ெ} + Sampled Policies  ߬ଵ ߬ଶ ߬ெ ߬ଷ Mini-Batch  … ࣦଵ ࣦଶ ࣦெ … Figure 1: The overview of our proposed method. W e formulate i t as a Min-Max game. The data of each batch is augmented by multiple pre-processing c omponents with sampled policies {τ1, τ 2, · · · , τ M }, respectively. Then, a target network is trained to minimiz e the loss of a large batch, which is formed by multiple augmented instances of th e input batch. W e extract the training losses of a target network corresponding to different augme ntation policies as the reward signal. Fi- nally, the augmentation policy network is trained with the g uideline of the processed reward signal, and aims to maximize the training loss of the target network t hrough generating adversarial policies. In this paper, we propose an efﬁcient data augmentation meth od to address the problems mentioned above, which can directly search the best augmentation poli cy on the full dataset during training a target network, as shown in Figure 1. W e ﬁrst organize the ne twork training and augmentation policy search in an adversarial and online manner. The augme ntation policy is dynamically changed along with the training state of the target network, rather t han ﬁxed throughout the whole training process like normal AutoAugment (Cubuk et al., 2018). Due to reusing the computation in policy evaluation and dispensing with the retraining of the target network, the computing cost and time overhead are extremely reduced. Then, the augmentation pol icy network is taken as an adversary to explore the weakness of the target network. W e augment the da ta of each min-batch with various adversarial policies in parallel, rather than the same data augmentation taken in batch augmentation (BA) (Hoffer et al., 2019). Then, several augmented instanc es of each mini-batch are formed into a large batch for target network learning. As an indicator of t he hardness of augmentation policies, the training losses of the target network are used to guide the po licy network to generate more aggres- sive and efﬁcient policies based on REINFORCE algorithm (Wi lliams, 1992). Through adversarial learning, we can train the target network more efﬁciently an d robustly. The contributions can be summarized as follows: • Our method can directly learn augmentation policies on targ et tasks, i.e., target networks and full datasets, with a quite low computing cost and time ov erhead. The direct policy search avoids the performance degradation caused by the pol icy transfer from proxy tasks to target tasks. • W e propose an adversarial framework to jointly optimize tar get network training and aug- mentation policy search. The harder samples augmented by ad versarial policies are con- stantly fed into the target network to promote robust featur e learning. Hence, the general- ization of the target network can be signiﬁcantly improved. • The experiment results show that our proposed method outper forms previous augmentation methods. For instance, we achieve a top-1 test error of 1.36% with PyramidNet+ShakeDrop (Y amada et al., 2018) on CIF AR-10, which is the state-of-the -art performance. On Ima- geNet, we improve the top-1 accuracy of ResNet-50 (He et al., 2016) from 76.3% to 79.4% without extra data, which is even 1.77% better than AutoAugment (Cubuk et al., 2018). 2Published as a conference paper at ICLR 2020 2 R ELATED WORK Common data augmentation, which can generate extra samples by some label-preserved transforma- tions, is usually used to increase the size of datasets and im prove the generalization of networks, such as on MINST , CIF AR-10 and ImageNet (Krizhevsky et al., 2012; W an et al., 2013; Szegedy et al., 2015). However, human-designed augmentation policies are speciﬁed for different datasets. For example, ﬂipping, the widely used transformation on CIF AR- 10/CIF AR-100 and ImageNet, is not suitable for MINST , which will destroy the property of origi nal samples. Hence, several works (Lemley et al., 2017; Cubuk et al., 2018 ; Lin et al., 2019; Ho et al., 2019) have attempted to automatically learn data augmentation polici es. Lemley et al. (2017) propose a method called Smart Augmentation, which merges two or more samples of a class to improve the generaliza- tion of a target network. The result also indicates that an au gmentation network can be learned when a target network is being training. Through well designing t he search space of data augmentation policies, AutoAugment (Cubuk et al., 2018) takes a recurren t neural network (RNN) as a sample controller to ﬁnd the best data augmentation policy for a sel ected dataset. T o reduce the computing cost, the augmentation policy search is performed on proxy t asks. Population based augmentation (PBA) (Ho et al., 2019) replaces the ﬁxed augmentation polic y with a dynamic schedule of aug- mentation policy along with the training process, which is m ostly related to our work. Inspired by population based training (PBT) (Jaderberg et al., 2017), t he augmentation policy search problem in PBA is modeled as a process of hyperparameter schedule lea rning. However, the augmentation schedule learning is still performed on proxy tasks. The lea rned policy schedule should be manually adjusted when the training process of a target network is non -matched with proxy tasks. Another related topic is Generative Adversarial Networks ( GANs) (Goodfellow et al., 2014), which has recently attracted lots of research attention due to its fascinating performance, and also been used to enlarge datasets through directly synthesizing new images (Tran et al., 2017; Perez & W ang, 2017; Antoniou et al., 2017; Gurumurthy et al., 2017; Frid-A dar et al., 2018). Although we formu- late our proposed method as a Min-Max game, there exists an ob vious difference with traditional GANs. W e want to ﬁnd the best augmentation policy to perform i mage transformation along with the training process, rather than synthesize new images. Pe ng et al. (2018) also take such an idea to optimize the training process of a target network in human po se estimation. 3 M ETHOD In this section, we present the implementation of Adversarial AutoAugment. First, the motivation for the adversarial relation between network learning and a ugmentation policy is discussed. Then, we introduce the search space with the dynamic augmentation policy. Finally, the joint framework for network training and augmentation policy search is pres ented in detail. 3.1 M OT IV AT IO N S Although some human-designed data augmentations have been used in the training of DNNs, such as randomly cropping and horizontally ﬂipping on CIF AR-10/ CIF AR-100 and ImageNet, limited randomness will make it very difﬁcult to generate effective samples at the tail end of the training. T o struggle with the problem, more randomness about image tr ansformation is introduced into the search space of AutoAugment (Cubuk et al., 2018) (described in Section 3.2). However, the learned policy is ﬁxed for the entire training process. All of possib le instances of each example will be send to the target network repeatedly, which still results i n an inevitable overﬁtting in a long-epoch training. This phenomenon indicates that the learned polic y is not adaptive to the training process of a target network, especially found on proxy tasks. Hence, th e dynamic and adversarial augmentation policy with the training process is considered as the crucia l feature in our search space. Another consideration is how to improve the efﬁciency of the policy search. In AutoAugment (Cubuk et al., 2018), to evaluate the performance of augment ation policies, a lot of child models should be trained from scratch nearly to convergence. The co mputation in training and evaluat- ing the performance of different sampled policies can not be reused, which leads to huge waste of computation resources. In this paper, we propose a computin g-efﬁcient policy search framework through reusing prior computation in policy evaluation. On ly one target network is used to evaluate the performance of different policies with the help of the tr aining losses of corresponding augmented 3Published as a conference paper at ICLR 2020 Epoch 30 Epoch 90 Epoch 120  …...  Epoch 60  …...  …...  …...  TranslateX, 6  Posterize, 5  Solarize, 6  Posterize, 5  Color, 8  Constrast, 8  Cutout, 3  ShearX, 9  Posterize, 5  Cutout, 7  TranslateX, 9  Cutout, 3  Rotate, 7  Color, 5  Equalize, 9  Invert, 8  TranslateX, 6 Posterize, 5  TranslateY, 8  Cutout, 5  Rotate, 9  Sharpness, 7  Equalize, 4  TranslateX, 6  TranslateX, 1 Color, 8  Rotate, 5  Invert, 7  ShearY, 8  TranslateX, 8  AutoContrast, 3  Posterize, 7  Original  ߬ଵ ߬ଶ ߬ଷ ߬ெ …...  Policy  …...  Figure 2: An example of dynamic augmentation policies learn ed with ResNet-50 on ImageNet. With the training process of the target network, harder augm entation policies are sampled to combat overﬁtting. Intuitively, more geometric transformations , such as TranslateX, ShearY and Rotate, are picked in our sampled policies, which is obviously diffe rent from AutoAugment (Cubuk et al., 2018) concentrating on color-based transformations. instances. The augmentation policy network is learned from the intermediate state of the target net- work, which makes generated augmentation policies more agg ressive and adaptive. On the contrary, to combat harder examples augmented by adversarial policie s, the target network has to learn more robust features, which makes the training more efﬁciently. 3.2 S E A RCH SPACE In this paper, the basic structure of the search space of Auto Augment (Cubuk et al., 2018) is re- served. An augmentation policy is deﬁned as that it is compos ed by 5 sub-policies, each sub-policy contains two image operations to be applied orderly, each op eration has two corresponding parame- ters, i.e., the probability and magnitude of the operation. Finally, the 5 best policies are concatenated to form a single policy with 25 sub-policies. For each image i n a mini-batch, only one sub-policy will be randomly selected to be applied. T o compare with Auto Augment (Cubuk et al., 2018) con- veniently, we just slightly modify the search space with rem oving the probability of each operation. This is because that we think the stochasticity of an operati on with a probability requires a certain epochs to take effect, which will detain the feedback of the i ntermediate state of the target network. There are totally 16 image operations in our search space, in cluding ShearX/Y , TranslateX/Y , Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Con trast, Color, Brightness, Sharpness, Cutout (Devries & T aylor, 2017) and Sample Pairing (Inoue, 2018). T he range of the magnitude is also discretized uniformly into 10 values. T o guarantee the convergence during adversarial learning,the magnitude of all the operations are set in a moderate range. 1 Besides, the randomness during the training process is introduced into our search space. Hence , the search space of the policy in each epoch has |S| = (16 ×10)10 ≈ 1. 1×1022 possibilities. Considering the dynamic policy, the number of possible policies with the whole training process can be e xpressed as |S|#epochs. An example of dynamically learning the augmentation policy along with the training process is shown in Figure 2. W e observe that the magnitude (an indication of difﬁculty ) gradually increases with the training process. 1 The more details about the parameter setting please refer to AutoAugment (Cubuk et al., 2018). 4Published as a conference paper at ICLR 2020 3.3 A DV E RS A RIA L LE A RN IN G In this section, the adversarial framework of jointly optim izing network training and augmentation policy search is presented in detail. W e use the augmentatio n policy network A(·, θ) as an adver- sary, which attempts to increase the training loss of the tar get network F(·, w) through adversarial learning. The target network is trained by a large batch form ed by multiple augmented instances of each batch to promote invariant learning (Salazar et al., 20 18), and the losses of different augmen- tation policies applied on the same data are used to train the augmentation policy network by RL algorithm. Considering the target network F(·, w) with a loss function L[F(x, w), y], where each example is transformed by some random data augmentation o(·), the learning process of the target network can be deﬁned as the following minimization problem w∗ = arg min w E x∼ Ω L[F(o(x), w), y], (1) where Ω is the training set, x and y are the input image and the corresponding label, respective ly. The problem is usually solved by vanilla SGD with a learning r ate η and batch size N, and the training procedure for each batch can be expressed as wt+1 = wt − η 1 N N∑ n=1 ∇wL[F(o(xn), w, y n]. (2) T o improve the convergence performance of DNNs, more random and efﬁcient data augmentation is performed under the help of the augmentation policy netwo rk. Hence, the minimization problem should be slightly modiﬁed as w∗ = arg min w E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y], (3) where τ(·) represents the augmentation policy generated by the networ k A(·, θ). Accordingly, the training rule can be rewritten as wt+1 = wt − η 1 M · N M∑ m=1 N∑ n=1 ∇wL[F(τm(xn), w), y n], (4) where we introduce M different instances of each input example augmented by adve rsarial policies {τ1, τ 2, · · · , τ M }. For convenience, we denote the training loss of a mini-batc h corresponding to the augmentation policy τm as Lm = 1 N N∑ n=1 L[F(τm(xn), w), y n]. (5) Hence, we have an equivalent form of Equation 4 wt+1 = wt − η 1 M M∑ m=1 ∇wLm. (6) Note that the training procedure can be regarded as a larger N · M batch training or an average over M instances of gradient computation without changing the lea rning rate, which will lead to a reduction of gradient variance and a faster convergence of t he target network Hoffer et al. (2019). However, overﬁtting will also come. T o overcome the problem , the augmentation policy network is designed to increase the training loss of the target netwo rk with harder augmentation policies. Therefore, we can mathematically express the object as the f ollowing maximization problem θ∗ = arg max θ J(θ), where J(θ) = E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y]. (7) Similar to AutoAugment (Cubuk et al., 2018), the augmentati on policy network is also implemented as a RNN shown in Figure 3. At each time step of the RNN controll er, the softmax layer will predict 5Published as a conference paper at ICLR 2020 Select the  type of Op0  Select the  magnitude  of Op0  Select the  type of Op1  Select the  magnitude  of Op1  Hidden  Layer  Softmax  Layer  Embedding Embedding Embedding Embedding  Embedding  Layer  Figure 3: The basic architecture of the controller for gener ating a sub-policy, which consists of two operations with corresponding parameters, the type and mag nitude of each operation. When a policy contains Q sub-policies, the basic architecture will be repeated Q times. Following the setting of AutoAugment (Cubuk et al., 2018), the number of sub-policie s Q is set to 5 in this paper. an action corresponding to a discrete parameter of a sub-pol icy, and then an embedding of the predicted action will be fed into the next time step. In our ex periments, the RNN controller will predict 20 discrete parameters to form a whole policy. However, there has a severe problem in jointly optimizing ta rget network training and augmentation policy search. This is because that non-differentiable aug mentation operations break gradient ﬂow from the target network F to the augmentation policy network A (W ang et al., 2017; Peng et al., 2018). As an alternative approach, REINFORCE algorithm (Wi lliams, 1992) is applied to optimize the augmentation policy network as ∇θ J(θ) = ∇θ E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y] ≈ ∑ m Lm∇θ pm = ∑ m Lmpm∇θ log pm = E τ ∼A (·, θ ) Lm∇θ log pm ≈ 1 M M∑ m=1 Lm∇θ log pm, (8) where pm represents the probability of the policy τm. T o reduce the variance of gradient ∇θ J(θ), we replace the training loss of a mini-batch Lm with ˆLm a moving average over a certain mini- batches2, and then normalize it among M instances as ˜Lm. Hence, the training procedure of the augmentation policy network can be expressed as ∇θ J(θ) ≈ 1 M M∑ m=1 ˜Lm∇θ log pm, θe+1 = θe + β 1 M M∑ m=1 ˜Lm∇θ log pm, (9) The adversarial learning of target network training and aug mentation policy search is summarized as Algorithm 1. 4 E XPERIME NT S AND ANALYSIS In this section, we ﬁrst reveal the details of experiment set tings. Then, we evaluate our proposed method on CIF AR-10/CIF AR-100, ImageNet, and compare it wit h previous methods. Results in Figure 4 show our method achieves the state-of-the-art perf ormance with higher computing and time efﬁciency 3. 2 The length of the moving average is ﬁxed to an epoch in our expe riments. 3 T o clearly present the advantage of our proposed method, we n ormalize the performance of our method in the Figure 4, and the performance of AutoAugment is plotted a ccordingly . 6Published as a conference paper at ICLR 2020 Algorithm 1 Joint Training of T arget Network and Augmentation Policy Ne twork Initialization: target network F(·, w), augmentation policy network A(·, θ) Input: input examples x, corresponding labels y 1: for 1 ≤ e ≤ epochs do 2: Initialize ˆLm = 0 , ∀m ∈ { 1, 2, · · · , M }; 3: Generate M policies with the probabilities {p1, p 2, · · · , p M }; 4: for 1 ≤ t ≤ T do 5: Augment each batch data with M generated policies, respectively; 6: Update we,t +1 according to Equation 4; 7: Update ˆLm through moving average, ∀m ∈ { 1, 2, · · · , M }; 8: Collect { ˆL1, ˆL2, · · · , ˆLM }; 9: Normalize ˆLm among M instances as ˜Lm, ∀m ∈ { 1, 2, · · · , M }; 10: Update θe+1 via Equation 9; 11: Output w∗ , θ∗ 4.1 E X P E RIM E N T SE T T IN G S The RNN controller is implemented as a one-layer LSTM (Hochr eiter & Schmidhuber, 1997). W e set the hidden size to 100, and the embedding size to 32. W e use Adam optimizer (Kingma & Ba, 2015) with a initial learning rate 0. 00035 to train the controller. T o avoid unexpected rapid conver- gence, an entropy penalty of a weight of 0. 00001 is applied. All the reported results are the mean of ﬁve runs with different initializations. 4.2 E X P E RIM E N T S O N CIFAR-10 A N D CIFAR-100 CIF AR-10 dataset (Krizhevsky & Hinton, 2009) has totally 60 000 images. The training and test sets have 50000 and 10000 images, respectively. Each image i n size of 32 × 32 belongs to one of 10 classes. W e evaluate our proposed method with the fo llowing models: Wide-ResNet- 28-10 (Zagoruyko & Komodakis, 2016), Shake-Shake (26 2x32d ) (Gastaldi, 2017), Shake-Shake (26 2x96d) (Gastaldi, 2017), Shake-Shake (26 2x112d) (Gast aldi, 2017), PyramidNet+ShakeDrop (Han et al., 2017; Y amada et al., 2018). All the models are tra ined on the full training set. T raining details: The Baseline is trained with the standard data augmentation , namely, randomly cropping a part of 32 × 32 from the padded image and horizontally ﬂipping it with a prob ability of 0. 5. The Cutout (Devries & T aylor, 2017) randomly select a 16 × 16 patch of each image, and then set the pixels of the selected patch to zeros. For our met hod, the searched policy is applied in addition to standard data augmentation and Cutout. For each image in the training process, standard data augmentation, the searched policy and Cutout are appli ed in sequence. For Wide-ResNet-28- 10, the step learning rate (LR) schedule is adopted. The cosi ne LR schedule is adopted for the other models. More details about model hyperparameters are suppl ied in A.1. Choice of M: T o choose the optimal M, we select Wide-ResNet-28-10 as a target network, and evaluate the performance of our proposed method verse diffe rent M, where M ∈ { 2, 4, 8, 16, 32}. From Figure 5, we can observe that the test accuracy of the mod el improves rapidly with the increase of M up to 8. The further increase of M does not bring a signiﬁcant improvement. Therefore, to balance the performance and the computing cost, M is set to 8 in all the following experiments. CIF AR-10 results: In T able 1, we report the test error of these models on CIF AR-1 0. For all of these models, our proposed method can achieve better perfor mance compared to previous methods. W e achieve 0. 78% and 0. 68% improvement on Wide-ResNet-28-10 compared to AutoAugment and PBA, respectively. W e achieve a top-1 test error of 1. 36% with PyramidNet+ShakeDrop, which is 0. 1% better than the current state-of-the-art reported in Ho et a l. (2019). As shown in Figure 6(a) and 6(b),we further visualize the probability distributio n of the parameters of the augmentation poli- cies learned with PyramidNet+ShakeDrop on CIF AR-10 over ti me. From Figure 6(a), we can ﬁnd that the percentages of some operations, such as TranslateY , Rotate, Posterize, and SampleParing, gradually increase along with the training process. Meanwh ile, more geometric transformations, such as TranslateX, TranslateY , and Rotate, are picked in th e sampled augmentation policies, which 7Published as a conference paper at ICLR 2020 $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013\u0013 \u0003 2XU\u00030HWKRG  $XWR$XJPHQW  $FFXUDF\\  RQ\u0003,PDJH1HW  $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013 \u0003 &RPSXWLQJ  (IILFLHQF\\  7LPH  (IILFLHQF\\  \u0014\u0013\u0013\b\u0003 \u0014\u0013\u0013\b\u0003\u0014\u0013\u0013\b\u0003 \u0014\u0013 \u0013\b \u0003 \u0014\u0013 \u0013\b \u0003 Figure 4: The Comparison of normalized performance between AutoAugment and our method. Please refer to the following tables for more details. /uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019/uni00000016/uni00000015 /uni00000030 /uni0000001c/uni0000001a/uni00000011/uni00000017 /uni0000001c/uni0000001a/uni00000011/uni00000018 /uni0000001c/uni0000001a/uni00000011/uni00000019 /uni0000001c/uni0000001a/uni00000011/uni0000001a /uni0000001c/uni0000001a/uni00000011/uni0000001b /uni0000001c/uni0000001a/uni00000011/uni0000001c /uni0000001c/uni0000001b/uni00000011/uni00000013 /uni0000001c/uni0000001b/uni00000011/uni00000014 /uni0000001c/uni0000001b/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c Figure 5: The T op-1 test accuracy of Wide- ResNet-28-10 on CIF AR-10 verse different M, where M ∈ { 2, 4, 8, 16, 32}. 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) ShearX ShearY TranslateX TranslateY Rotate AutoContrast Invert Equalize Solarize Posterize SampleParing Cutout Color Constrast Brightness Sharpness (a) Operations 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) 9 8 7 6 5 4 3 2 1 0 (b) Magnitudes Figure 6: Probability distribution of the parameters in the learned augmentation policies on CIF AR- 10 over time. The number in (b) represents the magnitude of on e operation. Larger number stands for more dramatic image transformations. The probability d istribution of each parameter is the mean of each ﬁve epochs. is different from color-focused AutoAugment (Cubuk et al., 2018) on CIF AR-10. Figure 6(b) shows that large magnitudes gain higher percentages during train ing. However, at the tail of training, low magnitudes remain considerable percentages. This indicat es that our method does not simply learn the transformations with the extremes of the allowed magnit udes to spoil the target network. CIF AR-100 results: W e also evaluate our proposed method on CIF AR-100, as shown i n T able 2. As we can observe from the table, we also achieve the state-of -the-art performance on this dataset. 4.3 E X P E RIM E N T S O N IM AG E NE T As a great challenge in image recognition, ImageNet dataset (Deng et al., 2009) has about 1.2 mil- lion training images and 50000 validation images with 1000 c lasses. In this section, we directly search the augmentation policy on the full training set and t rain ResNet-50 (He et al., 2016), ResNet- 50-D (He et al., 2018) and ResNet-200 (He et al., 2016) from sc ratch. T raining details: For the baseline augmentation, we randomly resize and crop e ach input image to a size of 224 × 224, and then horizontally ﬂip it with a probability of 0. 5. For AutoAugment (Cubuk et al., 2018) and our method, the baseline augmentati on and the augmentation policy are both used for each image. The cosine LR schedule is adopted in the training process. The model hyperparameters on ImageNet is also detailed in A.1. ImageNet results: The performance of our proposed method on ImageNet is presen ted in T able 3. It can be observed that we achieve a top-1 accuracy 79. 40% on ResNet-50 without extra data. T o 8Published as a conference paper at ICLR 2020 T able 1: T op-1 test error (%) on CIF AR-10. W e replicate the re sults of Baseline, Cutout and Au- toAugment methods from Cubuk et al. (2018), and the results o f PBA from Ho et al. (2019) in all of our experiments. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 3.87 3.08 2.68 2.58 1.90±0.15 Shake-Shake (26 2x32d) 3.55 3.02 2.47 2.54 2.36±0.10 Shake-Shake (26 2x96d) 2.86 2.56 1.99 2.03 1.85±0.12 Shake-Shake (26 2x112d) 2.82 2.57 1.89 2.03 1.78±0.05 PyramidNet+ShakeDrop 2.67 2.31 1.48 1.46 1.36±0.06 T able 2: T op-1 test error (%) on CIF AR-100. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 18.80 18.41 17.09 16.73 15.49±0.18 Shake-Shake (26 2x96d) 17.05 16.00 14.28 15.31 14.10±0.15 PyramidNet+ShakeDrop 13.99 12.19 10.67 10.94 10.42±0.20 the best of our knowledge, this is the highest top-1 accuracy for ResNet-50 learned on ImageNet. Besides, we only replace the ResNet-50 architecture with Re sNet-50-D, and achieve a consistent improvement with a top-1 accuracy of 80. 00%. T able 3: T op-1 / T op-5 test error (%) on ImageNet. Note that th e result of ResNet-50-D is achieved only through substituting the architecture. Model Baseline AutoAugment PBA Our Method ResNet-50 23.69 / 6.92 22.37 / 6.18 - 20.60±0.15 / 5.53±0.05 ResNet-50-D 22.84 / 6.48 - - 20.00±0.12 / 5.25±0.03 ResNet-200 21.52 / 5.85 20.00 / 4.90 - 18.68±0.18 / 4.70±0.05 4.4 A BL AT IO N ST U DY T o check the effect of each component in our proposed method, we report the test error of ResNet-50 on ImageNet the following augmentation methods in T able 4. • Baseline: Training regularly with the standard data augmentation an d step LR schedule. • Fixed: Augmenting all the instances of each batch with the standar d data augmentation ﬁxed throughout the entire training process. • Random: Augmenting all the instances of each batch with randomly an d dynamically generated policies. • Ours: Augmenting all the instances of each batch with adversaria l policies sampled by the policy network along with the training process. From the table, we can ﬁnd that Fixed can achieve 0. 99% error reduction compared to Baseline. This shows that a large-batch training with multiple augmen ted instances of each mini-batch can indeed improve the generalization of the model, which is con sistent with the conclusion presented in Hoffer et al. (2019). In addition, the test error of Random is 1. 02% better than Fixed. This indicates that augmenting batch with randomly generated policies can reduce overﬁtting in a certain extent. Furthermore, our method achieves the best test error of 20. 60% through augmenting samples with adversarial policies. From the result, we can conclude that these policies generated by the policy network are more adaptive to the training process, and make t he target network have to learn more robust features. 4.5 C O M P U T IN G CO S T A N D TIM E OV E RH E A D Computing Cost: The computation in target network training is reused for pol icy evaluation. This makes the computing cost in policy search become negligible . Although there exists an increase of 9Published as a conference paper at ICLR 2020 T able 4: T op-1 test error (%) of ResNet-50 with different aug mentation methods on ImageNet. Method Aug. Policy Enlarge Batch LR Schedule T est Error Baseline standard M = 1 step 23.69 Fixed standard M = 8 cosine 22.70 Random random M = 8 cosine 21.68 Ours adversarial M = 8 cosine 20.60 computing cost in target network training, the total comput ing cost in training one target network with augmentation policies is quite small compared to prior work. Time Overhead: Since we just train one target network with a large batch dist ributedly and simul- taneously, the time overhead of the large-batch training is equal to the regular training. Meanwhile, the joint optimization of target network training and augme ntation policy search dispenses with the process of ofﬂine policy search and the retraining of a targe t network, which leads to a extreme time overhead reduction. In T able 5, we take the training of ResNet-50 on ImageNet as an example to compare the computing cost and time overhead of our method and AutoAugment. From th e table, we can ﬁnd that our method is 12× less computing cost and 11× shorter time overhead than AutoAugment. T able 5: The comparison of computing cost (GPU hours) and tim e overhead (days) in training ResNet-50 on ImageNet between AutoAugment and our method. T he computing cost and time overhead are estimated on 64 NVIDIA T esla V100s. Method Computing Cost Time Overhead Searching Training T otal Searching Training T otal AutoAugment 15000 160 15160 10 1 11 Our Method ∼0 1280 1280 ∼0 1 1 4.6 T RA N S F E RA BIL IT Y ACRO S S DATA S E T S A N D ARCH IT E CT U RE S T o further show the higher efﬁciency of our method, the trans ferability of the learned augmentation policies is evaluated in this section. W e ﬁrst take a snapsho t of the adversarial training process of ResNet-50 on ImageNet, and then directly use the learned dyn amic augmentation policies to regu- larly train the following models: Wide-ResNet-28-10 on CIF AR-10/100, ResNet-50-D on ImageNet and ResNet200 on ImageNet. T able 6 presents the experimenta l results of the transferability. From the table, we can ﬁnd that a competitive performance can be st ill achieved through direct policy transfer. This indicates that the learned augmentation policies tra nsfer well across datasets and architectures. However, compared to the proposed method, t he policy transfer results in an obvious performance degradation, especially the transfer across d atasets. T able 6: T op-1 test error (%) of the transfer of the augmentat ion policies learned with ResNet-50 on ImageNet. Method Dataset AutoAugment Our Method Policy Transfer Wide-ResNet-28-10 CIF AR-10 2.68 1.90 2.45±0.13 Wide-ResNet-28-10 CIF AR-100 17.09 15.49 16.48±0.15 ResNet-50-D ImageNet - 20.00 20.20±0.05 ResNet-200 ImageNet 20.00 18.68 19.05±0.10 5 C ONCLUSIO N In this paper, we introduce the idea of adversarial learning into automatic data augmentation. The policy network tries to combat the overﬁtting of the target n etwork through generating adversarial policies with the training process. T o oppose this, robust f eatures are learned in the target network, which leads to a signiﬁcant performance improvement. Meanw hile, the augmentation policy search is performed along with the training of a target network, and the computation in network training is reused for policy evaluation, which can extremely reduce the search cost and make our method more computing-efﬁcient. 10Published as a conference paper at ICLR 2020 REFERENC ES Antreas Antoniou, Amos J. Storkey, and Harrison Edwards. Da ta augmentation generative adver- sarial networks. ICLR, 2017. Ekin D. Cubuk, Barret Zoph, Dandelion Man ´ e, V ijay V asudeva n, and Quoc V . Le. Autoaugment: Learning augmentation policies from data. CVPR, 2018. Jia Deng, W ei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li F ei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. T errance Devries and Graham W . T aylor. Improved regulariza tion of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017. Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldber ger, and Hayit Greenspan. Synthetic data augmentation using GAN for improved liver lesion class iﬁcation. IEEE International Sym- posium on Biomedical Imaging (ISBI), 2018. Xavier Gastaldi. Shake-shake regularization. CoRR, abs/1705.07485, 2017. Ian J. Goodfellow , Jean Pouget-Abadie, Mehdi Mirza, Bing Xu , David W arde-Farley, Sherjil Ozair, Aaron Courville, and Y oshua Bengio. Generative adversaria l networks. NIPS, 2014. Minghao Guo, Zhao Zhong, W ei Wu, Dahua Lin, and Junjie Y an. IR LAS: inverse reinforcement learning for architecture search. CoRR, abs/1812.05285, 2018. Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and V enkatesh Babu Radhakrishnan. Deli- gan : Generative adversarial networks for diverse and limit ed data. CVPR, 2017. Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal resi dual networks. CVPR, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep r esidual learning for image recog- nition. CVPR, 2016. T ong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. CoRR, abs/1812.01187, 2018. Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen . Population based augmentation: Efﬁcient learning of augmentation policy schedules. ICML, 2019. Sepp Hochreiter and Jrgen Schmidhuber. Long short-term mem ory. Neural Computation, 1997. Elad Hoffer, T al Ben-Nun, Itay Hubara, Niv Giladi, T orsten H oeﬂer, and Daniel Soudry. Augment your batch: better training with larger batches. CoRR, abs/1901.09335, 2019. Hiroshi Inoue. Data augmentation by pairing samples for ima ges classiﬁcation. CoRR, abs/1801.02929, 2018. Max Jaderberg, V alentin Dalibard, Simon Osindero, W ojciec h M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol V inyals, Tim Green, Iain Dunning, Karen Simon yan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural net works. CoRR, abs/1711.09846, 2017. Diederik P . Kingma and Jimmy Ba. Adam: A method for stochasti c optimization. ICLR, 2015. Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple l ayers of features from tiny images. T echnical report, University of T oronto, 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im agenet classiﬁcation with deep convo- lutional neural networks. NIPS, 2012. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation - learning an optimal data augmentation strategy. CoRR, abs/1703.08383, 2017. Chen Lin, Minghao Guo, Chuming Li, W ei Wu, Dahua Lin, W anli Ou yang, and Junjie Y an. Online hyper-parameter learning for auto-augmentation strategy . CoRR, abs/1905.07373, 2019. 11Published as a conference paper at ICLR 2020 Xi Peng, Zhiqiang T ang, Fei Y ang, Rog ´ erio Schmidt Feris, an d Dimitris N. Metaxas. Jointly op- timize data augmentation and network training: Adversaria l data augmentation in human pose estimation. CVPR, 2018. Luis Perez and Jason W ang. The effectiveness of data augment ation in image classiﬁcation using deep learning. CoRR, abs/1712.04621, 2017. Julian Salazar, Davis Liang, Zhiheng Huang, and Zachary C. L ipton. Invariant representation learn- ing for robust deep networks. NeurIPS W orkshop, 2018. Christian Szegedy, W ei Liu, Y angqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, V incent V anhoucke, and Andrew Rabinovich. G oing deeper with convolutions. CVPR, 2015. T oan Tran, Trung Pham, Gustavo Carneiro, Lyle J. Palmer, and Ian D. Reid. A bayesian data augmentation approach for learning deep models. NIPS, 2017. Li W an, Matthew Zeiler, Sixin Zhang, Y ann LeCun, and Rob Ferg us. Regularization of neural networks using dropconnect. ICML, 2013. Xiaolong W ang, Abhinav Shrivastava, and Abhinav Gupta. A-f ast-rcnn: Hard positive generation via adversary for object detection. CVPR, 2017. Ronald J. Williams. Simple statistical gradient-followin g algorithms for connectionist reinforcement learning. Machine Learning, 1992. Y oshihiro Y amada, Masakazu Iwamura, and Koichi Kise. Shake drop regularization. CoRR, abs/1802.02375, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networ ks. British Machine V ision Confer- ence, 2016. Zhao Zhong, Junjie Y an, and Cheng-Lin Liu. Practical networ k blocks design with Q-learning. CVPR, 2018a. Zhao Zhong, Zichen Y ang, Boyang Deng, Junjie Y an, W ei Wu, Jin g Shao, and Cheng-Lin Liu. BlockQNN: Efﬁcient block-wise neural network architectur e generation. CoRR, abs/1808.05584, 2018b. Barret Zoph and Quoc V . Le. Neural architecture search with r einforcement learning. ICLR, 2016. Barret Zoph, V ijay V asudevan, Jonathon Shlens, and Quoc V . L e. Learning transferable architectures for scalable image recognition. CVPR, 2017. A A PPENDIX A.1 H Y P E RPA RA M E T E RS W e detail the model hyperparameters on CIF AR-10/CIF AR-100 and ImageNet in T able 7. 12Published as a conference paper at ICLR 2020 T able 7: Model hyperparameters on CIF AR-10/CIF AR-100 and I mageNet. LR represents learning rate, and WD represents weight decay. W e do not speciﬁcally t une these hyperparameters, and all of these are consistent with previous works, expect for the n umber of epochs. Dataset Model Batch Size (N · M) LR WD Epoch CIF AR-10 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-10 Shake-Shake (26 2x32d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x96d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x112d) 128 · 8 0.2 1e-4 600 CIF AR-10 PyramidNet+ShakeDrop 128 · 8 0.1 1e-4 600 CIF AR-100 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-100 Shake-Shake (26 2x96d) 128 · 8 0.1 5e-4 1200 CIF AR-100 PyramidNet+ShakeDrop 128 · 8 0.5 1e-4 1200 ImageNet ResNet-50 2048 · 8 0.8 1e-4 120 ImageNet ResNet-50-D 2048 · 8 0.8 1e-4 120 ImageNet ResNet-200 2048 · 8 0.8 1e-4 120 13",
      "references": [],
      "meta_data": {
        "arxiv_id": "1912.11188v1",
        "authors": [
          "Xinyu Zhang",
          "Qiang Wang",
          "Jian Zhang",
          "Zhao Zhong"
        ],
        "published_date": "2019-12-24T03:17:17Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Adversarial AutoAugment (AAA), an online, adversarial policy-search framework that jointly optimizes data augmentation and target network training on full datasets, reusing target-network computation for policy evaluation to greatly reduce search cost. Dynamic, adversarial augmentation policies adapt during training to generate harder examples, boosting robustness and generalization. Demonstrates state-of-the-art results on CIFAR-10 (1.36% top-1 with PyramidNet+ShakeDrop) and ImageNet (ResNet-50 79.40% top-1, ResNet-50-D 80.00%) with substantial efficiency gains (≈12× less compute and ≈11× faster than AutoAugment).",
        "methodology": "Formulates a Min-Max game where a policy network A(·, θ) acts as adversary to maximize the training loss of a target network F(·, w). The policy network is an LSTM-based controller that generates M augmentation policies per batch (M=8 chosen empirically). Each policy is a sequence of 5 sub-policies; each sub-policy applies two image operations chosen from 16 possible operations with discretized magnitudes (10 levels). The target network is trained on a large batch that concatenates M augmented variants of each input; losses Lm for each policy are used as rewards to train the policy network via REINFORCE. To improve efficiency, the computation used to evaluate policies is reused from target-network training, avoiding retraining. The search space is dynamic and epoch-dependent, with policies becoming more aggressive over training; a moving average of Lm is used to stabilize policy-gradient updates. ",
        "experimental_setup": "Datasets: CIFAR-10/100 and ImageNet. Architectures: Wide-ResNet-28-10, Shake-Shake variants, PyramidNet+ShakeDrop on CIFAR; ResNet-50, ResNet-50-D, and ResNet-200 on ImageNet. Baselines include standard augmentation, Cutout, AutoAugment, and PBA. Training details include large-batch training with M augmented instances per input (M=8), cosine learning-rate schedules, and epoch counts (CIFAR: typically 200–600; ImageNet: 120 epochs for some models). Evaluation via top-1 (and top-5 where applicable) accuracy; ablation studies compare fixed, random, and adversarial policies; policy-transfer experiments assess cross-dataset/architecture applicability.",
        "limitations": "Reliance on REINFORCE for policy optimization introduces high-variance gradient estimates and potentially unstable training; though mitigated by moving-average normalization, policy quality may still depend on hyperparameters and dataset. The dynamic policy space, while powerful, can be computationally intensive in extremely large settings, and transferability across very different tasks can be limited (policy transfer shows degradation, especially across datasets). The approach assumes augmentations are label-preserving and may not generalize to domains where transformations drastically alter semantics. There is no theoretical convergence guarantee for the joint min–max training in this setting.",
        "future_research_directions": "Explore differentiable or continuous augmentation parameters to enable gradient-based policy learning; investigate alternative RL algorithms or baselines to reduce variance; extend the framework to other modalities (audio, text) and multi-task settings; combine augmentation policy search with neural architecture search for joint optimization; study transferability more deeply with few-shot adaptation; optimize memory and computation further for ultra-large-scale data; investigate online/adaptive policy updates in streaming data contexts.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Deep AutoAugment",
      "full_text": "Published as a conference paper at ICLR 2022 DEEP AUTO AUGMENT Yu Zheng1, Zhi Zhang2, Shen Yan1, Mi Zhang1 1Michigan State University, 2Amazon Web Services zhengy30@msu.edu, zhiz@amazon.com, {yanshen6, mizhang}@msu.edu ABSTRACT While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of ﬁxing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching conver- gence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default aug- mentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA. 1 I NTRODUCTION Augmentation Policy Default Transformation  1 (A) Transformation  2 Transformation  2 Transformation  1 Transformation  1 Default Transformation  2 Augmentation Policy Transformation  2 Transformation  2 Transformation  1 Transformation  1 Transformation  K-1 Transformation  K-1 Transformation  K Transformation  K (B) ... Figure 1: (A) Existing automated data augmentation meth- ods with shallow augmentation policy followed by hand- picked transformations. (B) DeepAA with deep augmentation policy with no hand-picked transformations. Data augmentation (DA) is a powerful tech- nique for machine learning since it effec- tively regularizes the model by increas- ing the number and the diversity of data points (Goodfellow et al., 2016; Zhang et al., 2017). A large body of data aug- mentation transformations has been pro- posed (Inoue, 2018; Zhang et al., 2018; DeVries & Taylor, 2017; Yun et al., 2019; Hendrycks et al., 2020; Yan et al., 2020) to improve model performance. While ap- plying a set of well-designed augmentation transformations could help yield consider- able performance enhancement especially in image recognition tasks, manually select- ing high-quality augmentation transforma- tions and determining how they should be combined still require strong domain exper- tise and prior knowledge of the dataset of interest. With the recent trend of automated machine learning (AutoML), data augmen- tation search ﬂourishes in the image domain (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020; Liu et al., 2021), which yields signiﬁcant performance improvement over hand-crafted data augmentation methods. Although data augmentation policies in previous works (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020) contain multiple transformations applied sequentially, only one or two transformations of each sub-policy are found through searching whereas the rest transformations are hand-picked and applied by default in addition to the found policy (Figure 1(A)). From this perspective, we believe that previous automated methods are not entirely automatedas they are still built upon hand-crafted default augmentations. 1 arXiv:2203.06172v2  [cs.CV]  15 Mar 2022Published as a conference paper at ICLR 2022 In this work, we propose Deep AutoAugment (DeepAA), a multi-layer data augmentation search method which aims to remove the need of hand-crafted default transformations (Figure 1(B)). DeepAA fully automates the data augmentation process by searching a deep data augmentation policy on an expanded set of transformations that includes the widely adopted search space and the default transformations (e.g. ﬂips, Cutout, crop). We formulate the search of data augmentation policy as a regularized gradient matching problem by maximizing the cosine similarity of the gradients between augmented data and original data with regularization. To avoid exponential growth of dimensionality of the search space when more augmentation layers are used, we incrementally stack augmentation layers based on the data distribution transformed by all the previous augmentation layers. We evaluate the performance of DeepAA on three datasets – CIFAR-10, CIFAR-100, and ImageNet – and compare it with existing automated data augmentation search methods including AutoAugment (AA) (Cubuk et al., 2019), PBA (Ho et al., 2019), Fast AutoAugment (FastAA) (Lim et al., 2019), Faster AutoAugment (Faster AA) (Hataya et al., 2020), DADA (Li et al., 2020), RandAugment (RA) (Cubuk et al., 2020), UniformAugment (UA) (LingChen et al., 2020), TrivialAugment (TA) (M¨uller & Hutter, 2021), and Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Our results show that, without any default augmentations, DeepAA achieves the best performance compared to existing automatic augmentation search methods on CIFAR-10, CIFAR-100 on Wide-ResNet-28-10 and ImageNet on ResNet-50 and ResNet-200 with standard augmentation space and training procedure. We summarize our main contributions below: • We propose Deep AutoAugment (DeepAA), a fully automated data augmentation search method that ﬁnds a multi-layer data augmentation policy from scratch. • We formulate such multi-layer data augmentation search as a regularized gradient matching problem. We show that maximizing cosine similarity along the direction of low variance is effective for data augmentation search when augmentation layers go deep. • We address the issue of exponential growth of the dimensionality of the search space when more augmentation layers are added by incrementally adding augmentation layers based on the data distribution transformed by all the previous augmentation layers. • Our experiment results show that, without using any default augmentations, DeepAA achieves stronger performance compared with prior works. 2 R ELATED WORK Automated Data Augmentation.Automating data augmentation policy design has recently emerged as a promising paradigm for data augmentation. The pioneer work on automated data augmentation was proposed in AutoAugment (Cubuk et al., 2019), where the search is performed under reinforce- ment learning framework. AutoAugment requires to train the neural network repeatedly, which takes thousands of GPU hours to converge. Subsequent works (Lim et al., 2019; Li et al., 2020; Liu et al., 2021) aim at reducing the computation cost. Fast AutoAugment (Lim et al., 2019) treats data augmentation as inference time density matching which can be implemented efﬁciently with Bayesian optimization. Differentiable Automatic Data Augmentation (DADA) (Li et al., 2020) further reduces the computation cost through a reparameterized Gumbel-softmax distribution (Jang et al., 2017). RandAugment (Cubuk et al., 2020) introduces a simpliﬁed search space containing two interpretable hyperparameters, which can be optimized simply by grid search. Adversarial AutoAugment (AdvAA) (Zhang et al., 2019) searches for the augmentation policy in an adversarial and online manner. It also incorporates the concept of Batch Augmentaiton (Berman et al., 2019; Hoffer et al., 2020), where multiple adversarial policies run in parallel. Although many automated data augmentation methods have been proposed, the use of default augmentations still imposes strong domain knowledge. Gradient Matching. Our work is also related to gradient matching. In (Du et al., 2018), the authors showed that the cosine similarity between the gradients of different tasks provides a signal to detect when an auxiliary loss is helpful to the main loss. In (Wang et al., 2020), the authors proposed to use cosine similarity as the training signal to optimize the data usage via weighting data points. A similar approach was proposed in (M¨uller et al., 2021), which uses the gradient inner product as a per-example reward for optimizing data distribution and data augmentation under the reinforcement learning framework. Our approach also utilizes the cosine similarity to guide the data augmentation 2Published as a conference paper at ICLR 2022 search. However, our implementation of cosine similarity is different from the above from two aspects: we propose a Jacobian-vector product form to backpropagate through the cosine similarity, which is computational and memory efﬁcient and does not require computing higher order derivative; we also propose a sampling scheme that effectively allows the cosine similarity to increase with added augmentation stages. 3 D EEP AUTO AUGMENT 3.1 O VERVIEW Data augmentation can be viewed as a process of ﬁlling missing data points in the dataset with the same data distribution (Hataya et al., 2020). By augmenting a single data point multiple times, we expect the resulting data distribution to be close to the full dataset under a certain type of transformation. For example, by augmenting a single image with proper color jittering, we obtain a batch of augmented images which has similar distribution of lighting conditions as the full dataset. As the distribution of augmented data gets closer to the full dataset, the gradient of the augmented data should be steered towards a batch of original data sampled from the dataset. In DeepAA, we formulate the search of the data augmentation policy as a regularized gradient matching problem, which manages to steer the gradient to a batch of original data by augmenting a single image multiple times. Speciﬁcally, we construct the augmented training batch by augmenting a single training data point multiple times following the augmentation policy. We construct a validation batch by sampling a batch of original data from the validation set. We expect that by augmentation, the gradient of augmented training batch can be steered towards the gradient of the validation batch. To do so, we search for data augmentation that maximizes the cosine similarity between the gradients of the validation data and the augmented training data. The intuition is that an effective data augmentation should preserve data distribution (Chen et al., 2020) where the distribution of the augmented images should align with the distribution of the validation set such that the training gradient direction is close to the validation gradient direction. Another challenge for augmentation policy search is that the search space can be prohibitively large with deep augmentation layers (K ≥5). This was not a problem in previous works, where the augmentation policies is shallow ( K ≤2). For example, in AutoAugment Cubuk et al. (2019), each sub-policy contains K = 2transformations to be applied sequentially, and the search space of AutoAugment contains 16 image operations and 10 discrete magnitude levels. The resulting number of combinations of transformations in AutoAugment is roughly (16 ×10)2 = 25,600, which is handled well in previous works. However, when discarding the default augmentation pipeline and searching for data augmentations from scratch, it requires deeper augmentation layers in order to perform well. For a data augmentation with K = 5sequentially applied transformations, the number of sub-policies is (16 ×10)5 ≈1011, which is prohibitively large for the following two reasons. First, it becomes less likely to encounter a good policy by exploration as good policies become more sparse on high dimensional search space. Second, the dimension of parameters in the policy also grows with K, making it more computational challenging to optimize. To tackle this challenge, we propose to build up the full data augmentation by progressively stacking augmentation layers, where each augmentation layer is optimized on top of the data distribution transformed by all previous layers. This avoids sampling sub-policies from such a large search space, and the number of parameters of the policy is reduced from |T|K to T for each augmentation layer. 3.2 S EARCH SPACE Let O denote the set of augmentation operations ( e.g. identity, rotate, brightness), m denote an operation magnitude in the set M, and xdenote an image sampled from the space X. We deﬁne the set of transformations as the set of operations with a ﬁxed magnitude as T := {t|t= o(·; m), o∈ O and m∈M}. Under this deﬁnition, every tis a map t: X→X , and there are |T|= |M|·|O| possible transformations. In previous works (Cubuk et al., 2019; Lim et al., 2019; Li et al., 2020; Hataya et al., 2020), a data augmentation policy Pconsists of several sub-policies. As explained above, the size of candidate sub-policies grows exponentially with depth K. Therefore, we propose a practical method that builds up the full data augmentation by progressively stacking augmentation layers. The ﬁnal data augmentation policy hence consists of Klayers of sequentially applied policy P= {P1,··· ,PK}, where policy Pk is optimized conditioned on the data distribution augmented 3Published as a conference paper at ICLR 2022 by all previous (k−1) layers of policies. Thus we write the policy as a conditional distribution Pk := pθk(n|{P1,··· ,Pk−1}) where ndenotes the indices of transformations in T. For the purpose of clarity, we use a simpliﬁed notation as pθk to replace pθk(n|{P1,··· ,Pk−1}). 3.3 A UGMENTATION POLICY SEARCH VIA REGULARIZED GRADIENT MATCHING Assume that a single data point xis augmented multiple times following the policy pθ. The resulting average gradient of such augmentation is denoted as g(x,θ), which is a function of data xand policy parameters θ. Let vdenote the gradients of a batch of the original data. We optimize the policy by maximizing the cosine similarity between the gradients of the augmented data and a batch of the original data as follows: θ= arg max θ cosineSimilarity(v,g(x,θ)) (1) = arg max θ vT ·g(x,θ) ∥v∥·∥g(x,θ)∥ where ∥·∥denotes the L2-norm. The parameters of the policy can be updated via gradient ascent: θ←θ+ η∇θ cosineSimilarity(v,g(x,θ)), (2) where ηis the learning rate. 3.3.1 P OLICY SEARCH FOR ONE LAYER We start with the case where the data augmentation policy only contains a single augmentation layer, i.e., P= {pθ}. Let L(x; w) denote the classiﬁcation loss of data point xwhere w∈RD represents the ﬂattened weights of the neural network. Consider applying augmentation on a single data point xfollowing the distribution pθ. The resulting averaged gradient can be calculated analytically by averaging all the possible transformations in T with the corresponding probability p(θ): g(x; θ) = |T|∑ n=1 pθ(n)∇wL(tn(x); w) (3) = G(x) ·pθ where G(x) = [ ∇wL(t1(x); w),··· ,∇wL(t|T|(x); w) ] is a D×|T|Jacobian matrix, and pθ = [pθ(1),··· ,pθ(|T|)]T is a |T|dimensional categorical distribution. The gradient w.r.t. the cosine similarity in Eq. (2) can be derived as: ∇θ cosineSimilarity(v,g(x; θ)) =∇θpθ ·r (4) where r= G(x)T ( v ∥g(θ)∥−vTg(θ) ∥g(θ)∥2 · g(θ) ∥g(θ)∥ ) (5) which can be interpreted as a reward for each transformation. Therefore, pθ ·rin Eq.(4) represents the average reward under policy pθ. 3.3.2 P OLICY SEARCH FOR MULTIPLE LAYERS The above derivation is based on the assumption that g(θ) can be computed analytically by Eq.(3). However, when K ≥2, it becomes impractical to compute the average gradient of the augmented data given that the search space dimensionality grows exponentially with K. Consequently, we need to average the gradient of all |T|K possible sub-policies. To reduce the parameters of the policy to T for each augmentation layer, we propose to incrementally stack augmentations based on the data distribution transformed by all the previous augmentation layers. Speciﬁcally, let P= {P1,··· ,PK}denote the K-layer policy. The policy Pk modiﬁes the data distribution on top of the data distribution augmented by the previous (k−1) layers. Therefore, the policy at the kth layer is a distribution Pk = pθk(n) conditioned on the policies {P1,··· ,Pk−1} 4Published as a conference paper at ICLR 2022 where each one is a |T|-dimensional categorical distribution. Given that, the Jacobian matrix at the kth layer can be derived by averaging over the previous(k−1) layers of policies as follows: G(x)k = |T|∑ nk−1=1 ··· |T|∑ n1=1 pθk−1 (nk−1) ···pθ1 (n1)[∇wL((t1 ◦tnk−1 ···◦ tn1 )(x); w),··· , ∇wL((t|T|◦tnk−1 ◦···◦ tn1 )(x); w)] (6) where Gk can be estimated via the Monte Carlo method as: ˜Gk(x) = ∑ ˜nk−1∼pθk ··· ∑ ˜n1∼pθ1 [∇wL((t1 ◦t˜nk−1 ···◦ t˜n1 )(x); w),··· , ∇wL((t|T|◦t˜nk−1 ◦···◦ t˜n1 )(x); w)] (7) where ˜nk−1 ∼pθk−1 (n), ··· ,˜n1 ∼pθ1 (n). The average gradient at the kth layer can be estimated by the Monte Carlo method as: ˜g(x; θk) = ∑ ˜nk∼pθk ··· ∑ ˜n1∼pθ1 ∇wL((t˜nk ◦···◦ t˜n1 )(x); w) . (8) Therefore, the reward at the kth layer is derived as: ˜rk(x) = ( ˜Gk(x) )T ( v ∥˜gk(x; θk)∥−vT˜gk(x; θk) ∥˜gk(x; θk)∥2 · ˜gk(x; θk) ∥˜gk(x; θk)∥ ) . (9) To prevent the augmentation policy from overﬁtting, we regularize the optimization by avoiding optimizing towards the direction with high variance. Thus, we penalize the average reward with its standard deviation as rk = Ex{˜rk(x)}−c· √ Ex{(˜rk(x) −Ex{˜rk(x)})2}, (10) where we use 16 randomly sampled images to calculate the expectation. The hyperparameter c controls the degree of regularization, which is set to 1.0. With such regularization, we prevent the policy from converging to the transformations with high variance. Therefore the parameters of policy Pk (k≥2) can be updated as: θ←θk + η∇θk cosineSimilarity(v,g(θk)) (11) where ∇θ cosineSimilarity(v,gk(x; θ)) =∇θpθk ·rk. (12) 4 E XPERIMENTS AND ANALYSIS Benchmarks and Baselines.We evaluate the performance of DeepAA on three standard benchmarks: CIFAR-10, CIFAR-100, ImageNet, and compare it against a baseline based on standard augmentations (i.e., ﬂip left-righ, pad-and-crop for CIFAR-10/100, and Inception-style preprocesing (Szegedy et al., 2015) for ImageNet) as well as nine existing automatic augmentation methods including (1) AutoAugment (AA) (Cubuk et al., 2019), (2) PBA (Ho et al., 2019), (3) Fast AutoAugment (Fast AA) (Lim et al., 2019), (4) Faster AutoAugment (Hataya et al., 2020), (5) DADA (Li et al., 2020), (6) RandAugment (RA) (Cubuk et al., 2020), (7) UniformAugment (UA) (LingChen et al., 2020), (8) TrivialAugment (TA) (M¨uller & Hutter, 2021), and (9) Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Search Space.We set up the operation setO to include 16 commonly used operations (identity, shear- x, shear-y, translate-x, translate-y, rotate, solarize, equalize, color, posterize, contrast, brightness, sharpness, autoContrast, invert, Cutout) as well as two operations (i.e., ﬂips and crop) that are used as the default operations in the aforementioned methods. Among the operations in O, 11 operations are associated with magnitudes. We then discretize the range of magnitudes into 12 uniformly spaced levels and treat each operation with a discrete magnitude as an independent transformation. Therefore, the policy in each layer is a 139-dimensional categorical distribution corresponding to |T|= 139 {operation, magnitude}pairs. The list of operations and the range of magnitudes in the standard augmentation space are summarized in Appendix A. 5Published as a conference paper at ICLR 2022 4.1 P ERFORMANCE ON CIFAR-10 AND CIFAR-100 Policy Search.Following (Cubuk et al., 2019), we conduct the augmentation policy search based on Wide-ResNet-40-2 (Zagoruyko & Komodakis, 2016). We ﬁrst train the network on a subset of 4,000 randomly selected samples from CIFAR-10. We then progressively update the policy network parameters θk (k= 1,2,··· ,K) for 512 iterations for each of the Kaugmentation layers. We use the Adam optimizer (Kingma & Ba, 2015) and set the learning rate to 0.025 for policy updating. Policy Evaluation.Using the publicly available repository of Fast AutoAugment (Lim et al., 2019), we evaluate the found augmentation policy on both CIFAR-10 and CIFAR-100 using Wide-ResNet- 28-10 and Shake-Shake-2x96d models. The evaluation conﬁgurations are kept consistent with that of Fast AutoAugment. Results. Table 1 reports the Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d, respectively. The results of DeepAA are the average of four independent runs with different initializations. We also show the 95% conﬁdence interval of the mean accuracy. As shown, DeepAA achieves the best performance compared against previous works using the standard augmentation space. Note that TA(Wide) uses a wider (stronger) augmentation space on this dataset. Baseline AA PBA FastAA FasterAA DADA RA UA TA(RA) TA(Wide)1 DeepAA CIFAR-10WRN-28-10 96.1 97.4 97.4 97.3 97.4 97.3 97.3 97.33 97.46 97.46 97.56±0.14Shake-Shake(26 2x96d) 97.1 98.0 98.0 98.0 98.0 98.0 98.0 98.1 98.05 98.21 98.11±0.12 CIFAR-100WRN-28-10 81.2 82.9 83.3 82.7 82.7 82.5 83.3 82.82 83.54 84.33 84.02±0.18Shake-Shake(26 2x96d) 82.9 85.7 84.7 85.1 85.0 84.7 - - - 86.19 85.19±0.28 Table 1: Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.2 P ERFORMANCE ON IMAGE NET Policy Search.We conduct the augmentation policy search based on ResNet-18 (He et al., 2016). We ﬁrst train the network on a subset of 200,000 randomly selected samples from ImageNet for 30 epochs. We then use the same settings as in CIFAR-10 for updating the policy parameters. Policy Evaluation.We evaluate the performance of the found augmentation policy on ResNet-50 and ResNet-200 based on the public repository of Fast AutoAugment (Lim et al., 2019). The parameters for training are the same as the ones of (Lim et al., 2019). In particular, we use step learning rate scheduler with a reduction factor of 0.1, and we train and evaluate with images of size 224x224. Results. The performance on ImageNet is presented in Table 2. As shown, DeepAA achieves the best performance compared with previous methods without the use of default augmentation pipeline. In particular, DeepAA performs better on larger models (i.e. ResNet-200), as the performance of DeepAA on ResNet-200 is the best within the 95% conﬁdence interval. Note that while we train DeepAA using the image resolution (224×224), we report the best results of RA and TA, which are trained with a larger image resolution (244×224) on this dataset. Baseline AA Fast AA Faster AA DADA RA UA TA(RA)1 TA(Wide)2 DeepAA ResNet-50 76.3 77.6 77.6 76.5 77.5 77.6 77.63 77.85 78.07 78.30±0.14 ResNet-200 78.5 80.0 80.6 - - - 80.4 - - 81.32±0.17 Table 2: Top-1 test accuracy (%) on ImageNet for ResNet-50 and ResNet-200. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.3 P ERFORMANCE WITH BATCH AUGMENTATION Batch Augmentation (BA) is a technique that draws multiple augmented instances of the same sample in one mini-batch. It has been shown to be able to improve the generalization performance of the 1On CIFAR-10/100, TA (Wide) uses a wider (stronger) augmentation space, while the other methods including TA (RA) uses the standard augmentation space. 6Published as a conference paper at ICLR 2022 network (Berman et al., 2019; Hoffer et al., 2020). AdvAA (Zhang et al., 2019) directly searches for the augmentation policy under the BA setting whereas for TA and DeepAA, we apply BA with the same augmentation policy used in Table 1. Note that since the performance of BA is sensitive to the hyperparameters (Fort et al., 2021), we have conducted a grid search on the hyperparameters of both TA and DeepAA (details are included in Appendix D). As shown in Table 3, after tuning the hyperparameters, the performance of TA (Wide) using BA is already better than the reported performance in the original paper. The performance of DeepAA with BA outperforms that of both AdvAA and TA (Wide) with BA. AdvAA TA(Wide) (original paper) TA(Wide) (ours) DeepAA CIFAR-10 98.1±0.15 98.04±0.06 98.06±0.23 98.21±0.14 CIFAR-100 84.51±0.18 84.62±0.14 85.40±0.15 85.61±0.17 Table 3: Top-1 test accuracy (%) on CIFAR-10/100 dataset with WRN-28-10 with Batch Augmentation (BA), where eight augmented instances were drawn for each image. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.4 U NDERSTANDING DEEPAA Figure 2: Top-1 test accuracy (%) on ImageNet of DeepAA-simple, DeepAA, and other automatic augmentation methods on ResNet-50. Effectiveness of Gradient Matching. One uniqueness of DeepAA is the regularized gradient matching objective. To examine its effectiveness, we remove the impact coming from multiple aug- mentation layers, and only conduct search for a sin- gle layer of augmentation policy. When evaluating the searched policy, we apply the default augmenta- tion in addition to the searched policy. We refer to this variant as DeepAA-simple. Figure 2 compares the Top-1 test accuracy on ImageNet using ResNet- 50 between DeepAA-simple, DeepAA, and other automatic augmentation methods. While there is 0.22% performance drop compared to DeepAA, with a single augmentation layer, DeepAA-simple still outperforms other methods and is able to achieve similar performance compared to TA (Wide) but with a standard augmentation space and trains on a smaller image size (224×224 vs 244×224). Policy Search Cost.Table 4 compares the policy search time on CIFAR-10/100 and ImageNet in GPU hours. DeepAA has comparable search time as PBA, Fast AA, and RA, but is slower than Faster AA and DADA. Note that Faster AA and DADA relax the discrete search space to a continuous one similar to DARTS (Liu et al., 2018). While such relaxation leads to shorter searching time, it inevitably introduces a discrepancy between the true and relaxed augmentation spaces. Dataset AA PBA Fast AA Faster AA DADA RA DeepAA CIFAR-10/100 5000 5 3.5 0.23 0.1 25 9 ImageNet 15000 - 450 2.3 1.3 5000 96 Table 4: Policy search time on CIFAR-10/100 and ImageNet in GPU hours. Impact of the Number of Augmentation Layers.Another uniqueness of DeepAA is its multi-layer search space that can go beyondtwo layers which existing automatic augmentation methods were designed upon. We examine the impact of the number of augmentation layers on the performance of DeepAA. Table 5 and Table 6 show the performance on CIFAR-10/100 and ImageNet respectively with increasing number of augmentation layers. As shown, for CIFAR-10/100, the performance gradually improves when more augmentation layers are added until we reach ﬁve layers. The performance does not improve when the sixth layer is added. For ImageNet, we have similar 1TA (RA) achieves 77.55% top-1 accuracy with image resolution 224×224. 2TA (Wide) achieves 77.97% top-1 accuracy with image resolution 224×224. 7Published as a conference paper at ICLR 2022 Figure 3: The distribution of operations at each layer of the policy for CIFAR-10/100 and ImageNet. The probability of each operation is summed up over all 12 discrete intensity levels (see Appendix B and C) of the corresponding transformation. observation where the performance stops improving when more than ﬁve augmentation layers are included. 1 layer 2 layers 3 layers 4 layers 5 layers 6 layers CIFAR-10 96.3±0.21 96.6±0.18 96.9±0.12 97.4±0.14 97.56±0.14 97.6±0.12 CIFAR-100 80.9±0.31 81.7±0.24 82.2±0.21 83.7±0.24 84.02±0.18 84.0±0.19 Table 5: Top-1 test accuracy of DeepAA on CIFAR-10/100 for different numbers of augmentation layers. The results are averaged over 4 independent runs with different initializations with the 95% conﬁdence interval denoted by ±. 1 layer 3 layers 5 layers 7 layers ImageNet 75.27±0.19 78.18±0.22 78.30±0.14 78.30±0.14 Table 6: Top-1 test accuracy of DeepAA on ImageNet with ResNet-50 for different numbers of augmentation layers. The results are averaged over 4 independent runs w/ different initializations with the 95% conﬁdence interval denoted by ±. Figure 3 illustrates the distributions of operations in the policy for CIFAR-10/100 and ImageNet respectively. As shown in Figure 3(a), the augmentation of CIFAR-10/100 converges to identity transformation at the sixth augmentation layer, which is a natural indication of the end of the augmentation pipeline. We have similar observation in Figure 3(b) for ImageNet, where the identity transformation dominates in the sixth augmentation layer. These observations match our results listed in Table 5 and Table 6. We also include the distribution of the magnitude within each operation for CIFAR-10/100 and ImageNet in Appendix B and Appendix C. Validity of Optimizing Gradient Matching with Regularization.To evaluate the validity of opti- mizing gradient matching with regularization, we designed a search-free baseline named “DeepTA”. In DeepTA, we stack multiple layers of TA on the same augmentation space of DeepAA without using default augmentations. As stated in Eq.(10) and Eq.(12), we explicitly optimize the gradient similarities with the average reward minus its standard deviation. The ﬁrst term – the average reward Ex{˜rk(x)}– encourages the direction of high cosine similarity. The second term – the standard deviation of the reward √ Ex{(˜rk(x) −Ex{˜rk(x)})2}– acts as a regularization that penalizes the direction with high variance. These two terms jointly maximize the gradient similarity along the direction with low variance. To illustrate the optimization trajectory, we design two metrics that are closely related to the two terms in Eq.(10): the mean value, and the standard deviation of the improvement of gradient similarity. The improvement of gradient similarity is obtained by subtracting the cosine similarity of the original image batch from that of the augmented batch. In our experiment, the mean and standard deviation of the gradient similarity improvement are calculated over 256 independently sampled original images. 8Published as a conference paper at ICLR 2022 (a) Mean of the gradient similarity improvement (b) Standard deviation of the gradi- ent similarity improvement (c) Mean accuracy over different aug- mentation depth Figure 4: Illustration of the search trajectory of DeepAA in comparison with DeepTA on CIFAR-10. As shown in Figure 4(a), the cosine similarity of DeepTA reaches the peak at the ﬁfth layer, and stacking more layers decreases the cosine similarity. In contrast, for DeepAA, the cosine similarity increases consistently until it converges to identity transformation at the sixth layer. In Figure 4(b), the standard deviation of DeepTA signiﬁcantly increases when stacking more layers. In contrast, in DeepAA, as we optimize the gradient similarity along the direction of low variance, the standard deviation of DeepAA does not grow as fast as DeepTA. In Figure 4(c), both DeepAA and DeepTA reach peak performance at the sixth layer, but DeepAA achieves better accuracy compared against DeepTA. Therefore, we empirically show that DeepAA effectively scales up the augmentation depth by increasing cosine similarity along the direction with low variance, leading to better results. Comparison with Other Policies.In Figure 7 in Appendix E, we compare the policy of DeepAA with the policy found by other data augmentation search methods including AA, FastAA and DADA. We have three interesting observations: • AA, FastAA and DADA assign high probability (over 1.0) on ﬂip, Cutout and crop, as those transformations are hand-picked and applied by default. DeepAA ﬁnds a similar pattern that assigns high probability on ﬂip, Cutout and crop. • Unlike AA, which mainly focused on color transformations, DeepAA has high probability over both spatial and color transformations. • FastAA has evenly distributed magnitudes, while DADA has low magnitudes (common issues in DARTS-like method). Interestingly, DeepAA assigns high probability to the stronger magnitudes. 5 C ONCLUSION In this work, we present Deep AutoAugment (DeepAA), a multi-layer data augmentation search method that ﬁnds deep data augmentation policy without using any hand-picked default transforma- tions. We formulate data augmentation search as a regularized gradient matching problem, which maximizes the gradient similarity between augmented data and original data along the direction with low variance. Our experimental results show that DeepAA achieves strong performance without using default augmentations, indicating that regularized gradient matching is an effective search method for data augmentation policies. Reproducibility Statement: We have described our experiment settings in great details. The evaluation of the found data augmentation policy is based the public repository of Fast AutoAugment. We believe that our results can be readily reproduced. ACKNOWLEDGEMENT We thank Yi Zhu, Hang Zhang, Haichen Shen, Mu Li, and Alexander Smola for their help with this work. This work was partially supported by NSF Award PFI:BIC-1632051 and Amazon AWS Machine Learning Research Award. 9Published as a conference paper at ICLR 2022 REFERENCES Maxim Berman, Herv´e J´egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a uniﬁed image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019. Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21(245):1–71, 2020. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, volume 33, pp. 702–703, 2020. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224, 2018. Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. Drawing multiple augmentation samples per image during training efﬁciently decreases test error. arXiv preprint arXiv:2105.13343, 2021. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.Deep learning. MIT press Cambridge, 2016. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In European Conference on Computer Vision, pp. 1–16. Springer, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. International Conference on Learning Representations, 2020. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In European Conference on Computer Vision, pp. 580–595. Springer, 2020. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 10Published as a conference paper at ICLR 2022 Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Raﬁ Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. arXiv preprint arXiv:2003.14348, 2020. Aoming Liu, Zehao Huang, Zhiwu Huang, and Naiyan Wang. Direct differentiable augmentation search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12219–12228, 2021. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018. Samuel M¨uller, Andr´e Biedenkapp, and Frank Hutter. In-loop meta-learning with gradient-alignment reward. arXiv preprint arXiv:2102.03275, 2021. Samuel G. M¨uller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmenta- tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 774–782, October 2021. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015. Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig. Optimizing data usage via differentiable rewards. In International Conference on Machine Learning, pp. 9983–9995. PMLR, 2020. Ross Wightman, Hugo Touvron, and Herv ´e J ´egou. Resnet strikes back: An improved training procedure in timm. volume 34, 2021. Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. In arXiv preprint arXiv: 2001.00677, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. International Conference on Learning Representations, 2018. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. In International Conference on Learning Representations, 2019. 11Published as a conference paper at ICLR 2022 A A LIST OF STANDARD AUGMENTATION SPACE Operation Magnitude Identity - ShearX [-0.3, 0.3] ShearY [-0.3, 0.3] TranslateX [-0.45, 0.45] TranslateY [-0.45, 0.45] Rotate [-30, 30] AutoContrast - Invert - Equalize - Solarize [0, 256] Posterize [4, 8] Contrast [0.1, 1.9] Color [0.1, 1.9] Brightness [0.1, 1.9] Sharpness [0.1, 1.9] Flips - Cutout 16 (60) Crop - Table 7: List of operations in the search space and the corresponding range of magnitudes in the standard augmentation space. Note that some operations do not use magnitude parameters. We add ﬂip and crop to the search space which were found in the default augmentation pipeline in previous works. Flips operates by randomly ﬂipping the images with 50% probability. In line with previous works, crop denotes pad-and-crop and resize-and-crop transforms for CIFAR10/100 and ImageNet respectively. We set Cutout magnitude to 16 for CIFAR10/100 dataset to be the same as the Cutout in the default augmentation pipeline. We set Cutout magnitude to 60 pixels for ImageNet which is the upper limit of the magnitude used in AA (Cubuk et al., 2019). 12Published as a conference paper at ICLR 2022 B T HE DISTRIBUTION OF MAGNITUDES FOR CIFAR-10/100 Figure 5: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for CIFAR-10/100. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 13Published as a conference paper at ICLR 2022 C T HE DISTRIBUTION OF MAGNITUDES FOR IMAGE NET Figure 6: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for ImageNet. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 14Published as a conference paper at ICLR 2022 D H YPERPARAMETERS FOR BATCH AUGMENTATION The performance of BA is sensitive to the training settings (Fort et al., 2021; Wightman et al., 2021). Therefore, we conduct a grid search on the learning rate, weight decay and number of epochs for TA and DeepAA with Batch Augmentation. The best found parameters are summarized in Table 8 in Appendix. We did not tune the hyperparameters of AdvAA (Zhang et al., 2019) since AdvAA claims to be adaptive to the training process. Dataset Augmentation Model Batch Size Learning Rate Weight Decay Epoch CIFAR-10 TA (Wide) WRN-28-10 128×8 0.2 0.0005 100 DeepAA WRN-28-10 128×8 0.2 0.001 100 CIFAR-100 TA (Wide) WRN-28-10 128×8 0.4 0.0005 35 DeepAA WRN-28-10 128×8 0.4 0.0005 35 Table 8: Model hyperparameters of Batch Augmentation on CIFAR10/100 for TA (Wide) and DeepAA. Learning rate, weight decay and number of epochs are found via grid search. 15Published as a conference paper at ICLR 2022 E C OMPARISON OF DATA AUGMENTATION POLICY Sampling probability of each transformations cumulated over all augmentation layers  (a) DeepAA (b) AA (c) FastAA (d) DADA Figure 7: Comparison of the policy of DeepAA and some publicly available augmentaiotn policy found by other methods including AA, FastAA and DADA on CIFAR-10. Since the compared methods have varied numbers of augmentation layers, we cumulate the probability of each operation over all the augmentation layers. Thus, the cumulative probability can be larger than 1. For AA, Fast AA and DADA, we add additional 1.0 probability to ﬂip, Cutout and Crop, since they are applied by default. In addition, we normalize the magnitude to the range [-5, 5], and use color to distinguish different magnitudes. 16",
      "references": [],
      "meta_data": {
        "arxiv_id": "2203.06172v2",
        "authors": [
          "Yu Zheng",
          "Zhi Zhang",
          "Shen Yan",
          "Mi Zhang"
        ],
        "published_date": "2022-03-11T18:57:27Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Deep AutoAugment (DeepAA) provides a fully automated, multi-layer data augmentation search that builds a deep augmentation pipeline from scratch without relying on hand-picked default transformations. It introduces a regularized gradient matching objective to guide policy search by maximizing the cosine similarity between gradients of augmented data and original data, with a variance-based regularizer to stabilize learning as layers grow. The method stacks augmentation layers progressively to control the combinatorial growth of the search space, and demonstrates strong performance on CIFAR-10, CIFAR-100, and ImageNet, outperforming existing AutoAugment-style methods even without default augmentations.",
        "methodology": "Formulate data augmentation search as regularized gradient matching. A policy layer Pk selects among a fixed set of transformations T (size 139 when including 16 operations with 12 magnitudes, plus flips and crop). For one layer (K=1), the averaged gradient g(x; θ) is computed analytically as a weighted sum over transformations. For multiple layers (K>1), the kth layer gradient Gk is estimated by Monte Carlo over sequences of transformations defined by the previous layers, yielding gk(x; θk). The reward rk is the inner product between the gradient directions of augmented and original data, normalized by gradient norms, with a regularization term Ex{rk} - c·sqrt(Ex{(rk - Ex{rk})^2}} to favor low-variance directions (c=1.0). Policy parameters θk are updated by gradient ascent on the cosine similarity. The full policy is built by stacking K layers P = {P1,...,PK}, where Pk is conditioned on {P1,...,Pk-1} to avoid an explosion in search space. Magnitudes are discretized into 12 levels, creating a 139-dimensional categorical distribution per layer. Training details include training on CIFAR-10 subset (4k samples) for 512 iterations per layer with Adam (lr=0.025); evaluation uses standard AutoAugment baselines; batch augmentation experiments evaluate the benefit of multiple augmented instances per image.",
        "experimental_setup": "Datasets: CIFAR-10, CIFAR-100, ImageNet. Models: Wide-ResNet-28-10 and Shake-Shake-2x96d on CIFAR; ResNet-50 and ResNet-200 on ImageNet. Baselines include AutoAugment, PBA, Fast AutoAugment, Faster AutoAugment, DADA, RandAugment, UniformAugment, TrivialAugment, and Adversarial AutoAugment. Search space: 16 operations plus flips and crop; 12 magnitude levels; per-layer transformations |T|=139; multi-layer policy with K layers (K up to 6 in experiments). Policy search: CIFAR-10/100 on 4k subset, 512 iterations per layer, Adam with lr=0.025; ImageNet search on 200k subset for 30 epochs. Evaluation mirrors Fast AutoAugment’s settings; batch augmentation experiments use eight augmented instances per image. Reproducibility: four runs with different seeds; 95% confidence intervals reported.",
        "limitations": "Limitations include substantial computational cost for policy search (GPU-hours similar to or higher than several baselines), potential sensitivity to hyperparameters (regularization constant c, learning rate), and dependence on the chosen operation set and magnitude discretization. The layering approach, while mitigating combinatorial explosion, may still underperform if the optimal policy requires non-local interactions across layers. Generalization to domains beyond image classification and to non-standard data distributions remains to be validated.",
        "future_research_directions": "Extend DeepAA to other data modalities such as text and audio; explore continuous relaxation or differentiable optimization of the augmentation space beyond the discrete 139 operations; reduce search cost via meta-learning, warm starts, or shared policy components across layers; investigate theoretical properties of gradient-matching with regularization and its connection to distribution matching; integrate with other regularization strategies (mixup, CutMix) and batch augmentation more deeply; automate selection of the optimal number of augmentation layers K and extend to domain adaptation and semi-supervised learning.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Single-Photon Image Classification",
      "full_text": "Published as a conference paper at ICLR 2021 SINGLE -PHOTON IMAGE CLASSIFICATION Thomas Fischbacher Google Research tfish@google.com Luciano Sbaiz Google Research sbaiz@google.com ABSTRACT Quantum Computing based Machine Learning mainly focuses on quantum comput- ing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. We demonstrate the existence of a “quantum computing toy model” that illustrates key aspects of quantum information processing while being experimentally accessible with room temperature optics. Pondering the question of the theoretical classiﬁcation accuracy performance limit for MNIST (respectively “Fashion-MNIST”) classiﬁers, subject to the constraint that a decision has to be made after detection of the very ﬁrst photon that passed through an image-ﬁlter, we show that a machine learning system that is permitted to use quantum interference on the photon’s state can substantially outperform any machine learning system that can not. Speciﬁcally, we prove that a “classical” MNIST (respectively “Fashion-MNIST”) classiﬁer cannot achieve an accuracy of better than 22.96% (respectively 21.38% for “Fashion-MNIST”) if it must make a decision after seeing a single photon falling on one of the 28 ×28 image pixels of a detector array. We further demonstrate that a classiﬁer that is permitted to employ quantum interference by optically transforming the photon state prior to detection can achieve a classiﬁcation accuracy of at least 41.27% for MNIST (respectively 36.14% for “Fashion-MNIST”). We show in detail how to train the corresponding quantum state transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics. 1 I NTRODUCTION Both quantum mechanics and machine learning play a major role in modern technology, and the emerging ﬁeld of AI applications of quantum computing may well enable major breakthroughs across many scientiﬁc disciplines. Yet, as the majority of current machine learning practitioners do not have a thorough understanding of quantum mechanics, while the majority of quantum physicists only have an equally limited understanding of machine learning, it is interesting to look for “Rosetta Stone” problems where simple and widely understood machine learning ideas meet simple and widely understood quantum mechanics ideas. It is the intent of this article to present a setting in which textbook quantum mechanics sheds a new light on a textbook machine learning problem, and vice versa, conceptually somewhat along the lines of Google’s TensorFlow Playground (Smilkov et al. (2017),) which was introduced as a teaching device to illustrate key concepts from Deep Learning to a wider audience. Speciﬁcally, we want to consider the question what the maximal achievable accuracy on common one-out-of-many image classiﬁcation tasks is if one must make a decision after the detection of the very ﬁrst quantum of light (i.e. photon) that passed a ﬁlter showing an example image from the test set. In this setting, we do not have a one-to-one correspondence between example images from the training (respectively test) set and classiﬁcation problems. Instead, every example image deﬁnes a probability distribution for the (x,y) detector pixel location on which the ﬁrst photon passing an image ﬁlter lands, the per-pixel probability being the pixel’s brightness relative to the accumulated (across all pixels) image brightness. So, from every (28 ×28 pixels) example image, we can sample arbitrarily many photon-detection-event classiﬁer examples, where the features are a pair of integer pixel coordinates, and the label is the digit class. On the MNIST handwritten digit dataset (LeCun and Cortes (2010)), any machine learning sys- tem that only gets to see a single such “photon detected at coordinates (x,y)” event as its in- 1 arXiv:2008.05859v2  [cs.LG]  12 Mar 2021Published as a conference paper at ICLR 2021 put features, of the pixel that ﬂashed up are the only input features, is limited in accuracy by the maximum likelihood estimate, since we have: P(Image class C|Photon detected at (x,y)) =∑ E P(Image class C|Example E)P(Example E|Photon detected at (x,y)). On photon detection events generated each by ﬁrst randomly picking an example image, and then randomly picking a brightness-weighted pixel from that, we cannot do any better than predicting the most likely digit class given these input features – the two pixel coordinates. As performance is measured on the test set, no classiﬁer could possibly ever outperform one that is built to achieve maximal performance on the test set. This is obtained by determining, for each pixel, what the most likely class is, where examples from the test set are weighted by the fraction of total example-image brightness that comes from the pixel in question. Figure 2(b) shows the most likely image-class per pixel. (For MNIST, some pixels are dark in every test set example.) No classiﬁer can outperform one that simply looks up the pixel-coordinates at which a photon was detected in Figure 2(b) and returns the corresponding class, and this optimal classiﬁer’s accuracy is22.96% for the the MNIST dataset – substantially higher than random guessing (10%). Appendix A.2 provides a detailed (but mostly straightforward) optimality proof of this accuracy threshold. We cannot, for example, outperform it by redistributing light intensity between pixels, since any such redistribution could only destroy some of the available useful information, not magically create extra useful information. An entirely different situation arises when we allow quantum mechanics to enter the stage: For a single photon passing through a coherently illuminated image ﬁlter, with all pixels at the same optical phase on the incoming wave, we can imagine putting some precision optical device between the image ﬁlter and the detector array that redistributes not the probabilities (which correspond to light intensity when aggregating over many photons), but the amplitudes that make up the spatial part of the photon wave-function. Illuminating such a set-up with many photons would show a hologram-like interference pattern on the detector array. This transformation of the (single-)photon wave function by linear optical elements then has tuneable parameters which we can adjust to improve classiﬁer accuracy. Quantum mechanics tells us that every (lossless) linear optical device can be represented by a linear unitary transform on the photon state: The action of any complex optical device consisting of (potentially very many) components which transforms a N-component photon state (in our case, N = 282 amplitudes in the spatial part of the photon wave function) can be described by an element of the N2-dimensional unitary matrix Lie group U(N). Vice versa, Reck et al. (1994) describes a constructive algorithm by which any U(N) transformation matrix can be translated back to a network of optical beam splitters and phase shifters. 1.1 R ELATED WORK Conceptually, exploiting interference to enhance the probability of a quantum experiment producing the sought outcome is the essential idea underlying all quantum computing. The main difference between this problem and modern quantum computing is that the latter tries to perform calculations by manipulating quantum states of multiple “entangled” constituents, typically coupled two-state quantum systems called “qubits,” via “quantum gates” that are controlled by parts of the total quantum system’s quantum state. Building a many-qubit quantum computer hence requires delicate control over the interactions between constituent qubits. This usually requires eliminating thermal noise by going to millikelvin temperatures. For the problem studied here, the quantum state can be transformed with conventional optics at room temperature: the energy of a green photon is 2.5 eV , way above the typical room temperature thermal radiation energy of kT ≃25 meV . The price to pay is that it is challenging to build a device that allows multiple photons to interact in the way needed to build a many-qubit quantum computer. Nevertheless, Knill, Laﬂamme, and Milburn (Knill et al. (2001)) devised a protocol to make this feasible in principle, avoiding the need for coherency-preserving nonlinear optics (which may well be impossible to realize experimentally) by clever exploitation of ancillary photon qubits, boson statistics, and the measurement process. In all such applications, the basic idea is to employ coherent multiphoton quantum states to do computations with multiple qubits. In the problem studied here, there is only a single photon, the only relevant information that gets processed is encoded in the spatial part of its wave function (i.e. polarization is irrelevant), so the current work resembles the “optical simulation of quantum logic” proposed by Cerf et al. (1998) where a N-qubit system is represented by 2N spatial modes of a single photon. Related work studied similar “optical simulations of quantum computing” for implementing various algorithms, in particular (small) integer factorization (Clauser and Dowling (1996); Summhammer (1997)), but to the best of the present authors’ knowledge did not consider machine learning problems. 2Published as a conference paper at ICLR 2021 This work can be described as belonging to the category of machine learning methods on quantum non-scalable architectures. Alternatively, one can regard it as a quantum analogue of recent work that demonstrated digital circuit free MNIST digit classiﬁcation via classical nonlinear optics, for instance via saturable absorbers (Khoram et al. (2019).) Apart from providing an accessible and commonly understandable toy problem for both quantum and ML research communities, this simple- quantum/simple-ML corner also may be of interest for teaching the physics of the measurement process (“the collapse of the wave function”) in a more accessible setting. Whereas explanations of the measurement process are forced to remain vague where they try to model “the quantum states of the observer” (typically unfathomably many states that one would never hope to be able to model in terms of actual numbers), using machine learning as a sort-of cartoon substitute for high level mental processes actually allows us to come up with fully concrete toy models of the measurement process on low-dimensional (such as: D< 1000) Hilbert spaces that nevertheless capture many of the essential aspects – to the extent that “ML classiﬁes the measurement as showing the image of a shoe” can be regarded as a crude approximation to “observer sees a shoe”. Looking closer at the relation between the present article and Khoram et al. (2019), both articles study the general feasibility of realizing Machine Learning classiﬁers in the form of an analog optical computer at the theoretical level, using numerical optimization to produce a blueprint of a device that can perform inference for a speciﬁc problem. In both articles, the primary problem under study is MNIST handwritten digit classiﬁcation, the input is encoded as spatial dependency of a (monochromatic) laser beam’s light intensity, and classiﬁcation happens by using interference to funnel optical energy onto a detector array. In both cases, stochastic gradient descent is used to shape how this funneling of optical energy happens. Indeed, even the loss function used for training (cross entropy) is essentially equivalent. The key differences are that Khoram et al. (2019) only considers the many-photon limit of classical wave optics, which allows one the luxury of using non-linear optical components, speciﬁcally saturable absorbers, to implement non-linearities. This has no analog for the single photon case. Also, having many photons available allows identifying the target class that receives most laser light and calling this the prediction of the model. This is clearly not possible when a decision has to be made after seeing only a single photon. If one sent many photons through an interference device as described in this article and picked the target class with the highest photon count, one would observe classiﬁcation accuracies of about 90% rather than the claimed about-40% for a single photon. This is considerably higher than the accuracies of about 80% presented inKhoram et al. (2019) as the focus of that article is on manufacturability, running gradient backpropagation directly on a Finite Difference Frequency Domain PDE simulation of Maxwell’s equations and taking materials engineering constraints into account, whereas our work focuses on upper and lower bounds for achievable accuracy, exploiting the one-to-one equivalence between linear optical devices and unitary transforms. Our work directly trains the parameters of the unitary transform, which only afterwards get mapped to a blueprint for an experimental device realization. Speculatively, if a device were built experimentally that was designed by the methods inKhoram et al. (2019), subject to the extra constraint that no non-linear elements can be used, and then deployed in a low-light-intensity single-photon setting, using a suitable detector such as a SPAD array, it may manage to realize better-than-classically-achievable classiﬁer performance, for reasons explained in the current work. 1.2 T HE MEASUREMENT PROCESS How well one can one solve a mental processing task, such as identifying a handwritten digit, if one is permitted to only measure a single quantum? This question leads to a Hilbert space basis factorization that parallels the factorization needed to study the quantum mechanical measurement process. Let us consider a gedankenexperiment where our quantum system (see Feynman et al. (2010); Landau and Lifshitz (1981) for an introduction to quantum mechanics) is a single atom that has two experiment-relevant quantum states, ‘spin-up’ and ‘spin-down’, |ψAtom⟩= c0|ψAtom=↑⟩+ c1|ψAtom=↓⟩. (1) This atom undergoes a measurement by interacting, over a limited time period, with an apparatus. The measurement process may involve for instance an atom emitting a photon that is detected by a camera, and it may include a human observing the result. We describe a quantum state in the potentially enormous Hilbert space of apparatus states with the vector |ψApparatus⟩.If, in this gedankenexperiment, we actually assume that we have maximal information about the (quantum) 3Published as a conference paper at ICLR 2021 state of the measurement apparatus (which, however, in practical terms would be unfathomably complicated) at the beginning of the experiment, then the full quantum state of the initial system is the tensor product |ψSystem, initial⟩= |ψAtom, initial⟩⊗|ψApparatus, initial⟩. (2) This factorization implies that atom and apparatus states are independent before the interaction. Without interaction between the apparatus and the atom, the time-evolution of the total system factorizes. A measurement requires an interaction between the apparatus and the atom, the solution of the Schrödinger equation is equivalent to the application of a unitary operator U to the state |ψSystem, initial⟩. This has the effect of combining the state components of the atom and the apparatus and, as a consequence, the joint time evolution no longer can be factorized. The overall state is |ψSystem, ﬁnal⟩= U|ψSystem, initial⟩and can be always decomposed in the sum: |ψSystem, ﬁnal⟩= α|ψAtom=↑⟩⊗|ψApparatus, ﬁnal=↑⟩+ β|ψAtom=↓⟩⊗|ψApparatus, ﬁnal=↓⟩, (3) where the apparatus states |ψApparatus, ﬁnal=↑⟩and |ψApparatus, ﬁnal=↓⟩represent the state of the apparatus after the measurement for the two basis states of the atom. Therefore, the apparatus is in a different state for the two cases, which leads to the apparent “collapse” of the wave function. The apparatus in the state |ψApparatus, ﬁnal=↑⟩perceives the “collapse” because the atom seems to have taken the state |ψAtom=↑⟩.The state of the apparatus includes also the representation of the thought process of a possible human observer, for instance asking herself at what instant the atom took a well determined state. This thought process disregards the superposed state |ψApparatus, ﬁnal=↓⟩which represents the alternative reality, where the apparatus observed a different outcome. Considering that a mental process could be seen as a measurement on the environment, one would naturally be inclined to think that high level mental concepts never would naturally lend themselves to a description in terms of some Hilbert space basis that has tensor product structure|ψgeneral concept⟩⊗ |ψdetails⟩.Machine learning is now making the question to what extent this may nevertheless work quantitatively testable for some simple cases, if we consider it as providing reasonably good (for this purpose) models for mental concepts. Let us consider the spatial part of a single photon’s quantum state as it traveled through a mask that has the shape of a complicated object. For instance, let’s assume that the mask is obtained from a random sample of the “Fashion-MNIST” dataset, Xiao et al. (2017), where each sample represents an object such as a shirt, a trouser, etc. One would generally expect that any sort of transformation that connects a highly regular and mathematically simple description of such a quantum system, such as in terms of per-picture-cell (“pixel”) amplitudes, with a description in human-interpretable terms, such as “the overall intensity pattern resembles a shirt,” would unavoidably involve very complicated entanglement, and one should not even remotely hope to be able to even only approximately express such photon states in terms of some factorization |ψphoton⟩≈ ∑ shape classC∈{shirt,trouser,...} ∑ style S cCS |ψshape class C⟩⊗|ψstyle S⟩, (4) since one would not expect the existence of a basis of orthonormal quantum states that can be (approxi- mately) labeled |ψshirt⟩, |ψshoe⟩, etc. Using machine learning, we can quantitatively demonstrate that, at least for some simple examples, precisely such a factorization does indeed work remarkably well, at least if we content ourselves with the concept of a “shirt shape” being that of a one-out-of-many machine learning classiﬁer, so not quite that of a human. In any case, it is reassuring to see that even near-future few-qbits quantum computers might be able to model high level concepts rather well. 2 S INGLE -QUANTUM OBJECT CLASSIFICATION Our gedankenexperiment starts with a single photon passing from faraway through a programmable LCD screen, which we here consider to consist of N ×N pixels and show an image, where for both the MNIST handwritten digit dataset of LeCun and Cortes (2010) and the “Fashion-MNIST” dataset of Xiao et al. (2017), we have N = 28. The size of the screen shall be sufﬁciently small for the photon’s quantum state to be at the same phase as it reaches each individual pixel. This does not mean that the screen has to be small in comparison to the wavelength. Rather, the light source must provide highly collimated illumination. 4Published as a conference paper at ICLR 2021 The relevant spatial part of the photon’s quantum state is described by an element of a N ×N- dimensional complex vector space. We can choose a basis for this Hilbert space such that the quantum state of a photon that managed to pass through the screen (rather than getting absorbed) has the form |ψPhoton⟩= ∑ row j, column k cjk|ψjk⟩ (5) where the |ψjk⟩basis functions correspond to a photon that went through pixel (j,k), and the coefﬁ- cients cjk are real, non-negative, proportional to the square roots of the image’s pixel-brightnesses, and are normalized according to ∑ j,k |cj,k|2 = 1. As we want to perform a rotation on this Hilbert space that maximizes alignment with a tensor product Hilbert space where one factor describes an image class, we pad this N2-dimensional Hilbert space into a larger Hilbert space with dimensionality M divisible by the number of object classes C, i.e. M = C ·S. This amounts to adding always-dark pixels (that may not form a complete row) to the image. The problem then amounts to engineering, for a problem P such as handwritten digit recognition, a single problem-speciﬁc unitary transform UP of the photon state, |ψPhoton⟩→ UP |ψPhoton⟩, such that we can meaningfully claim: UP |ψPhoton⟩= |ψPhoton∗ ⟩≈ ∑ example classc ∑ style s ccs|ψclass isc⟩⊗|ψstyle variant iss⟩ (6) Speciﬁcally, for each individual example image E, we would like to have UP |ψPhoton,E⟩≈| ψC(E)⟩⊗ ∑ style s cs|ψstyle variant iss(E)⟩, (7) where C(E) is the ground truth label of the example in a supervised learning setting. Using the method described in Reck et al. (1994), this trained matrix then can be translated to an optical network blueprint. The transformed quantum state at the output side of the network of beam splitters and phase shifters then gets measured by a detector array that can discriminate M = C·S quantum states which are labeled |ψdigit is a 0⟩⊗|ψstyle variant 1⟩, |ψdigit is a 0⟩⊗|ψstyle variant 2⟩, . . . , |ψdigit is a 3⟩⊗| ψstyle variant 57⟩, . . . ,|ψdigit is a 9⟩⊗| ψstyle variantSmax ⟩. If we detect the photon in any of the |ψdigit is a 7⟩⊗... cells, the classiﬁer output is a “7”, and likewise for the other digits. From a machine learning perspective, the trainable parameters hence are the complex entries of the matrix UP , which according to quantum mechanics have to satisfy an unitarity constraint,UP U† P = I, and this search space automatically covers all experimentally realizable linear optical devices. For MNIST, where examples have 28 ×28 pixels, the most obvious choice is padding to a M = 790-dimensional input vector. While one could implement the unitarity constraint in terms of a (regularizer) loss-function contribution that measures the degree of violation of unitarity, it here makes more sense to instead use a parametrization of UP that automatically guarantees unitarity, using Lie group theory. If WP is a 790 ×790 matrix of trainable (real) weights, then the hermitean matrix HP = −i(WP −WT P ) + (WP + WT P ) parametrizes the Lie algebra u(790), and UP = exp(iHP ) covers all of the (compact) unitary group U(790). This approach slightly over-parametrizes the problem, since, in the tensor-product basis that we are transforming to, we can freely re-deﬁne the basis on each of the ten 790/10 = 79-dimensional style subspaces. This means that 10% of the parameters are redundant. Overall, with all the trainable weights being provided by the matrix WP , and the brightness of the pixel at coordinates (y,x) for example Ebeing bE;yx, we have this formula for the probability of a photon travelling through an optical device that was designed by training weights and landing on a detector cell that predicts class c: p(c|E) = ∑ s ⏐⏐⏐⏐⏐⏐ ∑ j,k,y,x expm ( WP −WT P + i(WP + WT P ) ) kj √ bE;yx∑ ˜y,˜x bE;˜y˜x δN·y+x,jδj,c·S+s ⏐⏐⏐⏐⏐⏐ 2 . (8) Here, yand xare image row- and column-indices (for MNIST, running from 0 to 27), j,k are matrix row- and column-indices (in our example, running from 0 to 789, inclusive) for the exponentiated unitary matrix UP = expm(···), sis a style-index (here, running from 0 to S−1 = 78), the term 5Published as a conference paper at ICLR 2021 c = 0  c = 1 (a) c = 0  c = 1 (b) Figure 1: (a) The two shapes of the toy example. The four gray pixels correspond to a photon arrival probability of 1/4, i.e. a probability amplitude of 1/2. (b) The per-pixel photon arrival probability after the orthogonal transformation is applied. The dark gray pixels correspond to a probability of 1/8 and the light gray pixels to 1/2. under the square root is the relative contribution of the (y,x)-pixel to the total brightness of example image E, and the δ-factors are used for translating a pair of row,column image-indices to a linear pixel index, respectively an index on the UP -rotated quantum state vector to a pair of (class, style)- indices. Technically speaking, from the viewpoint of mapping an optical amplitude that describes light intensity passing through the image-ﬁlter to the quantum amplitude of a particular (class, style)- combination, this is simply a linear model (since quantum mechanics is linear), whose linear weights are however speciﬁed in a slightly unusual way, underneath a complex matrix exponential (since quantum mechanics is unitary, i.e. probability-preserving). The probability to predict a given class c is then obtained by summing over the probabilities associated with the given class (but different style-index). Model accuracy has to be evaluated with caution: as we need to make a prediction after detection of a single photon, accuracy is the quantum probability of the correct label, averaged over all examples. Naturally, we can not determine which of the Coutput classes would receive the most photons (= has highest probability) if all we have is a single photon detection event. This accuracy, about40% for the problems considered here, differs substantially from the accuracy that would be obtainable by looking at many photons coming from the same example image, which here typically exceeds 90%, roughly in alignment with the expected achievable performance of a linear model on MNIST. In other words, probabilities are uncalibrated, and the (non-linear “deep learning”) transformation that would be required to calibrate them cannot be expressed as a unitary operator. Let us consider a radically simpliﬁed example that illustrates why this method works. We want to discriminate between only two different shapes (with no further shape variation) on a 2 ×4 pixel screen where each pixel is either “on” or “off”, using only one photon. Speciﬁcally, let us consider the two Tetris “T” shapes represented in ﬁgure 1(a). For both shapes, the probability that the single photon arrives on one of the “on” pixels is 1/4; therefore, taking into account that for two pixels the correct shape is identiﬁed exactly and for two with 50% probability, we conclude that the baseline accuracy is 1/2 + 1/4 = 75%.Instead, we can apply a unitary transformation to reshape the probability amplitudes. Let us now consider the simple but not optimal transformation of the photon amplitude that replaces the pair of amplitudes (a,b) in each 2-pixel column with ((a−b)/ √ 2,(a+ b)/ √ 2), i.e. creates destructive interference in the top row and constructive interference in the bottom row. This gives the detection probability patterns shown in ﬁgure 1 (b). Maximum likelihood estimation here gives an accuracy of 1/2 + 3/8 = 87.5%. Obtaining the maximum achievable accuracy will here require a more complicated all-pixel amplitude transformation, obtained as follows: The quantum amplitude transformation is angle-preserving, and the angle αbetween the two amplitude quantum states q1, q2 is given by cos α = ⟨q1|q2⟩= 0.5. Hence, we can rotate these two states to lie in the plane of the ﬁrst two Cartesian coordinate axes of the Hilbert space, and at the same angle from their bisector. Identifying these coordinate axes with the correct labels, the accuracy is the cosine-squared of the angle between the transformed state and the corresponding axis, i.e. cos2(π/4 −α/2) = ( √ 3 + 2)/4 ≈93.30%. 6Published as a conference paper at ICLR 2021 Table 1: Results for the Fashion-MNIST and MNIST datasets. The “classic” accuracy and information refer to the observation of a single photon, while the “quantum” quantities are obtained after applying the quantum transformation. Dataset Entropy [bits] Accuracy Bound (classic) Information (classic) [bits] Accuracy (quantum) Information (quantum) [bits] Fashion-MNIST 3.32 21.38% 1.10 36.14% 1.85 MNIST 3.32 22.96% 1.20 41.27% 2.04 While the performance measure that we care about here is the probability for a correct classiﬁcation, one observes that model training is nevertheless more effective when one instead minimizes cross- entropy, as one would when training a conventional machine learning model. Intuitively, this seems to make sense, as a gradient computed on cross-entropy loss is expected to transport more information about the particular way in which a classiﬁcation is off than a gradient that is based only on maximizing the correct classiﬁcation probability. Overall, this task is somewhat unusual as a machine learning problem for three reasons: First, it involves complex intermediate quantities, and gradient backpropagation has to correctly handle the transitioning from real to complex derivatives where the loss function is the magnitude-squared of a complex quantity. TensorFlow is at the time of this writing the only widely used machine learning framework that can handle this aspect nicely. Appendix A.3 provides details on numerical aspects. Second, (as explained above), we cannot simply pick the class for which the predicted probability is highest as the predicted class. Rather, the probability for the single-photon measurement to produce the ground truth label sets the accuracy. Third, while most machine learning architectures roughly follow a logistic regression architecture and accumulate per-class evidence which gets mapped to a vector of per-class probabilities, we here have the probabilities as the more readily available data, so the computation of cross-entropy loss will have to infer logits from probabilities. Due to this need to compute logarithms of probabilities, it is very important that the training process does not intermediately see invalid probabilities outside the range (0 ... 1), and this is ensured by parametrizing unitary transforms as matrix exponentials of anti-hermitean matrices. TensorFlow code to both train such a model and also evaluate its performance is included in the supplementary material. 3 R ESULTS Figure 2 shows the most probable image class for each pixel, for the “Fashion-MNIST” and MNIST datasets. A classiﬁer that looks up and predicts the most likely class in this table achieves maximal accuracy among all single photon classiﬁers that do not employ quantum interference. This includes classiﬁers that have had access to the test set during training. This accuracy is reported in the third column of table 1. (We note that, as pointed out by Sun et al. (2007), the “Fashion-MNIST” dataset contains many mislabeled instances, which affects both classical and quantum results.) We can compute the amount of information provided by the photon by computing the difference between the class entropy, i.e. −log2(0.1) = 3.32,since there are 10 classes, and the entropy associated to the classiﬁcation errors, i.e. the accuracy. The mutual information for the classical classiﬁer is given in the fourth column of table 1. Training a unitary U(790) quantum transformation that gets applied after the photon passed the image ﬁlter and before it hits a bank of 790 detectors allows boosting accuracy for both the “Fashion-MNIST” and MNIST datasets, as reported in the ﬁfth column of table 1. The observation of the photon after the transformation provides a higher amount of mutual information with respect to the classical case. The values of mutual information in this case are given in the last column of table 1. Explicit matrices to perform the transformation for the two data sets have been made available with the supplementary material. The quantum transformation UP allows us to deﬁne the pixel-space projection operators: Pclass C := U−1 P (|ψC⟩⟨ψC|⊗Istyle) UP (9) 7Published as a conference paper at ICLR 2021 4 8 0 0 6 6 6 6 6 1 1 1 1 1 1 1 1 1 1 1 0 9 5 5 9 9 5 9 8 8 8 0 6 6 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 9 8 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 9 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 5 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 8 8 8 8 0 0 0 2 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 2 0 4 1 1 1 1 1 3 1 1 1 1 9 9 9 9 9 9 9 5 5 8 8 8 8 8 2 2 6 4 1 1 1 1 1 7 1 7 7 7 9 9 9 9 9 9 7 7 5 8 8 8 8 8 2 2 2 4 1 1 1 1 7 7 7 7 7 7 7 5 9 5 5 7 7 7 5 8 8 8 8 8 2 2 4 4 0 1 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 8 8 8 8 8 8 2 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 5 5 5 7 7 7 7 7 7 7 1 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 5 5 5 5 5 5 7 9 9 9 9 5 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 9 9 9 9 5 9 7 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 1 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 3 8 9 9 9 9 9 9 9 9 9 9 9 9 9 2 2 9 0 3 1 1 1 3 3 3 1 1 3 0 8 9 9 9 9 9 9 9 5 5 9 9 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 9 9 9 5 5 9 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 5 9 9 5 5 5 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 9 9 5 5 8 2 2 2 2 0 0 0 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 5 9 8 8 8 2 2 2 2 8 0 0 3 1 1 3 3 3 1 1 0 0 0 2 2 2 2 5 5 9 (a) 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 3 2 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 5 5 5 5 5 5 3 3 4 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 5 5 5 5 5 5 5 7 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 5 5 5 5 5 5 5 5 8 7 7 7 3 3 7 7 3 3 7 7 9 9 1 1 1 1 1 0 5 5 5 5 5 5 5 8 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 0 5 5 5 5 8 7 7 7 7 7 7 7 7 7 9 9 9 7 1 1 1 1 7 7 7 0 0 0 0 0 5 5 5 7 7 7 7 7 7 7 7 9 9 9 5 5 1 1 1 1 7 7 7 0 0 0 0 0 5 5 8 4 7 7 7 7 4 4 9 9 5 5 3 1 1 1 1 7 7 7 0 0 0 0 0 0 5 7 7 7 4 0 0 4 4 4 5 5 3 1 1 1 9 9 7 7 0 0 0 0 0 6 6 7 4 4 0 0 4 4 4 4 4 8 1 1 1 9 9 4 6 0 0 0 0 0 6 7 5 4 0 0 0 4 4 4 4 4 4 1 1 1 4 4 4 6 0 0 0 0 0 2 7 4 0 3 2 0 0 0 0 4 4 4 4 1 1 1 1 7 4 4 6 6 0 0 0 0 2 2 2 3 2 0 0 0 0 0 6 6 2 1 1 1 7 7 4 3 6 6 0 0 0 2 2 2 2 2 3 3 2 0 0 0 0 6 6 1 1 1 1 7 7 6 6 6 6 0 2 2 2 2 2 2 2 3 3 2 0 0 0 0 2 6 1 1 1 1 7 6 6 6 6 2 2 2 2 2 2 2 2 2 3 3 2 2 2 0 2 2 2 1 1 1 1 6 6 3 3 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 1 1 1 1 1 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 8 9 9 4 4 2 2 2 2 2 2 3 3 3 3 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 2 3 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 4 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 (b) Figure 2: (a) Fashion-MNIST most likely class given detection of a single photon at the corresponding pixel coordinates. Here, the classes are: 0=T-shirt/top, 1=Trouser, 2=Pullover, 3=Dress, 4=Coat, 5=Sandal, 6=Shirt, 7=Sneaker, 8=Bag, 9=Ankle Boot. (b) Most likely digit-class given detection of a single photon for MNIST. A non-quantum classiﬁer cannot outperform one that looks up its answer on the corresponding table. 0 1 2 3 4 5 6 7 8 9 (a) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (b) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (c) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (d) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 Figure 3: The confusion matrices for the “Fashion-MNIST” and MNIST datasets when classic and quantum classiﬁers are used: (a) “Fashion-MNIST”/classic, (b) MNIST/classic, (c) “Fashion- MNIST”/quantum, (d) MNIST/quantum. with which we can decompose any example into contributions that are attributable to the different classes. Here, one must keep in mind that such separation is done at the level of probability amplitudes, so while we can compute intensities/probabilities from these components, which are mutually orthogonal as quantum states, summing these per-component per-pixel intensities will not reproduce the example’s per-pixel intensities. This shows most clearly when considering the decomposition of an example “Trouser” from the “Fashion-MNIST” dataset’s test set with the model we trained for this task, as shown in ﬁgure 4(a). The dark vertical line between the legs in the original image mostly comes from destructive interference between a bright line from the “Trousers” component and a matching bright line from the “Dress” component. Due to the intrinsic quantum nature of this set-up, care must be taken when interpreting confusion matrices. Naturally, we never can claim of any single-photon classiﬁer that it would ‘classify a particular image example correctly’, since re-running the experiment on the same example will not see the photon always being counted by the same detector! So, strictly speaking, for any single-photon classiﬁer realized as a device, the “confusion matrix” could be determined experimentally only in the statistical sense, leaving uncertainty in the entries that decreases with the number of passes over the test set. Confusion matrices are shown in ﬁgure 3. 8Published as a conference paper at ICLR 2021 Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (a) Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (b) Figure 4: Projection of probability amplitudes for some samples of the “Fashion-MNIST” (a) and the MNIST (b) datasets. The ﬁrst image shows the original sample probability and the following images show the probability amplitudes for each class. We visualize the complex amplitude by using brightness to represent magnitude and hue for phase (the colormap for the phase is shown on the right of each row.) Our factorization ansatz appears to contain a hidden constraint: we are forcing each image class to use the same number of style-states. One could imagine, for instance, that a classiﬁer might achieve even higher accuracy by treating image classes unevenly. Any such model that allows more style-space dimesions for some classes can always be embedded into a model that allows more style-space dimensions for all classes, so this question can be answered by padding to a larger Hilbert space. Numerical experiments, e.g. padding to 1000 rather than 790 dimensions, suggest that this has no appreciable impact on classiﬁcation accuracy. 4 D ISCUSSION In summary, we demonstrated that, at least for the considered datasets, the space of the single observed photon state can be factorized remarkably well by a product of the example class space and a space collecting the remaining variables, such as the style. This factorization can be obtained easily by the proposed method and is experimentally realizable with optical elements placed in front of the sensor. The supplementary material contains a blueprint for an example circuit outperforming the classical limit (at 36.05% accuracy) on 10 ×10 downsampled MNIST. An experimental implementation of the proposed system would be a demonstration of a high- temperature and low- effective-qubit quantum ML device. With respect to other experimental approaches to quantum computing, such a device would have the limitation that it is built for the speciﬁc classiﬁcation problem and cannot be reconﬁgured easily. It would be interesting to see whether an advanced quantum protocol along the lines of Knill et al. (2001) might enable the realization of more sophisticated intermediate-scale high temperature quantum machine learning in a way that mostly (like here) bypasses the need for quantum logic built from common quantum gates. 5 T ENSOR FLOW CODE TensorFlow2 code to reproduce the experiments of this work and all the ﬁgures is provided in the ancillary ﬁles together with the computed unitary transformations for MNIST and “Fashion-MNIST”. REFERENCES Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Viégas, and Martin Wattenberg. Direct-manipulation visualization of deep networks. CoRR, abs/1708.03788, 2017. URL http://arxiv.org/abs/1708. 03788. 9Published as a conference paper at ICLR 2021 Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/. E. Knill, Laﬂamme, R., and G. Milburn. A scheme for efﬁcient quantum computation with linear optics. Nature, 409:46–52, 2001. URL https://doi.org/10.1038/35051009. N. J. Cerf, C. Adami, and P. G. Kwiat. Optical simulation of quantum logic. Phys. Rev. A , 57:R1477– R1480, Mar 1998. doi: 10.1103/PhysRevA.57.R1477. URL https://link.aps.org/doi/10. 1103/PhysRevA.57.R1477. John F. Clauser and Jonathan P. Dowling. Factoring integers with young’s n-slit interferometer.Phys. Rev. A, 53:4587–4590, Jun 1996. doi: 10.1103/PhysRevA.53.4587. URL https://link.aps.org/doi/10. 1103/PhysRevA.53.4587. Johann Summhammer. Factoring and fourier transformation with a mach-zehnder interferometer. Phys. Rev. A, 56:4324–4326, Nov 1997. doi: 10.1103/PhysRevA.56.4324. URL https://link.aps.org/doi/10. 1103/PhysRevA.56.4324. Erfan Khoram, Ang Chen, Dianjing Liu, Lei Ying, Qiqi Wang, Ming Yuan, and Zongfu Yu. Nanophotonic media for artiﬁcial neural inference. Photon. Res., 7(8):823–827, Aug 2019. doi: 10.1364/PRJ.7.000823. URL http://www.osapublishing.org/prj/abstract.cfm?URI=prj-7-8-823 . Richard Phillips Feynman, Robert Benjamin Leighton, and Matthew Sands. The Feynman lectures on physics; New millennium ed. Basic Books, New York, NY , 2010. URL https://cds.cern.ch/record/ 1494701. Originally published 1963-1965. L. D. Landau and L. M. Lifshitz. Quantum Mechanics Non-Relativistic Theory, Third Edition: Volume 3 . Butterworth-Heinemann, 3 edition, January 1981. ISBN 0750635398. URL http://www.worldcat. org/isbn/0750635398. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017. Michael Reck, Anton Zeilinger, Herbert J. Bernstein, and Philip Bertani. Experimental realization of any discrete unitary operator. Phys. Rev. Lett., 73:58–61, Jul 1994. doi: 10.1103/PhysRevLett.73.58. URL https://link.aps.org/doi/10.1103/PhysRevLett.73.58. J. Sun, F. Zhao, C. Wang, and S. Chen. Identifying and correcting mislabeled training instances. In Future Generation Communication and Networking (FGCN 2007), volume 1, pages 244–250, 2007. A A PPENDIX A.1 E XPERIMENT SCHEMATICS A        B         C          D Figure 5: Schematics of the experimental set-up. Top: Classical Baseline, Bottom: Quantum Set-Up. Figure 5 shows the schematics of an exper- imental set-up: The lens (A) stylizes the last optical component of the monochro- matic, coherent, linear-polarized (i.e. laser) light source that emits photons en- tering from the left and traveling to the right. Light intensity is controlled (e.g. by means of an absorbing ﬁlter, not shown) to be so low that photons travel through the apparatus individually. Any interference effects are hence due to self-interference of a single photon’s wave function (just as in the double slit gedankenexperiment). The laser photons then hit a coherently il- luminated N ×N screen (B) (e.g. a LCD screen, in this diagram 10 ×10) which allows light to pass through a given pixel with coordinates (y,x) with a probability that is proportional to the ink density on the example image. The photon arrives at each pixel with the same optical phase (i.e. 10Published as a conference paper at ICLR 2021 having traveled the same (fractional) number of wavelengths as seen from the laser). The diagram shows an ex- emplary raster image of a digit zero with maximal brightness (maximal ink density on the digitized ML example) on 14 pixels (with zero-based row/column coordinates (y,x) = (2,4),(2,5),(2,6),(3,2),..., (5,7)), with 75% brightness on three pixels (coordinates (3,3),(6,3),(6,5)), and 50% brightness on one pixel (coordinates (7,6)). The ‘Classical Baseline’ set-up does not use interference and would work just as well in a world where photons are ‘Particles of Light’ that cannot self-interfere (as envisioned by Newton). The dimensions of the apparatus need to be such that, when the image-ﬁlter is brightly illuminated, it casts a sharp shadow on the detector-array. For the classical case, use of a coherent source of light is not necessary. At very low light levels, photons coming from the light source (A) will keep hitting the screen (B), frequently getting absorbed by a dark pixel. At some point, a photon will (by chance) manage to hit a non-dark pixel and not get absorbed (the more likely the brighter the speciﬁc pixel) and travel on to the single-photon detector array (D) (such as: a SPAD array) and be detected as having passed through a speciﬁc pixel. Ignoring experimental imperfections that could in principle be made small such as optical losses, the only possible transform on the photon state one could perform with a passive-linear optical device at (C) would be equivalent to coupling the photon into an array of optical ﬁbers, routing each ﬁber from one pixel to some other pixel, and coupling out the photon at the other side of the device. This is equivalent to re-shufﬂing the pixels, which can always be un-done by re-shufﬂing the addresses of the cells of the detector array (D) and so does not affect classiﬁcation accuracy – the diagram hence omits such transformations that cannot affect performance. The quantum set-up (bottom) is perhaps easiest to analyze via Feynman’s path integral interpretation of Quantum Mechanics: There are different ‘histories’ which lead from the same initial point (a photon coming from the laser) to the same ﬁnal result (the photon being detected at a speciﬁc pixel, such as: ‘at coordinates (4, 5)’), and the prescription is that we have to attribute a complex quantum amplitude to each such ‘history’, summing over all amplitudes that connect the same initial and ﬁnal state to get the resultant amplitude, and obtaining the associated probability as the magnitude-square of the (complex) resultant probability. We can also consider the resultant per-pixel quantum amplitudes for a photon having traveled from the light source (A) not all the way to the detector but to some intermediate point, such as just after passing the screen (B). These are described by vectors with N2 entries, one per pixel, whose absolute-magnitude-squares sum to 1. In the example, the photon-state-vector on any ﬂat, screen-parallel surface between B and C, |ψBC ⟩, has zero entries for all but the 14+3+1=18 non-dark pixels. The entries ψBC [24],ψBC [25],... that correspond the 14 maximally-bright pixels at (2,4),(2,5),... are identical, as the photon reached each pixel at the same optical phase. Calling this amplitude cB (B for ‘bright’), the amplitudes for the three moderately-bright pixels cM , and the single dim-but-not-dark pixel’s amplitudecD, the amplitude magnitude-squares must be proportional to pixel brightness (probability for a photon to pass through the image) and sum to 1 (total probability for the photon that passed through the screen to have passed through a pixel). As the complex quantum amplitude phase matches the optical phase, these constraints ﬁx cB = u· √ 1/Z≈0.244, cM = u· √ 0.75/Z≈0.212, cD = u· √ 0.5/Z≈0.173, with Z being the overall normalization factor that makes the sum of magnitude-squares of all amplitudes 1, i.e. Z = 14·1 + 3·0.75 + 1·0.5, and ubeing some complex number of magnitude 1, the non-observable overall quantum phase factor. In the ‘Quantum’ set-up, we can employ a linear optical device, built from many beam-splitters and phase shifters (= ‘delay lines’), to adjust self-interference of the photon wave function. The diagram shows two example paths out of the total 10 ×10 different paths that the photon can take before it reaches the detector array. Quantum Mechanics tells us that ‘the single photon(!) travels along all these paths simultaneously’ – this is just Young’s double slit experiment in a slightly more complicated setting. There is a 1-to-1 correspondence between physically realizable linear optical devices and probability-preserving (generalized) rotations of the quantum state vector. The (in this downsampled example: 100 ×100) components of such a transformation form a unitary matrix. If we used a linear optical device that implemented a random such transformation, and sent many photons through the apparatus, they collectively would produce an image conceptually resembling the interference pattern on photo ﬁlm that codiﬁes a hologram. Using a basic Machine Learning procedure, stochastic gradient descent, we can train the parameters of the transform such that photons coming from an image that shows a digit ‘0’ preferentially land on the 0-th row of the detector array, photons coming from an image that shows a digit ‘4’ preferentially land on the 4-th row, etc. The supplementary material describes a speciﬁc set-up in terms of optical components that reaches >36% accuracy for MNIST downsampled to 10 ×10: If this were manufactured from ideal-quality optical components, the probability for a photon that passed through the screen (B) to land on the detector row matching its digit-class is better than 36%. A.2 C LASSICAL BASELINE ACCURACY THRESHOLD : M AXIMALITY PROOF Elementary statistical considerations allow us to obtain a stringent upper bound for the maximum accuracy that cannot be exceeded by any classiﬁer which satisﬁes these two properties: • P1: The classiﬁer must make a prediction using as its input only the index of the one detector in the detector-array that received the ﬁrst photon. It can not use any additional information about the 11Published as a conference paper at ICLR 2021 example (but may have been trained with arbitrary information about the example set, even including full knowledge of the training and test set). • P2: There is a one-to-one correspondence between image pixels and detector-cells: For each image- pixel, there is exactly one detector-cell such that when the example image Eis presented, then the probability for the ﬁrst photon to land on the detector cell kis proportional to the brightness of the associated pixel in image E. This bound is what we call the ‘classical accuracy bound’. The ‘quantum’ classiﬁer violates P2 by employing photon self-interference: The probability for the k-th detector to observe the ﬁrst photon depends on collective information which the photon ‘holographically’ transports about the input image once it passed the image-ﬁlter, rather than on a single pixel. The protocol for evaluating classiﬁer accuracy is as follows: We pick a random example from the dataset’s test set, set up the device to present this example as a problem, send light towards the ﬁlter-screen, and look at the ﬁrst photon that managed to pass the ﬁlter-screen and get counted by the detector array, then map the index of the detector that counted the photon to a predicted class. We register ‘successful prediction’ if the predicted class matches the example’s label, otherwise we register an ‘unsuccessful prediction’. The accuracy is the probability of the prediction to be successful. Somewhat unusually, this means that there is no such thing as ‘the predicted class of a given image’, as one normally would have it in a Machine Learning problem. This is due to the inherent randomness of quantum mechanics: Even repeating classiﬁcation for the same image multiple times, we will see photons land on detectors that correspond to different classiﬁcations. If we were to experimentally determine accuracy, this would then suffer from the usual problems of determining a probability via a statistical experiments: one can make the likelihood to be way off arbitrarily small, but never shrink it to zero. However, the aforementioned protocol makes it possible to directly compute the maximal achievable probability for any classiﬁer, without resorting to a statistical experiment. The gist of our argument parallels the reasoning behind the claim that we can show with simple statistics that no ML classiﬁer can possibly outperform an accuracy threshold of 29/36 for predicting from the eye total of rolling two dice whether any of the two dice showed a six. Here, the reason is that we can get maximal accuracy by looking look at all the possible realizations of any given eye total and make the best possible guess given the situation. For eye totals 2 −6 (1 + 2 + 3 + 4 + 5 = 15of 36 cases), we would predict ‘No’ and always be correct. For eye totals 11 and 12 (2 + 1additional cases), we would predict ‘Yes’ and also always be correct. For each other eye total, there are two realizations where one die shows a ‘six’, so we would want to predict ‘Yes’ for eye totals having at most four realizations, i.e. where we have at least a 50% chance of being correct, and ‘No’ otherwise. Using this approach, we would incorrectly classify three cases as ‘Yes’ (5 + 5,4 + 5,5 + 4), and incorrectly classify four cases as ‘No’ (6 + 1,1 + 6,6 + 2,2 + 6). Except for these 7/36 cases, we would make a correct prediction, so optimal accuracy is 29/36. Given the perhaps somewhat unfamiliar ‘quantum ML’ setting, and the need to rigorously justify the optimality claim, we prove it below. The only material difference to the dice-sum example is that relative weights of realizations are not determined by counting, but by looking at pixel brightnesses. In analogy to the the dice example, the key observation is that the classiﬁer’s input is a single pixel-index, and its output is an image-class. So, we can completely specify any (deterministic or not) classiﬁer’s action by tabulating, per-pixel-index, what the probability is for this classiﬁer to map the given input pixel-index kto each of the possible output classes c. The resulting matrix Kkc would, for a deterministic classiﬁer, simply be a matrix with one-hot encoded image class, one row per pixel-index. The classiﬁer’s accuracy is then given by Accuracy = P(Classiﬁcation is correct) =∑ E P(E) ∑ k ·P(γk|E) ·P(yE = C(γk)) = = ∑ E,k P(E) ·P(γk|E) ·Kk,c=yE. (10) Here, P(E) is the probability to pick example E from the test set (i.e. 1/{test set size}), P(γk|E) is the probability to detect the photon in the detector cell with index k, given the example E, and P(yE = C(γk)) is the probability that example E’s labelyE matches the classiﬁer’s output on the input “the photon was detected in cell k”. The probability for detecting a photon in cell kwhen randomly drawing an example image from the test set is P(γk) =∑ E P(E) ·P(γk|E). The probability for a fairly drawn example’s label to beyc when a photon was detected at cell kis P(yc|γk) =∑ E P(E) ·P(yE = yc) ·P(γk|E). Let us tabulate these P(yc|γk) in the {#pixels}×{#classes}matrix Rkc := P(yc|γk). We then have: Accuracy = ∑ Detector cellk ∑ Class c P(γk) ·P(yc|γk) ·Kkc = ∑ k,c P(γk)RkcKkc. (11) In words: We can compute accuracy by looking at each detector cell k and each class c, determining the probability P(γk) that, when fairly drawing examples from the test set, a photon gets detected at cell k, and 12Published as a conference paper at ICLR 2021 splitting up this probability into contributions from examples where the target class was 0, 1, 2, etc. These contributions are P(γk) ·P(yc|γk). We make a correct classiﬁcation when the classiﬁer also predicts class c given the input k. The classiﬁer’s behavior when given the inputkis speciﬁed by row kof the K-matrix, so this probability is Kkc. Here, P(γk) and Rkc are determined by the test set. Each admissible matrix Kkc that has ∑ c Kkc = 1speciﬁes a different classiﬁer, and the accuracy is a function of this matrix Konly. The question is now which admissible matrix Kmaximizes accuracy. Total classiﬁcation performance (accuracy) is a weighted sum over per-detector- cell performances (the weights being the probabilities to observe a photon in cell k when doing detection experiments on samples drawn fairly from the test set). Let K1 be a matrix that maximizes accuracy, and K2 be a matrix obtained by picking, for each cell-index k, a probability row-vector that maximizes ∑ c RkcK2,kc. We have Accuracy(K1) ≥Accuracy(K2) (since K1 is optimal), and also ∑ c Rkc(K2,kc −K1,kc) ≥0 (since K2 maximizes this value on each row k), so, taking a weighted sum with weights P(γk) ≥0, we ﬁnd ∑ k P(γk) ∑ c Rkc(K2,kc −K1,kc) ≥0, i.e. Accuracy(K2) ≥Accuracy(K1), hence Accuracy(K1) = Accuracy(K2). In words, we achieve maximal accuracy if we individually look at each “photon detected in cell k” case and make the optimal prediction there. Now, for a ﬁxed cell-index k, ∑ c RkcK2,kc is maximal if the matrix-row K2,kc has an entry 1 for the index cfor which Rkc is maximal, and is zero otherwise. To see this, let us assume K2,kc >0 for some index cfor which there is another class index dwith Rkd >Rkc. Then, incrementing K2,kd by K2,kc and subsequently setting K2,kc to zero increases ∑ c RkcK2,kc, which is a contradiction. So, optimal choices of K2,kc are zero for classes cfor which Rkc is not maximal. Also, we always attain the maximum when choosing each row-vector of Kto be one-hot and have its 1-entry in a place that maximizes Rkc, i.e. maximal achievable accuracy is obtained by a classiﬁer which, for every cell-index k, predicts the most likely digit-class subject to the constraint that a randomly drawn example from the test set had its ﬁrst photon-detection occur at detector cell k. This theoretical upper bound on classiﬁer accuracy hence is given by: Accuracy ≤ ∑ k P(γk)maxcRkc. (12) For the MNIST dataset, this is found to be 22.957% (rounded up to 22.96%), while for Fashion-“MNIST”, we get 21.375% (rounded up to 21.38%). Code that implements this calculation is available in the supplementary material. We should emphasize that the constructive procedure described here that yields a classiﬁer attaining this stringent upper bound does inspect the test set, and relevant deviations in statistical properties between training and test set would manifest in the form of lowering attainable accuracy for a classiﬁer that is trained on the training set only. A.3 B ACKPROPAGATION WITH COMPLEX INTERMEDIATE QUANTITIES As explained in the main text, the per-class probabilities deﬁned by Eq. (8), when used as input to a conventional softmax loss function, make training the real weight-parameters matrixWP in terms of which the unitary rotation is expressed a straightforward procedure. Nevertheless, this approach utilizes some capabilities which at the time of this writing are likely TensorFlow- speciﬁc. It hence may make sense to describe the training procedure in sufﬁcient detail to allow straightforward re-implementation on top of some other Machine Learning framework, or perhaps even directly without use of any such library. The loss function is deﬁned in terms of the magnitude-squared of a complex intermediate quantity, which here is the vector of complex quantum amplitudes, one entry per class/style combination. In this appendix, we henceforth consider the simpliﬁed 10 ×10 problem described in detail in appendix A.1. We can perform the calculation entirely in terms of real quantities by replacing every complex number C+ iD by a real 2 ×2 matrix block of the form C+ iD→ ( C −D D C ) . (13) This means in particular that a 100-dimensional (complex) amplitude-vector aj gets replaced by a 200 ×2- matrix Amn. If we interpret the a-index j as encoding class c and style s, i.e. j = c·S + s, the total probability for class c is p(c) = ∑ s |ac·S+s|2 = (Re ac·S+s)2 + (Imac·S+s)2, and this gets replaced by p(c) =∑ s ( A2 (c·S+s)·2,0 + A2 (c·S+s)·2,1 ) (reading off the real and imaginary part from the 1st column of the 2 ×2 block that represents aj). As the square root of the relative per-pixel intensity is real, the input-image amplitudes in this approach likewise get represented by a 200 ×2-matrix B. Speciﬁcally, if e.g. Q2,5 is the contribution of pixel (y= 2,x = 5)’s 13Published as a conference paper at ICLR 2021 brightness to the total image-brightness, this gets represented as: ( B(2·10+5)·2,0 B(2·10+5)·2+1,0 B(2·10+5)·2,1 B(2·10+5)·2+1,1 ) = ( B50,0 B51,0 B50,1 B51,1 ) = ( √ Q2,5 0 0 √ Q2,5 ) . (14) The off-diagonal part, which would correspond to the imaginary part of the amplitude, is zero here. The matrix that gets exponentiated is a real200×200 matrix, and its exponential, which also is a real200×200 matrix, gets multiplied from the right with the 200 ×2 matrix of input-image amplitudes and gives the real200 ×2-matrix A from above that contains the real and imaginary parts of class- and style-amplitudes. The real 200 ×200 matrix under the exponential only depends on 100 ×100 real parameters WP . Calling the 200 ×200-matrix M, the “2 ×2-blocking” prescription to obtain its entries from WP is: ( Mi·2 ,j·2 Mi·2 ,j·2+1 Mi·2+1,j·2 Mi·2+1,j·2+1 ) = ( (Wij −Wji) −(Wij + Wji) (Wij + Wji) ( Wij −Wji) ) . (15) Finally, we need a backpropagation-friendly prescription for computing a good approximation to the matrix exponential. The theory of compact Lie groups tells us that we can reach every ‘generalized’ (since complex) rotation matrix by exponentiating matrices where each entry is from some not too large interval. For matrices with small entries only, we can use a truncated Taylor polynomial to get a good numerical approximation of its exponential, using expm(M) ≈I+ M + 1 2M ·M + 1 6M ·M ·M + ..., (16) and we can reduce the problem of ﬁnding the matrix exponential of a matrix where this series requires many terms to give a good approximation by repeated halving and squaring, repeatedly using the property expm(M) = expm(M/2)2 = expm(M/2) ·expm(M/2). For the problem discussed here, the angle-ranges for rotations that need to be considered are limited, and this makes it feasible to in-advance pick both a number of squarings (such as: 8) and a maximal term in the Taylor expansion (such as: 10th power), and get very good results. The numerical computation implemented in the supplementary material, being based on TensorFlow, deviates from the procedure described here in two relevant ways. First, while TensorFlow’s differentiable matrix exponentiation algorithm employs repeated halving/squaring, it uses a Padé rather than Taylor approximation to compute the matrix exponential of a matrix with small entries. Second, TensorFlow can directly backpropagate through complex intermediate quantities, and handles the transition between real and complex gradients in just the way that one also obtains when expanding complex numbers to real 2 ×2 blocks as described above. It can however avoid the inefﬁciency associated with using actual real2 ×2 matrix blocks that make every real and every imaginary part show up in memory not once, but twice. 14",
      "references": [],
      "meta_data": {
        "arxiv_id": "2008.05859v2",
        "authors": [
          "Thomas Fischbacher",
          "Luciano Sbaiz"
        ],
        "published_date": "2020-08-13T12:37:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates a quantum-inspired, room-temperature optical toy model that shows how single-photon state transformations via linear optics can outperform any classical single-photon classifier for MNIST and Fashion-MNIST under a constraint: classification must occur after the first photon hits a pixel. It proves a classical accuracy bound for this setting (22.96% for MNIST, 21.38% for Fashion-MNIST) and shows that a quantum-interference-enabled classifier can reach 41.27% (MNIST) and 36.14% (Fashion-MNIST). It also provides a framework to train the required unitary transformations (UP) using TensorFlow and maps trained UPs to optical circuit blueprints via Reck’s decomposition, serving as an accessible teaching tool for quantum measurement.",
        "methodology": "The methodology encodes image pixels into the spatial mode amplitudes of a single photon (N^2 dimensional). A trainable unitary UP (dimension M, padded as M = C × S) acts on the photon's state, rotating amplitudes such that the output corresponds to a tensor product over (class, style). UP is parameterized by WP, with UP = exp(iH P) and H P derived from WP to ensure unitarity. The likelihood p(c|E) is computed by summing amplitudes across pixel-contributions and style indices, and training minimizes cross-entropy over the resulting probabilities. The matrix exponential is implemented with real-valued parameterizations and backprop through complex quantities (handled by TensorFlow). Reck et al.’s constructive decomposition translates UP into a network of beam splitters and phase shifters for physical realization. The training objective uses a cross-entropy-like loss derived from the complex amplitudes, and the setup augments the input with padding to create a fixed-size unitary transform (790×790 for MNIST, scaled for Fashion-MNIST).",
        "experimental_setup": "Datasets: MNIST and Fashion-MNIST (28×28 images). Device-level setup: a single-photon probe passing through a pixelated image mask, followed by a linear-optical network realizing a unitary UP, and a detector bank with M = C×S detectors. Training is performed with stochastic gradient descent in TensorFlow to optimize WP (and thus UP) under the unitarity constraint UP†UP = I. Evaluation measures the probability that a single detected photon yields the correct class, which is inherently different from typical multi-photon ML accuracy. Baselines: a classical single-photon classifier that predicts the most likely class per pixel, achieving the theoretical bound of 22.96% (MNIST) / 21.38% (Fashion-MNIST). Reported results show quantum-enhanced accuracies of 41.27% (MNIST) and 36.14% (Fashion-MNIST); mutual information calculations accompany accuracy. The paper provides a TensorFlow implementation and supplementary material with explicit UP matrices and an experimental blueprint for >36% accuracy on 10×10 MNIST downsampling.",
        "limitations": "The model is a room-temperature optical toy with a single photon, so it is not a scalable quantum computer and does not generalize to multi-photon or entangled regimes. The classical bound assumes a strict one-to-one pixel-to-detector mapping (P2) and a single-photon decision; quantum interference between pixels invalidates this bound. Probabilities at the detector are uncalibrated and cannot straightforwardly be converted to logits for standard ML pipelines. Real-world optical losses, component imperfections, and the need for precise phase control could degrade performance. The approach relies on padding and a specific class/style factorization; there is an implicit assumption that a fixed Hilbert-space partition suffices for the tasks considered, which may not hold for other datasets or more complex images. The study also depends on the test-set-specific distributions to compute the classical bound, and mislabeled Fashion-MNIST examples affect both baselines.",
        "future_research_directions": "Explore more general quantum protocols for optical ML beyond the single-photon toy model, including scalable schemes inspired by Knill–Laflamme–Milburn to enable interference-based ML with higher effective qubits at room temperature. Investigate multi-photon or entangled-state approaches, alternative datasets, and more complex unitary decompositions to increase accuracy and generalization. Develop hardware prototypes that implement more robust optical networks with calibration strategies, and study how non-linearities or adaptive measurements could further improve performance. Investigate theoretical upper bounds for quantum-enhanced ML in similar photon-limited regimes and extend TensorFlow-based training to other quantum-inspired architectures.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
      "full_text": "RandAugment: Practical automated data augmentation with a reduced search space Ekin D. Cubuk ∗, Barret Zoph∗, Jonathon Shlens, Quoc V . Le Google Research, Brain Team {cubuk, barretzoph, shlens, qvl}@google.com Abstract Recent work has shown that data augmentation has the potential to signiﬁcantly improve the generalization of deep learning models. Recently, automated augmentation strate- gies have led to state-of-the-art results in image classiﬁca- tion and object detection. While these strategies were op- timized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and im- proved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a sep- arate search phase which increases the training complex- ity and may substantially increase the computational cost. Additionally, due to the separate search phase, these ap- proaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAug- ment has a signiﬁcantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmenta- tion approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over base- line augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online. 1 ∗Authors contributed equally. 1github.com/tensorflow/tpu/tree/master/models/ official/efficientnet search CIFAR-10 SVHN ImageNet ImageNet space PyramidNet WRN ResNet E. Net-B7 Baseline 0 97.3 98.5 76.3 84.0 AA 1032 98.5 98.9 77.6 84.4 Fast AA 1032 98.3 98.8 77.6 - PBA 1061 98.5 98.9 - - RA (ours) 102 98.5 99.0 77.6 85.0 Table 1. RandAugment matches or exceeds predictive perfor- mance of other augmentation methods with a signiﬁcantly re- duced search space. We report the search space size and the test accuracy achieved for AutoAugment (AA) [5], Fast AutoAugment [25], Population Based Augmentation (PBA) [20] and the pro- posed RandAugment (RA) on CIFAR-10 [22], SVHN [34], and ImageNet [6] classiﬁcation tasks. Architectures presented include PyramidNet [15], Wide-ResNet-28-10 [53], ResNet-50 [17], and EfﬁcientNet-B7 [47]. Search space size is reported as the order of magnitude of the number of possible augmentation policies. All accuracies are the percentage on a cross-validated validation or test split. Dash indicates that results are not available. 1. Introduction Data augmentation is a widely used method for gen- erating additional data to improve machine learning sys- tems, for image classiﬁcation [43, 23, 7, 54], object detec- tion [13], instance segmentation [10], and speech recogni- tion [21, 16, 36]. Unfortunately, data augmentation meth- ods require expertise, and manual work to design policies that capture prior knowledge in each domain. This require- ment makes it difﬁcult to extend existing data augmentation methods to other applications and domains. Learning policies for data augmentation has recently emerged as a method to automate the design of augmen- tation strategies and therefore has the potential to address some weaknesses of traditional data augmentation methods [5, 57, 20, 25]. Training a machine learning model with a learned data augmentation policy may signiﬁcantly im- prove accuracy [5], model robustness [32, 52, 41], and per- formance on semi-supervised learning [50] for image clas- siﬁcation; likewise, for object detection tasks on COCO and PASCAL-VOC [57]. Notably, unlike engineering bet- 1 arXiv:1909.13719v2  [cs.CV]  14 Nov 2019ter network architectures [59], all of these improvements in predictive performance incur no additional computational cost at inference time. In spite of the beneﬁts of learned data augmentation poli- cies, the computational requirements as well as the added complexity of two separate optimization procedures can be prohibitive. The original presentation of neural architecture search (NAS) realized an analogous scenario in which the dual optimization procedure resulted in superior predictive performance, but the original implementation proved pro- hibitive in terms of complexity and computational demand. Subsequent work accelerated training efﬁciency and the ef- ﬁcacy of the procedure [30, 38, 28, 29], eventually making the method amenable to a uniﬁed optimization based on a differentiable process [30]. In the case of learned augmen- tations, subsequent work identiﬁed more efﬁcient search methods [20, 25], however such methods still require a sep- arate optimization procedure, which signiﬁcantly increases the computational cost and complexity of training a ma- chine learning model. The original formulation for automated data augmenta- tion postulated a separate search on a small, proxy task whose results may be transferred to a larger target task [59, 58]. This formulation makes a strong assumption that the proxy task provides a predictive indication of the larger task [28, 2]. In the case of learned data augmentation, we provide experimental evidence to challenge this core as- sumption. In particular, we demonstrate that this strategy is sub-optimal as the strength of the augmentation depends strongly on model and dataset size. These results suggest that an improved data augmentation may be possible if one could remove the separate search phase on a proxy task. In this work, we propose a practical method for auto- mated data augmentation – termed RandAugment – that does not require a separate search. In order to remove a sep- arate search, we ﬁnd it necessary to dramatically reduce the search space for data augmentation. The reduction in pa- rameter space is in fact so dramatic that simple grid search is sufﬁcient to ﬁnd a data augmentation policy that outper- forms all learned augmentation methods that employ a sep- arate search phase. Our contributions can be summarized as follows: •We demonstrate that the optimal strength of a data aug- mentation depends on the model size and training set size. This observation indicates that a separate opti- mization of an augmentation policy on a smaller proxy task may be sub-optimal for learning and transferring augmentation policies. •We introduce a vastly simpliﬁed search space for data augmentation containing 2 interpretable hyper- parameters. One may employ simple grid search to tailor the augmentation policy to a model and dataset, removing the need for a separate search process. •Leveraging this formulation, we demonstrate state-of- the-art results on CIFAR [22], SVHN [34], and Im- ageNet [6]. On object detection [27], our method is within 0.3% mAP of state-of-the-art. On ImageNet we achieve a state-of-the-art accuracy of 85.0%, a 0.6% increment over previous methods and 1.0% over base- line augmentation. 2. Related Work Data augmentation has played a central role in the train- ing of deep vision models. On natural images, horizon- tal ﬂips and random cropping or translations of the images are commonly used in classiﬁcation and detection mod- els [53, 23, 13]. On MNIST, elastic distortions across scale, position, and orientation have been applied to achieve im- pressive results [43, 4, 49, 42]. While previous examples augment the data while keeping it in the training set dis- tribution, operations that do the opposite can also be effec- tive in increasing generalization. Some methods randomly erase or add noise to patches of images for increased valida- tion accuracy [8, 55], robustness [46, 52, 11], or both [32]. Mixup [54] is a particularly effective augmentation method on CIFAR-10 and ImageNet, where the neural network is trained on convex combinations of images and their corre- sponding labels. Object-centric cropping is commonly used for object detection tasks [31], whereas [9] adds new objects on training images by cut-and-paste. Moving away from individual operations to augment data, other work has focused on ﬁnding optimal strategies for combining different operations. For example, Smart Augmentation learns a network that merges two or more samples from the same class to generate new data [24]. Tran et al. generate augmented data via a Bayesian approach, based on the distribution learned from the training set [48]. DeVries et al. use transformations (e.g. noise, interpo- lations and extrapolations) in the learned feature space to augment data [7]. Furthermore, generative adversarial net- works (GAN) have been used to choose optimal sequences of data augmentation operations[39]. GANs have also been used to generate training data directly [37, 33, 56, 1, 44], however this approach does not seem to be as beneﬁcial as learning sequences of data augmentation operations that are pre-deﬁned [40]. Another approach to learning data augmentation strate- gies from data is AutoAugment [5], which originally used reinforcement learning to choose a sequence of operations as well as their probability of application and magnitude. Application of AutoAugment policies involves stochasticity at multiple levels: 1) for every image in every minibatch, a sub-policy is chosen with uniform probability. 2) oper- ations in each sub-policy has an associated probability ofFigure 1. Example images augmented by RandAugment. In these examples N=2 and three magnitudes are shown corre- sponding to the optimal distortion magnitudes for ResNet-50, EfﬁcientNet-B5 and EfﬁcientNet-B7, respectively. As the dis- tortion magnitude increases, the strength of the augmentation in- creases. application. 3) Some operations have stochasticity over di- rection. For example, an image can be rotated clockwise or counter-clockwise. The layers of stochasticity increase the amount of diversity that the network is trained on, which in turn was found to signiﬁcantly improve generalization on many datasets. More recently, several papers used the Au- toAugment search space and formalism with improved op- timization algorithms to ﬁnd AutoAugment policies more efﬁciently [20, 25]. Although the time it takes to search for policies has been reduced signiﬁcantly, having to imple- ment these methods in a separate search phase reduces the applicability of AutoAugment. For this reason, this work aims to eliminate the search phase on a separate proxy task completely. Some of the developments in RandAugment were in- spired by the recent improvements to searching over data augmentation policies. For example, Population Based Augmentation (PBA) [20] found that the optimal magnitude of augmentations increased during the course of training, which inspired us to not search over optimal magnitudes for each transformation but have a ﬁxed magnitude schedule, which we discuss in detail in Section 3. Furthermore, au- thors of Fast AutoAugment [25] found that a data augmen- tation policy that is trained for density matching leads to improved generalization accuracy, which inspired our ﬁrst order differentiable term for improving augmentation (see Section 4.7). transforms = [ ’Identity’, ’AutoContrast’, ’Equalize’, ’Rotate’, ’Solarize’, ’Color’, ’Posterize’, ’Contrast’, ’Brightness’, ’Sharpness’, ’ShearX’, ’ShearY’, ’TranslateX’, ’TranslateY’] def randaugment(N, M): \"\"\"Generate a set of distortions. Args: N: Number of augmentation transformations to apply sequentially. M: Magnitude for all the transformations. \"\"\" sampled_ops = np.random.choice(transforms, N) return [(op, M) for op in sampled_ops] Figure 2. Python code for RandAugment based on numpy. 3. Methods The primary goal of RandAugment is to remove the need for a separate search phase on a proxy task. The reason we wish to remove the search phase is because a separate search phase signiﬁcantly complicates training and is com- putationally expensive. More importantly, the proxy task may provide sub-optimal results (see Section 4.1). In or- der to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model. Given that previ- ous learned augmentation methods contained 30+ parame- ters [5, 25, 20], we focus on vastly reducing the parameter space for data augmentation. Previous work indicates that the main beneﬁt of learned augmentation policies arise from increasing the diversity of examples [5, 20, 25]. Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation: • identity • autoContrast • equalize • rotate • solarize • color • posterize • contrast • brightness • sharpness • shear-x • shear-y • translate-x • translate-y In order to reduce the parameter space but still maintain im- age diversity, we replace the learned policies and probabili- ties for applying each transformation with a parameter-free procedure of always selecting a transformation with uni- form probability 1 K. Given N transformations for a training image, RandAugment may thus express KN potential poli- cies. The ﬁnal set of parameters to consider is the magnitude of the each augmentation distortion. Following [5], we em- ploy the same linear scale for indicating the strength of each transformation. Brieﬂy, each transformation resides on aninteger scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation. A data aug- mentation policy consists of identifying an integer for each augmentation [5, 25, 20]. In order to reduce the parame- ter space further, we observe that the learned magnitude for each transformation follows a similar schedule during train- ing (e.g. Figure 4 in [20]) and postulate that a single global distortion M may sufﬁce for parameterizing all transforma- tions. We experimented with four methods for the schedule of M during training: constant magnitude, random magni- tude, a linearly increasing magnitude, and a random magni- tude with increasing upper bound. The details of this exper- iment can be found in Appendix A.1.1. The resulting algorithm contains two parameters N and M and may be expressed simply in two lines of Python code (Figure 2). Both parameters are human-interpretable such that larger values of N and M increase regulariza- tion strength. Standard methods may be employed to efﬁ- ciently perform hyperparameter optimization [45, 14], how- ever given the extremely small search space we ﬁnd that naive grid search is quite effective (Section 4.1). We justify all of the choices of this proposed algorithm in this subse- quent sections by comparing the efﬁcacy of the learned aug- mentations to all previous learned data augmentation meth- ods. 4. Results To explore the space of data augmentations, we exper- iment with core image classiﬁcation and object detection tasks. In particular, we focus on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets as well as COCO object de- tection so that we may compare with previous work. For all of these datasets, we replicate the corresponding architec- tures and set of data transformations. Our goal is to demon- strate the relative beneﬁts of employing this method over previous learned augmentation methods. 4.1. Systematic failures of a separate proxy task A central premise of learned data augmentation is to con- struct a small, proxy task that may be reﬂective of a larger task [58, 59, 5]. Although this assumption is sufﬁcient for identifying learned augmentation policies to improve per- formance [5, 57, 36, 25, 20], it is unclear if this assumption is overly stringent and may lead to sub-optimal data aug- mentation policies. In this ﬁrst section, we challenge the hypothesis that for- mulating the problem in terms of a small proxy task is ap- propriate for learned data augmentation. In particular, we explore this question along two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size. To explore this hypothesis, we sys- tematically measure the effects of data augmentation poli- cies on CIFAR-10. First, we train a family of Wide-ResNet baseline PBA Fast AA AA RA CIFAR-10 Wide-ResNet-28-2 94.9 - - 95.9 95.8 Wide-ResNet-28-10 96.1 97.4 97.3 97.4 97.3 Shake-Shake 97.1 98.0 98.0 98.0 98.0 PyramidNet 97.3 98.5 98.3 98.5 98.5 CIFAR-100 Wide-ResNet-28-2 75.4 - - 78.5 78.3 Wide-ResNet-28-10 81.2 83.3 82.7 82.9 83.3 SVHN (core set) Wide-ResNet-28-2 96.7 - - 98.0 98.3 Wide-ResNet-28-10 96.9 - - 98.1 98.3 SVHN Wide-ResNet-28-2 98.2 - - 98.7 98.7 Wide-ResNet-28-10 98.5 98.9 98.8 98.9 99.0 Table 2. Test accuracy (%) on CIFAR-10, CIFAR-100, SVHN and SVHN core set. Comparisons across default data augmenta- tion (baseline), Population Based Augmentation (PBA) [20] and Fast AutoAugment (Fast AA) [25], AutoAugment (AA) [5] and proposed RandAugment (RA). Note that baseline and AA are replicated in this work. SVHN core set consists of 73K examples. The Shake-Shake model [12] employed a 26 2 ×96d conﬁgura- tion, and the PyramidNet model used the ShakeDrop regulariza- tion [51]. Results reported by us are averaged over 10 independent runs. Bold indicates best results. architectures [53], where the model size may be system- atically altered through the widening parameter governing the number of convolutional ﬁlters. For each of these net- works, we train the model on CIFAR-10 and measure the ﬁnal accuracy compared to a baseline model trained with default data augmentations (i.e. ﬂip left-right and random translations). The Wide-ResNet models are trained with the additional K=14 data augmentations (see Methods) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30] 2. Figure 3a demonstrates the relative gain in accuracy of a model trained across increasing distortion magnitudes for three Wide-ResNet models. The squares indicate the dis- tortion magnitude with which achieves the highest accu- racy. Note that in spite of the measurement noise, Figure 3a demonstrates systematic trends across distortion magni- tudes. In particular, plotting all Wide-ResNet architectures versus the optimal distortion magnitude highlights a clear monotonic trend across increasing network sizes (Figure 3b). Namely, larger networks demand larger data distor- tions for regularization. Figure 1 highlights the visual dif- ference in the optimal distortion magnitude for differently sized models. Conversely, a learned policy based on [5] provides a ﬁxed distortion magnitude (Figure 3b, dashed line) for all architectures that is clearly sub-optimal. A second dimension for constructing a small proxy task 2Note that the range of magnitudes exceeds the speciﬁed range of mag- nitudes in the Methods because we wish to explore a larger range of mag- nitudes for this preliminary experiment. We retain the same scale as [5] for a value of 10 to maintain comparable results.Figure 3. Optimal magnitude of augmentation depends on the size of the model and the training set. All results report CIFAR-10 validation accuracy for Wide-ResNet model architectures [53] averaged over 20 random initializations, where N = 1. (a) Accuracy of Wide-ResNet-28-2, Wide-ResNet-28-7, and Wide-ResNet-28-10 across varying distortion magnitudes. Models are trained for 200 epochs on 45K training set examples. Squares indicate the distortion magnitude that achieves the maximal accuracy. (b) Optimal distortion magnitude across 7 Wide-ResNet-28 architectures with varying widening parameters ( k). (c) Accuracy of Wide-ResNet-28-10 for three training set sizes (1K, 4K, and 10K) across varying distortion magnitudes. Squares indicate the distortion magnitude that achieves the maximal accuracy. (d) Optimal distortion magnitude across 8 training set sizes. Dashed curves show the scaled expectation value of the distortion magnitude in the AutoAugment policy [5]. is to train the proxy on a small subset of the training data. Figure 3c demonstrates the relative gain in accu- racy of Wide-ResNet-28-10 trained across increasing dis- tortion magnitudes for varying amounts of CIFAR-10 train- ing data. The squares indicate the distortion magnitude with that achieves the highest accuracy. Note that in spite of the measurement noise, Figure 3c demonstrates systematic trends across distortion magnitudes. We ﬁrst observe that models trained on smaller training sets may gain more im- provement from data augmentation (e.g. 3.0% versus 1.5% in Figure 3c). Furthermore, we see that the optimal distor- tion magnitude is larger for models that are trained on larger datasets. At ﬁrst glance, this may disagree with the expec- tation that smaller datasets require stronger regularization. Figure 3d demonstrates that the optimal distortion mag- nitude increases monotonically with training set size. One hypothesis for this counter-intuitive behavior is that aggres- sive data augmentation leads to a low signal-to-noise ratio in small datasets. Regardless, this trend highlights the need for increasing the strength of data augmentation on larger datasets and the shortcomings of optimizing learned aug- mentation policies on a proxy task comprised of a subset of the training data. Namely, the learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest. The dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task. This empirical result suggests that a distinct strategy may be necessary for ﬁnding an optimal data augmentation pol- icy. In particular, we propose in this work to focus on a uniﬁed optimization of the model weights and data augmen- tation policy. Figure 3 suggest that merely searching for a shared distortion magnitude M across all transformations may provide sufﬁcient gains that exceed learned optimiza- tion methods [5]. Additionally, we see that optimizing in- dividual magnitudes further leads to minor improvement in performance (see Section A.1.2 in Appendix). Furthermore, Figure 3a and 3c indicate that merely sam- pling a few distortion magnitudes is sufﬁcient to achieve good results. Coupled with a second free parameter N, we consider these results to prescribe an algorithm for learning an augmentation policy. In the subsequent sec- tions, we identify two free parameters N and M specify- ing RandAugment through a minimal grid search and com- pare these results against computationally-heavy learneddata augmentations based on proxy tasks. 4.2. CIFAR CIFAR-10 has been extensively studied with previous data augmentation methods and we ﬁrst test this proposed method on this data. The default augmentations for all methods include ﬂips, pad-and-crop and Cutout [8]. N and Mwere selected based on the validation performance on 5K held out examples from the training set for 1 and 5 settings for N and M, respectively. Results indicate that RandAug- ment achieves either competitive (i.e. within 0.1%) or state- of-the-art on CIFAR-10 across four network architectures (Table 2). As a more challenging task, we additionally com- pare the efﬁcacy of RandAugment on CIFAR-100 for Wide- ResNet-28-2 and Wide-ResNet-28-10. On the held out 5K dataset, we sampled 2 and 4 settings for N and M, respec- tively (i.e. N={1,2}and M={2,6,10,14}). For Wide- ResNet-28-2 and Wide-ResNet-28-10, we ﬁnd that N=1, M=2 and N=2, M=14 achieves best results, respectively. Again, RandAugment achieves competitive or superior re- sults across both architectures (Table 2). 4.3. SVHN Because SVHN is composed of numbers instead of nat- ural images, the data augmentation strategy for SVHN may differ substantially from CIFAR-10. Indeed, [5] identiﬁed a qualitatively different policy for CIFAR-10 than SVHN. Likewise, in a semi-supervised setting for CIFAR-10, a pol- icy learned from CIFAR-10 performs better than a policy learned from SVHN [50]. SVHN has a core training set of 73K images [34]. In addition, SVHN contains 531K less difﬁcult “extra” im- ages to augment training. We compare the performance of the augmentation methods on SVHN with and without the extra data on Wide-ResNet-28-2 and Wide-ResNet-28-10 (Table 2). In spite of the large differences between SVHN and CIFAR, RandAugment consistently matches or outper- forms previous methods with no alteration to the list of transformations employed. Notably, for Wide-ResNet-28- 2, applying RandAugment to the core training dataset im- proves performance more than augmenting with 531K ad- ditional training images (98.3% vs. 98.2%). For, Wide- ResNet-28-10, RandAugment is competitive with augment- ing the core training set with 531K training images (i.e. within 0.2%). Nonetheless, Wide-ResNet-28-10 with Ran- dAugment matches the previous state-of-the-art accuracy on SVHN which used a more advanced model [5]. 4.4. ImageNet Data augmentation methods that improve CIFAR-10 and SVHN models do not always improve large-scale tasks such as ImageNet. For instance, Cutout substantially improves CIFAR and SVHN performance [8], but fails to improve ImageNet [32]. Likewise, AutoAugment does not increase the performance on ImageNet as much as other tasks [5], especially for large networks (e.g. +0.4% for AmoebaNet- C [5] and +0.1% for EfﬁcientNet-B5 [47]). One plausible reason for the lack of strong gains is that the small proxy task was particularly impoverished by restricting the task to ∼10% of the 1000 ImageNet classes. Table 3 compares the performance of RandAugment to other learned augmentation approaches on ImageNet. Ran- dAugment matches the performance of AutoAugment and Fast AutoAugment on the smallest model (ResNet-50), but on larger models RandAugment signiﬁcantly outperforms other methods achieving increases of up to +1.3% above the baseline. For instance, on EfﬁcientNet-B7, the resulting model achieves 85.0% – a new state-of-the-art accuracy – exhibiting a 1.0% improvement over the baseline augmen- tation. These systematic gains are similar to the improve- ments achieved with engineering new architectures [59, 28], however these gains arise without incurring additional com- putational cost at inference time. 4.5. COCO To further test the generality of this approach, we next explore a related task of large-scale object detection on the COCO dataset [27]. Learned augmentation policies have improved object detection and lead to state-of-the-art results [57]. We followed previous work by training on the same architectures and following the same training schedules (see Appendix A.3). Brieﬂy, we employed RetinaNet [26] with ResNet-101 and ResNet-200 as a backbone [17]. Models were trained for 300 epochs from random initialization. Table 4 compares results between a baseline model, Au- toAugment and RandAugment. AutoAugment leveraged additional, specialized transformations not afforded to Ran- dAugment in order to augment the localized bounding box of an image [57]. In addition, note that AutoAugment expended ∼15K GPU hours for search, where as Ran- dAugment was tuned by on merely 6 values of the hyper- parameters (see Appendix A.3). In spite of the smaller li- brary of specialized transformations and the lack of a sep- arate search phase, RandAugment surpasses the baseline model and provides competitive accuracy with AutoAug- ment. We reserve for future work to expand the transforma- tion library to include bounding box speciﬁc transformation to potentially improve RandAugment results even further. 4.6. Investigating the dependence on the included transformations RandAugment achieves state-of-the-art results across different tasks and datasets using the same list of transfor- mations. This result suggests that RandAugment is largely insensitive to the selection of transformations for differ- ent datasets. To further study the sensitivity, we experi-baseline Fast AA AA RA ResNet-50 76.3 / 93.1 77.6 / 93.7 77.6 / 93.8 77.6 / 93.8 EfﬁcientNet-B5 83.2 / 96.7 - 83.3 / 96.7 83.9 / 96.8 EfﬁcientNet-B7 84.0 / 96.9 - 84.4 / 97.1 85.0 / 97.2 Table 3. ImageNet results. Top-1 and Top-5 accuracies (%) on ImageNet. Baseline and AutoAugment (AA) results on ResNet-50 are from [5]. Fast AutoAugment (Fast AA) results are from [25]. EfﬁcientNet results with and without AutoAugment are from [47]. Highest accuracy for each model is presented in bold. Note that Population Based Augmentation (PBA) [20] has not been implemented on ImageNet. model augmentation mAP search space Baseline 38.8 0 ResNet-101 AutoAugment 40.4 1034 RandAugment 40.1 102 Baseline 39.9 0 ResNet-200 AutoAugment 42.1 1034 RandAugment 41.9 102 Table 4. Results on object detection. Mean average precision (mAP) on COCO detection task. Higher is better. Search space size is reported as the order of magnitude of the number of possible augmentation policies. Models are trained for 300 epochs from random initialization following [57]. Figure 4. Average performance improves when more transfor- mations are included in RandAugment. All panels report me- dian CIFAR-10 validation accuracy for Wide-ResNet-28-2 model architectures [53] trained with RandAugment ( N = 3, M = 4) using randomly sampled subsets of transformations. No other data augmentation is included in training. Error bars indicate 30 th and 70th percentile. (a) Median accuracy for randomly sampled subsets of transformations. (b) Median accuracy for subsets with and with- out the Rotate transformation. (c) Median accuracy for subsets with and without the translate-x transformation. (d) Median accuracy for subsets with and without the posterize transfor- mation. Dashed curves show the accuracy of the model trained without any augmentations. mented with RandAugment on a Wide-ResNet-28-2 trained on CIFAR-10 for randomly sampled subsets of the full list of 14 transformations. We did not use ﬂips, pad-and-crop, or cutout to only focus on the improvements due to Ran- dAugment with random subsets. Figure 4a suggests that the median validation accuracy due to RandAugment improves as the number of transformations is increased. However, even with only two transformations, RandAugment leads to more than 1% improvement in validation accuracy on aver- age. To get a sense for the effect of individual transforma- tions, we calculate the average improvement in validation accuracy for each transformation when they are added to a random subset of transformations. We list the transforma- tions in order of most helpful to least helpful in Table 5. We see that while geometric transformations individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average. Note that while Table 5 shows the average effect of adding in- dividual transformations to randomly sampled subsets of transformations, Figure 4a shows that including all trans- formations together leads to a good result. The transfor- mation rotate is most helpful on average, which was also observed previously [5, 57]. To see the effect of represen- tative transformations in more detail, we repeat the anal- ysis in Figure 4a for subsets with and without ( rotate, translate-x, and posterize). Surprisingly, rotate can signiﬁcantly improve performance and lower variation even when included in small subsets of RandAugment transfor- mations, while posterize seems to hurt all subsets of all sizes. 4.7. Learning the probabilities for selecting image transformations RandAugment selects all image transformations with equal probability. This opens up the question of whether learning Kprobabilities may improve performance further. Most of the image transformations (except posterize, equal- ize, and autoContrast ) are differentiable, which permits back- propagation to learn the Kprobabilities [30]. Let us denote αij as the learned probability of selecting image transfor- mation ifor operation j. For K=14 image transformations and N=2 operations, αij constitutes 28 parameters. We ini- tialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these param- eters based on how well a model classiﬁes a held out set oftransformation ∆ (%) transformation ∆ (%) rotate 1.3 shear-x 0.9 shear-y 0.9 translate-y 0.4 translate-x 0.4 autoContrast 0.1 sharpness 0.1 identity 0.1 contrast 0.0 color 0.0 brightness 0.0 equalize -0.0 solarize -0.1 posterize -0.3 Table 5. Average improvement due to each transformation. Average difference in validation accuracy (%) when a particular transformation is added to a randomly sampled set of transfor- mations. For this ablation study, Wide-ResNet-28-2 models were trained on CIFAR-10 using RandAugment (N = 3, M = 4) with the randomly sampled set of transformations, with no other data augmentation. baseline AA RA + 1 st Reduced CIFAR-10 Wide-ResNet-28-2 82.0 85.6 85.3 85.5 Wide-ResNet-28-10 83.5 87.7 86.8 87.4 CIFAR-10 Wide-ResNet-28-2 94.9 95.9 95.8 96.1 Wide-ResNet-28-10 96.1 97.4 97.3 97.4 Table 6. Differentiable optimization for augmentation can im- prove RandAugment. Test accuracy (%) from differentiable Ran- dAugment for reduced (4K examples) and full CIFAR-10. The 1st-order approximation (1 st) is based on density matching (Sec- tion 4.7). Models trained on reduced CIFAR-10 were trained for 500 epochs. CIFAR-10 models trained using the same hyperpa- rameters as previous. Each result is averaged over 10 independent runs. validation images distorted by αij. This approach was in- spired by density matching [25], but instead uses a differen- tiable approach in lieu of Bayesian optimization. We label this method as a 1st-order density matching approximation. To test the efﬁcacy of density matching to learn the prob- abilities of each transformation, we trained Wide-ResNet- 28-2 and Wide-ResNet-28-10 on CIFAR-10 and the reduced form of CIFAR-10 containing 4K training samples. Ta- ble 6 indicates that learning the probabilities αij slightly improves performance on reduced and full CIFAR-10 (RA vs 1st). The 1 st-order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of ﬂips and pad-and-crop. On CIFAR-10, the 1st-order method improves accuracy by 0.9% on the smaller model and 1.2% on the larger model compared to the base- line. We further see that the 1 st-order method always per- forms better than RandAugment, with the largest improve- ment on Wide-ResNet-28-10 trained on reduced CIFAR-10 (87.4% vs. 86.8%). On CIFAR-10, the 1 st-order method outperforms AutoAugment on Wide-ResNet-28-2 (96.1% vs. 95.9%) and matches AutoAugment on Wide-ResNet- 28-10 3. Although the density matching approach is promis- 3As a baseline comparison, in preliminary experiments we additionally ing, this method can be expensive as one must apply all K transformations N times to each image independently. Hence, because the computational demand ofKN transfor- mations is prohibitive for large images, we reserve this for future exploration. In summary, we take these results to in- dicate that learning the probabilities through density match- ing may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future. 5. Discussion Data augmentation is a necessary method for achieving state-of-the-art performance [43, 23, 7, 54, 13, 36]. Learned data augmentation strategies have helped automate the de- sign of such strategies and likewise achieved state-of-the- art results [5, 25, 20, 57]. In this work, we demonstrated that previous methods of learned augmentation suffers from systematic drawbacks. Namely, not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance. To remedy this situation, we propose a simple parameterization for targeting augmentation to particular model and dataset sizes. We demonstrate that RandAugment is competitive with or outperforms previous approaches [5, 25, 20, 57] on CIFAR-10/100, SVHN, ImageNet and COCO without a separate search for data augmentation policies. In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data [5, 25]; population based augmentation was not re- ported for large-scale problems [20]. The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyper- parameters), but notable predictive performance gains. An open question remains how this method may improve model robustness [32, 52, 41] or semi-supervised learning [50]. Future work will study how this method applies to other ma- chine learning domains, where data augmentation is known to improve predictive performance, such as image segmen- tation [3], 3-D perception [35], speech recognition [19] or audio recognition [18]. In particular, we wish to better un- derstand if or when datasets or tasks may require a separate search phase to achieve optimal performance. Finally, an open question remains how one may tailor the set of trans- formations to a given tasks in order to further improve the predictive performance of a given model. learn αij based on differentiating through a virtual training step [30]. In this approach, the 2 nd-order approximation yielded consistently negative results (see Appendix A.1).6. Acknowledgements We thank Samy Bengio, Daniel Ho, Ildoo Kim, Jaehoon Lee, Zhaoqi Leng, Hanxiao Liu, Raphael Gontijo Lopes, Ruoming Pang, Ben Poole, Mingxing Tan, and the rest of the Brain team for their help.References [1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. 2 [2] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. Searching for efﬁcient multi-scale archi- tectures for dense image prediction. In Advances in Neural Information Processing Systems, pages 8699–8710, 2018. 2 [3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017. 8 [4] Dan Ciregan, Ueli Meier, and J ¨urgen Schmidhuber. Multi- column deep neural networks for image classiﬁcation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3642–3649. IEEE, 2012. 2 [5] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 1, 2, 3, 4, 5, 6, 7, 8 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 1, 2 [7] Terrance DeVries and Graham W Taylor. Dataset augmen- tation in feature space. arXiv preprint arXiv:1702.05538 , 2017. 1, 2, 8 [8] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 6 [9] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance de- tection. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1301–1310, 2017. 2 [10] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy- pasting. arXiv preprint arXiv:1908.07801, 2019. 1 [11] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019. 2 [12] Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017. 4, 13 [13] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll´ar, and Kaiming He. Detectron, 2018. 1, 2, 8 [14] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1487–1495. ACM, 2017. 4 [15] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyrami- dal residual networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6307–6315. IEEE, 2017. 1 [16] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. 1 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. 1, 6 [18] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi- tectures for large-scale audio classiﬁcation. In 2017 ieee in- ternational conference on acoustics, speech and signal pro- cessing (icassp), pages 131–135. IEEE, 2017. 8 [19] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012. 8 [20] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efﬁcient learn- ing of augmentation policy schedules. arXiv preprint arXiv:1905.05393, 2019. 1, 2, 3, 4, 7, 8 [21] Naoyuki Kanda, Ryu Takeda, and Yasunari Obuchi. Elastic spectral distortion for low resource speech recognition with deep neural networks. In 2013 IEEE Workshop on Auto- matic Speech Recognition and Understanding , pages 309– 314. IEEE, 2013. 1 [22] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Uni- versity of Toronto, 2009. 1, 2 [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In Advances in Neural Information Processing Sys- tems, 2012. 1, 2, 8 [24] Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. 2 [25] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv preprint arXiv:1905.00397, 2019. 1, 2, 3, 4, 7, 8 [26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 6 [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740–755. Springer, 2014. 2, 6 [28] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Mur- phy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017. 2, 6 [29] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representa-tions for efﬁcient architecture search. In International Con- ference on Learning Representations, 2018. 2 [30] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 2, 7, 8, 12 [31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European con- ference on computer vision, pages 21–37. Springer, 2016. 2 [32] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving robustness without sacriﬁcing accuracy with patch gaussian augmentation. arXiv preprint arXiv:1906.02611, 2019. 1, 2, 6, 8 [33] Seongkyu Mun, Sangwook Park, David K Han, and Hanseok Ko. Generative adversarial network based acoustic scene training set augmentation and selection using svm hyper- plane. In Detection and Classiﬁcation of Acoustic Scenes and Events Workshop, 2017. 2 [34] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Work- shop on Deep Learning and Unsupervised Feature Learning, 2011. 1, 2, 6 [35] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al- sharif, Patrick Nguyen, et al. Starnet: Targeted compu- tation for object detection in point clouds. arXiv preprint arXiv:1908.11069, 2019. 8 [36] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaug- ment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. 1, 4, 8 [37] Luis Perez and Jason Wang. The effectiveness of data aug- mentation in image classiﬁcation using deep learning. arXiv preprint arXiv:1712.04621, 2017. 2 [38] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efﬁcient neural architecture search via parameter sharing. In International Conference on Machine Learning, 2018. 2 [39] Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ´e. Learning to compose domain-speciﬁc transformations for data augmentation. In Advances in Neural Information Processing Systems , pages 3239–3249, 2017. 2 [40] Suman Ravuri and Oriol Vinyals. Classiﬁcation accuracy score for conditional generative models. arXiv preprint arXiv:1905.10887, 2019. 2 [41] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to im- agenet? arXiv preprint arXiv:1902.10811, 2019. 1, 8 [42] Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classiﬁcation with neural networks. arXiv preprint arXiv:1505.03229, 2015. 2 [43] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of International Confer- ence on Document Analysis and Recognition, 2003. 1, 2, 8 [44] Leon Sixt, Benjamin Wild, and Tim Landgraf. Render- gan: Generating realistic labeled data. arXiv preprint arXiv:1611.01331, 2016. 2 [45] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Prac- tical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959, 2012. 4 [46] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2 [47] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 6, 7, 13 [48] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learn- ing deep models. In Advances in Neural Information Pro- cessing Systems, pages 2794–2803, 2017. 2 [49] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using drop- connect. In International Conference on Machine Learning, pages 1058–1066, 2013. 2 [50] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019. 1, 6, 8 [51] Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018. 4, 13 [52] Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. arXiv preprint arXiv:1906.08988, 2019. 1, 2, 8 [53] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In British Machine Vision Conference, 2016. 1, 2, 4, 5, 7 [54] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 1, 2, 8 [55] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017. 2 [56] Xinyue Zhu, Yifan Liu, Zengchang Qin, and Jiahong Li. Data augmentation in emotion classiﬁcation using genera- tive adversarial networks. arXiv preprint arXiv:1711.00648, 2017. 2 [57] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data aug- mentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019. 1, 4, 6, 7, 8, 13 [58] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017. 2, 4 [59] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of IEEE Conference on Com- puter Vision and Pattern Recognition, 2017. 2, 4, 6A. Appendix A.1. Second order term from bilevel optimization For the second order term for the optimization of aug- mentation parameters, we follow the formulation in [30], which we summarize below. We treat the optimization of augmentation parameters and weights of the neural network as a bilevel optimization problem, whereαare the augmen- tation parameters and w are the weights of the neural net- work. Then the goal is to ﬁnd the optimal augmentation parameters αsuch that when weights are optimized on the training set using data augmentation given byαparameters, the validation loss is minimized. In other words: minαLval(w∗(α),α) s.t.w∗(α) = argminw Ltrain(w,α). (1) Then, again following [30], we approximate this bilevel op- timization by a single virtual training step, ∇αLval(w∗(α),α) ≈ ∇αLval(w−ξ∇wLtrain(w,α),α), (2) where ξ is the virtual learning rate. Eq. 2 can be expanded as ∇αLval(w∗(α),α) ≈ ∇αLval(w−ξ∇wLtrain(w,α),α) − ξ∇2 α,wLtrain(w,α)∇w′ Lval(w′,α), (3) where w′ = w−ξ∇wLtrain(w,α). In the case where the virtual learning rate, ξ, is zero, the second term disap- pears and the ﬁrst term becomes ∇Lval(w,α), which was called the ﬁrst-order approximation [30]. This ﬁrst-order approximation was found to be highly signiﬁcant for archi- tecture search, where most of the improvement (0.3% out of 0.5%) could be achieved using this approximation in a more efﬁcient manner (1.5 days as opposed to 4 days). Unfortu- nately, when α represents augmentation parameters, ﬁrst- order approximation is irrelevant since the predictions of a model on the clean validation images do not depend on the augmentation parameters α. Then we are left with just the second order approximation, where ξ >0, which we ap- proximate via ﬁnite difference approximation as ∇2 α,wLtrain(w,α)∇w′ Lval(w′,α) ≈ ∇αLtrain(w+,α) −∇αLtrain(w−,α) 2ϵ , (4) where w±= w±ϵ∇w′ Lval(w′,α) and ϵis a small number. A.1.1 Magnitude methods A random magnitude uniformly randomly samples the dis- tortion magnitude between two values. A constant mag- nitude sets the distortion magnitude to a constant number Magnitude Method Accuracy Random Magnitude 97.3 Constant Magnitude 97.2 Linearly Increasing Magnitude 97.2 Random Magnitude with Increasing Upper Bound 97.3 Table 7. Results for different ways of setting the global magni- tude parameter M. All magnitude methods were run on CIFAR- 10 with Wide-ResNet-28-10 for 200 epochs. The reported accu- racy is the average of 10 runs on the validation set for the best hyperparamter setting for that magnitude method. All magnitude methods searched over had 48 different hyperparameter settings tried. Figure 5. Performance when magnitude is changed for one im- age transformation. This plot uses a shared magnitude for all image transformations and then changes the magnitude of only one operation while keeping the others ﬁxed. Two different archi- tectures were tried (WRN-28-2 and WRN-28-10) and two differ- ent image transformations were changed (Rotate and TranslateX), which results in the 4 lines shown. Twenty different magnitudes were tried for the selected transformation ([0 − 19]). The squares indicate the optimal magnitude found and the diamonds indicate the magnitude used for all other transformations (4 for WRN-28-2 and 5 for WRN-28-10). during the course of training. A linearly increasing mag- nitude interpolates the distortion magnitude during training between two values. A random magnitude with increasing upper bound is similar to a random magnitude, but the upper bound is increased linearly during training. In preliminary experiments, we found that all strategies worked equally well. Thus, we selected a constant magnitude because this strategy includes only a single hyper-parameter, and we em- ploy this for the rest of the work. The results from our ex- periment on trying the different magnitude strategies can be see in Table 7.A.1.2 Optimizing individual transformation magni- tudes Figure 5 demonstrates that changing the magnitude for one transformation, when keeping the rest ﬁxed results in a very minor accuracy change. This suggests that tying all magni- tudes together into a single value M is not greatly hurting the model performance. Across all for settings in Figure 5 the difference in accuracy of the tied magnitude vs the opti- mal one found was 0.19% 0.18% for the rotation operation experiments and 0.07% 0.05% for the TranslateX experi- ments. Changing one transformation does not have a huge impact on performance, which leads us to think that tying all magnitude parameters together is a sensible approach that drastically reduces the size of the search-space. A.2. Experimental Details A.2.1 CIFAR The Wide-ResNet models were trained for 200 epochs with a learning rate of 0.1, batch size of 128, weight decay of 5e- 4, and cosine learning rate decay. Shake-Shake [12] model was trained for 1800 epochs with a learning rate of 0.01, batch size of 128, weight decay of 1e-3, and cosine learning rate decay. ShakeDrop [51] models were trained for 1800 epochs with a learning rate of 0.05, batch size of 64 (as 128 did not ﬁt on a single GPU), weight decay of 5e-5, and cosine learning rate decay. On CIFAR-10, we used 3 for the number of operations applied (N) and tried 4, 5, 7, 9, and 11 for magnitude. For Wide-ResNet-2 and Wide-ResNet-10, we ﬁnd that the op- timal magnitude is 4 and 5, respectively. For Shake-Shake (26 2x96d) and PyramidNet + ShakeDrop models, the opti- mal magnitude was 9 and 7, respectively. A.2.2 SVHN For both SVHN datasets, we applied cutout after RandAug- ment as was done for AutoAugment and related methods. On core SVHN, for both Wide-ResNet-28-2 and Wide- ResNet-28-10, we used a learning rate of 5e-3, weight de- cay of 5e-3, and cosine learning rate decay for 200 epochs. We set N = 3and tried 5, 7, 9, and 11 for magnitude. For both Wide-ResNet-28-2 and Wide-ResNet-28-10, we ﬁnd the optimal magnitude to be 9. On full SVHN, for both Wide-ResNet-28-2 and Wide- ResNet-28-10, we used a learning rate of 5e-3, weight de- cay of 1e-3, and cosine learning rate decay for 160 epochs. We set N = 3and tried 5, 7, 9, and 11 for magnitude. For Wide-ResNet-28-2, we ﬁnd the optimal magnitude to be 5; whereas for Wide-ResNet-28-10, we ﬁnd the optimal mag- nitude to be 7. A.2.3 ImageNet The ResNet models were trained for 180 epochs using the standard ResNet-50 training hyperparameters. The image size was 224 by 244, the weight decay was 0.0001 and the momentum optimizer with a momentum parameter of 0.9 was used. The learning rate was 0.1, which gets scaled by the batch size divided by 256. A global batch size of 4096 was used, split across 32 workers. For ResNet-50 the opti- mal distortion magnitude was 9 and ( N = 2). The distor- tion magnitudes we tried were 5, 7, 9, 11, 13, 15 and the values of N that were tried were 1, 2 and 3. The EfﬁcientNet experiments used the default hyper pa- rameters and training schedule, which can be found in [47]. We trained for 350 epochs, used a batch size of 4096 split across 256 replicas. The learning rate was 0.016, which gets scaled by the batch size divided by 256. We used the RM- SProp optimizer with a momentum rate of 0.9, epsilon of 0.001 and a decay of 0.9. The weight decay used was 1e-5. For EfﬁcientNet B5 the image size was 456 by 456 and for EfﬁcientNet B7 it was 600 by 600. For EfﬁcientNet B5 we tried N = 2and N = 3and found them to perform about the same. We found the optimal distortion magnitude for B5 to be 17. The different magnitudes we tried were 8, 11, 14, 17, 21. For EfﬁcientNet B7 we used N = 2and found the optimal distortion magnitude to be 28. The magnitudes tried were 17, 25, 28, 31. The default augmentation of horizontal ﬂipping and ran- dom crops were used on ImageNet, applied before Ran- dAugment. The standard training and validation splits were employed for training and evaluation. A.3. COCO We applied horizontal ﬂipping and scale jitters in addi- tion to RandAugment. We used the same list of data aug- mentation transformations as we did in all other classiﬁca- tion tasks. Geometric operations transformed the bounding boxes the way it was deﬁned in Ref. [57]. We used a learn- ing rate of 0.08 and a weight decay of 1e 4. The focal loss parameters are set to be α = 0.25 and γ = 1.5. We set N = 1 and tried distortion magnitudes between 4 and 9. We found the optimal distortion magnitude for ResNet-101 and ResNet-200 to be 5 and 6, respectively.",
      "references": [],
      "meta_data": {
        "arxiv_id": "1909.13719v2",
        "authors": [
          "Ekin D. Cubuk",
          "Barret Zoph",
          "Jonathon Shlens",
          "Quoc V. Le"
        ],
        "published_date": "2019-09-30T14:05:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "RandAugment introduces a minimal, proxy-task-free data augmentation policy by drastically reducing the search space to two interpretable hyperparameters (N, the number of augmentations applied per image, and M, a global magnitude). It shows that optimal augmentation strength depends on model size and dataset size, removing the need for training a separate proxy task. With a simple grid search over N and M, RandAugment matches or surpasses prior learned augmentation methods across CIFAR-10/100, SVHN, ImageNet, and COCO, while incurring negligible inference-time cost.",
        "methodology": "RandAugment randomly selects N transformations from a fixed 14-op Transform set and applies them sequentially, each with a global magnitude M on a 0–10 scale. Unlike AutoAugment-based methods, there are no per-operation probabilities learned, and the policy is parameterized by only N and M. A fixed magnitude schedule is used (constant M found to work well), with an option to vary M over time. A two-parameter grid search suffices to find strong policies. A further experiment explored learning transformation probabilities α_ij via differentiable optimization (density-matching style), but the core RandAugment uses N, M. The approach is validated against prior augmentation methods and different architectures.",
        "experimental_setup": "Evaluations on CIFAR-10/100, SVHN, ImageNet, and COCO using common backbones (Wide-ResNet, PyramidNet, Shake-Shake, ResNet variants, EfficientNet) with standard training regimes. Baselines include baseline augmentation, AutoAugment, Fast AutoAugment, and PBA. RandAugment uses a fixed 14-transform catalog; the two hyperparameters N and M are tuned via grid search. Key results: CIFAR-10: near state-of-the-art across multiple architectures; CIFAR-100 and SVHN: competitive to or surpassing prior learned augmentation methods; ImageNet: Top-1 85.0% with EfficientNet-B7 (1.0% above baseline and 0.6% above prior state-of-the-art); COCO: mAP 40.1–40.4 for ResNet-101/200 (competitive with AutoAugment with far less search cost); training details andAppendix provide exact schedules and hyperparameters.",
        "limitations": "Relies on a fixed set of 14 transformations; optimal N and M may not generalize to all domains; some transforms can hurt accuracy (e.g., certain color distortions); RA does not include bounding-box-specific augmentations for object detection (though compatible with standard transforms and some improvements); still requires hyperparameter tuning (N, M); reduces search cost but not zero cost; performance gaps exist on COCO compared to AutoAugment in mAP; generalization to segmentation/3D/audio remains to be tested.",
        "future_research_directions": "Extend RandAugment with domain-specific transformations (e.g., bounding-box-aware ops), develop per-task learned transformation catalogs, explore adaptive N/M schedules and per-batch variation, further study learning probabilities α_ij for large-scale tasks (with computational considerations), apply to vision tasks like segmentation, 3D perception, and audio, analyze impact on model robustness and semi-supervised learning, and investigate combining RandAugment with other augmentation strategies (Mixup, CutMix) for complementary gains.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Data-Efficient Learning with Neural Programs",
      "full_text": "Data-Efficient Learning with Neural Programs Alaia Solko-Breslin, Seewon Choi, Ziyang Li, Neelay Velingker, Rajeev Alur, Mayur Naik, Eric Wong University of Pennsylvania {alaia,seewon,liby99,neelay,alur,mhnaik,exwong}@seas.upenn.edu Abstract Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites “neural programs” and focus on the prob- lem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differen- tiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neu- ral programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosym- bolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner. 1 1 Introduction Many computational tasks cannot be solved by neural perception alone but can be naturally expressed as a composition of a neural modelMθ followed by a program P written in a traditional programming language or an API call to a large language model (LLM). We call such composites “neural programs” and study the problem of learning neural programs in an end-to-end manner with a focus on data and sample efficiency. One problem that is naturally expressed as a neural program is scene recognition [29], where Mθ classifies objects in an image and P prompts GPT-4 to identify the room type given these objects (Fig. 1). Neurosymbolic learning [2] is one instance of neural program learning in which P takes the form of a logic program. DeepProbLog (DPL) [14] and Scallop [13] are frameworks that extend ProbLog and Datalog, respectively, to ensure that the symbolic componentP is differentiable. This differentiability requirement is what facilitates learning in many neurosymbolic learning frameworks. There are also abductive learning frameworks that do not explicitly differentiate programs. Instead, they require that the symbolic component expose a method for abducing the function’s inputs for a given output, often using Prolog for the symbolic component as a result [6, 23]. While logic programming languages are expressive enough for these frameworks to solve tasks such as sorting [14], visual question answering [13], and path planning [23], they offer restricted features and a narrow range of libraries, making them incompatible with calls to arbitrary APIs or to modern LLMs. Learning neural programs when P is not expressed as a logic program is a difficult problem because gradients across black-box programs cannot be computed explicitly. One possible solution is to use REINFORCE [26] to sample symbols from distributions predicted by Mθ and compute the expected 1Code is available at https://github.com/alaiasolkobreslin/ISED 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.06246v2  [cs.LG]  31 Oct 2024Figure 1: Neural program decomposition for scene recognition. reward using the output label. However, REINFORCE is not sample-efficient as it produces a weak learning signal, especially when applied to programs with a large number of inputs. There are other REINFORCE-based methods that can be applied to the neural program learning setting, namely IndeCateR [21] and Neural Attention for Symbolic Reasoning (NASR) [ 5]. However, IndeCateR struggles with sample efficiency despite providing lower variance than REINFORCE, and NASR performs poorly when intermediate labels are unavailable for pretraining. Another possible solution is Approximate Neurosymbolic Inference (A-NeSI) [24], which trains a neural network to estimate the gradient of P, but learning the surrogate neural network becomes more difficult as the complexity of P increases. Moreover, the additional neural models in the learning framework in A-NeSI results in data inefficiency. In this paper, we propose an algorithm for learning neural programs, based on reinforcement learning, which is compatible with arbitrary programs. Our approach, called ISED (Infer-Sample-Estimate- Descend), yields a framework that expands the applicability of neural program learning frameworks by providing a data- and sample-efficient method of training neural models with randomly initialized weights. ISED uses outputs of Mθ as a probability distribution over inputs of P and samples representative symbols u from this distribution. ISED then computes outputs v of P corresponding to these symbols. The resulting symbol-output pairs can be viewed as a symbolic program consisting of clauses of the form if symbol = u then output = v summarizing P. The final step is to estimate the gradient across this symbolic summary, inspired by ideas from the neurosymbolic learning literature, to propagate loss across the composite model. Our evaluation considers 16 neural program benchmark tasks. Our results show that ISED outperforms purely neural networks and CLIP [19] on neural program tasks involving GPT-4 calls. Additionally, ISED outperforms neurosymbolic methods on 9 of the 14 benchmarks tasks that can be encoded in logic programming languages. ISED is also the top performer on 8 out of the 16 benchmark tasks when compared to REINFORCE-based and black-box gradient estimation baselines. Furthermore, we show that ISED is more data- and sample-efficient than baseline methods. In summary, the main contributions of this paper are as follows: 1) we introduce neural programs as a generalization of neurosymbolic programs, 2) we introduce new tasks involving neural programs that use Python and calls to GPT-4 called neuroPython and neuroGPT programs, respectively, 3) we present ISED, a general algorithm for data- and sample-efficient learning with neural programs, and 4) we conduct a thorough evaluation using existing techniques against a diverse set of benchmarks. 2 Neural Programs Problem Statement. In the neural program learning setting, we attempt to optimize model parameters Mθ which are being supervised by a fixed program P. Specifically, we are given a training dataset D of length N containing input-output pairs, i.e., D = {(x1, y1), . . .(xN , yN )}. Each xi represents unstructured data (e.g., image data) whose corresponding structured data (intermediate labels) are not given. Each yi is the result of applying P to the structured data corresponding to xi. Given a loss function L, we want to minimize the loss of L(P(Mθ(xi)), yi) for each (xi, yi) pair in order to optimize θ. Loss minimization is straightforward when there is some mechanism for automatically differentiating programs, but we focus on the setting of optimizing θ without assuming the differentiability of P. We now introduce three motivating applications that can be framed in this 2Figure 2: Illustration of our inference pipeline for the leaf classification task. leaf_id can be written with a decision tree (top program) or with a call to GPT-4 (bottom program). setting, namely classifying images of leaves [9], scene recognition [29], and hand-written formula evaluation (HWF) [12]. Leaf Classification. We consider a real-world example that deals with the problem of classifying leaf images. Traditional neural methods predict the species directly, without explicit notion of leaf features such as margin, shape, and texture, resulting in solutions that are data-inefficient, inaccurate, and harder to understand. We instead present a neural programming solution, making use of leaf classification decision trees [22]. These decision trees allow identifying plant species based on the visible characteristics of their leaves. Here, the neural model takes a leaf image and predicts its shape, margin, and texture. The program can then be written in two ways: one implementation involves encoding the decision tree in Python; another involves constructing a prompt using the predicted leaf features and calling GPT-4 (see Fig. 2). The latter is possible because ISED allows the use of black-box programs, so programs can also use state-of-the-art foundation models such as GPT-4 for computation. Scene Recognition. The goal of this task is to classify images according to their room types. The model receives an image from a scene dataset [16] and predicts among the 9 different room types: bedroom, bathroom, dining room, living room, kitchen, lab, office, house lobby, and basement. The traditional neural solution trains a convolutional neural network that directly predicts the room type. On the other hand, the neural program solution decomposes the task into detecting objects in the scene and identifying the room type based on those objects. We use an off-the-shelf object detection model YOLOv8 [20] and finetune it with a custom convolutional neural network to output labels related to scene recognition. We then make a GPT-4 call to predict the most likely room type given the list of detected objects. Hand-written Formula. In this task, a model is given a list of hand-written symbols containing digits (0-9) and operators (+, −, ×, and ÷) [12]. The dataset contains length 1-7 formulas free of syntax or divide-by-zero errors. The model is trained with supervision on the evaluated floating-point result without the label of each symbol. Since inputs are combinatorial and results are rational numbers, end-to-end neural methods struggle with accuracy. Meanwhile, neurosymbolic methods for this task either use specialized algorithms [12] or handcrafted differentiable programs [13]. With ISED, the program can be written in just a few lines of Python. It takes in a list of characters representing symbols, and simply invokes the Python eval function on the joined expression string. The hwf evaluation function can be used just like any other PyTorch [17] module since ISED internally performs sampling and probability estimation to estimate the gradient. 3 Learning Neural Programs In this section, we present the intuition behind ISED and the values it approximates. Next, we introduce the programming interface for ISED, which lays the groundwork for presenting the algorithm. We then formally describe the steps of ISED. 33.1 ISED Overview Assuming P is a black-box, we can collect symbol-output samples (u, v) from P. Such collection of samples can be viewed as a summary logic program consisting of rules of the form if r = u then y = v. For instance, in the task of adding two digits r1 and r2, one rule of the logic program would be r1 = 1 ∧ r2 = 2 → y = 3. Techniques from neurosymbolic literature via exact or approximate weighted model counting (WMC) [ 10] can then be used for computing the gradient across such a summary of P. However, having the complete summary of all combinations of symbols is not feasible for a black-box P. ISED samples symbols from the probability distribution predicted by the neural network Mθ, evaluates P on each sample, and takes the gradient across this partial summary of P. This is a good approximation of the complete summary since it is likely to contain symbols with high probability, which contribute the most in exact computation. This approach differs from REINFORCE in how it differentiates through this summary of P. RE- INFORCE rewards sampled symbols that resulted in the correct output through optimizing the log probability of each symbol, weighted by reward values. This weighted-sum style estimation provides a weaker learning signal compared to WMC used by ISED, making learning harder for REINFORCE as the number of inputs to P increases. See Appendix A for further details. 3.2 Preliminaries and Programming Interface ISED allows programmers to write black-box programs that operate on diverse structured inputs and outputs. To allow such programs to interact with neural networks, we define an interface named structural mapping. This interface serves to 1) define the data-types of black-box programs’ input and output, 2) marshall and un-marshall data between neural networks and logical black-box functions, and 3) define the loss. We define a structural mapping τ as either a discrete mapping (with Σ being the set of all possible elements), a floating point, a permutation mapping with n possible elements, a tuple of mappings, or a list of up to n elements. We define τ inductively as follows: τ ::= DISCRETE (Σ) | FLOAT | PERMUTATION n | TUPLE (τ1, . . . , τm) | LIST n(τ) Using this, we may further define data-types such as INTEGER k j = DISCRETE ({j, . . . , k}), DIGIT = INTEGER 9 0, and BOOL = DISCRETE ({true, false}). These types give ISED the flexibility learn neural programs with diverse types of inputs and outputs, e.g., PERMUTATION n input and output types for integer list sorting and LIST 9(LIST 9(DIGIT )) for sudoku solving. We also define a black-box program P as a function (τ1, . . . , τm) → τo, where τ1, . . . , τm are the input types and τo is the output type. For example, the structural input mapping for the hand-written formula task is LIST 7(DISCRETE ({0, . . . ,9, +, −, ×, ÷})), and the structural output mapping is FLOAT. The mappings suggest that the program takes a list of length up to 7 as input, where each element is a digit or an arithmetic operator, and returns a floating point number. There are two interpretations of a structural mapping: the set interpretation SET( τ) represents a mapping with defined values, e.g., a digit with value 8; the tensor interpretation DIST(τ) represents a mapping where each value is associated with a probability distribution, e.g., a digit that is 1 with probability 0.6 and 7 with probability 0.4. We use the set interpretation to represent structured program inputs that can be passed to a black-box program and the tensor interpretation to represent probability distributions for unstructured data and program outputs. These two interpretations are defined for the different structural mappings in Table 1. Table 1: Set and tensor interpretations of different structural mappings. Mapping (τ) Set Interpretation (SET(τ)) Tensor Interpretation (DIST(τ)) DISCRETE Σ Σ {⃗ v| ⃗ v∈ R|Σ|, vi ∈ [0, 1], i∈ 1 . . .|Σ|} FLOAT R n/a PERMUTATION n {ρ | ρ is a permutation of [1, ..., n]} { [ ⃗ v1, ..., ⃗ vn] | ⃗ vi ∈ Rn, vi,j ∈ [0, 1], i∈ 1 . . . n} TUPLE (τ1, . . . , τm) {(a1, ..., am) | ai ∈ SET(τi)} { (a1, .., am) | ai ∈ DIST(τi)} LIST n(τ′) {[a1, ..., aj] | j ≤ n, ai ∈ SET(τ′)} { [a1, .., aj] | j ≤ n, ai ∈ DIST(τ′)} In order to represent the ground truth output as a distribution to be used in the loss computation, there needs to be a mechanism for transforming SET( τ) mappings into DIST( τ) mappings. For 4this purpose, we define a vectorize function δτ : (SET(τ), 2τ ) → DIST(τ) for the different output mappings τ in Table 2. When considering a datapoint (x, y) during training, ISED samples many symbols and obtains a list of outputs ˆy. The vectorizer then takes the ground truth y and the outputs ˆy as input and returns the equivalent distribution interpretation of y. While ˆy is not used by δτ in most cases, we include it as an argument so that FLOAT output mappings can be discretized, which is necessary for vectorization. For example, if the inputs to the vectorizer for the hand-written formula task are y = 2.0 and ˆy = [1.0, 3.5, 2.0, 8.0], then it would return [0, 0, 1, 0]. Table 2: Vectorize and aggregate functions of different structural mappings. Mapping (τ) Vectorizer (δτ (y, ˆy)) Aggregator (στ (ˆr, ˆp)) DISCRETE n e(y) with dim n ˆp[ˆr] FLOAT [1y=ˆyi for i ∈ [1, . . . ,length(ˆy)]] n/a PERMUTATION n [δDISCRETE n(y[i]) for i ∈ [1, . . . , n]] ⊗n i=1σDISCRETE n(ˆr[i], ˆp[i]) TUPLE (τ1, . . . , τm) [ δτi(y[i]) for i ∈ [1, . . . , m]] ⊗m i=1στi(ˆr[i], ˆp[i]) LIST n(τ′) [ δτ′(ai) for ai ∈ y] ⊗n i=1στ′(ˆr[i], ˆp[i]) We also require a mechanism to aggregate the probabilities of sampled symbols that resulted in a particular output. With this aim, we define an aggregate function στ : (SET(τ), DIST(τ)) → R for different input mappings τ in Table 2. ISED aggregates probabilities either by taking their minimum or their product, and we denote both operations by ⊗. The aggregator takes as input sampled symbols ˆr and neural predictions ˆp from which ˆr was sampled. It gathers values in ˆp at each index in ˆr and returns the result of ⊗ applied to these values. For example, suppose we use min as the aggregator ⊗ for the hand-written formula task. Then if ⊗ takes ˆr = [1, +, 1] and ˆp as inputs where ˆp[0][1] = 0.1, ˆp[1][+] = 0.05, and ˆp[2][1] = 0.1, it would return 0.05. 3.3 Algorithm We now formally present the ISED algorithm. For a given task, there is a black-box program P, taking m inputs, that operates on structured data. Let τ1, ..., τm be the mappings for these inputs and τo the mapping for the program’s output. We write P as a function from its input mappings to its output mapping: P : (τ1, ..., τm) → τo. For each unstructured input i to the program, there is a neural model Mi θi : xi → DIST(τi). S is a sampling strategy (e.g., categorical sampling) that samples symbols using the outputs of a neural model, and k is the number of samples to take for each training example. There is also a loss function L whose first and second arguments are the predicted and ground truth values respectively. We present the pseudocode of the algorithm in Algorithm 1 and describe its steps with the hand-written formula task: Infer. The training pipeline starts with an example from the dataset, (x, y) = ([  ,  ,  ], 3.0), and uses a CNN to predict these images, as shown on lines 3-4. ISED initializes ˆp = Mθ(x). Sample. ISED samples ˆr from ˆp for k iterations using sampling strategy S. For each sample j, the algorithm initializes ˆrj to be the sampled symbols, as shown on lines 6-9. To continue our example, suppose ISED initializes ˆrj = [7, +, 2] for sample j. The next step is to execute the program on ˆrj, as shown on line 10, which in this example means setting ˆyj = P(ˆrj) = 9.0. Estimate. In order to compute the prediction value to use in the loss function, ISED must consider each output yl in the output mapping and accumulate the aggregated probabilities for all sampled symbols that resulted in the output yl. We specify ⊗ as the min function, and ⊕ as the max function in this example. Note that ISED requires that ⊗ and ⊕ represent either min and max or mult and add respectively. We refer to these two options as the min-max and add-mult semirings. We define an accumulate function ω that takes as input an element of the output mapping yl, sampled outputs ˆy, sampled symbols ˆr, and predicted input distributions ˆp. The accumulator performs the ⊕ operation on aggregated probabilities for elements of ˆy that are equal to yl and is defined as follows: ω(yl, ˆy, ˆr, ˆp) = ⊕k j=11ˆyj=ylστo(ˆrj, ˆpj) Continuing our example, suppose, among the samples, there are two symbolic combinations ([7, +, 2] and [3, ∗, 3]) that resulted in the output 9.0. Let us say that these sets of symbols had probabilities [0.3, 0.8, 0.8] and [0.1, 0.1, 0.1], respectively. Then the result of the probability aggregation for yl = 9.0 would be ω(9.0, ˆy, ˆr, ˆp) = max(min([0.3, 0.8, 0.8]), min([0.1, 0.1, 0.1])) = 0.3. 5Algorithm 1 ISED training pipeline Require: P is the black-box program (τ1, . . . , τm) → τo, Mi θi the neural model xi → DIST(τi) for each τi, S the sampling strategy, k the sample count, L the loss function, and D the dataset. 1: procedure TRAIN 2: for ((x1, . . . xm), y) ∈ Ddo 3: for i ∈ 1 . . . mdo 4: ˆp[i] ← Mi θi(xi) ▷ Infer 5: end for 6: for j ∈ 1 . . . kdo 7: for i ∈ 1 . . . mdo 8: Sample ˆrj[i] from ˆp[i] using S ▷ Sample 9: end for 10: ˆyj ← P(ˆrj) 11: end for 12: ˆw ← normalize([ω(yl, ˆy, ˆr, ˆp) for yl ∈ τo (or yl ∈ ˆy)]) ▷ Estimate 13: w ← δ(y, ˆy) 14: l ← L( ˆw, w) 15: Compute ∂l ∂θ by performing back-propagation on l 16: Optimize θ based on ∂l ∂θ ▷ Descend 17: end for 18: end procedure ISED then sets ˜w = [ω(yl, ˆy, ˆr, ˆp) for yl ∈ τo] in the case where τo is not FLOAT. When τo is FLOAT, as for hand-written formula, it only considers yl ∈ ˆy. Next, it performs L2 normalization over each element in ˜w and sets ˆw to this result. To initialize the ground truth vector, it sets w = δ(y, ˆy). ISED then initializes l = L( ˆw, w) and computes ∂l ∂θi for each input i. These steps are shown on lines 12-15. In our running example, since 9.0 is an incorrect output, the probability of the first symbol being equal to 7 (instead of the correct answer 1) will be penalized while the probabilities for predicting other symbols are unchanged. Descend. The last step is shown on line 16, where the algorithm optimizes θi for each input i based on ∂l ∂θi using a stochastic optimizer (e.g., Adam optimizer). This completes the training pipeline for one example, and the algorithm returns all final θi after iterating through the entire dataset. 4 Evaluation In this section, we evaluate ISED and aim to answer the following research questions: RQ1: How does ISED compare to state-of-the-art neurosymbolic, REINFORCE-based, and gradient estimation baselines in terms of accuracy? RQ2: What is the sample efficiency of ISED when compared to REINFORCE-based algorithms? RQ3: How data-efficient is ISED compared to neural gradient estimation methods? 4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic We first introduce two new neural program learning benchmarks which both contain a program component that can make a call to GPT-4. We call such models neuroGPT programs. Leaf Classification. In this task, we use a dataset, which we call LEAF-ID, containing leaf images of 11 different plant species [ 4], containing 330 training samples and 110 testing samples. We define custom DISCRETE types MARGIN , SHAPE , TEXTURE . With this, we define LEAF -TRAITS = TUPLE (MARGIN , SHAPE , TEXTURE ) and LEAF -OUTPUT to be the DISCRETE set of 11 plant species in the dataset. Neural program solutions either prompt GPT-4 (GPT leaf) or use a decision tree (DT leaf). Scene Recognition. We use a dataset containing scene images from 9 different room types [ 16], consisting of 830 training examples and 92 testing examples. We define custom types OBJECTS and SCENES to be DISCRETE set of 45 objects and 9 room types, respectively. We freeze the parameters 6Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.2 Accuracy (%) Method sum2 sum3 sum4 HWF DT leaf GPT leaf scene sudoku DPL 95.14 93 .80 TO TO 39.70 N/A N/A TO Scallop 91.18 91 .86 80 .10 96 .65 81 .13 N/A N/A TO A-NeSI 96.66 94.39 78 .10 3 .13 78 .82 72 .40 61 .46 26 .36 REINFORCE 74.46 19 .40 13 .84 88 .27 40 .24 53 .84 12 .17 79 .08 IndeCateR 96.48 93 .76 92 .58 95 .08 78 .71 69 .16 12 .72 66 .50 NASR 6.08 5 .48 4 .86 1 .85 16 .41 17 .32 2 .02 82.78 ISED (ours) 80.34 95.10 94 .10 97 .34 82 .32 79 .95 68 .59 80.32 of YOLOv8 and only optimize the custom neural network. The neural program solution prompts GPT-4 to classify the scene. We also consider several tasks from the neurosymbolic literature, including hand-written formula (HWF) evaluation and Sudoku solving. While the solutions to many of these tasks are usually presented as a logic program in neurosymbolic learning frameworks, neural program solutions can take the form of Python programs. We call such models neuroPython programs. MNIST-R. MNIST-R [13, 14] contains 11 tasks operating on inputs of images of handwritten digits from the MNIST dataset [11]. This synthetic test suite includes tasks performing arithmetic (sum2, sum3, sum4, mult2, mod2, add-mod-3, add-sub), comparison (less-than, equal), counting (count-3-or- 4), and negation (not-3-or-4) over the digits depicted in the images. Each task dataset has a training set of 5K samples and a testing set of 500 samples. HWF. The goal of the HWF task is to classify images of handwritten digits and arithmetic operators and evaluate the formula [12]. The dataset contains 10K formulas of length 1-7, with 1K length 1 formulas, 1K length 3 formulas, 2K length 5 formulas, and 6K length 7 formulas. Visual Sudoku. The goal of this task is to solve an incomplete 9x9 Sudoku, where the problem board is given as MNIST digits. We follow the experimental setting of NASR [5], including their pre-trained MNIST digit recognition models and sudoku solvers. We use the SatNet dataset consisting of 9K training samples and 500 test samples [25]. 4.2 Evaluation Setup and Baselines All of our experiments were conducted on a machine with two 20-core Intel Xeon CPUs, one NVIDIA RTX 2080 Ti GPU, and 755 GB RAM. Unless otherwise noted, the sample count, i.e., the number of calls to the program P per training example, is fixed at 100 for all relevant methods. For additional details on experimental setup, see Appendix B. We apply a timeout of 10 seconds per testing sample, and report the average accuracy and 1-sigma standard deviation obtained from 10 randomized runs. We pick as baselines neurosymbolic methods DeepProbLog (DPL) [14] and Scallop [13], A-NeSI [24] which performs neural approximation of the gradients, and sampling-based gradient approximation methods REINFORCE [26], IndeCateR [21], and NASR [5]. IndeCateR achieves provably lower variance than REINFORCE by using a specialized sampling method (Appendix A), and NASR is a variant specialized for efficient finetuning by using a single sample and a custom reward function. We also use purely neural baselines and CLIP [19] for GPT leaf and scene. CLIP is a multimodal model that supports zero-shot image classification by simply providing names of the output categories. 4.3 RQ1: Performance and Accuracy 2In an earlier version of this paper, we reported lower numbers for REINFORCE and IndeCateR forRQ1 but have since updated these numbers to reflect their performance with the MNIST network from the IndeCateR implementation. Unlike ISED’s network, IndeCateR’s network lacks a softmax layer. 7Table 4: Performance comparisons for sum8, sum12, and sum16 with different sample counts k. Accuracy (%) sum8 sum12 sum16 Method k = 80 k = 800 k = 120 k = 1200 k = 160 k = 1600 REINFORCE 8.32 8 .28 7 .52 8 .20 5 .12 6 .28 IndeCateR 5.36 89.60 4.60 77 .88 1 .24 5 .16 IndeCateR+ 10.20 88 .60 6 .84 86.92 4.24 83.52 ISED (Ours) 87.28 87.72 85.72 86.72 6.48 8.13 To answer RQ1, we evaluate ISED’s accuracy against those of the baselines. ISED matches, and in many cases surpasses, the accuracy of neurosymbolic and gradient estimation baselines. We highlight the results for sumn from MNIST-R and other benchmarks in Table 3. Tables 7-10 in Appendix C contain results for the remaining MNIST-R tasks, including standard deviations for all tasks. ISED is the top performer on 8 out of the 16 total tasks. On the GPT leaf and scene tasks, ISED outperforms the purely neural baseline by 3.82% and 31.42% respectively, and zero-shot CLIP by 59.80% and 17.50%. For many tasks, A-NeSI is the non- neurosymbolic method that comes closest to ISED, sometimes outperforming our method. However, A-NeSI achieves significantly lower performance than ISED on tasks involving complex programs, namely HWF and sudoku. This is likely due to the difficulty of training a neural model to estimate the output of P and its gradient when P is complex. ISED also outperforms REINFORCE on all but 3 tasks due to the REINFORCE learning signal being weaker for tasks where P involves multiple inputs. NASR outperforms ISED only on sudoku by 2.46% due to NASR being well-suited for fine-tuning as it restricts its algorithm to use a single sample. IndeCateR achieves similar performance compared to ISED on most tasks but achieves significantly lower accuracy on the scene classification task, which has a large input space with maximum 10 objects each with 47 possible values in each scene, demonstrating that IndeCateR is less sample-efficient than ISED. We elaborate more on this point in RQ2. ISED outperforms the neurosymbolic methods on 9 out of 14 tasks that can be written in logic programming languages. Despite treating P as a black-box, ISED even outperforms Scallop on HWF by 0.69% and comes within 1.16% of NGS, a specialized neurosymbolic learning framework that uses abductive reasoning [12]. Furthermore, DPL timed out on 4 tasks, and Scallop timed out on 1 (sudoku). These results demonstrate that even for tasks that can be written in a logic programming language, treating the program as a black-box can often yield optimal results. 4.4 RQ2: Sample Efficiency To answer RQ2, we evaluate the sample efficiency of ISED against REINFORCE, IndeCateR, and IndeCateR+ on adding MNIST digits. IndeCateR+ [21] is a variant of IndeCateR with a sampling method and loss computation customized for higher dimensional setting such as the addition of 16 MNIST digits. We vary the size of the input and output space (sum8, sum12, sum16) of P as well as the sample count, and report the average accuracy and standard deviation obtained from 5 randomized runs (Tables 4, 11-13). For a lower number of samples, ISED outperforms all other methods on the three tasks, outperforming IndeCateR by over 80% on sum8 and sum12. The experimental findings support the conceptual difference of REINFORCE-based methods providing a weak learning signal compared to ISED (Section 3.1). While ISED achieves accuracy similar to the top performer for sum8 and sum12 with a high sample count, it comes second on sum 16 with IndeCateR+ beating ISED by 75.39%. This suggests our approach is limited in scaling to high-dimensional inputs to P, and motivates exploring better sampling techniques, which is the core difference between IndeCateR and IndeCateR+. 4.5 RQ3: Data Efficiency We now examine how ISED compares to state-of-the-art baselines in terms of data efficiency. We compare ISED and A-NeSI in terms of training time and accuracy on sum3 and sum4. We choose these tasks for evaluation because A-NeSI has been shown to scale well to multi-digit addition tasks 80 200 400 600 0.2 0.4 0.6 0.8 1 Time (seconds) Accuracy ISED A-NeSI Figure 3: Accuracy vs. Time for sum3. 0 200 400 600 0.2 0.4 0.6 0.8 1 Time (seconds) Accuracy ISED A-NeSI Figure 4: Accuracy vs. Time for sum4. [24]. Furthermore, these tasks come from the MNIST-R suite in which we use 5K training samples, which is less than what A-NeSI would have used in its evaluation (20K training samples for sum3 and 15K for sum4). We plot the average test accuracy and standard deviation vs. training time (over 10 runs) in Figures 3 and 4, where each point represents the result of 1 epoch. While ISED and A-NeSI learn at about the same rate for sum 3 after about 5 minutes of training, ISED learns at a much faster rate for the first 5 minutes, reaching an accuracy of 88.22% after just 2 epochs (Fig. 3). The difference between ISED and A-NeSI is more pronounced for sum 4, with ISED reaching an accuracy of 94.10% after just 10 epochs while A-NeSI reaches 49.51% accuracy at the end of its 23rd epoch (Fig. 4). These results demonstrate that with limited training data, ISED is able to learn more quickly than A-NeSI, even for simple tasks. This result is likely due to A-NeSI training 2 additional neural models in its learning pipeline compared to ISED, with A-NeSI training a prior as well as a model to estimate the program output and gradient. 5 Limitations and Future Work The main limitation of ISED is the difficulty of scaling with the dimensionality of the space of inputs to the program P. There are interesting future directions in adapting and expanding ISED for high dimensionality. Specifically, improvements to the sampling strategy could help adapt ISED to a complex space of inputs. Techniques can be borrowed from the field of Bayesian optimization where such large spaces have traditionally been studied. Furthermore, there is merit to systematically combining white-box and black-box methods. ISED is especially useful when logic programs fail to encode reasoning components. Therefore, we believe that ISED can be used as an underlying engine for a new neurosymbolic language that blends the accessibility of black-box with the performance of white-box methods. 6 Related Work Neurosymbolic programming frameworks. These frameworks provide a general mechanism to define white-box neurosymbolic programs. DeepProbLog [ 14] and Scallop [13] abstract away gradient calculations behind a rule-based language. Others specialize in targeted applications, such as NeurASP [28] for answer set programming, or NeuralLog [3] for phrase alignment in NLP. ISED is similar in that it seeks to make classes of neurosymbolic programs easier to write and access; however, it diverges by offering an interface not bound by any specific domain or language syntax. RL and sampling-based neurosymbolic frameworks. ISED incorporates concepts found in the RL algorithm REINFORCE [ 26] such as the sampling of actions according to the current policy distribution, similar to NASR [5], and IndeCateR [21]. Other work has proposed a semantic loss function for neurosymbolic learning which measures how well neural network outputs match a given constraint [27]. While this technique resembles ISED in that it samples symbols from their predicted distributions to derive the loss, it relies on symbolic knowledge in the form of a constraint in Boolean logic, whereas ISED allows the program component to be any black-box program. Specialized neurosymbolic methods. The majority of the neurosymbolic learning literature pertains to point solutions for specific use cases [7, 25]. In the HWF example, NGS [12] and several of its 9variants leverage a hand-defined syntax defining the inherent structure within mathematical expres- sions. Similarly, DiffSort [18] leverages the symbolic properties of sorting to produce differentiable sorting networks. Other point solutions address broader problem setups, such as NS-CL [15] which provides a framework for visual question answering by learning symbolic representations in text and images. For reading comprehension, the NeRd [3] framework converts NL questions into executable programs over symbolic information extracted from text. ISED aligns with all of these point solutions by aiming to solve problems that have thus far required technically specific solutions in order to access the advantages of neurosymbolic learning, but it takes an opposite and easier approach by forgoing significant specializations and instead leverages existing solutions as black-boxes. Differentiable programming and non-differentiable optimization. Longstanding libraries in deep learning have grown to great popularity for their ability to abstract away automatic differen- tiation behind easy-to-use interfaces. PyTorch [17] is able to do so by keeping track of a dynamic computational graph. Similarly, JAX [1] leverages functional programming to abstract automatic differentiation. ISED follows the style of these frameworks by offering an interface to abstract away gradient calculations for algorithms used in deep learning, but ISED improves upon them by allowing systematic compatibility of non-differentiable functions. 7 Conclusion We proposed ISED, a data- and sample-efficient algorithm for learning neural programs. Unlike existing general neurosymbolic frameworks which require differentiable logic programs, ISED is compatible with Python programs and API calls to GPT, and it employs a sampling-based technique to learn neural model parameters using forward evaluation. We showed that for neuroGPT, neuroPython, and neurosymbolic benchmarks, ISED achieves better accuracy than end-to-end neural models and similar accuracy compared to neurosymbolic frameworks. ISED also often achieves superior accuracy on complex programs compared to REINFORCE-based and gradient estimation baselines. Furthermore, ISED learns in a more data- and sample-efficient manner compared to these baselines. 8 Acknowledgements We thank the anonymous reviewers for useful feedback. This research was supported by ARPA-H grant D24AC00253-00, NSF award CCF 2313010, and by a gift from AWS AI to ASSET (Penn Engineering Center on Trustworthy AI). References [1] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. [2] Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. Neurosymbolic programming. Foundations and Trends in Programming Languages, 7(3):158–243, 2021. [3] Zeming Chen, Qiyue Gao, and Lawrence S. Moss. NeuralLog: Natural language inference with joint neural and logical reasoning. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 78–88, 2021. [4] Siddharth Singh Chouhan, Uday Pratap Singh, Ajay Kaul, and Sanjeev Jain. A data repository of leaf images: Practice towards plant conservation with plant pathology. In2019 4th International Conference on Information Systems and Computer Networks (ISCON), pages 700–707, 2019. [5] Cristina Cornelio, Jan Stuehmer, Shell Xu Hu, and Timothy Hospedales. Learning where and when to reason in neuro-symbolic inference. In International Conference on Learning Representations, 2023. [6] Wang-Zhou Dai, Qiuling Xu, Yang Yu, and Zhi-Hua Zhou. Bridging machine learning and logical reasoning by abductive learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019. 10[7] Rajdeep Dutta, Qincheng Wang, Ankur Singh, Dhruv Kumarjiguda, Li Xiaoli, and Senthilnath Jayavelu. S-reinforce: A neuro-symbolic policy gradient approach for interpretable reinforce- ment learning. arXiv preprint arXiv:2305.07367, 2023. [8] Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. In Proceedings of the 35th International Conference on Neural Information Processing Systems, pages 25134–25145, 2021. [9] Paul Shekonya Kanda, Kewen Xia, and Olanrewaju Hazzan Sanusi. A deep learning-based recognition technique for plant leaf classification. IEEE Access, 9:162590–162613, 2021. [10] Angelika Kimmig, Guy Van den Broeck, and Luc De Raedt. Algebraic model counting. CoRR, 2012. [11] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [12] Qing Li, Siyuan Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, and Song-Chun Zhu. Closed loop neural-symbolic learning via integrating neural perception, grammar parsing, and symbolic reasoning. In Proceedings of the 37th International Conference on Machine Learning, page 5884–5894, 2020. [13] Ziyang Li, Jiani Huang, and Mayur Naik. Scallop: A language for neurosymbolic programming. In ACM International Conference on Programming Language Design and Implementation, page 1463–1487, 2023. [14] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, page 3753–3763, 2018. [15] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro- symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019. [16] Lukas Murmann, Michael Gharbi, Miika Aittala, and Fredo Durand. A multi-illumination dataset of indoor object appearance. In 2019 IEEE International Conference on Computer Vision (ICCV), Oct 2019. [17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, page 8026–8037, 2019. [18] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Differentiable sorting networks for scalable sorting and ranking supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021. [19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [20] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 779–788, 2016. [21] Lennert De Smet, Emanuele Sansone, and Pedro Zuidberg Dos Martires. Differentiable sampling of categorical distributions using the catlog-derivative trick. In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. [22] Dagher R. Talhouk S.N., Fabian M. Landscape plant database, 2015. [23] Efthymia Tsamoura, Timothy Hospedales, and Loizos Michael. Neural-symbolic integration: A compositional perspective. In AAAI Conference on Artificial Intelligence, 2020. [24] Emile van Krieken, Thiviyan Thanapalasingam, Jakub M. Tomczak, Frank van Harmelen, and Annette ten Teije. A-nesi: A scalable approximate method for probabilistic neurosymbolic in- ference. In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. 11[25] Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In Proceedings of the 36th International Conference on Machine Learning, pages 6545–6554, 2019. [26] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce- ment learning. Machine Learning, 8(3–4):229–256, 1992. [27] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In Proceedings of the 35th International Conference on Machine Learning, pages 5502–5511, 2018. [28] Zhun Yang, Adam Ishay, and Joohyung Lee. Neurasp: Embracing neural networks into answer set programming. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pages 1755–1762, 2020. [29] Delu Zeng, Minyu Liao, Mohammad Tavakolian, Yulan Guo, Bolei Zhou, Dewen Hu, Matti Pietikäinen, and Li Liu. Deep learning for scene classification: A survey, 2021. 12A Explanation of Differences Between ISED and Baseline Methods We explain the differences between ISED and prior techniques using the simple example of sum2, where digits are restricted to be between 0-2. Suppose that we are training a neural network Mθ for this task, and we are considering the symbol-output sample where the ground truth symbols are 1 and 2, i.e., r1 = 1, r2 = 2, and the ground truth output is y = 3. Suppose that the predicted distributions from Mθ for r1 and r2 are [0.1, 0.6, 0.3] and [0.2, 0.1, 0.7] respectively. We now explain how different methods perform their loss computations. A.1 ISED Suppose ISED is initialized with a sample count of 3, and the sampled symbol-output pairs are ((1, 2), 3), ((1, 0), 1), and ((2, 1), 3). We use the add-mult semiring in this example. ISED can be thought of as differentiating through the following summary logic program: r1 = 1 ∧ r2 = 2 → y = 3 r1 = 1 ∧ r2 = 0 → y = 1 r1 = 2 ∧ r2 = 1 → y = 3 As a result, the final vector calculated for the loss function before normalization would be   0.0 0.6 ∗ 0.2 0.0 0.6 ∗ 0.7 + 0.3 ∗ 0.1 0.0   where each value corresponds to the probability of the given output (possible outputs are in the range 0-4). Note that if there are duplicate samples, ISED includes the duplicate probabilities in its aggregation. In our implementation, we would perform normalization on this vector and then pass it into the binary cross-entropy loss function, with the ground truth vector being:   0.0 0.0 0.0 1.0 0.0   We would then minimize this loss and update Mθ accordingly. If we use the min-max semiring instead, ∗ is replaced by min and + by max in the final vector calculation, resulting in   0.0 0.2 0.0 0.6 0.0   A.2 REINFORCE Suppose REINFORCE is also initialized with a sample count of 3, and it samples the same symbol- output pairs. The final reward is computed by element-wise multiplication of the log probability of each sample with its reward value and taking the mean, as follows: 1 3 ∗ \"log(0.6) + log(0.7) log(0.6) + log(0.2) log(0.3) + log(0.1) # ∗ \"1.0 0.0 1.0 # and the goal is to optimize Mθ to maximize this reward. While this approach resembles ISED’s loss computation for the add-mult semiring, it does not involve the mult step. As it rewards possible values instead of possible combinations, the final reward would have been the same when (1,1) and (2,2) were the correct samples, instead of (1,2) and (2,1). Hence, the learning signal is weaker compared to ISED when there is more than one input to P. 13A.3 IndeCateR and NASR IndeCateR is an extension of the REINFORCE estimator that is unbiased with a provably lower variance. It assumes and exploits the factoring of the underlying multivariate distribution into independent categorical variables by summing out one dimension while keeping a sample for other dimensions fixed. For each sample drawn, IndeCateR systematically creates additional samples that differ on a single entry by enumerating all possible values for each variable. NASR targets efficient finetuning by setting the sample count to one and customizing the reward function. The loss computation for IndeCateR and NASR are identical to that of REINFORCE, also providing weak signals with fewer samples. Furthermore, both set the reward to 0 for samples leading to incorrect predictions, effectively ignoring them, unlike ISED which penalizes such symbols. Since only the correct symbol contribute to the final reward, the signal is sparser than ISED, making it sample-inefficient. A.4 A-NeSI A-NeSI trains two additional neural networks: a prediction model Qθ′ as a surrogate for P, and a prior model Rα for learning the parameters α for the Dirichlet distribution Dα. Suppose A-NeSI is also initialized with a sample count of 3. At each training step, A-NeSI first updates α using ˆy = Mθ(x). Next, it samples a single symbol from each of the 3 distributions sampled from Dα, and uses the sampled symbol-output pair and the standard cross entropy loss to update Qθ′. Then, A-NeSI optimizes Mθ by minimizing the loss L(Qθ′(Mθ(r1, r2))) using the prediction model instead of P. A.5 DeepProbLog DeepProbLog (DPL) enumerates all possible proofs for each output and aggregates probabilities accordingly. For example, the proofs for y = 1 include r1 = 0, r2 = 1 and r1 = 1, r2 = 0. Thus, the probability of this output is 0.6 ∗ 0.2 + 0.1 ∗ 0.1. The final vector calculated would be  0.1 ∗ 0.2 0.6 ∗ 0.2 + 0.1 ∗ 0.1 0.1 ∗ 0.7 + 0.6 ∗ 0.1 + 0.3 ∗ 0.2 0.6 ∗ 0.7 + 0.3 ∗ 0.1 0.3 ∗ 0.7   and we would pass this vector into some loss function (e.g., cross-entropy), with the same ground truth vector that ISED would use. DPL would then minimize this loss and update Mθ accordingly. A.6 Scallop Suppose Scallop is configured to use the diff-top-1-proofs semiring. This means that for each possible output, Scallop will use the proof of that output with the highest probability. For instance, the most likely proof for y = 1 is r1 = 1 and r2 = 0, and the probability of the output y = 1 is 0.6 ∗ 0.2. The final vector calculated would be  0.1 ∗ 0.2 0.6 ∗ 0.2 0.1 ∗ 0.7 0.6 ∗ 0.7 0.3 ∗ 0.7   and we would pass this vector into some loss function (e.g., binary-cross-entropy), with the same ground truth vector that ISED would use. Scallop would then minimize this loss and update Mθ accordingly. This probability estimation would change depending on the choice of semiring (e.g., diff-top-k-proofs for a different value of k). B Evaluation Setup For tasks with the MNIST dataset as unstructured data, we employ LeNet [11], a 2-layer CNN-based model, except for sum8, sum12, and sum16 tasks where we choose a smaller 2-layer CNN used by 14IndeCateR [21]. For HWF, we also use a 2-layer CNN-based model. For leaf classification tasks, images are scaled down and passed to a simple CNN-based network with 4 convolutional layers. For the scene recognition, we use YOLOv8 and a 3-layer convolutional network for neural program methods, 7-layer CNN for the purely neural solution, and CLIP with ViT-B/32. For all tasks included in RQ1, other than sudoku, we remove the final softmax function at the end of each network when evaluating IndeCateR since its sampling procedure yields optimal results without the softmax. We also do that same with REINFORCE if it results in higher accuracy. Since sudoku uses a pretrained CNN, we use the same CNN across all methods, including IndeCateR and REINFORCE. We use the Adam optimizer with the best learning rate among {1e−3, 5e−4, 1e−4}. We train for maximum 100 epochs, but stop early if the training saturates. For MNIST-R tasks, we used learning rate 1e−4 and trained ISED for 10 epochs, REINFORCE and IndeCateR for 50 epochs, and A-NeSI and NASR for 100 epochs. We trained ISED for 30 epochs, A-NeSI for 100 epochs, and the rest for 50 epochs for HWF and Leaf Classification with learning rate 1e−4. For the Scene Recognition task, we trained A-NeSI and the purely neural baseline 50 epochs and the rest 100 epochs with learning rate 5e−4. For tasks sum8 to sum16 we trained ISED for 50 epochs and the rest for 100 epochs with learning rate 1e−3. For Visual Sudoku, we follow the setting in NASR [5] and train for 10 epochs with learning rate 1e−5. We configure ISED to use the min-max semiring for HWF and the add-mult semiring for all other tasks. We use categorical sampling and binary cross-entropy loss for ISED. B.1 Neural-GPT Experiment Prompts For leaf classification and scene recognition, the neural-GPT experiments, we used the up-to-date version of GPT-4, gpt-4-1106-preview and gpt-4o respectively, with the parameter top-p set to 1e−8. We present the prompts used for the experiments in Tables 5 and 6. Table 5: GPT-4 prompt for the leaf classification task. System message You are an expert in classifying plant species based on the margin, shape, and texture of the leaves. You are designed to output a single JSON. User message <PLANT NAME >. Classify into one of: <M ARGIN /SHAPE /TEXTURE >. Give your answer without explanation. Table 6: GPT-4 prompt for the scene recognition task. System message You are an expert at identifying room types based on the object detected. Give short single responses. User message There are <DETECTED OBJECTS >. What type of room is most likely? Choose among <SCENES >. We use MARGIN = {entire, dentate, lobed, serrate, serrulate, undulate}, SHAPE ={ellipti- cal, lanceolate, obovate, oblong, ovate}, TEXTURE ={glossy, leathery, smooth, medium}, and PLANT NAME ∈{Alstonia Scholaris, Citrus limon, Jatropha curcas, Mangifera indica, Ocimum basilicum, Platanus orientalis, Pongamia Pinnata, Psidium guajava, Punica granatum, Syzygium cumini, Terminalia Arjuna}. Furthermore, SCENES = {bathroom, bedroom, dining room, living room, kitchen, lab, office, home lobby, basement} and DETECTED OBJECTS is a list of maximum length 10 with duplicates. C Full Performance Summary We report the accuracy of all benchmarks with 1-sigma standard deviation in Tables 7, 8, 9, and 10. We further provide the performance comparison with varying sample counts with 1-sigma standard deviation in Tables 11, 12, and 13. 15Table 7: Performance comparison for DT leaf, GPT leaf, scene, and sudoku. Accuracy (%) Method DT leaf GPT leaf scene sudoku DPL 39.70 ± 6.55 N/A N/A TO Scallop 81.13 ± 3.50 N/A N/A TO A-NeSI 78.82 ± 4.42 72 .40 ± 12.24 61 .46 ± 14.18 26 .36 ± 12.68 REINFORCE 40.24 ± 0.08 53 .84 ± 0.04 12 .17 ± 0.02 79 .08 ± 0.87 IndeCateR 78.71 ± 5.59 69 .16 ± 2.35 12 .72 ± 2.51 66 .50 ± 1.37 NASR 16.41 ± 1.79 17 .32 ± 1.92 2 .02 ± 0.23 82.78 ± 1.06 ISED (ours) 82.32 ± 4.15 79 .95 ± 5.71 68 .59 ± 1.95 80.32 ± 1.79 Table 8: Performance comparison for HWF, sum2, sum3, and sum4. Accuracy (%) Method HWF sum 2 sum3 sum4 DPL TO 95.14 ± 0.80 93 .80 ± 0.54 TO Scallop 96.65 ± 0.13 91 .18 ± 13.43 91 .86 ± 1.60 80 .10 ± 20.4 A-NeSI 3.13 ± 0.72 96.66 ± 0.87 94.39 ± 0.77 78 .10 ± 19.0 REINFORCE 88.27 ± 0.02 74 .46 ± 26.29 19 .40 ± 4.52 13 .84 ± 2.26 IndeCateR 95.08 ± 0.41 96 .48 ± 0.53 93 .76 ± 0.47 92 .58 ± 0.80 NASR 1.85 ± 0.27 6 .08 ± 0.77 5 .48 ± 0.77 4 .86 ± 0.93 ISED (ours) 97.34 ± 0.26 80.34 ± 16.14 95.10 ± 0.95 94 .1 ± 1.6 Table 9: Performance comparison for mult2, mod2, less-than, and add-mod-3. Accuracy (%) Method mult2 mod2 less-than add-mod-3 DPL 95.43 ± 0.97 96 .34 ± 1.06 96.60 ± 1.02 95.28 ± 0.93 Scallop 87.26 ± 24.70 77 .98 ± 37.68 80 .02 ± 3.37 75 .12 ± 21.64 A-NeSI 96.25 ± 0.76 96 .89 ± 0.84 94 .75 ± 0.98 77 .44 ± 24.60 REINFORCE 96.62 ± 0.23 94.40 ± 2.81 78 .92 ± 2.31 95.42 ± 0.37 IndeCateR 96.32 ± 0.50 97.04 ± 0.39 94.98 ± 1.50 78 .52 ± 23.26 NASR 5.34 ± 0.68 20 .02 ± 2.67 49 .30 ± 2.14 33 .38 ± 2.81 ISED (ours) 96.02 ± 1.13 96 .68 ± 0.93 96 .22 ± 0.95 83 .76 ± 12.89 Table 10: Performance comparison for add-sub, equal, not-3-or-4, and count-3-4. Accuracy (%) Method add-sub equal not-3-or-4 count-3-4 DPL 93.86 ± 0.87 98.53 ± 0.37 98.19 ± 0.55 TO Scallop 92.02 ± 1.58 71 .60 ± 2.29 97 .42 ± 0.73 93 .47 ± 0.83 A-NeSI 93.95 ± 0.60 77 .89 ± 36.01 98 .63 ± 0.50 93 .73 ± 2.93 REINFORCE 17.86 ± 3.27 78 .26 ± 3.96 99.28 ± 0.21 87.78 ± 1.14 IndeCateR 93.74 ± 0.44 98 .18 ± 0.39 99 .26 ± 0.16 94 .30 ± 1.26 NASR 5.26 ± 1.10 81 .72 ± 1.94 68 .36 ± 1.54 25 .26 ± 1.66 ISED (ours) 95.32 ± 0.81 96.02 ± 1.74 98 .08 ± 0.72 95.26 ± 1.04 16Table 11: Performance comparison for sum8 with different sample counts k. Accuracy (%) sum8 Method k = 80 k = 800 REINFORCE 8.32 ± 2.52 8 .28 ± 0.39 IndeCateR 5.36 ± 0.26 89.60 ± 0.98 IndeCateR+ 10.20 ± 1.12 88 .60 ± 1.09 ISED (Ours) 87.28 ± 0.76 87.72 ± 0.86 Table 12: Performance comparison for sum12 with different sample counts k. Accuracy (%) sum12 Method k = 120 k = 1200 REINFORCE 7.52 ± 1.92 8 .20 ± 1.80 IndeCateR 4.60 ± 0.24 77 .88 ± 6.68 IndeCateR+ 6.84 ± 2.06 86.92 ± 1.36 ISED (Ours) 85.72 ± 2.15 86.72 ± 0.48 Table 13: Performance comparison for sum16 with different sample counts k. Accuracy (%) sum16 Method k = 160 k = 1600 REINFORCE 5.12 ± 1.91 6 .28 ± 1.04 IndeCateR 1.24 ± 1.68 5 .16 ± 0.52 IndeCateR+ 4.24 ± 0.95 83.52 ± 1.75 ISED (Ours) 6.48 ± 0.50 8.13 ± 1.10 D License Information For implementing the baselines, we adapted the code from the official repositories of DeepProblog [14] (Apache 2.0), Scallop [8] (MIT), A-NeSI [24] (MIT), NASR [5] (MIT), and IndeCateR [21] (Apache 2.0). Additionally, our benchmarks use Multi-illumination dataset [16] (CC BY 4.0), HWF dataset (CC BY-NC-SA 3.0) from NGS [12], a subset of the leaf database [4] (CC BY 4.0), YOLOv8 (AGPL-3.0) and CLIP (MIT). 17",
      "references": [],
      "meta_data": {
        "arxiv_id": "2406.06246v2",
        "authors": [
          "Alaia Solko-Breslin",
          "Seewon Choi",
          "Ziyang Li",
          "Neelay Velingker",
          "Rajeev Alur",
          "Mayur Naik",
          "Eric Wong"
        ],
        "published_date": "2024-06-10T13:23:00Z",
        "github_url": "https://github.com/alaiasolkobreslin/ISED"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes neural programs as a generalization of neurosymbolic programs, and introduces ISED (Infer-Sample-Estimate-Descend), a data- and sample-efficient learning algorithm designed to train neural models when the program component P is a black box (including Python programs and GPT calls). The paper also introduces new neural-program benchmarks that couple neural components with modern LLMs (neuroPython and neuroGPT) and shows that ISED achieves competitive performance relative to state-of-the-art neurosymbolic frameworks on logic-program-compatible tasks while offering strong data-efficiency vs purely neural baselines on GPT-based tasks.",
        "methodology": "ISED treats P as a black box and learns neural parameters by sampling symbolic inputs from Mθ's output distribution. It constructs a symbolic summary of P from sampled symbol-output pairs (rules of the form if symbol=u then output=v). It then estimates gradients through this partial symbolic program using weighted model counting (WMC) across the summary, choosing semirings (min-max or add-mult) to aggregate probabilities. The approach relies on a structural data-type interface τ (DISCRETE, FLOAT, PERMUTATION, TUPLE, LIST) and a vectorize function δτ to connect ground-truth outputs to probability distributions. ISED updates parameters by backpropagating through a differentiable loss L over the aggregated symbol-output predictions. It supports arbitrary black-box programs, including calls to GPT-4, and uses sampling strategies to keep the gradient signal data-efficient.",
        "experimental_setup": "Evaluated on 16 neural-program benchmarks spanning neuroPython, neuroGPT, and traditional neurosymbolic problems (HWF, Sudoku, etc.). Datasets include LEAF-ID (leaf species), a 9-scene dataset for scene recognition, MNIST-R tasks (sum2, sum3, sum4, etc.), and the SatNet Sudoku dataset. Comparisons include neurosymbolic baselines (DeepProbLog, Scallop, A-NeSI, NASR, IndeCateR) and gradient-estimation baselines (REINFORCE, IndeCateR, NASR) as well as purely neural methods and CLIP. Experimental protocol uses multiple random seeds, fixed per-example program-call counts (typically 100), network architectures (LeNet-like CNNs for MNIST tasks, a CNN for leaf images, YOLOv8 for object detection in scene tasks), and GPT-4 prompts for GPT-based tasks. Evaluation reports accuracy with 1-sigma deviations across seeds and analyzes data/time efficiency and sample-efficiency across tasks.",
        "limitations": "ISED scales poorly with high-dimensional input spaces for the program P due to combinatorial symbol spaces and sampling cost. Its performance degrades as the dimensionality of the program inputs grows, requiring more sophisticated sampling or dimension reduction. The method relies on the quality of the sampled symbolic summary and the choice of semiring, and may not always match white-box neurosymbolic methods on all tasks. It also assumes discrete symbol spaces and relies on the ability to query black-box components (e.g., GPT) at training time, which incurs cost and latency.",
        "future_research_directions": "Develop better, potentially Bayesian-inspired sampling strategies to handle high-dimensional input spaces; investigate hybrid white-box/black-box approaches to combine symbolic reasoning with neural priors; scale ISED to more complex P with many inputs; extend the neural-program language to larger APIs and more LLMs; optimize sampling and prompting to reduce GPT call overhead; explore more domains and benchmarks to test adaptability and efficiency; and consider integrating ISED as a core engine for a neurosymbolic programming language that blends ease of use with differentiable optimization.",
        "experimental_code": "Code excerpts focused on the Infer-Sample-Estimate-Descend (ISED) computation engine as implemented in the repository. Key components include:\n\n1) baselines/a-nesi/anesi/anesi.py\n- Core ANeSIBase class implementing the training loop, sampling, and gradient-based updates through a differentiable wrapper over a symbolic component. Key methods include:\n  - __init__ (setup of Dirichlet alpha, q-model, perception model, optimizers, and sampling parameters)\n  - joint_matching_loss (computes loss comparing P over symbol outputs to w, including observed y consistency checks)\n  - off_policy_loss (gradient step when using prior or percept input P and symbolic constraints)\n  - log_q_loss (perception loss maximizing log probability of label under the NRM model)\n  - train_all (Algorithm 3 orchestration: off-policy loss, perception losses, and gradient steps)\n  - test (Algorithm 1: sampling and beam search evaluation)\n  - sampled_loss (computes perception loss by sampling w and evaluating the neural program)\n- The module also defines abstract methods for the problem domain to implement initial_state, symbolic_function, preprocess_y, and success.\n\n2) baselines/a-nesi/anesi/fit_dirichlet_hwf.py\n- Helpers to fit a Dirichlet posterior over world probabilities from ground-truth beliefs (beliefs). It implements a unit that updates the Dirichlet parameters via maximizing a log-likelihood, with optional L2 regularization.\n\n3) baselines/a-nesi/anesi/experiments/hwf/anesi_hwf_run.py\n- Derived model _HWFModel extending ANeSIBase tailored for HWF (Handwritten Formula) tasks. It wires the perception network (SymbolNet) and the InferenceModel (In MT/HWF variants) and implements the exact training loop (Algorithm 3) with off-policy and on-policy losses, gradient updates, and Dirichlet parameter fitting via fit_dirichlet.\n\n4) baselines/a-nesi/anesi/experiments/hwf/im_hwf.py\n- InferenceModelHWF: defines neural-IR for HWF setting with a neural RNG-based model that outputs world distributions over symbolic outputs (base-10 digits). The distribution method ties a neural world model (InHWFF implies a neural parameterized world given the symbol sequences) to a Dirichlet-distributed symbolic summary.\n\n5) baselines/a-nesi/anesi/experiments/hwf/fit_dirichlet_hwf.py\n- Variant of Dirichlet fitting specialized for HWF with beliefs collected from the perception policy.\n\n6) baselines/a-nesi/anesi/experiments/mnist_op/im_mnist_op.py\n- InferenceModelMnist and IndependentIMMnist: radiate the MNIST features to produce an output distribution over target digits as part of the ISED loop for neurosymbolic tasks such as arithmetic on MNIST digits.\n\n7) baselines/a-nesi/anesi/experiments/leaves/im_leaf.py / im_leaf_llm.py\n- Similar neural modules for leaves-based experiments, showing how ISED components interface with perception models and symbol sampling.\n\nThis collection encompasses the core experimental code implementing ISED across various domains, including HWF, MNIST-oriented tasks, leaves, and the HWF variant with dirichlet fitting.\n",
        "experimental_info": "Extraction summarizes the experimental settings used to evaluate ISED across tasks:\n- Methodology: Infer-Sample-Estimate-Descend (ISED) with P treated as a black-box (e.g., Python function or GPT) and a differentiable neural objective guided by a learned symbolic summary. Gradients flow through a differentiable loss L that aggregates symbol-output predictions using weighted model counting semantics via Dirichlet priors.\n- Experimental Setup: Evaluated on 16 neural-program benchmarks (neuroPython, neuroGPT, traditional neurosymbolic tasks like HWF, Sudoku, etc.) including neuro-symbolic programming tasks and GPT-based tasks. Datasets span MNIST-based arithmetic tasks (sum, mod, etc.), leaf classification datasets, and HWF arithmetic.\n- Data Efficiency: Emphasis on data- and sample-efficient training through sampling of symbolic inputs from P’s output distribution; uses off-policy and on-policy losses with a Dirichlet prior over P, and a learned Dirichlet posterior (alpha) via fit_dirichlet.\n- Architectures: LeNet-like CNNs for MNIST-like tasks; SymbolNet and InferenceModel variants for HWF; perception networks (e.g., LeavesNet, SceneNet) interfacing with neural components of the ISED system. Some experiments use GPT-4 prompts for GPT-based tasks.\n- Experimental Protocol: Multiple seeds for reported results; fixed per-example program-call counts (typically around 100) across tasks; architecture-specific hyperparameters (q_lr, perception_lr, dirichlet_lr, dirichlet_iters, K_beliefs); training with both perception loss and symbolic consistency penalties; evaluation reports accuracy with 1-sigma deviations; data and time efficiency analyses across tasks.\n- Limitations and Scope: ISED scales poorly with high-dimensional program inputs due to combinatorial symbol spaces; performance can degrade compared to white-box neurosymbolic methods; relies on discrete symbol spaces and the ability to query black-box components, with cost implications for GPT prompts.\n- Future directions: investigate Bayesian-inspired sampling, hybrid white-box/black-box integration, scaling to more complex P, extended neural-program APIs, and GPT prompt optimizations.\n"
      }
    },
    {
      "title": "Data-Efficient Pipeline for Offline Reinforcement Learning with Limited Data",
      "full_text": "Data-Efﬁcient Pipeline for Ofﬂine Reinforcement Learning with Limited Data Allen Nie∗ Yannis Flet-Berliac Deon R. Jordan William Steenbergen Emma Brunskill Department of Computer Science Stanford University *anie@stanford.edu Abstract Ofﬂine reinforcement learning (RL) can be used to improve future performance by leveraging historical data. There exist many different algorithms for ofﬂine RL, and it is well recognized that these algorithms, and their hyperparameter settings, can lead to decision policies with substantially differing performance. This prompts the need for pipelines that allow practitioners to systematically perform algorithm- hyperparameter selection for their setting. Critically, in most real-world settings, this pipeline must only involve the use of historical data. Inspired by statistical model selection methods for supervised learning, we introduce a task- and method- agnostic pipeline for automatically training, comparing, selecting, and deploying the best policy when the provided dataset is limited in size. In particular, our work highlights the importance of performing multiple data splits to produce more reliable algorithm-hyperparameter selection. While this is a common approach in supervised learning, to our knowledge, this has not been discussed in detail in the ofﬂine RL setting. We show it can have substantial impacts when the dataset is small. Compared to alternate approaches, our proposed pipeline outputs higher-performing deployed policies from a broad range of ofﬂine policy learning algorithms and across various simulation domains in healthcare, education, and robotics. This work contributes toward the development of a general-purpose meta-algorithm for automatic algorithm-hyperparameter selection for ofﬂine RL. 1 Introduction Ofﬂine/batch reinforcement learning has the potential to learn better decision policies from existing real-world datasets on sequences of decisions made and their outcomes. In many of these settings, tuning methods online is infeasible and deploying a new policy involves time, effort and potential negative impact. Many of the existing datasets for applications that may beneﬁt from ofﬂine RL may be fairly small in comparison to supervised machine learning. For instance, the MIMIC intensive care unit dataset on sepsis that is often studied in ofﬂine RL has 14k patients (Komorowski et al., 2018), the number of students frequently interacting with an online course will often range from hundreds to tens of thousands (Bassen et al., 2020), and the number of demonstrations collected from a human operator manipulating a robotic arm is often on the order of a few hundred per task (Mandlekar et al., 2018). In these small data regimes, recent studies (Mandlekar et al., 2021; Levine et al., 2020) highlight that with limited data, the selection of hyperparameters using the training set is often challenging. Yet hyperparameter selection also has a substantial inﬂuence on the resulting policy’s performance, particularly when the algorithm leverages deep neural networks. One popular approach to address this is to learn policies from particular algorithm-hyperparameter pairs on a training set and then use ofﬂine policy selection, which selects the best policy given a validation set (Thomas et al., 2015a, 2019; Paine et al., 2020; Kumar et al., 2021). However, when 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.08642v2  [cs.LG]  12 Jan 2023Common Practices Non-Markov Env Data Efﬁcient (re-train) Compare Across OPL Considers Evaluation Variation Considers Training Variation Policy selection (1 split) Internal Objective / TD-Error (Thomas et al., 2015b, 2019) (depends) \u0017 \u0017 \u0017 \u0017 OPE methods (Komorowski et al. (2018); Paine et al. (2020) (depends) \u0017 \u0013 \u0017 \u0017 OPE + BCa Val. (Thomas et al., 2015b) (depends) \u0017 \u0013 \u0013 \u0017 BVFT (Xie and Jiang, 2021) \u0017 \u0017 \u0017 \u0017 \u0017 BVFT + OPE (Zhang and Jiang, 2021) \u0017 \u0017 \u0013 \u0013 \u0017 Q-Function Workﬂow (Kumar et al., 2021) \u0017 \u0013 \u0017 \u0017 \u0017 Ours: Ai selection (multi-split) Cross-Validation \u0013 \u0013 \u0013 \u0013 \u0013 Repeated Random Subsampling \u0013 \u0013 \u0013 \u0013 \u0013 Table 1: A summary of commonly used approaches for choosing a deployment policy from a ﬁxed ofﬂine RL dataset. We deﬁne Data Efﬁcient as: the approach assumes the algorithm can be re-trained on all data points; (depends) as: depends on whether the underlying OPL or OPE methods make explicit Markov assumption or not. the dataset is limited in size, this approach can be limited: (a) if the validation set happens to have no or very few good/high-reward trajectories, then trained policies cannot be properly evaluated; (b) if the training set has no or very few such trajectories, then no good policy behavior can be learned through any policy learning algorithm; and (c) using one ﬁxed training dataset is prone to overﬁtting the hyperparameters on this one dataset and different hyperparameters could be picked if the training set changes. One natural solution to this problem is to train on the entire dataset and compare policy performance on the same dataset, which is often referred to as the internal objective approach. In Appendix A.1 we conduct a short experiment using D4RL where this approach fails due to the common issue of Q-value over-estimation (Fujimoto et al., 2019). There has been much recent interest in providing more robust methods for ofﬂine RL. Many rely on the workﬂow just discussed, where methods are trained on one dataset and Ofﬂine Policy Evaluation (OPE) is used to do policy selection (Su et al., 2020; Paine et al., 2020; Zhang and Jiang, 2021; Kumar et al., 2021; Lee et al., 2021; Tang and Wiens, 2021; Miyaguchi, 2022). Our work highlights the impact of a less studied issue: the challenge caused by data partitioning variance. We ﬁrst motivate the need to account for train/validation partition randomness by showing the wide distribution of OPE scores the same policy can obtain on different subsets of data or the very different performing policies the same algorithm and hyperparameters can learn depending on different training set partitions. We also prove a single partition can have a notable failure rate in identifying the best algorithm-hyperparameter to learn the best policy. We then introduce a general pipeline for algorithm-hyperparameters (AH) selection and policy de- ployment that: (a) uses repeated random sub-sampling (RRS) with replacement of the dataset to perform AH training, (b) uses OPE on the validation set, (c) computes aggregate statistics over the RRS splits to inform AH selection, and (d) allows to use the selected AH to retrain on the entire dataset to obtain the deployment policy. Though such repeated splitting is common in supervised learning, its impact and effect have been little studied in the ofﬂine RL framework. Perhaps surprisingly, we show that our simple pipeline leads to substantial performance improvements in a wide range of popular benchmark tasks, including D4RL (Fu et al., 2020) and Robomimic (Mandlekar et al., 2021). 22 Related work Ofﬂine Policy Learning (OPL). In OPL, the goal is to use historical data from a ﬁxed behavior policy πb to learn a reward-maximizing policy in an unknown environment (Markov Decision Process, deﬁned in Section 3). Most work studying the sampling complexity and efﬁciency of ofﬂine RL (Xie and Jiang, 2021; Yin et al., 2021) do not depend on the structure of a particular problem, but empirical performance may vary with some pathological models that are not necessarily Markovian. Shi et al. (2020) have precisely developed a model selection procedure for testing the Markovian hypothesis and help explain different performance on different models and MDPs. To address this problem, it is inherently important to have a fully adaptive characterization in RL because it could save considerable time in designing domain-speciﬁc RL solutions (Zanette and Brunskill, 2019). As an answer to a variety of problems, OPL is rich with many different methods ranging from policy gradient (Liu et al., 2019), model-based (Yu et al., 2020; Kidambi et al., 2020), to model-free methods (Siegel et al., 2020; Fujimoto et al., 2019; Guo et al., 2020; Kumar et al., 2020) each based on different assumptions on the system dynamics. Practitioners thus dispose of an array of algorithms and corresponding hyperpa- rameters with no clear consensus on a generally applicable evaluation tool for ofﬂine policy selection. Ofﬂine Policy Evaluation (OPE). OPE is concerned with evaluating a target policy’s performance using only pre-collected historical data generated by other (behavior) policies (V oloshin et al., 2021). Each of the many OPE estimators has its unique properties, and in this work, we primarily consider two main variants (V oloshin et al., 2021): Weighted Importance Sampling (WIS) (Precup, 2000) and Fitted Q-Evaluation (FQE) (Le et al., 2019). Both WIS and FQE are sensitive to the partitioning of the evaluation dataset. WIS is undeﬁned on trajectories where the target policy does not overlap with the behavior policy and self-normalizes with respect to other trajectories in the dataset. FQE learns a Q-function using the evaluation dataset. This makes these estimators very different from mean-squared errors or accuracy in the supervised learning setting – the choice of partitioning will ﬁrst affect the function approximation in the estimator and then cascade down to the scores they produce. Ofﬂine Policy Selection (OPS).Typically, OPS is approached via OPE, which estimates the expected return of candidate policies. Zhang and Jiang (2021) address how to improve policy selection in the ofﬂine RL setting. The algorithm builds on the Batch Value-Function Tournament (BVFT) (Xie and Jiang, 2021) approach to estimating the best value function among a set of candidates using piece-wise linear value function approximations and selecting the policy with the smallest projected Bellman error in that space. Previous work on estimator selection for the design of OPE methods include Su et al. (2020); Miyaguchi (2022) while Kumar et al. (2021); Lee et al. (2021); Tang and Wiens (2021); Paine et al. (2020) focus on ofﬂine hyperparameter tuning. Kumar et al. (2021) give recommendations on when to stop training a model to avoid overﬁtting. The approach is exclusively designed for Q-learning methods with direct access to the internal Q-functions. On the contrary, our pipeline does policy training, selection, and deployment on any ofﬂine RL method, not reliant on the Markov assumption, and can select the best policy with potentially no access to the internal approximation functions (black box). We give a brief overview of some OPS approaches in Table 1. 3 Background and Problem Setting We deﬁne a stochastic Decision Process M = ⟨S,A,T,r,γ ⟩, where Sis a set of states; Ais a set of actions; T is the transition dynamics (which might depend on the full history); ris the reward function; and γ ∈(0,1) is the discount factor. Let τ = {si,ai,s′ i,ri}L i=0 be the trajectory sampled from π on M. The optimal policy π is the one that maximizes the expected discounted return V(π) = Eτ∼ρπ[G(τ)] where G(τ) = ∑∞ t=0 γtrt and ρπ is the distribution of τ under policy π. For simplicity, in this paper we assume policies are Markov π : S →A, but it is straightforward to consider policies that are a function of the full history. In an ofﬂine RL problem, we take a dataset: D= {τi}n i=1, which can be collected by one or a group of policies which we refer to as the behavior policy πb on the decision process M. The goal in ofﬂine/batch RL is to learn a decision policy πfrom a class of policies with the best expected performance Vπ for future use. Let Ai to denote an AH pair, i.e. an ofﬂine policy learning algorithm and its hyperparameters and model architecture. An ofﬂine policy estimator takes in a policy πe and a dataset D, and returns an estimate of its performance: ˆV : Π ×D→ R. In this work, we focus on two popular Ofﬂine Policy Evaluation (OPE) estimators: Importance Sampling (IS) (Precup, 2000) and Fitted Q-Evaluation (FQE) (Le et al., 2019) estimators. We refer the reader to V oloshin et al. (2021) for a more comprehensive discussion. 3(a) (b) OPE Estimates on 10 Partitions True Reward of Policy trained on 10 Partitions Figure 1: True performance and evaluation of 6Ai pairs on the Sepsis-POMDP (N=1000) domain. (a) shows the OPE estimations and (b) shows the variation in terms of true performance. The variations are due to the differentAH pairs of the policiesbut alsoto the sensitivity to the training/validation splits. 4 The Challenge of Ofﬂine RL Ai Selection An interesting use-case of ofﬂine RL is when domain experts have access to an existing dataset (with potentially only a few hundred trajectories) about sequences of decisions made and respective outcomes, with the hope of leveraging the dataset to learn a better decision policy for future use. In this setting, the user may want to consider many options regarding the type of RL algorithm (model-based, model-free, or direct policy search), hyperparameter, or deep network architecture to use. Automated algorithm selection is important because different Ai (different AH pairs) may learn very diverse policies, each with signiﬁcantly different performance VAi. Naturally, one can expect that various algorithms lead to diverse performance, but using a case-study experiment on a sepsis simulator (Oberst and Sontag, 2019), we observe in Figure 1(b) that the sensitivity to hyperparameter selection is also substantial (cf. different average values in box plots for each method). For example, MBS-QI (Liu et al., 2020) learns policies ranging from over -12 to -3 in their performance, depending on the hyperparameters chosen. Precisely, to address hyperparameter tuning, past work often relies on executing the learned policies in the simulator/real environment. When this is not feasible, as in many real-world applications, including our sepsis dataset example, where the user may only be able to leverage existing historical data, we have no choice but to rely on off-policy evaluation. Prior work (Thomas et al., 2015b; Farajtabar et al., 2018; Thomas et al., 2019; Mandlekar et al., 2021) have suggested doing so using a hold-out method, after partitioning the dataset into training and validation sets. Unfortunately, the partitioning of the dataset itself may result in substantial variability in the training process (Dietterich, 1998). We note that this problem is particularly prominent in ofﬂine RL where high-reward trajectories are sparse and affect both policy learning and policy evaluation. To explore this hypothesis, we consider the inﬂuence of the train/validation partition in the same sepsis domain, and we evaluate the trained policies using the Weighted Importance Sampling (WIS) (Precup, 2000) estimator. Figure 1(a) shows the policies have drastically different OPE estimations with sensitivity to randomness in the dataset partitioning. We can observe the same phenomena in Figure 1(b) with largely different true performances depending on the dataset splitting for most of the policiesAi. This is also illustrated on the left sub-ﬁgure of Figure 4 where in the case where a single train-validation split is used, an Ai that yields lower-performing policies will often be selected over those that yield higher-performing policies when deployed. 4.1 Repeated Experiments for Robust Hyperparameter Evaluation in Ofﬂine RL We now demonstrate why it is important to conduct repeated random sub-sampling on the dataset in ofﬂine RL. Consider a ﬁnite set of J ofﬂine RL algorithms A. Let the policy produced by algorithm Aj on training dataset Dbe πj, its estimated performance on a validation set ˆVπj, and its true (unknown) value be Vπj. Denote the true best resulting policy as πj∗ = arg maxjVπj and the corresponding algorithm Aj∗. Let the best policy picked based on its validation set performance as πˆj∗ = arg maxj ˆVπj and the corresponding algorithm Aˆj∗. Theorem 1. There exist stochastic decision processes and datasets such that (i) using a single train/validation split procedure that selects an algorithm-hyperparameter with the best performance 4on the validation dataset will select a suboptimal policy and algorithm with signiﬁcant ﬁnite prob- ability, P(πˆj∗ ̸= πj∗) ≥C, with corresponding substantial loss in performance O(Vmax), and, in contrast, (ii) selecting the algorithm-hyperparameter with the best average validation performance across Ns train/validation splits will select the optimal algorithm and policy with probability 1: limNs→∞P(πˆj∗ = πj∗) →1. Proof Sketch. Due to space constraints we defer the proof to Appendix A.3. Brieﬂy, the proof proceeds by proof by example through constructing a chain-like stochastic decision process and considers a class of algorithms that optimize over differing horizons (see e.g. Jiang et al. (2015); Cheng et al. (2021); Mazoure et al. (2021)). The behavior policy is uniformly random meaning that trajectories with high rewards are sparse. This means there is a notable probability that in a single partition of the dataset, the resulting train and/or validation set may not contain a high reward trajectory, making it impossible to identify that a full horizon algorithm, and resulting policy, is optimal. In the proof and our experiments, we focus on when the training and validation sets are of equal size. If we use an uneven split, such as 80/20%, the failure probability can further increase if only a single partition of the dataset is used. We provide an illustrative example in the Appendix. Note that Leave-one-out Cross-Validation (LooCV) will also fail in our setting if we employ, as we do in our algorithm, WIS, because as a biased estimator, WIS will return the observed return of the behavior policy if averaging over a single trajectory, independent of the target policy to be evaluated . We explain this further in Appendix A.11. 5 SSR: Repeated Random Sampling for Ai Selection and Deployment In this paper, we are interested in the following problem: If ofﬂine RL training and evaluation are very sensitive to the partitioning of the dataset, especially in small data regimes, how can we reliably produce a ﬁnal policy that we are conﬁdent is better than others and can be reliably deployed in the real-world? Instead of considering the sensitivity to data partition as an inherent obstacle for ofﬂine policy selection, we view this as statistics to leverage for Ai selection. We propose a general pipeline: Split Select Retrain (SSR) (of which we provide a pseudo-code in Algorithm 1, Appendix A.4) to reliably optimize for a good deployed policy given only: an ofﬂine dataset, an input set of AH pairs and an off-policy evaluation (OPE) estimator. This deployment approach leverages the random variations created by dataset partitioning to select algorithms that perform better on average using a robust hyperparameter evaluation approach which we develop below. First, we split and create different partitions of the input dataset. For each train/validation split, each algorithm-hyperparameter ( AH) is trained on the training set and evaluated using the input OPE method to yield an estimated value on the validation set. These estimated evaluations are then averaged, and the best AH pair (A∗) is selected as the one with the highest average score. Now the last step of the SSR pipeline is to re-use the entire dataset to train one policy π∗using A∗. Repeated Random Sub-sampling (RRS). As Theorem 1 suggests, one should ensure a sufﬁcient amount of trajectories in the evaluation partition to lower the failure rateC. We propose to create RRS train-validation partitions. This approach has many names in the statistical model selection literature, such as Predictive Sample Reuse Method (Geisser, 1975), Repeated Learning-Test Method (Burman, 1989) or Monte-Carlo Cross-Validation (Dubitzky et al., 2007). It has also been referred to as Repeated Data Splitting (Chernozhukov et al., 2018) in the heterogeneous treatment effect literature. We randomly select trajectories in D and put them into into two parts: Rtrain and Rvalid. We repeat this splitting process K times to generate paired datasets: (Rtrain 1 ,Rvalid 1 ),(Rtrain 2 ,Rvalid 2 ),..., (Rtrain K ,Rvalid K ). We compute the generalization performance estimate as follows: GA,RSK = 1 K K∑ k=1 [ˆV(A(Rtrain k ); Rvalid k ) ] (1) A key advantage of overlap partitioning is that it maintains the size of the validation dataset as K increases. This might be favorable since OPE estimates are highly dependent on the state-action coverage of the validation dataset – the more data in the validation dataset, the better OPE estimators can evaluate a policy’s performance. AsK →∞, RRS approaches the leave-p-out cross-validation 5(CV), where pdenotes the number of examples in the validation dataset. Since there are (n p ) possible selections of pdata points out of nin our dataset, it is infeasible to use exact leave-p-out CV when p> 2, but a ﬁnite K can still offer many advantages. Indeed, Krzanowski and Hand (1997) point out that leave-p-out estimators will have lower variance compared to leave-one-out estimators, which is what the more commonly used M-fold cross-validation method converges to when M = n−1. We discuss more in Appendix A.2. 6 Experiments In this section, we answer the following questions: (a) how does the pipeline SSR-RRS compare to other methods? (b) does the proposed pipeline for Ai selection and policy deployment allow us to generate the best policy trained on the whole dataset? (c) does re-training on the whole dataset (data efﬁciency) generate better policies than policies trained on half of the dataset when Ai is selected by the pipeline? In addition, we conduct two ablation studies to answer to: what number of splits should we use for SSR-RRS, and what is the impact of dataset size on the pipeline results? 6.1 Task/Domains The experimental evaluation involves a variety of real-world and simulated domains, ranging from tabular settings to continuous control robotics environments. We evaluate the performance of SSR in selecting the best algorithm regardless of task domains and assumptions on task structure. We conduct experiments on eight datasets (Figure 2) from ﬁve domains (details in Appendix A.14), which we give a short description below, and use as many as 540 candidate AH pairs for the Sepsis POMDP domain. Figure 2: Illustrations from left to right of the D4RL, Robomimic, TutorBot and Sepsis domains. Sepsis. The ﬁrst domain is based on the simulator and work by Oberst and Sontag (2019) and revolves around treating sepsis patients. The goal of the policy for this simulator is to discharge patients from the hospital. In this domain, we experiment on two tasks: Sepsis-MDP and Sepsis-POMDP, a POMDP version of Sepsis-MDP. TutorBot. The second domain includes a TutorBot simulator that is designed to support 3-5th grade elementary school children in understanding the concept of volume and engaging them while doing so. An online study was conducted using a policy-gradient-based RL agent, which interacted with about 200 students. We took the observations from this online study and built a simulator that reﬂects student learning progression, combined with some domain knowledge. Robomimic. Robomimic (Mandlekar et al., 2021) is composed of various continuous control robotics environments with suboptimal human data. We use the Can-Paired and Transport dataset composed of 200 mixed-quality human demonstrations. Mandlekar et al. (2021) attempted to use the RL objective loss on a 20% split validation set to select the best AH pair, but reported that the selected AH did not perform well in the simulator, which makes this task an interesting testbed for our pipeline. D4RL. D4RL (Fu et al., 2020) is an ofﬂine RL standardized benchmark designed and commonly used to evaluate the progress of ofﬂine RL algorithms. We use 3 datasets (200k samples each) with different qualities from the Hopper task: hopper-random from a randomly initialized policy, hopper-medium from a policy trained to approximately 1/3 the performance of a policy trained to completion with SAC (\"expert\"), and hopper-medium-expert from a 50-50 split of medium and expert data. 6.2 Baselines One-Split OPE. The simplest method to train and verify an algorithm’s performance without access to any simulator is to split the data into a train Dtrain and valid set Dvalid. All policies are trained 6OPE +  Bootstrapped Val. Cross Validation Nested CV Ours BVFT (a) (b) Sepsis-POMDP  N=1000 Sepsis-MDP  N=200 Robomimic Can-Paired N=200 Robomimic Transport N=200 Figure 3: (a) We ﬁrst compare our proposed pipeline to various other policy selection approaches in the Sepsis POMDP task. Our approach SSR-RRS 5-split consistently obtains policies that on average perform close to the optimal policy, signiﬁcantly outperforming other approaches. (b) We investigate the importance of re-training in the small (N=200) to medium (N=1000) data regime. We show the true reward obtained by all policies from allAH pairs either trained on 50% of the data or 100% of the data. 95% conﬁdence intervals are depicted as error bars. Most policies achieve higher rewards when trained on more data, even more so when the dataset is small or when tasks are more difﬁcult (Robomimic). on the same training set and evaluated on the same valid set. As we explained before, this method has the high potential of overﬁtting the chosen hyperparameter for one data partition – it might pick the best policy, but does not guarantee we can use the same hyperparameter to re-train on the full dataset. OPE on Bootstrapped Val. Bootstrapping is a popular re-sampling technique that estimates pre- diction error in supervised learning models (Efron, 1983, 1986). The idea of using bootstrapping for OPE estimate is ﬁrst utilized in HCOPE (Thomas et al., 2015b). Compared to the one-split method and the SSR pipeline, bootstrapping trains all policies on the same training dataset, and only considers variations in the validation set by creating bootstrapped samples. We refer to the considered Bias-corrected accelerated (BCa) bootstrap method as BCa in the experiments. Cross-Validation (CV).One other natural alternative of repeated experiment validation is the popular M-fold Cross-Validation method (Stone, 1974). M-fold CV constructs a non-overlapping set of trajectories from the original dataset. For example, a 5-fold CV will train a policy on 80% of data and evaluate the policy on 20% of data, as it divides the dataset into 5 non-overlapping partitions. However, as we increase the number of splits M, which allows us to train/test our algorithms under more data split variations, each non-overlapping set Dm becomes smaller. When M = n−1, M-fold CV becomes leave-one-out CV (LooCV). In this extreme case, many OPE estimators will not work properly, as we have shown in Appendix A.3. We further investigate a variant of M-fold CV called Nested M-fold CV (Nested CV), which repeats the M-fold non-overlapping partitioning K times. This procedure is computationally very expensive. Considering the fairness of comparison and computational efﬁciency, we only evaluate K×2-fold CV . OPE with BVFT. Batch Value Function Tournament is the closest competitor to our method, which is a meta-algorithm for hyperparameter-free policy selection (Xie and Jiang, 2021; Zhang and Jiang, 2021). For a set of Q-value functions, BVFT makes pairwise comparisons of each (tournament-style) to select the best out of the entire set based on the BVFT-Loss. Compared to our method, BVFT incurs O(J2) comparison given J AH pairs, practically infeasible for large J. The original BVFT can only compare Q-functions, therefore only usable with OPL that directly learns Q-functions. Zhang and Jiang (2021) offers an extension to BVFT by using BVFT to compare between FQEs, therefore allowing BVFT to be OPL-agnostic. We adopt the two strategies recommended by the paper. GivenJ AH pairs and BFQEs, strategy 1 compares J×BFQE’s Q-functions jointly (πx FQE) and strategy 2 compares BFQEs within each AH and pick the best FQE as the estimate of the AH’s value estimate (π+ FQE). We discuss more in Appendix A.6 (calculations) and A.7 (time complexity). 6.3 Training and Evaluation Ofﬂine Policy Learning. In the following, we outline a variety of Ofﬂine RL algorithms used in the evaluation of the SSR pipeline to demonstrate the generality of our approach and that it can reliably 7Category Algorithms Internal Q-function Imitation Learning BC (Pomerleau, 1991) BCRNN (Mandlekar et al., 2018) \u0017 Conservative Model-Free BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020), IRIS (Mandlekar et al., 2020) \u0013 Policy Gradient POIS (Metelli et al., 2018), BC+POIS (ours), BC+mini-POIS (ours) \u0017 Ofﬂine Model-Based MOPO (Yu et al., 2020), P-MDP (ours) \u0013 Table 2: List of AH we are comparing in our experiments. High  performing  policy One-Split OPE SSR RRS-2 SSR RRS-5 (a) (b)Low performing  policies High performing  policies Diff in  Picked  Policy N=200 N=1000 N=5000 No Diff 5 5 2 2 Figure 4: (a) We show the effect of choosing K (the number of train/valid splits) for SSR-RRS. We ablate K and run the simulation 500 times. The heatmaps shows the frequency at which each policy is chosen by our method and its ﬁnal performance in the environment. In this experiment, SSR-RRS chooses over 540 AH pairs. As we can see, when the number of splits Kis larger, SSR-RRS consistently pick better policies. (b) In the Sepsis-POMDP domain, we show that as the number of trajectories (N) in the ofﬂine dataset increases, data partitioning becomes less important. Though RRS still outperforms the 1-split policy selection. produce the optimal ﬁnal policy using the selected AH pair. This marks a departure from workﬂows designed for speciﬁc algorithms, such as Kumar et al. (2021). We experiment with popular ofﬂine RL methods (see Table 2 and we provide algorithmic and hyperparameter details in Table A.2). Ofﬂine Policy Evaluation. We use WIS estimators for the tabular and discrete action domains: Sepsis and TutorBot. We use FQE for continuous action domains: Robomimic and D4RL. For each task, with a given dataset, we use the splitting procedure described in Section 5 to generate the partitioning. We describe how we compute the results for each ﬁgure in Appendix A.13. 7 Results Ai Selection Comparison. In Figure 3(a), we compare ﬁve approaches (One-Split OPE, BCa, BVFT-FQE, CV , andSSR-RRS) on the Sepsis-POMDP domain with 1000 patients. For each approach, we compute a score per AH pair, and select the best algorithm according to each. For fairness in comparison, all selected Ai are re-trained on the full dataset and we report the ﬁnal performance in the real environment. As expected, One-Split OPE performed the worst. Surprisingly, using the lower bound of bootstrapped (BCa (LB)) conﬁdence interval also does not allow to pick good policies, LB being perhaps too conservative. We see that CV 2-fold and CV 5-fold do not perform well either. CV 2-fold does not allow enough repetition and CV 5-fold makes the validation set size too small. We observe clearly that SSR-RRS 5-split performs the best and selected policies that are on average very close to the optimal policy’s performance.BVFT-FQE relies on FQE, which is a misspeciﬁed model on the Sepsis domain and difﬁcult to optimize given the small dataset size; hence it does not select good policies in Sepsis. However, in Robomimic Can-Paired and D4RL Hopper, BVFT-FQE is able to pick good policies, albeit not signiﬁcantly better or worse than other methods, and still worse than SSR-RRS 5-split in the mixed (more realistic) “medium-expert” dataset. We show more analysis of BVFT compared to our method in the Appendix. Table 3 aggregates the results for all the considered domains in our study. Our approach SSR-RRS 5-split is distinctly able to more consistently select policies that, once deployed, perform close to the optimal policy across all tasks. 8Re-trained on full dataset BVFT-FQE πx FQE BVFT-FQE π+ FQE CV-2 CV-5 SSR RRS-2 SSR RRS-5 Optimal Policy Robomimic: Can-Paired 0.65 0.71 0.72 0.72 0.71 0.73 0.75 Transport 0.21 0.0 0.42 0.42 0.62 0.70 0.74 D4RL (Hopper): random 321.75 317.72 325.37 325.37 324.92 325.37 325.37 medium 934.71 1227.81 1227.81 1304.53 1296.87 1304.54 1392.93 medium-expert 2677.93 2677.93 2530.04 2530.04 3481.34 3657.80 3657.80 Sepsis: MDP (n=200) — -19.32 -10.26 -20.32 -13.01 -7.85 -1.94 POMDP (n=1000) — -1.92 0.74 -1.92 2.40 6.75 7.86 TutorBot: POMDP (n=200) — — 1.34 1.19 1.30 1.38 1.43 Table 3: Comparison of the performance obtained by a policy deployed using the SSR pipeline vs. using 1-split policy selection approaches on a wide range of application domains. Cells = average true return. We note that (πx FQE) is very computationally expensive when we search through a large AH space (in Sepsis and TutorBot), therefore we exclude them. The Beneﬁts of Re-training Policies Selected with SSR. In Figure 3(b), we plot the true reward of a selected policy Ai when only trained on 50% of the dataset (the training set) compared to when trained on 100% of the dataset. As expected, in the small data regime, every single trajectory matters. Policies trained on the full dataset signiﬁcantly outperform policies trained only on half of it. This experiment provides strong evidence in favor ofAH selection (done with RRS on the full dataset) over policy selection (done on the training set) in ofﬂine RL. The Impact of Number of Repeats for SSR-RRS. The proposed pipeline SSR-RRS has a hyperpa- rameter Kfor the number of repeated data splitting. In Figure 4(a), we show the true performance of the policy that is being selected by SSR-RRS with K = 1,2,5 by running 500 simulations with heatmaps on the frequency of each policy is selected. We observe that whenK = 1 (equivalent to the One-Split OPE method), policies are picked quite uniformly; many of which are performing poorly. When K = 5, higher-performing policies are selected much more frequently. From Table 3, we conclude that K = 5 generally works well across various domains. Naturally, the number of split K will be chosen in line with the computing budget available; K = 5 appears to be a reasonable choice. The Impact of Dataset Size. Finally, we investigate to which extent the proposed pipeline is nec- essary when the dataset size is sufﬁciently large. We use the Sepsis-POMDP domain with 200, 1000, and 5000 patients. We show the best policies that are most frequently selected by our approach in Fig- ure 4(b). Unsurprisingly, policies trained on larger datasets perform better. In the 200-patient dataset, having SSR-RRS 5-split is crucial in picking the best policy, as most policies perform quite poorly. The gap between different approaches becomes smaller with 1000 patients, and even smaller when there are 5000 patients in the dataset. However, it is worth noting that even in the large dataset regime (N=5000), SSR-RRS still outperforms the One-Split OPE method in selecting the best algorithm. Additional Analysis. Our method SSR-RRS can also be used to select hyperparameters for a single algorithm, as we demonstrate in Appendix A.9. One might also wonder how sensitive is SSR- RRS pipeline to the choice of OPE method used inside the pipeline. OPE methods are known to signiﬁcantly vary in accuracy for different domains, and unsurprisingly, using a reasonable OPE method for the domain is important (see Appendix A.8). Note that the OPE estimators we use in our results are very popular ones, and it is possible to use standard approaches, though additional beneﬁts may come from using even better OPE methods. Finally, related to this question, one might wonder if particular OPE methods might be biased towards certain OPL algorithms which make similar assumptions (such as assuming a Markov structure): interestingly in preliminary experiments, FQE estimators did not seem to give FQI algorithms higher performance estimations (see Appendix A.10). 8 Discussion and Conclusion We presented SSR, a pipeline for training, comparing, selecting and deploying ofﬂine RL policies in a small data regime. The approach performs automated AH selection with a robust hyperparameter 9evaluation process using repeated random sub-sampling. SSR allows to consistently and reliably deploy best-performing policies thanks to jointly avoiding overﬁtting on a single dataset split and being data efﬁcient in re-using the whole dataset for ﬁnal training. We prove that a single split has a high failure rate of discovering the optimal AH because of reward sparsity. We have demonstrated its strong empirical performance across multiple and various challenging domains, including real-world applications where AH tuning cannot be performed online. There exist many interesting areas for future work. The proposed ofﬂine RL pipeline assumes the user/practitioner has selected a particular OPE method. OPE is an important subarea of its own and different approaches have different bias/variance tradeoffs. Recent work on automated model selection algorithms for OPE (Su et al., 2020; Lee et al., 2021) are a promising approach for producing good internal estimators. A second issue is that while our approach aims to produce a high-performing policy, it does not also produce an accurate estimate of this policy since the entire dataset is used at the end for training. An interesting issue is whether cross-splitting (Chernozhukov et al., 2016) or other methods could be used to compute reliable estimators as well as perform policy optimization. 9 Acknowledgment Research reported in this paper was supported in part by a Hoffman-Yee grant, NSF grant #2112926 and the DEVCOM Army Research Laboratory under Cooperative Agreement W911NF-17-2-0196 (ARL IoBT CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the Army Research Laboratory or the U.S.Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. We would like to thank Jonathan N. Lee, Henry Zhu, Matthew Jorke, Tong Mu, Scott Fleming, and Eric Zelikman for discussions. References Bassen, J., Balaji, B., Schaarschmidt, M., Thille, C., Painter, J., Zimmaro, D., Games, A., Fast, E., and Mitchell, J. C. (2020). Reinforcement learning for the adaptive scheduling of educational activities. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–12. Burman, P. (1989). A comparative study of ordinary cross-validation, v-fold cross-validation and the repeated learning-testing methods. Biometrika, 76(3):503–514. Cheng, C.-A., Kolobov, A., and Swaminathan, A. (2021). Heuristic-guided reinforcement learning. Advances in Neural Information Processing Systems, 34. Chernozhukov, V ., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W., and Robins, J. (2016). Double/debiased machine learning for treatment and causal parameters. arXiv preprint arXiv:1608.00060. Chernozhukov, V ., Demirer, M., Duﬂo, E., and Fernandez-Val, I. (2018). Generic machine learning inference on heterogeneous treatment effects in randomized experiments, with an application to immunization in india. Technical report, National Bureau of Economic Research. Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural computation, 10(7):1895–1923. Dubitzky, W., Granzow, M., and Berrar, D. P. (2007).Fundamentals of data mining in genomics and proteomics. Springer Science & Business Media. Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on cross-validation. Journal of the American statistical association, 78(382):316–331. Efron, B. (1986). How biased is the apparent error rate of a prediction rule? Journal of the American statistical Association, 81(394):461–470. Farajtabar, M., Chow, Y ., and Ghavamzadeh, M. (2018). More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1447–1456. PMLR. 10Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020). D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219. Fujimoto, S., Meger, D., and Precup, D. (2019). Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pages 2052–2062. PMLR. Futoma, J., Hughes, M. C., and Doshi-Velez, F. (2020). Popcorn: Partially observed prediction constrained reinforcement learning. arXiv preprint arXiv:2001.04032. Geisser, S. (1975). The predictive sample reuse method with applications. Journal of the American statistical Association, 70(350):320–328. Gilpin, A. R. (1993). Table for conversion of kendall’s tau to spearman’s rho within the context of measures of magnitude of effect for meta-analysis. Educational and psychological measurement, 53(1):87–92. Guo, Y ., Feng, S., Le Roux, N., Chi, E., Lee, H., and Chen, M. (2020). Batch reinforcement learning through continuation method. In International Conference on Learning Representations. Jiang, N., Kulesza, A., Singh, S., and Lewis, R. (2015). The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 1181–1189. Citeseer. Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020). Morel: Model-based ofﬂine reinforcement learning. Advances in neural information processing systems, 33:21810–21823. Komorowski, M., Celi, L. A., Badawi, O., Gordon, A. C., and Faisal, A. A. (2018). The artiﬁcial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine, 24(11):1716–1720. Krzanowski, W. and Hand, D. (1997). Assessing error rate estimators: the leave-one-out method reconsidered. Australian Journal of Statistics, 39(1):35–46. Kumar, A., Singh, A., Tian, S., Finn, C., and Levine, S. (2021). A workﬂow for ofﬂine model-free robotic reinforcement learning. In 5th Annual Conference on Robot Learning. Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for ofﬂine reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191. Le, H., V oloshin, C., and Yue, Y . (2019). Batch policy learning under constraints. InInternational Conference on Machine Learning, pages 3703–3712. PMLR. Lee, J. N., Tucker, G., Nachum, O., and Dai, B. (2021). Model selection in batch policy optimization. arXiv preprint arXiv:2112.12320. Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Ofﬂine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643. Liao, P., Greenewald, K., Klasnja, P., and Murphy, S. (2020). Personalized heartsteps: A reinforce- ment learning algorithm for optimizing physical activity. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1):1–22. Liu, Y ., Swaminathan, A., Agarwal, A., and Brunskill, E. (2019). Off-policy policy gradient with stationary distribution correction. In Globerson, A. and Silva, R., editors, Proceedings of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pages 1180–1190. AUAI Press. Liu, Y ., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020). Provably good batch reinforcement learning without great exploration. Advances in neural information processing systems, 33. Mandlekar, A., Ramos, F., Boots, B., Savarese, S., Fei-Fei, L., Garg, A., and Fox, D. (2020). Iris: Implicit reinforcement without interaction at scale for learning control from ofﬂine robot manipulation data. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 4414–4420. IEEE. 11Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y ., and Martín-Martín, R. (2021). What matters in learning from ofﬂine human demonstrations for robot manipulation. In 5th Annual Conference on Robot Learning. Mandlekar, A., Zhu, Y ., Garg, A., Booher, J., Spero, M., Tung, A., Gao, J., Emmons, J., Gupta, A., Orbay, E., et al. (2018). Roboturk: A crowdsourcing platform for robotic skill learning through imitation. In Conference on Robot Learning, pages 879–893. PMLR. Mazoure, B., Mineiro, P., Srinath, P., Sedeh, R. S., Precup, D., and Swaminathan, A. (2021). Improving long-term metrics in recommendation systems using short-horizon reinforcement learning. arXiv preprint arXiv:2106.00589. Metelli, A. M., Papini, M., Faccio, F., and Restelli, M. (2018). Policy optimization via importance sampling. Advances in Neural Information Processing Systems, 31. Miyaguchi, K. (2022). A theoretical framework of almost hyperparameter-free hyperparameter selection methods for ofﬂine policy evaluation. arXiv preprint arXiv:2201.02300. Oberst, M. and Sontag, D. (2019). Counterfactual off-policy evaluation with gumbel-max structural causal models. In International Conference on Machine Learning, pages 4881–4890. PMLR. Owen, A. B. (2013). Monte carlo theory, methods and examples. Paine, T. L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A., Wang, Z., and de Fre- itas, N. (2020). Hyperparameter selection for ofﬂine reinforcement learning. arXiv preprint arXiv:2007.09055. Pomerleau, D. A. (1991). Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural computation, 3(1):88–97. Precup, D. (2000). Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80. Shi, C., Wan, R., Song, R., Lu, W., and Leng, L. (2020). Does the markov decision process ﬁt the data: Testing for the markov property in sequential decision making. In International Conference on Machine Learning, pages 8807–8817. PMLR. Siegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020). Keep doing what worked: Behavior modelling priors for ofﬂine reinforcement learning. In International Conference on Learning Representations. Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. Journal of the royal statistical society: Series B (Methodological), 36(2):111–133. Su, Y ., Srinath, P., and Krishnamurthy, A. (2020). Adaptive estimator selection for off-policy evaluation. In International Conference on Machine Learning, pages 9196–9205. PMLR. Tang, S. and Wiens, J. (2021). Model selection for ofﬂine reinforcement learning: Practical con- siderations for healthcare settings. In Machine Learning for Healthcare Conference, pages 2–35. PMLR. Thomas, P. and Brunskill, E. (2016). Data-efﬁcient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139–2148. PMLR. Thomas, P., Theocharous, G., and Ghavamzadeh, M. (2015a). High conﬁdence policy improvement. In Bach, F. and Blei, D., editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2380–2388, Lille, France. PMLR. Thomas, P., Theocharous, G., and Ghavamzadeh, M. (2015b). High conﬁdence policy improvement. In International Conference on Machine Learning, pages 2380–2388. PMLR. Thomas, P. S., Castro da Silva, B., Barto, A. G., Giguere, S., Brun, Y ., and Brunskill, E. (2019). Preventing undesirable behavior of intelligent machines. Science, 366(6468):999–1004. 12V oloshin, C., Le, H. M., Jiang, N., and Yue, Y . (2021). Empirical study of off-policy policy evaluation for reinforcement learning. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). Xie, T. and Jiang, N. (2021). Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404–11413. PMLR. Yin, M., Bai, Y ., and Wang, Y .-X. (2021). Near-optimal ofﬂine reinforcement learning via double variance reduction. Advances in neural information processing systems, 34. Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y ., Levine, S., Finn, C., and Ma, T. (2020). Mopo: Model-based ofﬂine policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142. Zanette, A. and Brunskill, E. (2019). Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304–7312. PMLR. Zhang, S. and Jiang, N. (2021). Towards hyperparameter-free policy selection for ofﬂine reinforce- ment learning. Advances in Neural Information Processing Systems, 34. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 7 and Section 8. (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 4.1 and Section A.3. (b) Did you include complete proofs of all theoretical results? [Yes] See Section 4.1 and Section A.3. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main exper- imental results (either in the supplemental material or as a URL)? [Yes] See Supple- mentary Material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Sections 6, A.14, A.16, A.17, A.18 and A.19 (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] Open-source (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Appendix A.1 Prelude Experiment In this section, we put ourselves in a situation where model selection would be performed by comparing different AH pairs on their internal objective or value function estimates on a given dataset, as described near the beginning of Section 1. We use three datasets of different qualities (random, medium, and medium-expert) of the popular Hopper task from the D4RL benchmark (see Appendix A.14 for a detailed description) to train a total of 36 policies with different AH pairs and then calculate the resulting TD-Errors and Q-values on the whole dataset at the end of training. To evaluate the performance one would obtain by employing such an approach to select the best policy, we report in Table A.1 the performance (true return in the environment) of the selected policies and compare them with the performance of the optimal policy for each of the datasets. The policies are selected either by ﬁnding the one which corresponds to the lowest TD-Error, or the one which corresponds to the highest Q-value. We also include the Kendall rank correlation coefﬁcient (Gilpin, 1993) for each of the ranking methods (ranking with respect to TD-Error or Q-value) compared with the “true ranking” of policies ranked with respect to the performance in the environment: τ = ( number of concordant pairs ) −( number of discordant pairs )( n 2 ) where n is the number of policies, and where “concordant pairs” are pairs from the two compared rankings for which the sort order agrees. A coefﬁcient of 1 means the agreement between the two rankings is perfect. TD-Error Q-value Policy Selected (True Return) Kendall Policy Selected (True Return) Kendall Optimal Policy (True Return) random 334.24 -0.09 333.65 -0.15 345.39 medium 1475.82 0.42 2381.37 0.21 2469.81 medium-expert 327.97 -0.18 327.97 -0.09 3657.80 Table A.1: Average Return (True Return obtained in the simulator) of the policy selected with respect to min(TD-Error) or max(Q-value) on the training dataset with a comparison to the True Return obtained by the Optimal Policy. Kendall rank correlation coefﬁcient when ranking with respect to the same metrics. Policies are trained and validated on the same dataset. Task: Hopper. Unsurprisingly, Table A.1 shows that one cannot rely on this straightforward pipeline to select a best-performing AH pair. Actually, for most of the datasets (the medium-expert dataset should resemble the most to what a dataset would look like in a real-world situation as it is composed of both high-quality and medium-quality data), following such an approach would produce and deploy a very bad performing policy. A.2 Connection between Leave-p-Out CV and RRS Our RSS is a ﬁnite approximation of Leave-p-out (Lp0) cross-validation1. LpO is known in supervised learning, but rarely used due to the computational burden. The correctness of LpO is proved itecelisse2014optimal in a supervised learning setting with projection estimators. Unlike K-fold cross-validation, Leave-p-out CV selects pdata points for evaluation and the rest for training. In our proposed RSS method, we set p = n/2, and instead of exhaustively enumerating all possible selections of pdata points out of n data points, we only repeat this process Ktimes. Asymptotically as the amount of data goes to inﬁnity, this approach should be correct, but also a single train/test split will also be correct in such a setting. The key challenges arise in the ﬁnite data setting, where the choice of dataset partitioning is key. 1https://scikit-learn.org/stable/modules/generated/sklearn.model_selection. LeavePOut.html 15A.3 Proof of Theorem 1 Consider a ﬁnite set ofJofﬂine RL algorithmsA. Let the policy produced by algorithmAj on training dataset Dbe πj, its estimated performance on a validation set ˆVπj, and its true (unknown) value be Vπj.Denote the true best resulting policy as πj∗ = arg maxjVπj and the corresponding algorithm Aj∗. Let the best policy picked based on its validation set performance as πˆj∗ = arg maxj ˆVπj and the corresponding algorithm Aˆj∗. Theorem 1. Then there exist stochastic decision processes and datasets such that (i) using a single train/validation split procedure will select a suboptimal policy and algorithm with signiﬁcant ﬁnite probability, P(πˆj∗ ̸= πj∗) ≥C, with corresponding substantial loss in performance O(Vmax), and, in contrast, (ii) averaging across Ns train/validation splits will select the optimal policy with probability 1: limNs→∞P(πˆj∗ = πj∗) →1. Proof. We proceed by constructing a stochastic decision process. A common domain to illustrate the importance of strategic exploration is a chain MDP. Here consider an episodic, ﬁnite horizon, ﬁnite chain, deterministic decision process with 6 states, s1,...,s H, (H = 6) with two actions. a1 moves the state one down except for at the starting state, anda2 increments the state one up except for the ﬁnal state: more formally, p(si−1|si,a1) = 1 except for p(s1|s1,a1) = 1; p(si+1|si,a2) except for p(sH|sH,a2) = 1 . The reward is 0 in all states except R(s1) = 1 /6 and R(sH) = 201 . All episodes are length H = 6 and start in state s1. The optimal policy always takes a2 and achieves Vmax = R(sH).Any other policy achieves at most H∗1/6 = 1 reward. The behavior policy is uniform random over the two actions, πb(a1|s) = 0 .5 = πb(a2).Let the available ofﬂine dataset D consist of 200 episodes gathered using πb.Given the behavior policy, each of the 64= 2H unique trajectories has an equal probability of being observed, and only one of these τh = (s1,0,a2,s2,0,a2,,s 3,0,a2,s4,0,a2,s5,a2,sH,R(sH)) achieves the highest return. On average out of 200 episodes 2, nτh=3(=round(|D|/(2H))) episodes will match τh. All other episodes will have a return of 1 or less. Let there be a set of H ofﬂine RL algorithms Ah, each which optimizes the reward over a different horizon h= 1 : H, by constructing a maximum-likelihood estimate (MLE) MDP model Mgiven a training dataset Dtr, and then computing a policy πh that optimizes the h-step value given the learned MDP Mmodel3 For example, algorithm A2 will take the MLE MDP model and construct a policy to optimize the sum over rewards for the next two time steps π2(s) = arg max ar(s) +∑ s′p(s′|s,a)r(s′). We think this is a reasonable set of algorithms to consider as an illustrative example: the horizon length can directly inﬂuence the amount of data needed to compute an optimal policy, and recent work has explored using shorter horizons (Cheng et al., 2021; Mazoure et al., 2021; Liao et al., 2020), so choosing the right horizon can be viewed as a bias/variance tradeoff, suitable for automatic model selection. Observe that even if given access to the true (unknown) MDP parameters, algorithms A1,..., AH−1 will compute a policy that is suboptimal: due to the shortened horizon length, to optimize the expected total reward, the resulting policy computed for s1 will be πh(s1) = π1(s1) = a1 for these algorithms Ah, h= 1 : H−1. As the MDP is deterministic, this will also be true for any input dataset. We now consider the impacts of partitioning the input dataset into a training datasetDtr taken as input by each algorithm Ah to compute a policy πˆh, and an evaluation/test dataset Dte: D= Dtr ∪Dte. For algorithm AH to learn the optimal policy π∗which achieves Vmax, it must learn over a dataset Dtr that includes one or more examples of the highest return trajectory τh. Note that a single episode of τh in the training set is sufﬁcient to learn the optimal policy4. 2Our calculations can easily be extended to cases where there are different numbers of observed τh, but for simplicity we assume a dataset where the average expected number of τh are observed. 3During planning with the learned MDP model, we restrict taking the maximum value over actions for a given state sto only actions that have been taken at least once in that state in the dataset, e.g. maxa s.t. n(s,a)≥1, where n(s,a) is the counts of the number of times action awas taken in state sin the dataset. Note that in a ﬁnite dataset, some states and/or actions may not be observed, and this common choice simply ensures that the algorithm does not overestimate the value of untried actions. 4A single example of τh will induce a MLE ˆM with the correct reward model for all states, and the dynamics model for action a2. From the procedure used to compute an optimal policy ˆM, this will result in an optimal policy. 16Assume that the ofﬂine evaluation of the policies learned by the algorithm on Dtr is performed using importance sampling on Dte: note, our results will still apply, with minor modiﬁcations, if off policy evaluation is performed on Dte using ﬁtted Q evaluation (Le et al., 2019) or using a certainty-equivalent MDP constructed from Dte. Then the off policy evaluation of the policy learned by the full horizon algorithm AH, ˆVπH(s1), will only be greater than 1 if there also exists at least one episode of the highest return trajectory τh. Assume the training dataset and validation dataset are constructed by randomly sampling 50% of the episodes to be in each. By assumption, there are nτh samples of τh, which have an equal chance of being in either the training or validation set. There are nτh + 1 ways to partition the nτh exchangable episodes of τh into the training and validation sets, here ([3,0],[2,1],[1,2],[0,3]). Note the training and validation set are identical in size ( |D|/2 trajectories each), and we only care about whether a trajectory τ is identical to τh or not. The probability that each of these partitions occurs is : P([3,0]) = P([0,3]) = 100 200 ∗ 99 199 ∗ 98 198 ≈0.123. From the above analysis, AH can only learn an optimal policy, and its estimated value ˆVπ5 > 1 on Dte if there is at least one τh in both the training and validation set datasets, which occurs in partitions ([2,1],[1,2]). This occurs with probability 0.754. Otherwise, either (a) AH will not learn an optimal policy, and instead will learn πH(s1) = π1(s1) = a1, or (b) AH will learn an optimal policy πH(s1) = a2 but as the validation dataset does not contain τh, ˆVπH = 1/H < ˆVπ1 . In both cases, the selected policy given its performance on the validation set will be π1(s1) = a1. The resulting loss in performance is Vmax −Vπ1 = Vmax −1 = O(Vmax).This failure occurs with substantial probability 24.6%. This proves part (i) of the theorem. To prove part (ii) of the proof, we consider cases where at least one τh is in both Dtr and Dte. Note ˆVπ1 ≤R(s1) 1/2H = 1 1/2H. Deﬁne Ess as a \"successful split\": the event that 1 or more ofτh (high returns) episodes are in Dte, but not all nτh. On event Ess, the optimal policy (which will be computed by AH on the training set), will have an estimated value on Dte, using importance sampling: ˆVπ∗ ss ≥ 1 |Dte| R(sH) 1/2H = 1 1/2H ∗201 100 >2 ˆVπ1 (2) since there are at least 1 τh trajectories, each with propensity weight 1 1/2H and reward R(sH). Therefore on Event Ess the optimal policy can be learned and estimated as having high reward. The probability of event Ess is greater than 0.5: P(Ess) = 0.754. In the repeated train-validation split setting, the algorithm selected is the one that has the best performance on the validation set, on average across all Ns splits. Let Eh be the event that at least half the train-validation dataset splits are successful (Event Ess holds for that split). In this case then the average performance of A5 will be at least ˆVA5 ≥ 1 Ns (Ns 2 ˆVπ∗ ss + 0 ) ≥ 1 Ns (Ns 2 2 ˆVπ1 + 0 ) = ˆVπ1 , where the ﬁrst line uses a lower bound of 0 when the event Ess fails to hold, and substitutes in Equation 2. Therefore as long as event Eh holds, the optimal policy π∗(which will be computed by algorithm AH will be selected. Since P(Ess) >0.5, the probability5 as the number of splits goes to inﬁnity that Ess holds on least half of those splits goes to 1: limNs→∞P(Eh) →1. 5**calculate for ﬁnite S. 17A.4 SSR pseudo-code Algorithm 1 SSR-RRS: Ai Selection with Repeated Random Sub-sampling Input: ofﬂine RL data D; set of AH pairs [A1,A2,..., Az], OPE estimator ˆV, split number K ∈N. Output: policy ˆπ∗for deployment R= ∅ for i←1...Kdo Rtrain i ,Rvalid i = Subsample (D,0.5) R= R∪(Rtrain i ,Rvalid i ) end G= [] for i←1...zdo S= [] for j ←1...Kdo πi = A(Rtrain j ) Sij = ˆV(πi; Rvalid j ) end Gi = 1 K ∑K j=1 Sij end A∗= Ao+ where o= arg max(G) π∗= A∗(D) return π∗ A.5 Code We include the implementation and experiment code here: https://github.com/ StanfordAI4HI/Split-select-retrain A.6 Experiment Detail Summary We choose different sets of algorithms to evaluate our pipeline in every domain to demonstrate the generality of our approach and because some algorithms have limitations inherent to certain types of domains to which they can be applied. We list them in Table A.2. Running a large number of algorithm-hyperparameter pairs many times is very computationally expensive. In order to save time and resources, we leverage the fact that multiple approaches can share resources. We describe how we compute the numbers for each approach as follows: For each ofﬂine RL dataset in Sepsis, TutorBot, Robomimic, and D4RL, we produce the following partitions (we refer to this as the “partition generation procedure”): 1. 2-fold CV split (2 partitions consisted of (Si)) 2. 5-fold CV split (5 partitions consisted of (Si)) 3. 5 RRS split (5 partitions consisted of (Rtrain i ,Rvalid i )) Here, we brieﬂy describe how to use these data partitions to select algorithms with alternative approaches. One-Split OPE. The One-Split OPE method can be conducted to train and evaluate an algorithm on any of the RRS splits being produced, but only look at one split, without considering other splits. We let for a particular i, we let Dtrain = Rtraini and Dvalid = Rvalid i . BCa Bootstrap. Similar to the One-Split OPE method, we can use RRS split for bootstrap. For a particular i, we let Dtrain = Rtraini and Dvalid = Rvalid i . Bootstrapping will re-sample with 18Experiment Domain Number of Trajectories (N) Average Trajectory Length Number of Transitions in Total AH Pairs Evaluated Algorithms in Experiment Sepsis-POMDP 200 14 2792 540 BC, POIS, BC+POIS, BC+mini-POIS, BCQ, MBSQI, pMDP, MOPO Sepsis-POMDP 1000 14 13708 540 BC, POIS, BC+POIS, BC+mini-POIS, BCQ, MBSQI, pMDP, MOPO Sepsis-POMDP 5000 14 68576 148 BC, POIS, BCQ, MBS-QI, pMDP, MOPO Sepsis-MDP 200 14 2792 383 BC, BCQ, MBSQI, pMDP, POIS, BC + POIS BC+mini-POIS TutorBot 200 5 987 81 BC, POIS, BC+POIS, BC+mini-POIS Robomimic Can-Paired 200 235 47,000 35 BC, BCRNN, CQL, IRIS, BCQ Robomimic Transport 200 470 94,000 10 BC, BCRNN, CQL, IRIS, BCQ D4RL Hopper 500 1000 500,000 4 x 4 BCQ D4RL HalfCheetah 500 1000 500,000 4 x 4 BCQ Table A.2: List of algorithms being used in which domain. 4 x 4 means we evaluate 4 AH pairs for the policy learning and 4 AH pairs for the policy evaluation estimators (FQE). replacement on trajectories in Dvalid to create (largely) overlapping subsets B1,B2,...,B N, with |Bi|= n. We then evaluate πe on each subset using ˆV. The ﬁnal score is computed through a bias correction process with an added acceleration factor (BCa). Nested K ×2-fold Cross-Validation. We can also use the RRS split partitions to produce K×2 Nested CV by taking one RRS split (Rtrain i ,Rvalid i ) by doing the following procedure: si = ˆV(A(Rtrain i ); Rvalid i ) + ˆV(A(Rvalid i ); Rtrain i ) 2 (3) GA,NCVK = 1 K K∑ i=1 si (4) Intuitively, for K×2 Nested CV , we just need to swap the train and valid set produced by repeated sub-sampling and average to produce the algorithm performance score for a particular split i. Then we average the scores to get a ﬁnal score for the algorithm. 2-fold Cross-Validation. Similar to theK×2 Nested CV , we can choose thei-th partition generated by the 10 RRS split procedure, and compute the score according to Equation 3. We do this for the Sepsis and TutorBot domains, but we do not do this for the Robomimic domain. Batch Value Function Tournament (BVFT) Xie and Jiang (2021); Zhang and Jiang (2021) proposed to use pairwise Q-function comparisons to select the optimal Q-function from a set of Q-functions. Given Qi,Qj, let Gij be the piecewise constant function class induced by binning (s,a) and (s′,a′) if Qi(s,a) = Qj(s′,a′). Given an ofﬂine dataset D, we can compute the BVFT loss as 19follow: ˆTGijQ:= arg min g∈Gij 1 |D| ∑ [(g(s,a) −r−γmax a′ Q(s′,a′))2] (5) Eϵk(Qi,Qj) = ∥Qi −ˆTGijQj∥2,D (6) Eϵk(Qi) = max j Eϵk(Qi,Qj) (7) Zhang and Jiang (2021) proposed a method to automatically search through different discretization resolutions (ϵk). In our experiment, we search through [0.1,0.2,0.5,0.7,1.0,3.0,10.0]. We use the BVFT code provided by Xie and Jiang (2021). Because BVFT can only compare Q-functions, Zhang and Jiang (2021) offered two strategies to perform policy selection for any model/algorithm. Here we brieﬂy describe two strategies: • Strategy 1 (πx FQE): if we have 4 policies, and each policy is evaluated by 4 FQEs, then this strategy will compare 16 Q-functions (4 πx 4 FQE). • Strategy 2 (π+ FQE): if we have 4 policies, and each policy is evaluated by 4 FQEs, then this strategy will ﬁrst run BVFT to compare 4 Q-functions (1 πx 4 FQE), select the best Q-function for each π(4 πx 1 FQE), then we select the best policy by the average Q-value computed by each FQE. We generally ﬁnd strategy 2 more computationally efﬁcient (because it makes a smaller number of comparisons). BVFT generally has O(J2) time complexity where J is the number of Q-functions that need to be compared – it’s easy to see that162 = 256 is much larger than 42 = 16. Our repeated experiment protocol (RRS) is reliant on choosing a good FQE. In order to compare fairly, for π x FQE strategy, we only use the optimal FQE (the ones used in RRS and CV and one-split). We can see that in this condition, BVFT can do pretty well (even outperforming RRS in the D4RL-Hopper medium setting). For π+ FQE, because it focuses on the selection of FQE, we try 4 different FQE hyperparameters. We discuss this more in D4RL Experiment Details (in Section A.19). A.7 Computational Complexity Most of the approaches we discussed in Section A.6 leverage multiple repetitions (resampling) to account for data allocation randomness. We provide a time complexity table below and deﬁne the following terms: • H = number of AH pairs to evaluate • N = total data samples. We assume the training time for each trajectory is N1 and evaluation time for each trajectory is N2, where N = N1 + N2 • M = number of folds in multi-fold cross-validation • B = number of bootstraps (this number is 100 in our experiment) • P = number of resolutions for BVFT’s grid (proposed in Zhang and Jiang (2021)) • F = number of FQE hyperparameters (proposed in Zhang and Jiang (2021)) For BVFT, one can amortize the computational cost by caching (storing Q(s,a) for all (s,a) in the dataset). If caching is done only once, we treat the actual computation time for the validation data set as n2. P is usually between 5 and 10. When H is relatively large, for example, H = 540 (in our experiment), H * H = 2.916e5. It’s easy to see that RRS is slightly more expensive than M-Fold CV but less expensive than the pairwise comparison tournament algorithm (BVFT). Zhang and Jiang (2021) proposed BVFT-FQE that only makes pairwise tournament comparison between FQE hyperparameters – F is 5 in our experiments. It’s also worth noting that BCa has a high evaluation cost when B is large – when B = 100, BCa evaluation cost is signiﬁcantly higher than CV and RRS. A.8 Sensitivity to OPE Methods OPE is often a critical part of OPL, which has motivated signiﬁcant research into OPE. Thus the employed OPE method will likely impact the performance of our proposed pipeline. As has been 20Training Complexity Evaluation Complexity One-Split H ×N1 H ×N2 Bootstrapping (BCa) H ×N1 H ×B ×N2 M-Fold Cross-Validation (H ×M ×N×(M-1))/M = H ×N ×(M-1) (H ×M ×N ×1)/M = H ×N K-Repeat RRS H ×K ×N1 H ×K ×N2 BVFT (Xie and Jiang, 2021) H ×N1 (H ×H) ×N2 or (H ×H) ×n2 BVFT-auto (Zhang and Jiang, 2021) H ×N1 P ×(H ×H) ×N2 or P ×(H ×H) ×n2 BVFT-FQE (Zhang and Jiang, 2021) H ×N1 P ×H ×(F ×F) ×N2 or P ×H ×(F ×F) ×n2 demonstrated in a recent bake-off paper (V oloshin et al., 2021), minimal-assumption OPE methods like weighted doubly robust methods (e.g. Jiang et al. (2015); Thomas and Brunskill (2016)) may be most consistently accurate for many domains. However if the domain is known to be Markov and the models are well speciﬁed, FQE methods will likely be more accurate in small data regimes. To explore further the impact of the choice of OPE method, we conducted an additional experiment on the Sepsis-POMDP domain. The aim to was to look at the sensitivity of SSR-RRS for picking the best AH to the choice of OPE estimators. In addition to the prior OPE methods used in the main text, we included clipped IS (importance sampling), CWPDIS (Thomas and Brunskill, 2016), and 8 different FQE OPE variants, in which different networks, learning rate and epochs were used. Sepsis-POMDP Parameters Best AH Performance Chosen by SSR-RRS K=5 FQE-1 [64], lr=3e-4, epoch=20 2.84 FQE-2 [64], lr=1e-5, epoch=20 -74.26 FQE-3 [64], lr=3e-4, epoch=50 -20.88 FQE-4 [64], lr=1e-5, epoch=50 -14.16 FQE-5 [128], lr=3e-4, epoch=20 -75.26 FQE-6 [128], lr=1e-5, epoch=20 -14.48 FQE-7 [128], lr=3e-4, epoch=50 -75.54 FQE-8 [128], lr=1e-5, epoch=50 -74.26 IS N/A 4.47 CWPDIS N/A 4.68 WIS N/A 6.75 Table A.3: Using different OPE estimators in the SSR-RRS pipeline. FQE-1 denotes the FQE with the optimal FQE hyperparameter (heuristically chosen). First, using FQE does generally much worse in this setting which is not very surprizing: FQE assumes the domain is Markov, which Sepsis-POMDP is not. All importance-sampling based OPE methods yield quite similar performing algorithm- hyperparameter choices in this setting. While there are some clear differences, if some basic information about the domain is known (Markov or not), it is likely possible to select a pretty good OPE. In addition, prior work has proposed heuristics (V oloshin et al., 2021) or automatic methods for automatic OPE selection (Su et al., 2020; Lee et al., 2021). An interesting direction for future work would be to include such methods in the pipeline. 21We highlight that while it is well known that OPE methods are important, our paper focused on an under-explored issue: that the dataset partitioning can also introduce a substantial amount of additional impact on learning good policies / selecting good AH. A.9 Robustness of SSR-RRS In Table 3, we only show the performance of the best policy among all AH pairs. Here we show that SSR-RRS can still robustly select a good hyperparameter for a given ofﬂine RL policy learning algorithm (the gap between best AH selected and true best AH is relatively small). Sepsis-POMDP Range of True Policy Performance (95%CI) Percentile of AH Chosen by SSR-RRS Performance of AH Chosen by SSR-RRS True Best AH Performance BCQ [-10.8, -0.73] 94% 5.98 7.86 MBSQI [-7.34, -2.26] 95% 6.40 7.42 BC [-8.98, -8.37] 58% -8.46 -7.42 BC+PG [-5.55, -4.26] 78% -3.68 2.52 P-MDP [-31.17, -21.26] 83% 0.23 2.82 Table A.4: We show the relative position (percentile) of theAH selected by SSR-RRS K=5 pipeline. For each algorithm, we evaluate over 24 to 72 hyperparameters, and we compute the 95% conﬁ- dence interval of all these policies’ true performance. Except for behavior cloning, we are picking hyperparameters that are out-performing 78%-95% of other hyperparameters in the same algorithm. A.10 Is FQE biased towards FQI algorithms? In our evaluation on the Sepsis domain, FQE is used to evaluate both BCQ and MBSQI (both FQI-based) and BC and BCPG (policy-gradient algorithms). We designed the following analysis experiment using our logged results. We ﬁrst rank all AH pairs (540 of them) with their true performance in the simulator, and then we count the percentage of FQI (BCQ, MBSQI) algorithms that appear in the top 10%, 20%, and 50% percentile. The number in each cell should be read as: “90.7% of AH pairs in the top-10% based on True Performance are FQI-based”. If FQE is biased towards FQI algorithms, we expect to see a higher percentage of BCQ and MBSQI AH pairs selected than the true performance baseline and compared to other OPE methods. Sepsis-POMDP OPE Method % of BCQ and MBSQI AHs in Top-10% AHs % of BCQ and MBSQI AHs in Top-20% AHs True Performance 90.7% 61.1% FQE-1 0% 0% WIS 9.4% 35.5% RRS-5 WIS 68.5% 58.3% Table A.5: Examining whether FQE as an estimator will prefer FQI policy learning algorithms. Based on this analysis, we believe that FQE is not biased to select FQI-based algorithms in the Sepsis-POMDP domain. However, our analysis is limited to one domain and only on two FQI-based algorithms. Further investigation is needed but beyond the scope of our paper. A.11 Additional Discussions Sensitivity to K in small and large datasets In general, we expect the issue of data partitioning into a train and test split is most important in small datasets: as the dataset gets very large, a single train/test split will generally work well. Therefore, we suggest using a larger K for smaller datasets, but for larger datasets, a smaller K will likely be sufﬁcient. Using our theoretical example in the 22appendix (chain-MDP), this can also be observed – with a larger N, the failure probability for smaller numbers of repeats decreases. This N-K tradeoff has computational beneﬁts if there is a limited computational budget (larger datasets will require more training, therefore, harder to use a larger K). Weighted importance sampling (WIS) as a biased estimator WIS is a self-normalizing impor- tance sampling estimator. We refer readers to Owen (2013) Chapter 9 for a more detailed discussion on the statistical properties of this type of estimator. In Section 4.1 (line 174), we state: WIS will return the observed return of the behavior policy if averaging over a single trajectory, independent of the target policy to be evaluated. In brief, WIS works by ﬁrst computing the probability of the dataset trajectory appearing under the evaluation policy and behavior policy: wi = L∏ t=1 πe(at|st) πb(at|st) Then, this coefﬁcient is normalized before multiplying with the trajectory return, therefore: WIS(D) = 1 n n∑ i=1 wi∑n j=1 wj ( L∑ t=1 γtRi t). Perhaps surprisingly, if there is a single trajectory,n= 1, this implies WIS(D) = wi wi ( L∑ t=1 γtRi t) = L∑ t=1 γtRi t. Here WIS is a biased estimator that returns the trajectory weighted reward, independent of wi. A.12 Additional Experiment We report the D4RL HalfCheetah result over the same setting as D4RL Hopper, where the result is averaged over 20 runs. Re-trained on full dataset BVFT πx FQE BVFT π+ FQE CV-2 CV-5 SSR RRS-2 SSR RRS-5 Optimal Policy D4RL (HalfCheetah): random -1.14 1106.94 -1.13 -1.13 1922.07 1922.07 1922.07 medium 4421.95 4290.33 4290.33 4290.33 4290.33 4290.33 4517.96 medium-expert 8118.84 8799.66 8118.84 8118.84 9681.78 9681.78 10364.36 Table A.6: Additional comparison of the performance obtained by a policy deployed using the SSR pipeline vs. using 1-split policy selection approaches on D4RL HalfCheetah. Cells = average true return. A.13 Figure Generation Procedure Given our partition generation procedure, there are some methods (One-Split OPE,K×2 Nested CV , and SSR-RRS Kwhen K <5) that have a few different partitions to choose from. For example, out of the 5 RRS split partitions, which partition should we choose for the One-Split OPE method? If we choose one partition, and the One-Split method cannot select the best algorithm, does that mean the One-Split method is bad, or could the 9 other partitions do better for the One-Split method? In order to evaluate these approaches fairly, we exhaustively train and evaluate on the 5 RRS splits, swap the train/valid set, and train/evaluate on them again, generating 20 scores. For the aforementioned methods, we randomly sample from these 10 (or 20, if Nested CV is being evaluated) scores to simulate the setting that we happen to get one particular split. We run this sampling procedure multiple times and compute the average performance of the policies that are chosen by conditioning on one or Kparticular partitions. 23A.14 Domain Descriptions Sepsis. The ﬁrst domain is based on the simulator and works by Oberst and Sontag (2019) and revolves around treating sepsis patients. The goal of the policy for this simulator is to discharge patients from the hospital. There are three treatments the policy can choose from antibiotics, vasopressors, and mechanical ventilation. The policy can choose multiple treatments at the same time or no treatment at all, creating 8 different unique actions. The simulator models patients as a combination of four vital signs: heart rate, blood pressure, oxygen concentration and glucose levels, all with discrete states (for example, for heart rate low, normal and high). There is a latent variable called diabetes that is present with a20% probability which drives the likelihood of ﬂuctuating glucose levels. When a patient has at least 3 of the vital signs simultaneously out of the normal range, the patient dies. If all vital signs are within normal ranges and the treatments are all stopped, the patient is discharged. The reward function is +1 if a patient is discharged, −1 if a patient dies, and 0 otherwise. We follow the process described by Oberst and Sontag (2019) to marginalize an optimal policy’s action over 2 states: glucose level and whether the patient has diabetes. This creates the Sepsis- POMDP environment. We sample 200, 1000, and 5000 patients (trajectories) from Sepsis-POMDP environment with the optimal policy that has 5% chance of taking a random action. We also sample 200 trajectories from the original MDP using the same policy; we call this the Sepsis-MDP environment. Robomimic. Our approach is further evaluated on a third domain, Robomimic (Mandlekar et al., 2021), consisting of various continuous control robotics environments along with corresponding sets of suboptimal human data. More speciﬁcally, we use the Can-Paired dataset composed of mixed-quality human data. These 200 demonstrations include an equal combination of “good” (the can is picked up and placed in the correct bin) and “bad” trajectories (the can is picked up and thrown out of the robot workspace). The initializations of the tasks being identical, it is expected that algorithms dealing with suboptimal data will be able to ﬁlter out the good trajectories from the bad ones and achieve near-optimal performance. Interestingly, state-of-the-art batch RL algorithms do not reach maximum performance (Mandlekar et al., 2021), making this task a good testbed for our procedure. We also use the Transport dataset, where two robot arms must transfer an object from one bin to another. The dataset contains 200 successful trajectories collected by one human operator. D4RL. D4RL (Fu et al., 2020) is an ofﬂine RL standardized benchmark designed and commonly used to evaluate the progress of ofﬂine RL algorithms. We use 3 datasets of different quality from the Hopper task: hopper-random with 200k samples from a randomly initialized policy, hopper-medium with 200k samples from a policy trained to approximately 1/3 the performance of a policy trained to completion with SAC (\"expert\"), and hopper-medium-expert with 200k samples from a 50-50 split of medium and expert data. The Hopper task is to make a hopper with three joints, and four body parts hop forward as fast as possible. A.15 TutorBot Domain We introduce a new TutorBot simulator that is designed to mimic 3-5th grade elementary school children in understanding the concept of calculating volume, and engaging them while doing so. We base certain aspects of this simulator on some experimental studies of this learning environment, where an RL policy can learn to teach. The state space includes children’s pre-test score, anxiety level, thinking time, and whether it’s the last question in the tutoring session. The action is to offer encouragement, a guided prompt, or a hint at each step of the tutoring. The dynamics of TutorBot is a 4th-order Markov transition function that takes in anxiety and the amount of thinking time and updates a latent parameter that captures learning progress. For each simulated student learning trajectory, we pre-determine how many times this student will interact with the TutorBot. We denote this as T, which is the trajectory length. We calculated the relationship between T and the pre-test score based on the aforementioned experimental study. T = round(7 −0.46 ∗pre-test + l),l ∼U([−1,2]) θx = [0,−0.05,−0.2,−0.5],θh = [0.5,0.3,0.2,0] T(st+1|st,at) = [ pre-test,[st−3,st−2,st−1,st]θT x,[st−3,st−2,st−1,st]θT h,1 {t+ 1 = T} ] 24TutorBot Dimension Description State 4 Pre-test ∈{0,1,..., 8}, Anxiety-level ∈[−1,0] Thinking ∈[0,1]+, Pre-termination ∈{0,1} Action 1 0 = Encourage, 1 = Guided Prompt, 2 = Hint Reward 1 0 for all steps if not last step Table A.7: MDP speciﬁcation for TutorBot. The reward is always 0 at all steps except for the ﬁnal step. We use xto denote anxiety and hto denote thinking. Note that anxiety is always negative. We calculate the ﬁnal reward as follows: RT = 1 {U[0,1] <p}∗rimprov + (1 −1 {U[0,1] <p}) ∗rbase,p = x+ h Under this simulator, a student will improve a small amount even if the chatbot fails to teach optimally. rimprov ∼N(µimprov,1),rbase ∼N(µbase,0.4) We provide the full simulator code in the GitHub repo. A.16 Sepsis-POMDP and Sepsis-MDP Experiment Details Our algorithm-hyperparameter search is trying to be as realistic as possible to the setting of ofﬂine RL practitioners. We search over hyperparameters that could potentially have a strong inﬂuence on the downstream performance. Since this is an ofﬂine RL setting, we are particularly interested in searching over hyperparameters that have an inﬂuence on how pessimistic/conservative the algorithm should be. A.16.1 BCQ Batch Constrained Q-Learning (BCQ) is a commonly used algorithm for batch (ofﬂine) reinforcement learning (Fujimoto et al., 2019). We search over the following hyperparameters: BCQ Hyperparameter Range Actor/Critic network dimension [32, 64, 128] Training Epochs [15, 20, 25] BCQ Threshold δ [0.1, 0.3, 0.5] Table A.8: BCQ Hyperparams for Spesis-POMDP N=200, 1000. Sepsis-MDP N=200. TutorBot N=200. BCQ threshold determines if the Q-network can take the max over action to update its value using (s,a) – it can only update Q-function using (s,a) if µ(s) >δ and π(a|s) >0. The higher δ(BCQ threshold) is, the less data BCQ can learn from. δdetermines whether (s′,a′) ∈B. Q(s,a) ←(1 −α)Q(s,a) + α(r+ γ max a′s.t.(s′,a′)∈B Q′(s′,a′)) (8) We search through the cross-product of these, in total 27 combinations. For Sepsis-POMDP N=5000, we realize the network size is too small to ﬁt a relatively large dataset of 5000 patients. So we additionally search over Table A.9. The actor/critic network uses a 2-layer fully connected network. This resulted in 6 additional combinations for BCQ in Sepsis-POMDP N=5000. 25BCQ Hyperparameter Range Actor/Critic network dimension [256, 256], [512,512], [1024,1024] Training Epochs [25] V AE Latent Dim [512] BCQ Threshold δ [0.3, 0.4] Table A.9: BCQ Hyperparams for Spesis-POMDP N=5000. A.16.2 MBS-QI The MBS-QI algorithm is very similar to BCQ, but MBS-QI also clips the states (Liu et al., 2020). We searched through similar hyperparameters as BCQ. MBS-QI Hyperparameter Range Actor/Critic network dimension [32, 64, 128] Training Epochs [15, 20, 25] BCQ Threshold δ [0.1, 0.3, 0.5] Beta β [1.0, 2.0, 4.0] Table A.10: MBS-QI Hyperparams for Spesis-POMDP N=200, 1000. Sepsis-MDP N=200. TutorBot N=200. The beta (β) hyperparameter in MBS-QI is a threshold for the V AE model’s reconstruction loss. When the reconstruction loss of the next state is larger than beta, MBS-QI will not apply the Q function on this next state to compute future reward (to avoid function approximation over unfamiliar state space). ζ(s,a; ˆµ,b) = 1 (ˆµ(s,a) ≥β) ( ˜Tf)(s,a) := r(s,a) + γEs′[max a′ ζ◦f(s′,a′)] (9) We search through the cross-product of these, in total 81 combinations. Similar to BCQ situation, we realize the network size is too small to ﬁt a relatively large dataset of Sepsis-POMDP N=5000. So we additionally search over Table A.11. The actor/critic network uses a 2-layer fully connected network. This results in 18 additional combinations for MBS-QI in Sepsis-POMDP N=5000. A.16.3 MOPO We also experiment with Model-based Ofﬂine Policy Optimization (MOPO) (Yu et al., 2020). The original MOPO paper only experimented on Mujoco-based locomotion continuous control tasks. We want to experiment with whether MOPO can work well in environments like the Sepsis-POMDP simulator, which is not only a healthcare domain but also partially observable with a discrete state and action space. We do not expect MOPO to do well. We re-implemented two versions of MOPO 26MBS-QI Hyperparameter Range Actor/Critic network dimension [256, 256], [512,512], [1024,1024] Training Epochs [25] V AE Latent Dim [512] BCQ Threshold δ [0.3, 0.4] Beta ζ [1.0, 2.0, 4.0] Table A.11: MBS-QI Hyperparams for Spesis-POMDP N=5000. with Tensorﬂow 2.0 and PyTorch, and used the PyTorch version to run our experiments. Our implementation of MOPO matches the original’s performance in a toy environment. MOPO is fairly slow to run – because it needs ﬁrst to train a model to approximate the original environment, and then sample from this model to train an RL algorithm. We did not evaluate it for Sepsis-POMDP N=5000. MOPO Hyperparameter Range Actor/Critic network dimension dim [32, 32], [64, 64], [128, 128] Training Iterations [1000, 2000, 3000] MOPO Lambda λ [0, 0.1, 0.2] Number of Ensembles [3, 4, 5] Table A.12: MOPO hyperparameters for Spesis-POMDP N=200, 1000. Number of ensembles refers to MOPO Algorithm 2, which trains an ensemble of N probabilistic dynamics on batch data. N should be adjusted according to the dataset size. Each dynamics model is trained on 1 N of the data during each epoch. ˆTi(s′,r|s,a) = N(µi(s,a),Σi(s,a)) (10) MOPO λhyperparameter controls how small we want the reward to be, adjusting for state-action pair uncertainty. Generally, the more uncertain we are about (s,a), the more we should ignore the reward that’s outputted by the learned MDP model. Its use is also described in Algorithm 2: ˜r(s,a) := r(s,a) −λ N max i=1 ||Σi(s,a)||F (11) We search through the cross-product of these, in total 81 combinations. In our initial experiments, MOPO does not seem to perform well in a tabular setting where both state and action are discrete. Therefore, we simpliﬁed the idea of MOPO to introduce Pessimistic Ensemble MDP (P-MDP). 27A.16.4 P-MDP As noted in the previous section, inspired by MOPO and MoREL (Kidambi et al., 2020), we develop a tabular version of MOPO. We instantiate N tabular MDP models. For each epoch, each MDP model only updates on 1/Nportion of the data. During policy learning time, for each timestep, we randomly sample 1 of the N MDP for the next state and reward; and use Hoeffding bound to compute a pessimistic reward, similar to MOPO’s variance penalty on reward: Let N(s,a) be the number of times (s,a) is observed in the dataset: ϵ= β∗ √ 2 log(1/δ) N(s,a) ˜r(s,a) = min(max(r−ϵ,−1),1) (12) In the last step we bound the reward to (-1, 1) for the Sepsis setting – but it can be changed to apply to any kind of reward range. We note that Hoeffding bound is often loose when N(s,a) is small, therefore, might make the reward too small to learn any good policy. However, empirically, we observe that in the Sepsis-POMDP, P-MDP is often the best-performing algorithm. We additional add a temperature hyperparameter α, that changes the peakness/ﬂatness of the softmax distribution of the learned policy: P-MDP Hyperparameter Range Training Iterations [1000, 5000, 10000] Penalty Coefﬁcient β [0, 0.1, 0.5] Number of Ensembles [3, 5, 7] Temperature α [0.05, 0.1, 0.2] Table A.13: P-MDP Hyperparams for Spesis-POMDP N=200, 1000. Not surprisingly, since planning algorithms (such as Value Iteration or Policy Iteration) need to enumerate through the entire state space, we ﬁnd it too slow to train a policy in Sepsis-MDP domain, because Sepsis-POMDP has 144 unique states, yet Sepsis-MDP has 1440 unique states (glucose level has 5 unique states and diabetes status has 2 unique states). TutorBot and Robomimic both have continuous state space, therefore are not suitable for our P-MDP algorithm without binning. We search through the cross-product of these, in total 81 combinations. For Sepsis-POMDP N=5000, we realize we can increase the number of MDPs and increase training iterations to ﬁt a relatively large dataset of 5000 patients. So we additionally search over Table A.14. This results in 16 additional combinations for P-MDP in Sepsis-POMDP N=5000. P-MDP Hyperparameter Range Training Iterations [20000, 40000] Penalty Coefﬁcient β [0.05, 0.1] Number of Ensembles [15, 25] Temperature α [0.01, 0.05] Table A.14: P-MDP Hyperparams for Spesis-POMDP N=5000. 28A.16.5 BC Behavior Cloning (BC) is a type of imitation learning method where the policy is learned from a data set by training a policy to clone the actions in the data set. It can serve as a great initialization strategy for other direct policy search methods which we will discuss shortly. One pessimistic hyperparameter we can introduce to behavior cloning is similar in spirit to BCQ and MBS-QI, we can train BC policy only on actions that the behavior policy has a high-enough probability to take, optimizing the following objective: ζ = πb(a|s) ≥α arg min θ E(s,a)∼D||πθ(s) −ζ◦πb(a|s)||2 (13) We refer to αas the “safety-threshold”. We search through the cross-product of these, in total 27 combinations. BC Hyperparameter Range Policy network dimension [32, 32], [64, 64], [128, 128] Training Epochs [15, 20, 25] Safety Threshold α [0, 0.01, 0.05] Table A.15: BC Hyperparams for Spesis-POMDP N=200, 1000, 5000. Sepsis-MDP N=200. TutorBot N=200. A.16.6 POIS Policy Optimization via Importance Sampling (Metelli et al., 2018) uses an importance sampling estimator as an end-to-end differentiable objective to directly optimize the parameters of a policy. In our experiment, we refer to this as the “ PG” (policy gradient) method. Similar to BC method, we can set a safety threshold αthat zeros out any behavior probability of an action that’s not higher than α, and then re-normalizes the probabilities of other actions. Metelli et al. (2018) also introduces another penalty hyperparameter λto control the effective sample size (ESS) penalty. ESS measures the Renyi-divergence between πb and πe. Let ˆV be the differentiable importance sampling estimator – we write the optimization objective similar to Futoma et al. (2020), but without the generative model: J(Dtrain) = ˆV(πθ; Dtrain) − λESS ESS(θ) θ= arg max θ J(Dtrain) (14) We search through the following hyperparameters in Table A.16. There are 81 combinations in total. A.16.7 BC+POIS BC + POIS is a method that ﬁrst ﬁnds a policy using BC as an initialization strategy to make sure that the policy stayed close (at ﬁrst) to the behavior policy. This is particularly useful for neural network-based policy classes, as a form of pre-training using behavior cloning objective. We use the same set of hyperparameters displayed in Table A.16, resulting in 81 combinations in total. A.16.8 BC+mini-POIS In both Metelli et al. (2018) and Futoma et al. (2020), the loss is computed on the whole dataset Dtrain, which makes sense – importance sampling computes the expected reward (which requires averaging over many trajectories to have an estimation with low variance). However, inspired by the success of randomized optimization algorithms such as mini-batch stochastic gradient descent 29BC Hyperparameter Range Policy network dimension [32, 32], [64, 64], [128, 128] Training Epochs [15, 20, 25] Safety Threshold α [0, 0.01, 0.05] ESS Penalty λ [0, 0.01, 0.05] Table A.16: POIS, BC+POIS, BC+mini-POIS Hyperparams for Spesis-POMDP N=200, 1000, 5000. Sepsis-MDP N=200. TutorBot N=200. (SGD), we decided to attempt a version of BC + POIS with ˆV over a small batch of trajectories instead of over the entire dataset. Our batch size is 4 (4 trajectories/patients) for Sepsis-POMDP N=200 and 1000, which is very small. However, this strategy seems to be quite successful, resulting in learning high-performing policies competitive with other more principled methods. This can be seen in Figure A.2 (“BCMINIPG”). We leave the exploration of why this is particularly effective to future work, and hope others who want to try POIS style method to include our variant in their experiment. We use the same set of hyperparameters displayed in Table A.16, resulting in 81 combinations in total. A.17 TutorBot Experiment Details The details of this environment is shown in the code ﬁle in the supplementary material. We trained BC+POIS, POIS, and BC+mini-POIS on this domain. A.18 Robomimic Experiment Details We refer the reader to Mandlekar et al. (2021) for a full review of the ofﬂine RL algorithms used in our experiment. For Robomimic, we include the range of hyperparameters we have considered below: • BC: – Actor NN dimension: [300,400], [1024,1024] – Training epochs: 600, 2000 – GMM actions: 5, 25 • BCRNN: – RNN dimension: [100], [400] – Training epochs: 600, 2000 – GMM actions: 5, 25 • BCQ: – Critic NN size: [300,400], [1024,1024] – Training epochs: 600, 2000 – Action samples: [10,100], [100,1000] • CQL: – Critic NN size: [300,400], [1024,1024] – Training epochs: 600, 2000 – Lagrange threshold: 5, 25 • IRIS: – Critic NN size: [300,400], [1024,1024] – Training epochs: 600, 2000 – LR critic: 0.001, 0.0001 30A.19 D4RL-Hopper Experiment Details For the D4RL experiments, we include the range of hyperparameters we have considered below: • BCQ: – Policy NN size: [512,512], [64,64] – LR policy: 0.001, 0.0001 • CQL: – Policy NN size: [256,256,256], [64,64,64] – LR policy: 0.001, 0.0001 • AW AC: – Policy NN size: [256,256,256,256], [64,64,64,64] – LR policy: 0.001, 0.0001 For BVFT Strategy 1 πx FQE, we use the optimal FQE hyperparameter on all hyperparameters of BCQ, CQL and AWAC. For BVFT Strategy 2π+ FQE, we use 4 FQE hyperparameters but only with 4 hyperparameters of BCQ. For RRS and CV , we use the optimal FQE hyperparameter on 4 hyperparameters of BCQ as well. A.20 Computing Resources For the overall experimental study in this paper, an internal cluster consisting of 2 nodes with a total of 112 CPUs and 16 GPUs was used. A.21 Additional Ofﬂine RL Sensitivity Study A.21.1 Sensitivity to data splitting: One-Split OPE Figure A.1 shows that procedure produces policies with drastically different estimated, and true, performances subject to randomness in data selection process. Because training and validation set randomness are directly conﬂated, it becomes difﬁcult to accurately select a better AH pair (and its associated higher-performing policy) based on a single train/validation set partition. Figure A.1: We show that policies learned from ofﬂine RL algorithms are sensitive to the variation of training and validation dataset: an algorithm-hyperparameter (AH) pair can obtain wildly different policies based on which portion of the data they were trained on. We obtained 5400 policies from 540 AH combinations on Sepsis-POMDP (N=1000) domain. The variation is not just in terms of the policy’s true performance in the real environment, but also in terms of OPE estimations. Note that FQE estimate on Robomimic exceeded the range of possible achievable rewards (between 0 and 1). The true reward is calculated by evaluating the policy in the real environment. A.21.2 Sensitivity to hyperparameters In Figure A.2, we show that ofﬂine RL algorithms are sensitive to the choice of hyperparameters. In the Sepsis-POMDP N=1000 task and the Robomimic Can-Paired N=200 task, all popular ofﬂine RL algorithms show a wide range of performance differences even when trained on a ﬁxed partition of the dataset. 31Figure A.2: Sensitivity of ofﬂine RL algorithms due to the choice of hyperparameters. 32",
      "references": [],
      "meta_data": {
        "arxiv_id": "2210.08642v2",
        "authors": [
          "Allen Nie",
          "Yannis Flet-Berliac",
          "Deon R. Jordan",
          "William Steenbergen",
          "Emma Brunskill"
        ],
        "published_date": "2022-10-16T21:24:53Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Problem: In offline/batch RL with limited data, hyperparameters, algorithms, and data-splits dramatically affect the learned policy and its deployment performance. Contribution: a general, data-efficient, task- and method-agnostic pipeline (SSR-RRS) for automatic algorithm-hyperparameter (AH) selection and deployment using only historical data. It relies on repeated random sub-sampling (RRS) to create multiple train/validation splits, trains a wide set of AH pairs on the training parts, evaluates them on the corresponding validation parts with off-policy evaluation (OPE), aggregates scores across splits to select the best AH, and finally retrains the chosen AH on the full dataset to produce the deployment policy. The approach yields higher-performing deployed policies across diverse domains (healthcare sepsis simulators, TutorBot education, Robomimic robotics, and D4RL/robotics benchmarks) and demonstrates that multi-split validation can substantially improve AH selection when data are scarce. The paper also provides theoretical support showing that a single split can misidentify the best AH with non-trivial probability, while averaging over many splits converges to selecting the optimal AH as splits grow.",
        "methodology": "Key techniques include: (1) framing AH pairs as candidates for offline policy learning, (2) using SSR-RRS: Generate K repeated random train/validation splits from the offline dataset, train each AH on the training part, evaluate on the validation part with off-policy evaluation (OPE) estimators (Weighted Importance Sampling for discrete domains; Fitted Q-Evaluation for continuous domains), (3) aggregate the validation scores across splits to obtain GA_RSK and select the AH with the highest average score, (4) retrain the selected AH on the full dataset to obtain the deployment policy, and (5) compare against baselines such as One-Split OPE, BCa bootstrap, cross-validation (2-fold/5-fold), and BVFT. The framework is designed to be algorithm- and model-agnostic and does not require access to internal Q-functions (black-box policies). Theoretical result (Theorem 1) shows that with a single split there is a non-zero probability of selecting a suboptimal AH, while increasing the number of splits yields consistent selection of the optimal AH as K→∞.",
        "experimental_setup": "Datasets spanning five domains and multiple tasks: Sepsis-POMDP (N=1000) and Sepsis-MDP (N=200) simulated patient-treatment domains; TutorBot (N=200); Robomimic can-paired and transport (N=200 each); D4RL Hopper (N=200k samples with hopper-random, hopper-medium, hopper-medium-expert); D4RL HalfCheetah (for supplementary results). The experiments evaluate up to 540 AH pairs per domain and compare SSR-RRS against One-Split OPE, BCa bootstrap, 2-fold and 5-fold CV, and BVFT variants. OPE estimators used include WIS for discrete/tabular domains and FQE for continuous-action domains. Policies are trained on training splits, evaluated on validation splits via OPE, G_A,R_S is averaged over K splits, and the best AH is retrained on the full dataset to yield the deployment policy. The paper conducts ablations on K (number of splits) and dataset size (N ∈ {200, 1000, 5000}) and reports true environment rewards with confidence intervals.",
        "limitations": "Limitations include dependence on the quality and bias/variance of the chosen OPE estimator, potentially high computational cost due to training many AH across splits, sensitivity to non-Markov or partially observable settings (FQE may be mis-specified in POMDP-like domains), and the inability to produce an unbiased performance estimate for the deployed policy since the final retraining uses the full dataset. The theoretical guarantees rely on specific constructions (chain MDP) and assume the ability to retrain on full data; effectiveness depends on data coverage of the validation splits, and the pipeline’s benefits may diminish with very large datasets where a single split already performs well. The approach also relies on the availability of a diverse AH pool and suitable hyperparameter grids.",
        "future_research_directions": "Future work could explore automated OPE model selection within SSR, cross-splitting or cross-fitting techniques to improve reliable inference while reducing bias, and theoretical analysis of finite-sample performance of SSR-RRS with realistic OPE estimators. Additional directions include reducing computational burden (e.g., sharing computations across AH, smarter search strategies), extending to non-Markov/POMDP settings with OPE methods tailored to partial observability, integrating cross-validation-like procedures that provide both reliable policy deployment and calibrated policy evaluation estimates, and applying SSR-RRS to additional real-world domains or online-offline hybrids (e.g., safe online fine-tuning after offline selection).",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "CaSE-style contextual adapters reduce overfitting to per-task context via a simple L2 anchoring regularization on per-channel scales gamma toward 1. However, this prior is global and task-agnostic, and does not capture how channel-scale adaptation should be shared and constrained across multiple domains, tasks, and layers. It also lacks a principled mechanism to learn how much and where gamma should vary across domains, leading to suboptimal cross-domain adaptation and limited data efficiency under distribution shifts. The gap is a lack of a hierarchical, learnable prior over adapter scales that can (i) promote cross-task consistency of channel modulation, (ii) allow selective task-domain-specific adaptation, and (iii) remain plug-in and computation-light for practical use. Proving or validating such a prior would advance data-efficient, robust image classification in diverse settings.",
    "method": "Refine CaSE by introducing a hierarchical prior over per-channel gamma that operates across tasks and layers, and augment the gamma-regularization with three components: (i) anchor_loss: penalize deviation of gamma from 1 (as in CaSE-Reg), (ii) var_loss: penalize high covariance/variance of gamma across tasks within a training batch or dataset to promote cross-task consistency, and (iii) kl_loss: impose a KL divergence between the per-task, per-layer gamma statistics and a learnable prior N(1, sigma^2) with learnable sigma (and optionally a domain-conditioned prior p(domain)=D). This yields a plug-in, data-efficient regularization that balances adaptation with global priors. Inference remains CaSE-like, but these hierarchical priors guide gamma updates to be robust to domain shift. The approach is novel in that it combines per-task gamma regularization with a learnable probabilistic prior and cross-task variance control, forming a two-level adaptation constraint (task-level and domain-level) that is not previously reported for CaSE or similar adapters.",
    "experimental_setup": "Setup mirrors the data-efficient image classification objective. Use CaSE as the backbone with multiple places to insert adaptive channels (e.g., 2–3 blocks per backbone). Compare three methods on VTAB+MD (26 datasets) and ORBIT personalization: (a) CaSE (baseline), (b) CaSE-Reg (existing L2 gamma regularization toward 1), and (c) CaSE-Reg+ (our hierarchical prior with anchor+variance+KL terms). Evaluate on EfficientNetB0 and ResNet50-S, following the CaSE experimental protocol. Report per-dataset accuracies and average accuracy, with standard deviations across datasets, along with adaptation cost (MACs) and training time. Hyperparameters include reg_anchor, reg_var, reg_kl, and domain-prior configuration, chosen via a small validation sweep. We also include ablations varying the number and location of CaSE blocks, and whether a domain-conditioned prior is used. All experiments are designed to run with standard Python/PyTorch on a single GPU for reproducibility.",
    "primary_metric": "accuracy",
    "experimental_code": "# Refined CaSE with Hierarchical Prior (CaSE-Reg+)\nimport torch\nfrom torch import nn\n\nclass CaSE_HierPrior(nn.Module):\n    def __init__(self, cin, reduction=64, min_units=16, out_mul=2.0,\n                 reg_anchor=0.01, reg_var=0.01, reg_kl=0.01, init_log_sigma=-2.0):\n        super(CaSE_HierPrior, self).__init__()\n        self.cin = cin\n        self.standardize = True\n        self.out_mul = out_mul\n        self.reg_anchor = reg_anchor\n        self.reg_var = reg_var\n        self.reg_kl = reg_kl\n        # Gamma generator network remains the same spirit as CaSE\n        hidden_features = max(min_units, cin // reduction)\n        self.gamma_generator = nn.Sequential(\n            nn.Linear(cin, hidden_features),\n            nn.SiLU(),\n            nn.Linear(hidden_features, cin),\n            nn.Sigmoid(),\n        )\n        # Learnable prior over gamma scales (log-variance)\n        self.log_sigma_gamma = nn.Parameter(torch.tensor(init_log_sigma))\n\n    def forward(self, x):\n        # Global context for gamma (mean over spatial dims as in CaSE)\n        ctx = x.mean(dim=[0,2,3], keepdim=True)  # [1, C, 1, 1] if x=[N,C,H,W]\n        gamma = self.gamma_generator(ctx.view(-1))  # [C]\n        gamma = gamma.clamp(min=0.0, max=2.0).view(1, -1, 1, 1)\n        # Anchor term: keep gamma near 1\n        anchor_loss = self.reg_anchor * torch.mean((gamma - 1.0) ** 2)\n        # Variance term across batch channels to encourage cross-task consistency\n        gamma_batch = gamma.squeeze().detach()  # [C]\n        var_loss = self.reg_var * torch.var(gamma_batch)\n        # KL divergence to a learnable prior N(1, exp(log_sigma_gamma)^2)\n        mu = gamma_batch  # per-channel mean\n        sigma = torch.exp(self.log_sigma_gamma)\n        kl_loss = 0.5 * torch.sum(((mu - 1.0) ** 2) / (sigma ** 2) + torch.log(sigma ** 2) - 1)\n        kl_loss = self.reg_kl * kl_loss / max(self.cin, 1)\n        reg_loss = anchor_loss + var_loss + kl_loss\n        self.gamma = gamma\n        return gamma * x, reg_loss\n\n# Example usage in a training step (conceptual)\n# model outputs: y, reg_loss via CaSE-like forward\n# loss = ce_loss(y, target) + reg_loss\n\n# Note: This is a compact, plug-in illustration and omits inner-loop meta-adaptation details.\n\n# The full implementation should integrate CaSE_HierPrior into the backbone at chosen blocks, accumulate reg_loss per forward, and backpropagate jointly with the classification loss across tasks.\n",
    "expected_result": "We expect CaSE-Reg+ to outperform both the CaSE baseline and CaSE-Reg on VTAB+MD and ORBIT benchmarks. Specifically, on average across the 26 VTAB+MD datasets and ORBIT personalization tasks, we predict: - Average accuracy improvement over CaSE-Reg by 0.6–1.5 percentage points and over CaSE by 1.0–2.0 percentage points. - Reduced per-dataset accuracy variance by roughly 5–15% points relative to CaSE-Reg, indicating more stable cross-domain adaptation. - Similar or slightly increased training time and MACs (due to the small additive regularization terms) compared to CaSE-Reg, with a negligible overhead (≤ 5%). - Improved cross-domain generalization when the domain shift is large (e.g., ORBIT personalization with unseen user contexts) due to hierarchy-informed regulation of per-channel gamma.",
    "expected_conclusion": "CaSE-Reg+ demonstrates that combining a learnable hierarchical prior with anchor-based and cross-task regularization yields significantly better data-efficient, cross-domain image classification than per-task gamma anchoring alone. This approach provides stronger theoretical and empirical grounding for stable context-conditioned adaptation across varied domains, enabling more reliable deployment of lightweight adapters in real-world settings with limited labeled data and distribution shifts. The plug-in nature and modest compute overhead facilitate adoption across architectures and datasets, advancing both the academic understanding and societal impact of robust, data-efficient vision systems."
  },
  "experimental_design": {
    "experiment_summary": "Task: Validate that CaSE-Reg+ hierarchical prior improves cross-domain data-efficient image classification by regulating per-channel gamma in CaSE adapters across multiple domains and layers. Setup: implement CaSE-Reg+ (anchor_loss, var_loss, kl_loss with a learnable gamma prior) as a plug-in regularization for per-channel gamma scales inserted in 2–3 CaSE blocks within an EfficientNetB0-like backbone. Compare three methods on VTAB+MD (26 datasets) plus ORBIT personalization: (a) CaSE baseline, (b) CaSE-Reg (L2 anchoring toward 1), and (c) CaSE-Reg+ (hierarchical prior). Training is conducted on a single GPU (NVIDIA A100/H100 class) to satisfy resource constraints; use standard data-efficient training (CaSE-style) with a small validation sweep to set reg_anchor, reg_var, reg_kl, and domain_priors. Datasets: VTAB+MD (26 datasets) and ORBIT personalization; metrics reported per-dataset accuracy, plus average accuracy and their standard deviation across datasets. Report adaptation cost (MACs) and wall-clock training time. Ablations/variants include number/location of CaSE blocks and domain-conditioned priors. The experiment design emphasizes plug-in, computation-light regularization to improve cross-domain adaptation and data efficiency under distribution shifts while keeping inference latency near CaSE. Scale and hyperparameters are chosen to fit a single-GPU Run under the provided Runner constraints, with careful logging and reproducibility (seed, deterministic ops).",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA A100 or H200, VRAM: 80 GB or more, RAM: 2048 GB or more"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Primary evaluation metric: per-dataset top-1 accuracy. Correctness: for each dataset, compute the proportion of correctly predicted test samples and report as a percentage. Calculation: accuracy_dataset = (# correct predictions) / (total samples). Then report mean_accuracy = average of accuracy_dataset across the 26 VTAB+MD datasets and ORBIT personalization, and std_accuracy = standard deviation of per-dataset accuracies. Task appropriateness: classification with multiple datasets of diverse domains; ensures cross-domain data efficiency and generalization. Visualizations: bar chart of per-dataset accuracy (sorted by value) and a forest/point plot showing mean ± std across datasets."
      },
      {
        "name": "std_accuracy",
        "description": "Secondary metric capturing cross-domain stability. Correctness: same per-dataset accuracies as above; Calculation: std_accuracy = sqrt(mean((accuracy_dataset - mean_accuracy)^2)) across all datasets. Task appropriateness: helps quantify consistency of adaptation across domains. Visualizations: box plot of per-dataset accuracies and a violin/box plot."
      },
      {
        "name": "adaptation_cost_MACs",
        "description": "Compute the additional MACs introduced by CaSE adapters (per forward pass) compared to a baseline backbone without adapters. Correctness: MACs computed via framework-provided profiling or a standardized MAC estimation; Report delta_MACs as (MACs_with_adapters - MACs_baseline). Calculation: delta_MACs and percentage increase. Task appropriateness: assesses computational overhead of CaSE-Reg+ relative to CaSE baseline and primary method, ensuring practicality. Visualizations: bar plot of delta_MACs across methods."
      },
      {
        "name": "training_time",
        "description": "Total wall-clock time to reach target performance or convergence criteria across datasets. Correctness: measure the cumulative training time per method; if convergence criteria differ across datasets, report time to reach target accuracy, otherwise total time per experiment; Visualizations: learning-curve style plot of accuracy vs time and/or cumulative time to reach accuracy thresholds."
      }
    ],
    "models_to_use": [
      "EfficientNetB0-5.3M"
    ],
    "datasets_to_use": [
      "VTAB+MD"
    ],
    "proposed_method": {
      "method_name": "CaSE-Reg+-HierPrior",
      "description": "Refined CaSE by introducing a hierarchical, learnable prior over per-channel gamma that operates across tasks and layers, augmented with three regularization components: anchor_loss (penalizes gamma deviation from 1, as in CaSE-Reg), var_loss (penalizes high cross-task covariance/variance of gamma to promote cross-task consistency), and kl_loss (KL divergence between per-task/per-layer gamma statistics and a learnable prior N(1, sigma^2) with learnable sigma, optionally domain-conditioned p(domain)). This plug-in prior guides gamma updates to be robust to domain shift while maintaining CaSE-like inference, enabling cross-domain consistency and selective domain-specific adaptation with minimal compute overhead. The prior is learnable, enabling data-efficient cross-domain adaptation with a two-level constraint: task-level and domain-level, across multiple backbone blocks.",
      "training_config": {
        "learning_rate": 0.0002,
        "batch_size": 32,
        "epochs": 12,
        "optimizer": "adamw",
        "warmup_steps": 1000,
        "weight_decay": 0.01,
        "gradient_clip": 1.0,
        "scheduler": "cosine",
        "seed": 42,
        "additional_params": "reg_anchor=0.05; reg_var=0.01; reg_kl=0.02; domain_prior='learnable'; domain_conditioned_prior=True; gamma_clamp=(0,2); num_caose_blocks=2-3"
      },
      "optuna_config": {
        "enabled": true,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "learning_rate",
            "distribution_type": "loguniform",
            "low": 1e-05,
            "high": 0.0005
          },
          {
            "param_name": "reg_anchor",
            "distribution_type": "uniform",
            "low": 0.0,
            "high": 0.1
          },
          {
            "param_name": "reg_var",
            "distribution_type": "uniform",
            "low": 0.0,
            "high": 0.1
          },
          {
            "param_name": "reg_kl",
            "distribution_type": "uniform",
            "low": 0.0,
            "high": 0.05
          },
          {
            "param_name": "init_log_sigma",
            "distribution_type": "uniform",
            "low": -4.0,
            "high": 0.0
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "CaSE-Reg",
        "description": "Baseline CaSE with L2 gamma regularization toward 1. Regularization term anchored to gamma. Training preserves CaSE inference as a plug-in adapter.",
        "training_config": {
          "learning_rate": 0.0002,
          "batch_size": 32,
          "epochs": 12,
          "optimizer": "adamw",
          "warmup_steps": 1000,
          "weight_decay": 0.01,
          "gradient_clip": 1.0,
          "scheduler": "cosine",
          "seed": 42,
          "additional_params": "anchor_only=True; reg_anchor=0.01; reg_var=0.0; reg_kl=0.0"
        },
        "optuna_config": {
          "enabled": false,
          "n_trials": 0
        }
      }
    ]
  }
}