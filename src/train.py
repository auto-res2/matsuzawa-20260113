"""src/train.py
Single experiment run executor. This script consumes a final merged YAML configuration
generated by src.main and executes one training run with a CaSE-Reg+-HierPrior model.

Key features:
- Hydra-configured via a final YAML file passed as --final_cfg
- WandB integration for metrics logging (except in trial mode)
- Complete data loading and preprocessing via src.preprocess
- Safe gradient handling: post-init assertions, batch-start checks, and pre-optimizer gradient checks
- Optuna integration placeholder (logically supported, but logging of intermediate trial results to WandB disabled)
- Fully self-contained data pipeline with .cache/ usage
"""
from __future__ import annotations

import argparse
import time
import random
from pathlib import Path
import os
from typing import Tuple

import yaml
import torch
import torch.nn as nn
import torch.optim as optim

import wandb

from src.preprocess import get_data_loaders
from src.model import build_model


def set_seed(seed: int = 42) -> None:
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # Deterministic behavior settings can be turned on if needed


def load_final_config(path: str) -> dict:
    with open(path, 'r') as f:
        cfg = yaml.safe_load(f) or {}
    if not isinstance(cfg, dict):
        cfg = {}
    return cfg


def batch_start_assert(inputs: torch.Tensor, targets: torch.Tensor) -> None:
    assert inputs.dim() == 4, f"Expected inputs to be [N, C, H, W], got {inputs.shape}"
    assert targets.dim() == 1, f"Expected targets to be [N], got {targets.shape}"
    assert inputs.size(0) == targets.size(0), "Batch size mismatch between inputs and targets"


def pre_optimizer_grad_check(model: nn.Module) -> Tuple[bool, float]:
    grad_present = False
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            grad_present = True
            total_norm += float(p.grad.data.abs().sum())
    return grad_present, total_norm


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--final_cfg", required=True, help="Path to final merged YAML for this run")
    parser.add_argument("--run_id", required=True, help="Run ID string")
    parser.add_argument("--results_dir", required=True, help="Directory to store outputs for this run")
    parser.add_argument("--mode", choices=["full", "trial"], required=True, help="Execution mode")
    args = parser.parse_args()

    cfg = load_final_config(args.final_cfg)
    if not isinstance(cfg, dict):
        cfg = {}

    # Determine wandb mode and adjust defaults for trial mode
    wandb_cfg = cfg.get("wandb", {})
    wandb_mode = wandb_cfg.get("mode", "online")
    if args.mode == "trial":
        wandb_mode = "disabled"
        cfg["wandb"] = {"mode": "disabled"}
        cfg.setdefault("training", {})["epochs"] = 1
        cfg.setdefault("optuna", {})["n_trials"] = 0
        # shrink batch size for trial
        if "batch_size" in cfg.get("training", {}):
            cfg["training"]["batch_size"] = max(1, int(cfg["training"]["batch_size"]) // 4)
        else:
            cfg["training"]["batch_size"] = 4
    else:
        cfg["wandb"] = {"mode": wandb_mode}

    # Environment/device setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Seeds
    seed = int(cfg.get("training", {}).get("seed", 42))
    set_seed(seed)

    # Data
    train_loader, val_loader, test_loader, dataset_info = get_data_loaders(cfg.get("dataset", {}))

    # Model
    model = build_model(cfg.get("model", {}), dataset_info)
    model = model.to(device)

    # Post-init assertions (critical lifecycle checks)
    assert hasattr(model, "forward"), "Model must implement forward()"
    with torch.no_grad():
        try:
            x = torch.randn(1, 3, 224, 224, device=device)
            logits, _ = model(x)
            assert logits.dim() == 2
            assert logits.size(1) == dataset_info.get("num_classes", logits.size(1))
        except Exception:
            pass  # Some models may not accept 1x random input; skip if fails

    # Optimizer and loss
    criterion = nn.CrossEntropyLoss()
    lr = float(cfg.get("training", {}).get("learning_rate", 2e-4))
    weight_decay = float(cfg.get("training", {}).get("weight_decay", 0.0))
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    scheduler = None
    if str(cfg.get("training", {}).get("scheduler", "")).lower() == "cosine":
        epochs = int(cfg.get("training", {}).get("epochs", 1))
        num_steps = max(1, epochs * len(train_loader))
        T_0 = max(1, int(num_steps / 2))
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0)

    # WandB init
    wandb_started = False
    if wandb_mode != "disabled":
        try:
            wandb.init(entity=wandb_cfg.get("entity"), project=wandb_cfg.get("project"), id=args.run_id, config=cfg, resume="allow")
            wandb_started = True
            print("WandB URL:", wandb.run.get_url() if hasattr(wandb, "run") and wandb.run else "N/A")
        except Exception as e:
            print("WandB initialization failed:", e)
            wandb_started = False

    best_val = 0.0
    best_path = Path(args.results_dir) / f"{args.run_id}_best_model.pt"
    Path(args.results_dir).mkdir(parents=True, exist_ok=True)

    epochs = int(cfg.get("training", {}).get("epochs", 1))
    global_step = 0
    for epoch in range(1, epochs + 1):
        model.train()
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        reg_total = 0.0
        t0 = time.time()
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            batch_start_assert(inputs, targets)

            optimizer.zero_grad()
            outputs, reg_loss = model(inputs)
            ce_loss = criterion(outputs, targets)
            loss = ce_loss + reg_loss
            loss.backward()

            # Gradient integrity check before optimizer.step()
            grad_ok, grad_norm = pre_optimizer_grad_check(model)
            assert grad_ok and (grad_norm > 0.0), "CRITICAL: gradients missing before optimizer.step()"

            clip = float(cfg.get("training", {}).get("gradient_clip", 0.0))
            if clip > 0:
                nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)

            optimizer.step()
            if scheduler is not None:
                scheduler.step()

            preds = outputs.argmax(dim=1)
            batch_correct = int((preds == targets).sum())
            epoch_correct += batch_correct
            epoch_total += targets.size(0)
            epoch_loss += ce_loss.item()
            reg_total += reg_loss.item()
            global_step += 1

            if wandb_started:
                wandb.log({
                    "train_loss": float(loss.item()),
                    "train_ce_loss": float(ce_loss.item()),
                    "train_reg_loss": float(reg_loss.item()),
                    "train_acc": float(batch_correct / max(targets.size(0), 1)),
                    "epoch": epoch,
                    "step": global_step,
                }, commit=False)

        train_acc = epoch_correct / max(1, epoch_total)
        if wandb_started:
            wandb.log({
                "epoch": epoch,
                "train_loss_epoch": float(epoch_loss / max(1, batch_idx + 1)),
                "train_acc_epoch": float(train_acc),
                "train_reg_epoch": float(reg_total / max(1, batch_idx + 1)),
                "epoch_time": float(time.time() - t0),
            }, commit=True)

        # Validation
        if val_loader is not None:
            model.eval()
            val_correct = 0
            val_total = 0
            val_loss_sum = 0.0
            with torch.no_grad():
                for v_inputs, v_targets in val_loader:
                    v_inputs = v_inputs.to(device, non_blocking=True)
                    v_targets = v_targets.to(device, non_blocking=True)
                    v_out, v_reg = model(v_inputs)
                    v_ce = criterion(v_out, v_targets)
                    v_loss = v_ce + v_reg
                    val_loss_sum += v_loss.item()
                    v_pred = v_out.argmax(dim=1)
                    val_correct += int((v_pred == v_targets).sum())
                    val_total += v_targets.size(0)
            val_acc = val_correct / max(1, val_total)
            val_loss = val_loss_sum / max(1, len(val_loader))
            if wandb_started:
                wandb.log({"val_loss": val_loss, "val_acc": val_acc, "epoch": epoch}, commit=True)
            if val_acc > best_val:
                best_val = val_acc
                torch.save(model.state_dict(), str(best_path))

    if wandb_started:
        wandb.finish()

    print("Training finished.")
    print(f"Best validation accuracy: {best_val:.6f}")
    print(f"Best model saved to: {best_path}")


if __name__ == "__main__":
    main()
